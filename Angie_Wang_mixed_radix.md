view and understand https://www.youtube.com/watch?v=mogO0at3-RU then read :everyone today I'll be talking about a memory based runtime reconfigurable mixed radix hardware fft generator and chisel so first F of tees are everywhere hardware implemented where F of tees have been reimplemented countless times and their core DSP in everything from radio astronomy audio signal processing MRI and software-defined radio so if we just look at wireless communication we think okay all the different standards that are involved everything from Internet of Things which requires has

not doesn't really have a significant power has an extremely significant power constraint but doesn't have as stringent of the throughput requirement all the way to digital TV which requires high data throughput but doesn't have as much of a limitation on power all of these different Wi-Fi stuffs different standards require different F of T or other DEA's people out configurations so this implies that one-off designs whether they're runtime reconfigurable or otherwise aren't resource and time

efficient from a designer perspective also in the case of LTE and Wi-Fi you actually need to support runtime reconfigurability so how do we approach Hardware DSP design well we don't really want to reinvent the wheel we want to leverage your understanding of algorithms and architecture regularity and instead of building instances build generators so generators support a wide range of applications this enables you to do fast systems prototyping and design space exploration they allow you to if you design a generator well give

you state-of-the-art performance and unlike commercial runtime reconfigurable IP cores which target specific applications like LT for example you're able to get an extensive and also an extensive extendable feature set with the help of the open source community so how do we approach this well it's actually the same general approach as one-off instance design but we just need to change our mindset slightly so if you look at higher art for memory based F of T instances specifically you'll see that a lot of the papers

focus on one aspect of the FFT design for example how do you come up with conflict free conflict free calculation schedules using multiple butterflies but they don't consider the implications of this enhancement so to speak on the overall system complexity and in the case of calc the conflict free calculation schedules how does it affect for example input/output addressing or even like twiddle memory usage so moving on from F of T instances if you were to look at FFT generators you'll see none

of the current ones that exist in that are available for users support runtime reconfigurability spiral from CMU requires foreign memory and their algorithms are fixed limited to fixed two to the FFTs Stanford's Genesis two for example also doesn't support IO unscrambling so how do we develop FFT John Hardware generators with an F of T as an example so smart design choices benefit both instances and generators so if you look at the standard radix to Cooley to key FFT signal flow graph you'll see that there are sort of rules

that govern how twiddles are computed how inputs and outputs are connected and general internet interconnect patterns and these rules can be applied for whatever radix to FFT size you care about for example if you look at the twiddle factors the twiddle factors of subsequent stages are always a subset of the previous stages twiddle factors and also for radix to FFTs in particular if you look at a decimation and frequency signal flow graph the inputs are indexed in order the outputs are indexed in bit

reversed order and for a decimation and time FFT you actually get the reverse of that so this is actually kind of problematic if you just naively think about an FFT design because you have this bit reversed unscrambling stuff that needs to occur you assume that IO cannot be performed in place and this will limit you in saying that okay maybe you need a 3 end memory to if you wanted to support memory-based FFTs so your first symbol goes in memory a it's done calculated on and then the output gets stored gets output and it

requires three memories to keep the pipeline going however leveraging the fact that decimation and frequency and decimation and time signal photographs are actually mirrored images of each other you can basically allow you to switch between decimation and time and decimation and frequency every to eighth symbol allowing you to do in place computation with only two end memory so for example simple a zero is input and decimation and frequency like indexing it's calculated using the just standard

decimation and frequency signal flow graph and the output is actually using the decimation and time index mapping and then vice versa happens when you start feeding and symbol to so then you can perform in place readout basic in place memory reads and writes and then by allowing you to switch from three and memory to chew on memory you're able to save thirty three thirty three percent reduction in memory which is great for power in area so if you wanted to generalize from standard radix two or

power of 2 F of T's to 2 to the N 3 to them five to the seventh a 5 to the K seven to the LX cetera you could use the Cooley - key algorithm which is what you use which was applied before but with Cooley to key you actually have to compute additional twiddle factors and if you leverage the fact that mixed radix F of T's can be decomposed into co-prime elements you're able to use the prime factor algorithm followed by Cooley to keep recursively for scalability and this minimizes the number of twiddle factors needed because

the prime factor algorithm is actually a Chu jack bi-directional transform so similarly you can apply the decimation and frequency and decimation and time duality exploited with the 2 to the N F of T for the more general case but you need to so to speak unlock dual index mappings and this would allow you to achieve in place aiyoh with to end memory even more generally you'll see that the same rules and optimizations that you apply for two to the twenty four point FFT applies to a sixty point F of T or an eight

sixty-four point F of T and this leads to the conclusion that you can compile time and recon you can perform a reconfigurability at compile time or at runtime via parametrized control logic and this is actually really nice because it's extensible to using chisel for generators so briefly Winograd short Fourier transform algorithm allows you to reduce a number of Hardware intensive multiplies you need in favor of addition an increased number of adders exploiting a cyclic convolution specifically for

the radix to case just if you were to look at a radix to butterfly and a radix to a butterfly you'll see radix to butterfly requires two compact adders radix three butterfly requires six adders and two multipliers and obviously radix three requires more hardware resources and radix too so if you wanted to support a bunch of different wireless standards for example including Chinese digital television which requires this nasty three thousand seven hundred eighty point FFT and radix seven you

would get a butterfly that can look something like this this has a bunch of additional registers and other logic inside that allows you to support runtime reconfigurability and that means that I'm reusing radix seven hardware for other radix modes and all of the multiplication constants they are programmable so at any given time you're using up to 36 adders and eight multipliers for example if you wanted to use this radix seven butterfly four radix three at a particular computation step only the process only the calamus

and purple are active so for the radix three case you'd reduce it down to six adders and two multipliers that are active so that was talking about runtime configuration if you wanted to look at compile time configuration this radix template this butterfly template supports custom stage pipelining and also tunable internal base so you can perform some design space exploration and optimize for this also you achieve optimal stage by passing with unused array disease and that means that I have this template that supports

my memory configurability from radix to three four or five and seven but for example I only want to generate a two to the n FFT so instead of basically this butterfly template just ends up reducing down to a radix to only output for Hardware if you only need that case so as you can see unlike a standard IP core which is only configurable runtime you're actually able to save area if you don't need it so for to key as I mentioned earlier for two the two to the N FFTs it basically allows you to

recursively and uniquely map and index for example n to a vector and this these two index mappings that give you this big bit reversed relationship between the input and the output if you were to generalize that to mixed radix FFTs you might think okay instead of bit reversal how about digit reversal right that makes sense but one of the things with digit reversal is if you wanted to support prime factor algorithm in addition to Cooley to key and you wanted to be able to support in place continuous io rather than using the more

like frequently quoted Chinese remainder theorem or good mapping you actually want to use a more general one day 2 to 2 index mapping and this can actually be simplified into Cooley to key for specific cases so but then in the prime factor case you have this particular mapping that allows you to reverse the decomposition order between for example decimation and frequency and decimation and time equivalent that allows you to say that the K decimation and frequency output is the same index mapping as the

N decimation and time mapping and this allows you to use the same index counters for either forward or reverse decomposition and allows you to achieve in place to end memory the demonstrate for all of this is you have this nasty mod unit which is not really Hardware friendly but if you design all of your logic and think of everything in terms of mixed radix arithmetic actually mod operations for free because I'm just instead of masking out bits I'm masking out digits and the unscrambling scheme

when I do prime factor followed by Cooley to key decomposition is actually not specific to us with 50 flavors and you still apply the digit reversal at the end so that was talking about the FFT i/o if you now look at the F of T calculation step right the simplest case for memory based F of T is you have a single butterfly iterating down this signal flow graph and if you do that for LTE Wi-Fi sizes worst case you require 2x the clock rate to compute the full FFT for a 2x calculation clock rate to

compute the FFT but even if you look at that for 648 and 864 point FFT s the compute doesn't finish in two times in two n so this kind of makes you think like given a specific three point in the stream what you really want to be able to do is use the optimal number of parallel butterflies so that you can achieve for example all of your companies your computation and only n cycles unfortunately this complicates conflict rescheduling so let's stare at the signal flow graph some more and take for

now that banking relies on the mixed radix representations of operand locations a couple of observations you'll note is that radix two-stage has the most butterflies but they don't have any twiddle multiplications so you couldn't reorder the twiddle of the butterflies and parallelized them without incurring bank conflict if you were to do that this is actually what was taped out just several years two years ago ish now three years ago you'll see that I was able to reduce the number

of compute cycles needed from something that was exceeding 2 in these two cases to below 2 and you can similarly reduce for some of the other tee's so how do you generalize this for a mixed radix case well if you look at an N equals 48 calculation schedule with one processing element as each of the stages are iterated through you're actually just counting the processing elements the digits associated with a particular stage or zerode and this banking scheme allows you to guarantee that all the operands for a single

processing element come from different things but for example if now you wanted to do two processing elements in parallel you'll see that the first in the second row have conflicting banks and therefore you can't actually accomplish this so now if you were to look and say okay the maximum radix that's needed for a forty eight point F of T is four and for example you wanted to be able to support for processing elements in parallel instead of using four memory banks you will actually want

to have 16 memory banks accessed simultaneously you can change the banking scheme to ensure that that's all possible all the memory locations are processed and you can achieve this basically by reordering one like this reordering the digits used to count the processing elements and now you'll see that all four of these can be grouped in simultaneously with the rest and none of these banks conflict so all of these concepts were used to develop this fft generator the F of T generator is built

in chisel it includes what I call Hardware template which just has look-up tables for reconfiguration and twiddle factors they have controllable blocks configurable blocks controlling data flow between the i/o the SRAM's and the processing element and actually like as would be expected for a memory based design the design complexity is mostly in the control logic this Hardware template is populated after some processing work done by what I call Scala firmware this firmware takes in user inputs for example the desired FFT

link and calculates constants for lookup table generation and also Hardware parameters for example the FFT length determines the number of banks memory banks how how large these memory banks need to be and the specific butterfly array disease that must be supported they also are used basically calculate the size and also the contents of the Twitter look-up tables using with the help of the breeze scholar numerical processing library so this is a more full-blown picture of what the firmware does and essentially

after the user inputs it turns on it for a little bit outputs a hardware template populates the hardware template in chisel and also populates a test vector generator that creates test vectors that can be usable all the way down to post place in Route design for verification so this is what I aimed to actually apply my FFT generator to originally it supports Wi-Fi a total of an AC and LTE specifically Wi-Fi and LTE if you just don't ignore the stuff on the right include requires reconfigurability for

power of 2 f of T's with the exception of this 1536 which is 512 times 3 but if you wanted to support scfdma precoding for LTE you actually need to support this wide assortment of two and three m5k FFT sizes and actually once you figure that out seven L comes for free and you can extend on to other prime rate of C's so if you were to look at static versus runtime reconfigurable FFT areas you'll see a for memory based F of T chip area is dominated by SRAM so if you're able to save 33% that's great and also if you

wanted to support all Wi-Fi and LTE sizes reconfigure ibly compared to and just a standard N equals 2004 to 22,000 48 point FFT you only have the 24 percent area penalty so this kind of touches upon some of the stuff that was work we were working on with chisel DSP you can integrate this fft generator with more general like DSP infrastructure that allows you to verify mathematical correctness and decouple algorithmic errors from degraded performance due to quantization noise by allowing you to plug in for example

either fixed point types into the F of T or DSP reels which are just essentially wrappers on system rare log reels and this allows you to study system sensitivity to block level performance and one of the nice things is because we're designing inches old we can verify black functionality in a system model that's constructed partially with synthesizable hardware and also with nonsense of sizable Scala models and the same since infrastructure can be used and although these interfaces don't necessarily need to be

cycle accurate they allow you to do fast systems exploration and prototyping so this is the F of T generaiiy accelerator that was taped out in craft in 16 nanometer basically it's attached to rocket risk 5 core and also has this hand laid out ring oscillator this was in 60 nanometer it was taped out within one month from when we got our first 16 nanometer FinFET PDK and basically for control the rocket ship just sets up the F of T for a particular F of T that you want to configure it at runtime and then

feeds in the data waits until its output and you can do all the testing and see so the performance is comparable to state-of-the-art this same F of T generator now translated from chisel two to trizol three was used in a real-time one point eight nine gigahertz bandwidth 175 kilohertz resolution spar spectral analysis RIS five SOC also in 60 nanometers the ATC's were designed using the berkeley analog generator they were actually modeled first and chiseled because they were swapped with the real

IP in innovates and thankfully the modeling worked it also included mixed radix f50s that work about that came from this generator and appealing reconstruction back-end that was designed in chisel so in this case the chisel and the bad generator has enabled the first automatically generated fully integrated sparse spectral analysis SOC and it was designed in two months so the code for the tape outs can be found here and the work was sponsored by NSF DARPA PwC aspire and adult [Applause] questions yes so if anyone has any

questions they can come up to this mic in the middle I can start with an easy one maybe if you have a bunch of different entities that you want but you want them to be configuration like non runtime configurable can you still share resources between them oh but for example if you wanted to compose your own FFTs wiz D you want them to be different radix --is you can't share resources but you could potentially reuse some of the blocks because I implemented it kind of in unit elements to implement like an alternative that

would be able to do that I guess do you see these techniques as being generalizable to other similar transforms like wavelet for example in the sense that you can create for example DCT transforms using FFT hardware I just like just like chisel code generally like like do you see a way to use chisel to library of components that are extensible to other transform I definitely think that's the case you just have to basically zero in on components that you think are reusable amongst all the different algorithms and

transforms that you care about create don't create a big jumbled mess of like for one particular instance but you have to systematically think okay I want to partition it some way in some way because I think this block is more usable reusable so you have a very parameterize design here yeah what was missing from chisel in the beginning and what was added in order to be able to do all the fancy stuff here man it's been a while okay so for me like actually just not having any complex type support originally was big

pain having to do all the manual floating off binary point conversions myself was one of the reasons that we decided we should come up with chisel DSP and try to simplify the design process for DSP hardware engineers there were some bugs and chisel - that were like man I think this should be right but it's not and is it my fault or is it chisel I'm not really sure but I think they've actually cleaned that up a lot and now when you use it it's just like the design productivity increase I think

is pretty significant especially if you want to design generators that are parametrized and even potentially runtime reconfigurable if things change again and our next speaker

