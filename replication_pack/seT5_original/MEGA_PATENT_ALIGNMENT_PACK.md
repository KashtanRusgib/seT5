2/14/26, 1:58 PM

CA2872831C - Flexible radix switch - Google Patents

Patents
Back to results

10 of 125,048

high radix

(high radix);

Flexible radix switch
Abstract
A system and method for interconnecting nodes and routing data packets in high radix networks
includes constructing or redefining a network structure to provide improved performance.

CA2872831C
Canada

Computation and data storage nodes are connected to a network of switching nodes that provide
Download PDF

near optimum bandwidth and latency for networks of any size. Specialized interconnection patterns

Find Prior Art

Similar

and addressing methods ensure reliable data delivery in very large networks with high data traffic
volume.

Other languages: French
Inventor: Ratko V. Tomic, Christopher John Williams, Leigh

Classifications

Richard Turner, Reed Graham Lewis

H04L12/462 LAN interconnection over a bridge based backbone
H04L41/0806 Configuration setting for initial configuration or provisioning, e.g. plug-and-play
H04L45/06 Deflection routing, e.g. hot-potato routing

Current Assignee : J Scott Benson Living Trust

Worldwide applications
2012 CA US WO EP

H04L49/118 Address processing within a device, e.g. using internal ID or tags for routing
within a switch

Application CA2872831A events

H04L49/1553 Interconnection of ATM switching modules, e.g. ATM switching fabrics

2012-05-08

Application filed by J Scott Benson Living Trust

H04L49/356 Switches specially adapted for specific applications for storage area networks

2012-11-15

Publication of CA2872831A1

2019-10-29

Application granted

2019-10-29

Publication of CA2872831C

Status

Active

2032-05-08

Anticipated expiration

Hide more classifications

Landscapes

Engineering & Computer Science
Computer Networks & Wireless Communication

Info: Patent citations (11), Cited by (73), Legal events, Similar
documents, Priority and Related Applications

Show more

External links: Espacenet, Global Dossier, CIPO, Discuss

Claims (20)

Hide Dependent

Claims
1. A method of constructing a network for the transfer of data from a source device to a destination device the method comprising:
selecting a base symmetric network structure, wherein the topology of the base symmetric network structure substantially corresponds to a Cayley graph;
defining at least one of:
a number of source and destination devices to be connected to the network, a number switches to be used in the network, a number of ports per switch, and an
oversubscription characteristic of the network;
determining a generator matrix as a function of at least one of:
the number of source and destination devices to be connected to the network, the number switches to be used in the network, the number of ports per switch, and the
oversubscription characteristic of the network;
determining a wiring pattern for interconnecting each of the switches as a function of the generator matrix; and interconnecting the switches of the network with
interconnecting wires according to the wiring pattern.
2. The method according to claim 1 wherein the base network structure substantially corresponds to a hypercube having a dimension d.
3. The method according to claim 2 wherein the generator matrix is determined as a function of the number of interconnections between switches of the network and the
dimension, d, of the hypercube.
4. The method according to claim 1 wherein the generator matrix is an error correcting code (ECC) generating matrix and the wiring pattern is determined by rotating the
error correcting code generating matrix.
5. The method according to claim 4, wherein the error correcting code generating matrix is rotated counterclockwise.
6. The method according to claim 1 wherein the oversubscription characteristic of the network is determined as a function of a number of ports defined for connection to
source computers and destination computers and a bisection of the network.
7. The method according to claim 6 wherein the bisection is determined as a function of a Walsh function.
8. The method according to claim 7 wherein the bisection is determined by constructing primary equipartitions defined by patterns of 1's and 0s in a Walsh function.

https://patents.google.com/patent/CA2872831C/en?q=(high+radix)&oq=high+radix

1/31

2/14/26, 1:58 PM

CA2872831C - Flexible radix switch - Google Patents

9. The method according to claim 7 wherein the bisection is determined by constructing primary equipartitions defined by the sign pattern in an algebraic Walsh function.
10. The method according to claim 1 wherein the generator matrix is an error correcting code (ECC) generating matrix derived from digital (t,m,s) nets parameters and
the wiring pattern is determined by rotating the error correcting code generating matrix.
11. The method according to claim 4 wherein ECC distance metrics are constructed using a Lee distance.
12. The method according to claim 4 wherein ECC distance metrics are constructed using a Hamming distance.
13. A network constructed according to the method of claim 1.
14. A network constructed by connecting a plurality of switched, the network comprising a defined number of switches, each switch being connected to at least one other
switch by an internal switch connection and the network including a defined number of internal switch connections;
the switched being arranged in a symmetric network structure, wherein the topology of the base symmetric network structure substantially corresponds to a Cayley graph;
the switches being interconnected according to a wiring pattern, the wiring pattern being determined as a function of a generator matrix, wherein the generator matrix is
determined as a function of the number of internal switch connections.
15. A network according to claim 14 wherein the base network structure substantially corresponds to a hypercube having a dimension d.
16. A network according to claim 14 wherein the generator matrix is determined as a function of the number of internal switch connections and the dimension, d, of the
hypercube.
17. A network according to claim 14 wherein the generator matrix is an error correcting code generating matrix and the wiring pattern is determined by rotating the error
correcting code generating matrix.
18. A network according to claim 17, wherein the error correcting code generating matrix is rotated counterclockwise.
19. A network according to claim 14, wherein the generator matrix is determined as a function of at least one of:
a number of source and destination devices to be connected to the network, the number switches used in the network, a number of ports per switch, and an
oversubscription characteristic of the network.
20. A network according to claim 19 wherein the oversubscription characteristic of the network is determined as a function of the number of ports defined for connection
to source and destination devices and a bisection of the network.

Description

Flexible Radix Switch STATEMENT REGARDING FEDERALLY SPONSORED RESEARCH
[0001] Not Applicable REFERENCE TO MICROFICHE APPENDIX
[0002] Not Applicable FIELD OF THE INVENTION
10003] The invention relates generally to the interconnection of nodes in a network. More specifically, the invention relates to interconnected nodes of a communication
network that provide some combination of computation and/or data storage and provide an efficient interchange of data packets between the interconnected nodes.
[0004] The network can be defined by a base network structure that can be optimized by selectively defining and connecting long hops between sections of the network,
for example, to reduce the network diameter.
[0005] The invention provides a system and method for creating a cost-effective way of connecting together a very large number of producers and consumers of data
streams.
BACKGROUND OF THE INVENTION
[0006] One real world analogy is the methods for constructing roadway networks that allow drivers to get from a starting location to a destination while satisfying real
world constraints such as 1) a ceiling on the amount of tax people are willing to pay to fund roadway construction; 2) a desire to maximize speed of travel subject to
safety constraints; and 3) a desire to avoid traffic jams at peak travel times of day.
[0007] In this analogy, the cars are similar to the data sent over a computer network and the starting locations and destinations represent the host computers connected
to the network. The constraints translate directly into cost, speed and congestion constraints in computer networks.
[0008] The basic quality of solving these types of problems is that they get much harder to solve efficiently as the number of starting locations and destinations (e.g.,
host computers) increases. As a starting point, consider how three destinations can be connected together. Figs. lA and 113 show that there are really only two
alternatives. The number of ways to connect the destinations together grows along with the number of destinations, for example, with four destinations, some of the
possible methods of connection are shown in Figs. IC, 1D, lE and 1F.
[0009] As can be seen from the figures, both the number of connections between nodes and the number of different ways of making those connections grows faster than
the number of connections. For example, a set of 6 nodes can have more than twice as many alternative ways to connect the nodes as a set of 3 nodes.
Also, the possible number of connections between the nodes can vary from, on the low side, the number of nodes (N) minus 1 for destinations connected, for example,
along a single line as shown in Fig. 1C, to N(N-1)/2 connections as shown in Fig. IF, where every single node has a direct connection to every other node.
[00101 Another measure of the performance of a network is the diameter of the network, which refers to how many connections need to be traveled in order to get from
any one destination to another. In the network shown in Fig. 1C, its economy in the number of connections (3) is offset by the consequence that the only path, from one
end of the network to the other, requires travel across three connections, thus slowing the journey. On the other hand as shown in Fig. 1F, the large number of connections
results in every destination only being one connection away from any other, permitting more rapid travel.
[0011] The two networks shown in Figs. 1C and 1F can also have very different behavior at peak traffic times. Assuming that each connection can support the same rate
of traffic flow, the two end point nodes of the network shown in Fig. 1C
will be affected if there is a lot of traffic traveling between the two nodes in the middle of the line. Conversely, in network shown in Fig. 1F, since there is an individual
connection between every possible combination of nodes, traffic flowing between two nodes is not affected at all by traffic flowing between a different pair of nodes.
[0012] Another difficulty arises in the construction of computer networks: It is difficult to have a large number of connections converging on a single point, such as shown
in Fig. IF. In a computer data center, the devices that allow multiple connections to converge are called switches. These switches that allow multiple connections to
converge typically have physical limitations on the number of connections or ports, for example, around 50 ports for inexpensive switches, and can approach 500 ports
for more modem, expensive switches. This means that for a fully-meshed network like that shown in Fig. 1F where delays and congestion are minimized, no more than,
for example, 499 destination hosts could be connected together.
SUMMARY OF THE INVENTION
[0013] The sample network layouts shown in Figs. 1A ¨ 1F, 2A ¨ 2C, and in fact all other network layouts conceived to date, suffer from a fundamental tradeoff between

https://patents.google.com/patent/CA2872831C/en?q=(high+radix)&oq=high+radix

2/31

2/14/26, 1:58 PM

CA2872831C - Flexible radix switch - Google Patents

the cost and difficulty of building the network, and the ability of the network to support high traffic rates. The present invention allows for the design of networks that can
include a very large number of connections and a high level of complexity of the switches that manage those connections, while providing very high immunity from the
congestion that limits the ability of all nodes to communicate with each other at maximum speed, no matter how other nodes are using the network.
[0014] The emergence of "cloud computing", supported by huge data centers where hundreds of thousands of computers all connected to one network provide
economies of scale and thereby reduced costs, has stressed the ability of current network designs to provide a reliable and cost effective way of allowing data to be
exchanged between the computers.
[0015] A number of approaches have been tried by both academia and industry, but to date, all the approaches fall short of theoretical limits by a factor of 2 to 5 times.
Some embodiments of the invention include a method for constructing networks that can be within 5-10% of the theoretical maximum for data throughput across
networks with multiple simultaneously communicating hosts, a highly prevalent use case in modem data centers.
[0016] In accordance with some embodiments of the invention, methods for constructing highly ordered networks of hosts and switches are disclosed that make
maximum use of available switch hardware and interconnection wiring. The basic
3 approach can include the following: selecting a symmetrical network base design, such as, a hypercube, a star, or another member of the Cayley graph family;
developing an appropriate topological routing method that simplifies data packet forwarding; and adding short cuts or long hops to the base symmetrical network to
reduce the network diameter.
[0017] The regularity of symmetrical networks makes them well suit for topological addressing schemes.
[0018] It is one of the objects of the present invention to provide an improved network design that can be expanded greatly without performance penalty.
[0019] It is another object of the present invention to provide an improved network design allows the network to be more easily operated and managed. In some
embodiments, the entire network can be operated and managed as a single switch.
[0020] It is another object of the present invention to provide an improved network design that provided improved network performance. In some embodiments, the
network can have 2 to 5 times greater bisection bandwidth than with conventional network architectures that use the same number of component switches and ports.
[0021] The invention also includes flexible methods for constructing physical embodiments of the networks using commercially available switches and method for
efficiently, accurately and economically interconnecting (wiring) the switched together to form a high performance network having improved packet handing.
4 Description of Drawings [0022] Figures IA ¨ 1F show sample network layouts.
[0023] Figures 2A ¨ 2C show symmetrical network structures according to some embodiments of the invention.
[0024] Figures 3A and 3B show an example of topological routing.
[0025] Figure 4A shows an order 3 hypercube and Figure 4B shows an order 3 hypercube with shortcuts added.
[0026] Figure 5 illustrates a typical large data center layer 2 network architecture.
[0027] Figure 6 illustrates hypercube notation and construction.
[0028] Figure 7 illustrates partitioning between topology and external ports.
[0029] Figure 8 illustrates packet non-blocking with 4 switches and 8 paths.
[0030] Figure 9 illustrates a network bisection according to some embodiments of the invention.
[0031] Figure 10 illustrates an 8 node network with long hops added.
[0032] Figures 11 ¨ 15 are charts comparing long hop networks with alternative network configurations..
[0033] Figure 16 illustrates data center available bandwidth and cost for 4x external/topology port ratio.
[0034] Figure 17 illustrates data center available bandwidth and cost for lx external/topology port ratio.
[0035] Figure 18 illustrates the reduction in average and maximum hops.
[0036] Figure 19 illustrates optimized wiring pattern using port dimension mapping according to an embodiment of the invention.
[0037] Figure 20 illustrates the integrated super switch architecture across an entire data center according to an embodiment of the invention.
[0038] Figure 21 illustrates a network architecture showing a flexible radix switch fabric according to an embodiment of the invention.
[0039] Figure 22 illustrates the flow of a data packet from an ingress switch through a network according to an embodiment of the present invention.
[0040] Figure 23 illustrates various network logical topographies according to an embodiment of the present invention.
[00411 Figure 24 illustrates a network architecture according to one embodiment of the invention.
[0042] Figure 25 illustrates a system including a Data Factory according to some embodiments of the invention.
[0043] Figure 26 illustrates a system interconnecting a control plane executive (CPX) according to some embodiments of the invention.
DETAILED DESCRIPTION OF EMBODIMENTS OF THE INVENTION
[0044] The present invention is directed methods and systems for designing large networks and the resulting large networks. In accordance with some embodiments of
the invention, a way of connecting large numbers of nodes, consisting of some combination of computation and data storage, and providing improved behaviors and
features. These behaviors and features can include: a) practically unlimited number of nodes, b) throughput which scales nearly linearly with the number of nodes,
without bottlenecks or throughput restriction, c) simple incremental expansion where increasing the number of nodes requires only a proportional increase in the number
of switching components, while maintaining the throughput per node, d) maximized parallel multipath use of available node interconnection paths to increase node-tonode bandwidth, e) Long hop topology enhancements which can simultaneously minimize latency (average and maximum path lengths) and maximize throughput at any
given number of nodes, 0 a unified and scalable control plane, g) a unified management plane, h) simple connectivity ¨ nodes connected to an interconnection fabric do
not need to have any knowledge of topology or connection patterns, i) streamlined interconnection path design -dense interconnections can be between physically near
nodes, combined with a reduced number of interconnections between physically distant nodes, resulting in simple interconnection or wiring.
[0045] In one embodiment of the invention, the nodes can represent servers or hosts and network switches in a networked data center, and the interconnections
represent the physical network cables connecting the servers to network switches, and the network switches to each other.
[0046] In another embodiment of the invention, the nodes can represent geographically separated clusters of processing or data storage centers and the network
switches that connect them over a wide area network. The interconnections in this case can be the long distance data transfer links between the geographically
separated data centers.
100471 Those skilled in the art will realize that the described invention can be applied to many other systems where computation or data storage nodes require high
bandwidth interconnection, such as central processing units in a massively parallel supercomputer or other multiple CPU or multi-core CPU processing arrays.
[0048] In accordance with some embodiments of the invention, component switches can be used as building blocks, wherein the component switches are not managed
by data center administrators as individual switches. Instead, switches can be managed indirectly via the higher level parameters characterizing collective behavior of the
network, such as latency (maximum and average shortest path lengths), bisection (bottleneck capacity), all-to-all capacity, aggregate oversubscription, ratio of external
and topological ports, reliable transport behavior, etc. Internal management software can be used to translate selected values for these collective parameters into the
internal configuration options for the individual switches and if necessary into rewiring instructions for data center technicians. This approach makes management and
monitoring scalable.
100491 Hypercubes and their variants have atti acted great deal of attention within parallel and supercomputer fields, and recently for data center architectures as well
due to their highly efficient communications, high fault tolerance and reliable diagnostics, lack of bottlenecks, simple routing & processing logistics, and simple, regular
construction. In accordance with some embodiments of the invention, a method of designing an improved network includes modifying a basic hypercube network
structure in order to optimize latency and bandwidth across the entire network. Similar techniques can be used to optimize latency and bandwidth across other Cayley

https://patents.google.com/patent/CA2872831C/en?q=(high+radix)&oq=high+radix

3/31

2/14/26, 1:58 PM

CA2872831C - Flexible radix switch - Google Patents

graph symmetrical networks such as star, pancake and truncated hypercube networks.
100501 A symmetrical network is one that, from the perspective of a source or a destination looks the same no matter where you are in the network and which allows
some powerful methods to be applied for developing both routing methods for moving traffic through the network and for adding short cuts to improve throughput and
reduce congestion. One commonly known symmetrical network structure is based on the structure of a hypercube. The hypercube structured network can include a set
of destinations organized as the corners of a cube, such as shown in Fig. 2A.
The structure shown in Fig. 2A is known as an order 3 hypercube, based on each destination having three connections to neighboring destinations. To generate a higher
order hypercube, copy the original hypercube and connect all the destinations in the first hypercube with the corresponding destination in the copy as shown in Fig. 2B.
[0051] Hypercubes are just one form of symmetrical network. Another form of symmetrical network is the star graph shown in Fig. 2C. There are many other types of
symmetrical networks, known formally as Cayley graphs that can be used as a basis on which to apply the methods of the invention.
100521 In accordance with some embodiments of the present invention, topological routing can be used route messages through the symmetrical network.
Topological routing can include a method for delivering messages from a source node to a destination node through a series of intermediate locations or nodes where
the destination address on the message describes how to direct the message through the network. A simple analogy is the choice of method for labeling streets and
numbering houses in a city. In some planned areas such as Manhattan, addresses not only describe a destination location, "425 17th Street", but also describe how to get
there from a starting point. If it is known that house numbers are allocated 100 per block, and the starting location is 315 19th Street, it can be determined that the route
includes going across one block and down two streets to get to the destination.
Similarly, for the organi7ation shown in Fig. 3A, traveling from N 200 W. 2nd Street to N 100 E 1st Street can include going east 3 blocks and south one block.
[00531 In contrast, a typical unplanned town like Concord, MA, shown in Fig.
3B, has road that are not laid out in any regular pattern and the names for streets have no pattern either. This "plan" requires a "map" to determine how to get from one
place to another.
[0054] Topological addressing is important in large networks because it means that a large map does not have to be both generated and then consulted at each step
along the way of sending a message to a destination. Generating a map is time consuming and consumes a lot of computing resources, and storing a map at every step
along the way between destinations consumes a lot of memory storage resources and requires considerable computation to look up the correct direction on the map
each time a message needs to be sent on its way towards its destination. The small maps required by topological addressing are not just a matter of theoretical concern.
Present day data centers have to take drastic, performance impacting measures to keep their networks divided into small enough segments that the switches that
control the forwarding of data packets do not get overwhelmed with building a map for the large number of destinations for which traffic flows through each switch.
[0055] The regularity of symmetrical networks makes them excellent candidates for having topological addressing schemes applied to them, just as a regular, basically
symmetrical, arrangement of streets allows addresses to provide implied directions for getting to them.
[0056] In accordance with some embodiments of the invention, the performance of these symmetrical networks can be greatly improved by the select placement of
"short cuts" or long hops according to the invention. The long hops can simultaneously reduce the distance between destinations and improve the available bandwidth
for simultaneous communication. For example, Fig. 4A show a basic order 3 hypercube, where the maximum distance of three links between destination nodes occurs at
the opposite corners. In accordance with some embodiments of the invention, adding shortcuts across all three corners as shown in Fig. 4B
reduces the distance between the destinations that used to have the worst case distance of three to a distance of one link.
[0057] In accordance with some embodiments of the invention, this method can be applied to hypercubes of higher order with many more destinations. In accordance
with some of the embodiments of the invention, a method for identifying select long hops in higher order hypercube networks and symmetric networks can include
determining a generator matrix using linear error correcting codes to identify potential long hops within the network.
[0058] Figure 5 shows a diagram of a typical commercial data center.
Figure also shows the typical port oversubscription ratios, and hence bottlenecks, at each level (core, aggregation, and edge) of the network, that result from the
traditional approaches to building data centers. In addition, none of these approaches work well as the number of devices connected to the network increase
exponentially, as has happened as a result of adoption of highly centralized data centers with large numbers of host computers or servers at a single location.
[0059] All real world network implementations are limited by the physical constraints of constructing switches and wiring them together. With the limitations of
conventional wiring techniques, one of the parameters that can be adjusted to improve network performance is to increase the number of ports per network switch,
which allows that group of ports to exchange data with very high throughput within the single physical device. Problems then arise maintaining that high throughput
when groups of switches have to be assembled in order to connect a large number of servers together. Switch manufacturers have been able to increase the number of
ports per switch into the several hundreds (e.g., 500), and some new architectures claim the ability to create switch arrays that have several thousand ports. However,
that is two to three orders of magnitude less than the number of servers in large data centers. The number of switch ports is referred to as the "radix" of the switch.
100601 In accordance with some embodiments of the invention, one difference between networks according to the invention and the prior art, networks according to the
invention can be expanded (increasing the number of host computer ports) practically, without limit or performance penalty. The expansion can be flexible, using
commodity switches having a variable radix. Although there are presently switches which can be upgraded from an initial configuration with a smaller radix to a
configuration with a higher radix, the latter maximum radix is fixed in advance to at most a few hundred ports. Further, the 'radix multiplier' switching fabric for the
maximum configuration is hardwired in the switch design. For example, a typical commercial switch such as the Arista 7500 can be expanded to 384 ports by adding up
to 8 line cards, each providing 48 ports; but the switching fabric gluing the 8 separate 48 port switches into one 384 port switch is rigidly fixed by the design and it is even
included in the basic unit. In contrast, the networks constructed according some embodiments of the invention have no upper limit on the maximum number of ports it
can provide. And this holds for an initial network design as well as any subsequent expansion of the same network. In accordance with some embodiments of the
invention, for any given type of switch having radix R, the upper limit for simple expansion without performance penalty is 2R-1 component switches. Since typical R is at
least 48, even this conditional limit of 1.4.1014 on the radix expansion is already far larger than the number of ports in the entire intemet, let alone in any existing or
contemplated data center.
[0061] Another difference between networks according to some embodiments of the invention and prior art data centers is that data center layer 2 networks are typically
operated and managed as networks of individual switches where each switch requires individual installation, configuration, monitoring and management. In accordance
with some embodiments of the invention, the data center network can be operated and managed as a single switch. This allows the invention to optimize all aspects of
performance and costs (of switching fabric, cabling, operation and management) to a far greater degree than existing solutions.
[0062] In addition, networks according to some embodiments of the invention can provide improved performance over any existing data center Layer 2 networks, on the
order of 2 to 5 times greater bisection bandwidth than conventional network architectures that use the same number of component switches and ports.
[0063] The invention also describes novel and flexible methods for realizing physical embodiments of the network systems described, both in the area of wiring switches
together efficiently, accurately and economically, as well as ways to use existing functionality in commercial switches to improve packet handing.
100641 Hypercubes can be characterized by their number of dimensions, d.
To construct a (d+1)-cube, take two d-cubes and connect all 2" corresponding nodes between them, as shown in Figure 6 for transitions d: 0 --> 1 2 3 (red lines indicate
added links joining two d-cubes).
100651 For purpose of illustrating one embodiment of the invention, a d-cube can be a d-dimensional binary cube (or Harming cube, hypercube graph) with network
switches as its nodes, using d ports per switch for the d connections per node.
By convention, coordinate values for nodes can be 0 or 1, e.g. a 2-cube has nodes at (x, y) = (0,0), (0,1), (1,0), (1,1), or written concisely as binary 2-bit strings: 00, 01, 10
and 11.
[0066] Each switch can have some number of ports dedicated to interconnecting switches, and hosts can be connected to some or all of the remaining ports not used to

https://patents.google.com/patent/CA2872831C/en?q=(high+radix)&oq=high+radix

4/31

2/14/26, 1:58 PM

CA2872831C - Flexible radix switch - Google Patents

interconnect switches. Since the maximum number of switches N in a d-cube is N=2", the dimensions d of interest for typical commercial scalable data center
applications can include, for example, d = 10..16, i.e. d-cubes with 1K-switches, which corresponds to a range of 20K-1280K physical host (computers or servers),
assuming a typical subscription of 20 hosts per switch.
[0067] In accordance with some embodiments of the invention, a concise binary d-bit notation for nodes (and node labels) of a d-cube can be used. The hops, defined as
the difference vectors between directly connected nodes, can be d-bit strings with a single bit=1 and (d-1) bits=0. The jumps (difference vectors) between any two nodes
Si and S2 can be: 112 S, ^ S2 (A is a bitwise XOR) and the minimum number of hops (distance or the shortest path) L between them is the Hamming weight (count of l's)
of the jump 112 i.e. L L(42) = 11121. There are exactly L!
distinct shortest paths of equal length L between any two nodes S1 and S2 at distance L. The diameter D (maximum shortest path over all node pairs) of a d-cube is D
=
log(N) =d hops, which is also realized for each node. For any node S, its bitwise complement (--,c) is at the maximum distance D from S. The average number of hops
between two nodes is dI2 and bisection (minimum number of links to cut in order to split a d-cube into 2 equal halves) is N/2.
100681 In accordance with some embodiments of the invention, the d-cube coordinates of the switches (d-bit strings with d ¨ 10..16) can be used as their physical MAC
addresses M, and the optimal routing becomes very simple. Routing can be done entirely locally, within each switch using only 0(log(/V)) resources (where 0 can be ____
and N is the maximum number of switches). When a frame with Mdst arrives at a switch M, the switch M computes J = MAJ./do and if J=0, then the switch M is the
destination. Otherwise it selects the next hop h corresponding to any bit = 1 in J which will bring the frame one hop closer to the Mdst since the next node after the hop at
Mmct = M A h will have one less bit = 1, hence one less hop, in its jump vector to Mdst which is Jnxt= MmaA M.
100691 In accordance with some embodiments of the invention, the total number of switches N, in the network is not an exact power of 2, so in this case, the d-cubes
can be truncated so that for any accessible M the relation M<N, holds, where bit string M is interpreted as an integer (instead of M< 2" which is used for a complete dcube). Hence, instead of the 0(N) size forwarding table and an 0(N) routing tree, the switches only need one number N, and their own MAC address M to forward frames
along the shortest paths.
100701 In accordance with some embodiments of the invention, one useful parameter of the hypercubic network topology is the port distribution or ratio of the internal
topology ports (T-ports) used to interconnect switches and external ports(E-ports) that the network uses to connect the network to hosts (servers and routers).
Networks built according to some embodiments of the invention can use a fixed ratio:
X,L¨= Elf (E¨#E-ports, T _____________________________________ #T-ports) for all IPA switches. In accordance with one embodiment, the ratio is X=1 (ports are split evenly
between E and T), is shown in Figure 7 for d=2.
[0071] For a hypercube of dimension d there are it:/ 2d total T-ports and E-ports, d ports per switch for either type. Since the ports are duplex, each E-port can be
simultaneously a source and a sink (destination) of data packets. Hence, there are m sources X1, and m destinations Yi, /725. = = Yan= The non-blocking (NB) property of
a network or a switch can usually be defined via performance on the 'permutation task'¨ each source Xi (1=1 ..m) is sending data to a distinct destination l'j (where j
=7t,õ[1] and 'irõ, a permutation of m elements), and if these m transmissions can occur without collisions/blocking, for all m! permutations of Ys, the network is NB. The
evaluation of the NB property of a network can depend on the specific meaning of "sending data" as defined by the queuing model. Based on kinds of "sending data",
there can be two forms of NB, Circuit NB (NB-C) and Packet NB
(NB-P). For NB-C, each source X can send a continuous stream at its full port capacity to its destination I'. For NB-P, each source can send one frame to its destination I'.
In both cases, for NB to hold, for any it( F) there exists a set of m paths (sequences of hops), each path connecting its XY pair. The difference in these paths for the two
forms of NB is that for NB-C each XY path has to have all its hops reserved exclusively for its XY pair at all times, while for NB-P, the XY path needs to reserve a hop only
for the packet forwarding step in which the XY frame is using it.
Hence NB-C is a stronger requirement, i.e. if a network is NB-C then it is also NB-P.
[0072] In accordance with some embodiments of the invention, a hypercube network with a X=1 has Packet Non-Blocking Property. This is self-evident for d=1, where
there are only 2 switches, two ports per switch, one T-port and one E-port. In this case m=2, hence there are only 2!=2 sets of XY pairing instances to consider: II=
[X1-0 , X2-0Y2] and 12= WC". Y2 , X2"4 Yd. The set of nt--2 paths for are: {
(X10 Yi), (X2 1 1'2)}, each taking 0 hops to reach its destination (i.e. there were no hops between switches, since the entire switching function in each path was done
internally within the switch). The paths are shown as (X S1 S2 ... SkY), where Si sequence specifies switches visited by the frame in each hop from X. This path requires klhops between the switches (X and Y are not switches but ports on S1 and Sk respectively). For the pairing 12, the two paths are { (X10 1 IT2), (X2 1 0 Y1)}, each 1 hop
long. Since there were no collisions in either instance II or 12, the d=1 network is NB-P. For the next size hypercube, d=2, m=8 and there are 8! (40320) XY
pairings, so we will look at just one instance (selected to maximize the demands over the same links) and show the selection of the n8 collision free paths, before proving
the general case.
[0073] Fig. 8 shows the 8 paths with their properties discernable by splitting the diagram into (a) and (b) parts, but the two are actually running on the same switches and
lines simultaneously. The short arrows with numbers show direction of the frame hop and the switching step/phase at which it takes place. It is evident that at no stage
of the switching, which lasts 3 hops, is any link required to carry 2 or more frames in the same direction (these are duplex lines, hence 2 frames can share a link in
opposite direction) hence NB-P holds for this instance. Not all paths are the shortest ones possible (e.g. X1¨Y3 which took 3 hops, although the shortest path is 1 hop,
the same one as the path X2¨' Y4).
[0074] To prove that in the general case all m= dN frames sent by X1, X2,.. .Xõ, can be delivered to proper destinations, in a finite time and without collisions or dropped
frames, the following routing algorithm can be used. In the initial state when in frames are injected by the sources into the network, each switch receives d frames from
its d E-ports. If there were just one frame per switch instead of d, the regular hypercube routing could solve the problem, since there are no conflicts between multiple
frames targeting the same port of the same switch. Since each switch also has exactly d T-ports, if each switch sends d frames, one frame to each port in any order, in
the next stage each switch again has exactly d frames (received via its d T-ports), without collisions or frame drops so far. While such 'routing' can go on forever without
collisions/frame drops, it does not guarantee delivery.
In order to assure a finite time delivery, each switch must pick out of the maximum d frames it can have in each stage, the frame closest to its destination (the one with
the lowest Hamming weight of its jump vectorDstACurrent) and send it to the correct port.
The remaining d-1 frames (at most; there may be fewer) are sent on the remaining d-ports applying the same rule (the closest one gets highest priority, etc).
Hence after this step is done on each of the N switches, there are at least N frames (the N
"winners" on N switches) which are now closer by 1 hop to their destinations i.e.
which are now at most d-1 hops away from their destination (since the maximum hop distance on a hypercube is d). After k such steps, there will be at least N
frames which are ¨ at most d-k hops away from their destinations. Since the maximum distance on a hypercube is d hops, in at most d steps from start at least N
frames are delivered to their destinations and there are no collisions/drops. Since the total number of frames to deliver is d-N, the above sequence of steps need not be
repeated more than d times, therefore all frames are delivered in at most d2 steps after the start.
QED.
[0075] In accordance with some embodiments of the invention, load balancing can be performed locally at each switch. For each arriving frame, the switch can select the
next hop along a different d-cube dimension than the last one sent, if one is available. Since for any two points with distance (shortest path) L there are L!
alternative paths of equal length L, there are plenty of alternatives to avoid congestion, especially if aided by a central control and management system with a global
picture of traffic flows.
[0076] Much of this look ahead at the packet traffic flow and density at adjacent nodes required to decide which among the equally good alternatives to pick can be done
completely locally between switches with a suitable lightweight one-hop (or few hops) self-terminating (time to live set to 1 or 2) broadcast through all ports, notifying
neighbors about its load. The information packet broadcast in such manner by a switch M can also combine its knowledge about other neighbors (with their

https://patents.google.com/patent/CA2872831C/en?q=(high+radix)&oq=high+radix

5/31

2/14/26, 1:58 PM

CA2872831C - Flexible radix switch - Google Patents

weight/significance scaled down geometrically, e.g. by a factor 1/d for each neighbor). The division of labor between this local behavior of switches and a central control
and management system can be that switching for short distance and near time regions can be controlled by switches and that switching for long distance and long time
behavior can be controlled by the central control and management system.
[0077] In accordance with some embodiments of the invention, symmetrical networks with long hop shortcuts are used to achieve high performance in the network,
however additional forwarding management can be used to optimize the network and achieve higher levels of performance. As the size of the network (number of hosts)
becomes large, it is useful to optimize the forwarding processes to improve network performance.
[0078] One reason for current data center scaling problems is the non-scalable nature forwarding tables used in current switches. These tables grow as 0(N2) where N is
the number of edge devices (hosts) connected to the network. For large networks, this quickly leads to forwarding tables that can not be economically supported with
current hardware, leading to various measures to control forwarding table size by segmenting networks, which leads to further consequences and sub-optimal network
behavior.
[0079] In accordance with some embodiments of to the invention, each switch can maintain a single size forwarding table (of size 0(N)) and network connection matrix
(of size 0(NR), where R is the switch radix and N the number of switches).
The scalable layer 2 topology and forwarding tables maintained by the switches can be based on hierarchical labeling and corresponding hierarchical forwarding
behavior of the switches, which require only m=Nihn table entries for the m-level hierarchy (where m is a small integer parameter, typically m = 2 or 3).
100801 In accordance with one embodiment of the invention, the network can be divided into a hierarchy of clusters, which for performance reasons align with the actual
network connectivity. The 1st level clusters contain R nodes (switches) each, while each higher level cluster contains R sub-clusters of the previous lower level.
Hence, each node belongs to exactly one 18' level cluster, which belongs to exactly one 2nd level cluster, etc. The number of levels m needed for a network with N
nodes and a given R, is then determined from the relations R"<N5je, i.e. m =
[log(N)/log(R)1. The forwarding identifier (FID or Forwarding ID) of a node consists of m separate fields (digits of the node ordinal 0..N-1 expressed in radix R), FID =
Fi.F2...Fõ, where F1 specifies the node index (number 0.. R-1) within its 1st level cluster, F2 the index of the node's 1st level cluster within its second level cluster, etc.
[0081] For example, in an N=100 node network and selecting R=10, each node is labeled via two decimal digits, e.g. a node 3.5 is a node with index 3 in a cluster with
index 5. In this embodiment, if node 3.5 needs to forward to some node 2.8, all that 3.5 needs to know is how to forward to a single node in cluster 8, as long as each
node within the cluster 8 knows how to forward within its own cluster.
For multi-path topologies, nodes have more than single destination forwarding address.
Or, for a general node ¨ each node needs to know how to forward to 9 nodes in its own cluster and to a single node in each of other 9 clusters, hence it needs tables with
only 2*9=18 elements (instead of 99 elements that conventional forwarding uses).
[0082] In accordance with some embodiments of the invention, the forwarding tables for each node can consist of m arrays Ti, 1=1..m, each of size R
elements (the elements are forwarding ports). For example, for R=16 and a network with N=64*1024 switches (corresponding to a network with 20*N=1280*1024 hosts),
the forwarding tables in each switch consist of 4 arrays, each of size 16 elements, totaling 64 elements.
[0083] For any node F with FID(F) = Fi.F2...Fõ, the array Ti[R]
contains ports which F needs to use to forward to each of the R nodes in its own 1st level cluster. This forwarding is not assumed to be a single hop, so the control
algorithm can seek to minimize the number of hops when constructing these tables. A
convenient topology, such as hypercube type, makes this task trivial since each such forwarding step is a single hop to the right cluster. In accordance with some
embodiments of the invention, in the hypercube network, the control algorithm can harmonize node and cluster indexing with port numbers so that no forwarding tables
are needed at all. The array 7'2 contains ports F needed for forwarding to a single node in each of the R 2' level clusters belonging to the same third level cluster as node
F; T3 contains ports F needed for forwarding to a single node in each of the R 3"
level clusters belonging to the same 4th level cluster as F,... and finally Tõ, contains ports F needs to use to forward to a single node in each of the Rmth level cluster
belonging to the same (m+l)h cluster (which is a single cluster containing the whole network).
[0084] In accordance with some embodiments of the invention, forwarding can be accomplished as follows. A node F with FID(F) = Fit.F2...F,õ receiving a frame with final
destination FID(Z) = Zi.22..2õ, determines the index i = 1..m of the highest 'digit' 2; that differs from its own corresponding 'digit' f; and forward the frame to the port T[Zi].
The receiving node G then has (from the construction of tables TO for its i-th digit the value G, 4. Hence, repeating the procedure, node G
determines the index j<i of the highest digit 2rj differing from corresponding Gi and forwards to port Ti[4]... etc., until j1, at which point the node is performing the final
forwarding within its own cluster.
[0085] In accordance with some embodiments of the invention, the implementation of this technique can involve the creation of hierarchical addresses.
Since the forwarding to clusters at levels > 1 involves approximation (a potential loss of information, and potentially sub-optimal forwarding), for the method to forward
efficiently it can be beneficial to a) reduce the number of levels m to the minimum needed to fit the forwarding tables into the CAMs (content addressable memories) and
b) reduce the forwarding approximation error for m >1 selecting the formal clustering used in the construction of the network hierarchy to match as closely as possible
the actual topological clustering of the network.
[0086] Forwarding efficiency can be improved by reducing the number of levels m to the minimum needed to fit the forwarding tables into the CAMs. In situations where
one can modify only the switch firmware but not the forwarding hardware to implement hierarchical forwarding logic, the conventional CAM
tables can be used. The difference from the conventional use is that instead of learning the MAC addresses, which introduce additional approximation and forwarding
inaccuracy, the firmware can program the static forwarding tables directly with the hierarchical tables.
[0087] Since m levels reduce the size of the tables from N to m./V1/1"
entries (e.g. m=2 reduces the tables from N entries to 24N entries), a 2-3 level hierarchy may be sufficient to fit the resulting tables in the C=16K entries CAM memory
(e.g.
C---16K allows 2.8K entries, or N=64.206 nodes). Generally, m is the lowest value satisfying inequality: m=Nlim < C.
[0088] In order to reduce the forwarding approximation error for m >1, the formal clustering used in the construction of the hierarchical should match as closely as
possible the actual topological clustering of the network. For enhanced hypercube topologies used by the invention, optimum clustering is possible since hypercubes are
a clustered topology with m=log(N). In practice, where minimum m is preferred, the hypercubes of dimension d are intrinsically clustered into lower level hypercubes
corresponding to partition of d into m parts. E.g. partition d = a+b corresponds to 2' clusters (hypercube of dim=a) of size 2beach (hypercubes of dim=b). The following
clustering algorithm performs well in practice and can be used for general topologies:
[0089] A node which is the farthest node from the existent complete clusters is picked as the seed for the next cluster (the first pick, when there are no other clusters, is
arbitrary). The new cluster is grown by adding to it one of the unassigned nearest neighbors x based on the scoring function: V(x) = #i - #e, where #i is the number of
intra-cluster links and #e is the number of extra-cluster links in the cluster resulting from adding node x to it. The neighbor x with max value of V(x) score is then assigned
to the cluster. The cluster growth stops when there are no more nodes or when the cluster target size is reached (whichever comes first). When no more unassigned
nodes are available the clustering layer is complete. The next layer clusters are constructed by using the previous lower layer clusters as the input to this same algorithm.
[0090] In accordance with some embodiments of the invention, networks can be considered to include n "switches" (or nodes) of radix (number of ports per switch) Ri for
the i-th switch, where i =1..n. The network thus has the total of PT =
Ei ports. Some number of ports PI is used for internal connections between switches ("topological ports") leaving P = PT ¨ P1 ports free ("external ports"), available for
use by servers, routers, storage,... etc. The number of cables C1 used by the internal connections is CI = P1/2. For regular networks (graphs), those in which all nodes
have the same number of topological links per node m (i.e. m is a node degree), it follows that Pi =

https://patents.google.com/patent/CA2872831C/en?q=(high+radix)&oq=high+radix

6/31

2/14/26, 1:58 PM

CA2872831C - Flexible radix switch - Google Patents

[0091] The network capacity or throughput is commonly characterized via the bisection (bandwidth) which is defined in the following manner: network is partitioned into
two equal subsets (equipartition) Si + S2 so that each subset contains n/2 nodes (within 1 for odd n). The total number of links connecting S1 and S2 is called a cut for
partition Si+S2. Bisection B is defined as the smallest cut (min-cut) for all possible equipartitions Si+S2 of the network. Fig. 9 illustrates this defmition on an 8 node
network with B=2.
[0092] Bisection is thus an absolute measure of the network bottleneck throughput. A related commonly used relative throughput measure is the network
oversubscription 4) defined by considering the P/2 free ports in each min-cut half, Si and S2, with each port sending and receiving at its maximum capacity to/from the
ports in the opposite half. The maximum traffic that can be sent in each direction this way without overloading the network is B link (port) capacities since that's how
many links the bisection has between the halves. Any additional demand that free ports are capable of generating is thus considered to be an "oversubscription" of the
network.
Hence, the oversubscription 4) is defined as the ratio:
4) - (3.1) [0093] The performance comparisons between network topologies, such as [1]-[5], [9]-[10], typically use non-oversubscribed networks (4)=1) and compare the
costs in terms of number of switches n of common radix R and number of internal cables C1 used in order to obtain a given target number of free ports P. Via eq. (3.1),
that is equivalent to comparing the costs n and CI needed to obtain a common target bisection B.
[0094] Therefore, the fundamental underlying problem is how to maximize B
given the number of switches n each using some number of topological ports per switch in (node degree). This in turn breaks down into two sub-problems:
(1) Compute bisection B for given network (ii) Modify/select links which maximize B computed via (i) [0095] For general networks (graphs), both sub-problems are
computationally intractable, i.e. NP-complete problems. For example, the 'easier' of the two tasks is (i), since (ii) requires multiple evaluations of (i) as the algorithm (ii)
iterates/searches for the optimum B. Task (i) involves finding the graph equipartition H0+111 which has the minimum number of links between the two halves, in general
case would have to examine every possible equipartition Ho+Hi and in each case count the links between the two, then pick the one with the lowest count. Since there
are C(n, n/2) =
2n/2 irV7-1/2 ways to split the set of n nodes into two equal halves, the exact brute force solution has exponential complexity. The problem with approximate bisection
algorithms is the poor solution quality as network size increases ¨ the polynomial complexity algorithms bisection applied to general graphs cannot guarantee to find an
approximate cut even to within merely a constant factor from the actual minimum cut as n increases. And without an accurate enough measure of network throughput,
the subtask (ii) cannot even begin to optimize the links.
100961 An additional problem with (ii) becomes apparent, that even for small networks such as those with few dozen nodes, for which one can compute exact B
via brute force and also compute the optimum solution by examining all combinations of the links. Namely, a greedy approach for solving (ii), successively computes B
for all possible addition of the next link, then picks the link which produces the largest increment of B among all possible additions. That procedure continues until the
target number of links per node is reached. The numerical experiments on small networks show that in order to get the optimum network in step m --> m+1 links per
node, one often needs to replace one or more existent links as well, the links which were required for optimum at previous smaller values of m.
[0097] In addition to bandwidth optimization for a given number of switches and cables, the latency, average or maximum (diameter), is another property that is often a
target of optimization. Unlike the B optimization, where an optimum solution dramatically reduces network costs, yielding ¨2-5 fewer switches and cables compared to
conventional and approximate solutions, the improvements in latency are less sensitive to the distinction between the optimal and approximate solutions, with typical
advantage factors of only 1.2-1.5. Accordingly, greater optimization can be achieved in LH networks by optimizing the bisection than by optimizing the network to
improve latency.
100981 The present invention is directed to Long Hop networks and methods of creating Long Hop networks. The description provides illustrative examples of methods
for constructing a Long Hop network in accordance with the invention.
In accordance with one embodiment, one function of a Long Hop network is to create a network interconnecting a number of computer hosts to transfer data between
computer hosts connected to the network. In accordance some embodiments, the data can be transferred simultaneously and with specified constraints on the rate of
data transmission and the components (e.g., switches and switch interconnect wiring) used to build the network.
[0099] In accordance with the invention, a Long Hop network includes any symmetrical network whose topography can be represented by a Cayley graph, and the
corresponding Cayley graphs have generators corresponding to the columns of Error Correcting Code (ECC) generator matrices G (or their isometric equivalents, also
instead of G one can use equivalent components of the parity check matrix H).
In addition, the Long Hop networks in accordance with some embodiments of the invention can have performance (bisection in units of n/2) within 90% of the lower
bounds of the related ECC, as described by the Gilbert-Varshamov bound theorem.
In accordance with some embodiments of the invention, Long Hop networks will include networks having 128 or more switches (e.g., dimension 7 hypercube or greater)
and/or direct networks. In accordance with some embodiments of the invention, Long Hop networks can include networks having the number of interconnections m not
equal to d, d+1,..d+d-1 and m not equal to n-1, n-2. In accordance with some embodiments of the invention, the wiring pattern for connecting the switches of the network
can be determined from a generator matrix that is produced from the error correcting code that corresponds to the hypercube dimension and the number of required
interconnections determined as function of the oversubscription ratio.
[00100] In other embodiments of the invention, similar methods can be used to create networks for interconnecting central processing units (CPUs) as is typically used in
supercomputers, as well as to interconnect data transfer channels within integrated circuits or within larger hardware systems such as backplanes and buses.
[00101] In accordance with some embodiments of the invention, the Long Hop network can include a plurality of network switches and a number of network cables
connecting ports on the network switches to ports on other network switches or to host computers.
[00102] Each cable connects either a host computer to a network switch or a network switch to another network switch. In accordance with some embodiments of the
invention, the data flow through a cable can be bidirectional, allowing data to be sent simultaneously in both directions. In accordance with some embodiments of the
invention, the rate of data transfer can be limited by the switch or host to which the cable is connected. In accordance with other embodiments of the invention, the data
flow through the cable can be uni-directional. In accordance with other embodiments of the invention, the rate of data transfer can be limited only the physical
capabilities of the physical cable media (e.g., the construction of the cable). In accordance with some embodiments, the cable can be any medium capable of
transferring data, including metal wires, fiber optic cable, and wired and wireless electromagnetic radiation (e.g., radio frequency signals and light signals). In accordance
with some embodiments, different types of cable can be used in the same Long Hop network.
[00103] In accordance with some embodiments of the invention, each switch has a number of ports and each port can be connected via a cable to another switch or to a
host. In accordance with some embodiments of the invention, at least some ports can be capable of sending and receiving data, and at least some ports can have a
maximum data rate (bits per second) that it can send or receive. Some switches can have ports that all have the same maximum data rate, and other switches can have
groups of ports with different data rates or different maximum data transfer rates for sending or receiving. In accordance with some embodiments, all switches can have
the same number of ports, and all ports can have the same send and receive maximum data transfer rate. In accordance with other embodiments of the invention, at
least some of the switches in a Long Hop network can have different numbers of ports, and at least some of the ports can have different maximum data transfer rates.
[001041 The purpose of a switch is to receive data on one of its ports and to send that data on out another port based on the content of the packet header fields.
Switches can receive data and send data on all their ports simultaneously. A
switch can be thought of as similar to a rail yard where incoming train cars on multiple tracks can be sent onward on different tracks by using a series of devices that
control which track among several options a car continues onto.
[00105] In accordance with some embodiments of the invention, the Long Hop network is constructed of switches and cables. Data transferred between a host computer

https://patents.google.com/patent/CA2872831C/en?q=(high+radix)&oq=high+radix

7/31

2/14/26, 1:58 PM

CA2872831C - Flexible radix switch - Google Patents

or a switch and another switch over a cable. The data received from a sending host computer enters a switch, which can then forward the data either directly to a
receiving host computer or to another switch which in turn decides whether to continue forwarding the data to another switch or directly to a host computer connected to
the switch. In accordance with some embodiments of the invention, all switches in the network can be both connected to other switches and to hosts.
In accordance with other embodiments of the invention, there can be interior switches that only send and receive to other switches and not to hosts as well.
1001061 In accordance with some embodiments, the Long Hop network can include a plurality of host computers. A host computer can be any device that sends and/or
receives data to or from a Switch over a Cable. In accordance with some embodiments of the invention, host computers can be considered the source and/or destination
of the data transferred through the network, but not considered to be a direct part of the Long Hop network being constructed. In accordance with some embodiments of
the invention, host computers cannot send or receive data faster than the maximum data transfer rate of the Switch Port to which they are connected.
1001071 In accordance with some embodiments of the invention, at least some of following factors can influence the construction of the network. The factors can
include 1) the number of Hosts that must be connected; 2) the number of switches available, 3) the number of ports on each switch; 4) the maximum data transfer rate
for switch ports; and 5) the sum total rate of simultaneous data transmission by all hosts. Other factors, such as the desired level of fault tolerance and redundancy can
also be factor in the construction of a Long Hop network.
1001081 In accordance with some embodiments of the invention, the desired characteristics of the Long Hop network can limit combinations of the above factors used
in the construction of a Long Hop network that can actually be built. For example, it is not possible to connect more hosts to a network than the total number of switches
multiplied by the number of ports per switch minus the number of ports used to interconnect switches. As one ordinary skill would appreciate, a number of different
approaches can be used to design a network depending on the desired outcome. For example, for a specified number of hosts, switches with a given maximum data
transfer rate, and ports per switch, how many switches are needed and how should they be connected in order to allow all hosts to send and receive simultaneously at
50% of their maximum data transfer rate, Alternatively, for a specified number of hosts, number of switches with a given number of ports and maximum data transfer
rate, how much data can be simultaneously transferred across the network and what switch connection pattern(s) supports that performance.
[001091 For purposes of illustration, the following description explains how to construct a Long Hop network according to some embodiments of the invention.
In this embodiment, the Long Hop network includes 16 switches and uses up to 7 ports per switch for network interconnections (between switches). As one of ordinary
skill will appreciate any number of switches can be selected and the number ports for network interconnection can be selected in accordance with the desired
parameters and performance of the Long Hop network.
[001101 In accordance with some embodiments of the invention, the method includes determining how to wire the switches (or change the wiring of an existing network
of switches) and the relationship between the number of attached servers per switch and the oversubscription ratio.
[00111] In accordance with some embodiments of the invention, the ports on each switch can be allocated to one of two purposes, external connections (e.g., for
connecting the network to external devices including host computers, servers and external routers or switches that serve as sources and destinations within the
network), and topological or internal connections. An external network connection is a connection between a switch and a source or destination device that enables data
to enter the network from a source or exit the network to a destination. A
topological or internal network connection is a connection between networks switches that form the network (e.g., that enables data to be transferred across network).
[00112] In accordance with some embodiments of the invention, the oversubscription ratio can be determined as the ratio between the total number of host connections
(or more generally, external ports) and the bisection (given as number of links crossing the min-cut partition). In accordance with some embodiments of the invention, an
oversubscription ratio of 1 indicates that in all cases, all hosts can simultaneously send at the maximum data transfer rate of the switch port. In accordance with some
embodiments of the invention, an oversubscription ratio of 2 indicates that the network can only support a sum total of all host traffic equal to half of the maximum data
transfer rate of all host switch ports. In accordance with some embodiments of the invention, an oversubscription ratio of 0.5 indicates that the network has twice the
capacity required to support maximum host traffic, which provides a level of failure resilience such that if one or more switches or connections between switches fails,
the network will still be able to support the full traffic volume generated by hosts.
[00113] In accordance with some embodiments of the invention, the base network can be an n-dimensional hypercube. In accordance with other embodiments of the
invention, the base network can be another symmetrical network such as a star, a pancake and other Cayley graphs based network structure. In accordance with some
embodiments of the invention, an n-dimensional hypercube can be selected as a function of the desired number of switches and interconnect ports.
[001141 In accordance with some embodiments of the invention, a generator matrix is produce for the linear error correcting code that matches the underlying hypercube
dimension and the number of required interconnections between switches as determined by the network oversubscription ratio. In accordance with some embodiments
of the invention, the generator matrix can be produced by retrieving it from one of the publicly available lists, such as the one maintained by the MinT
project (http://mint.sbg.ac.at/index.php). In accordance with other embodiments of the invention, the generator matrix can be produced using a computer algebra
system such as the Magma package (available fromhttp://magma.maths.usyd.edu.au/magmaJ). For example, in Magma package a command entered into Magma
claculator (http://magma.maths.usyd.edu.au/calc/):
C:=BKLC(GF(2),7,4); C;
produces as output the generator matrix for the binary linear code [7,4,3]:
[7, 4, 3] Linear Code over GF(2) Generator matrix:
[1 0 0 0 0 1 1]
[0 1 0 0 1 0 1]
[0 0 1 0 11 01 [0 0 0 1 1 1 1]
[00115] In accordance with some embodiments of the invention, a linear error correcting code generator matrix can be converted into a wiring pattern matrix by rotating
the matrix counterclockwise 90 degrees, for example, as shown in Table 4.9.
[00116] In the illustrative example shown in Table 4.9, each switch has 7 ports connected to other switches and 16 total switches corresponding to an LH
augmented dimension 4 hypercube. Generators h1 through h7 correspond to the original columns from rotated [G4,7] matrix that can be used to determine how the
switches are connected to each other by cables.
[00117] In accordance with some embodiments of the invention, the 16 switches can be labeled with binary addresses, 0000, 0001, through 1111. The switches can be
connected to each other using the 7 ports assigned for this purpose, labeled hl through h7, by performing the following procedure for each of the sixteen switches. For
example, connect a cable between each source switch network port (1 ¨
7) and the same port number on the destination switch whose number is determined by performing an exclusive or logical operation between the source switch number
and the value of the Cayley graph generator hi to h7 (column 2 in the table below) for the network port number.
[00118] For example, to determine how to connect the 7 wires going from switch number 3 (binary 0011), take the graph generator (number in 2nd column) and exclusive
or (XOR) it with 0011 (the source switch number), which results in "Destination switch number" in the following connection map (the XOR of columns 2 and 3 yields
column 4):
Port on switch Generators h1-h7 Source switch Destination switch 3 number number 1 0001 (1) 0011 (3) 0010(2) 2 0010(2) 0011 (3) 0001 (1) 3 0100 (4) 0011 (3) 0111
(7) 4 1000(8) 0011(3) 1011(11) 0111(7) 0011 (3) 0100(4) 6 1110(14) 0011 (3) 1101 (13) 7 1011(11) 0011(3) 1000(8) [00119] This wiring procedure describes how to
place the connections to send from a source switch to a destination switch, so for each connection from a source switch to a destination switch, there is also a
connection from a destination switch to a source switch. As a practical matter, in this embodiment, a single bi-directional cable is used for each pair of connections.
Construction of Long Hop Networks 1001201 The LH networks are direct networks constructed using general Cayley graphs Cay(G. S.) for the topology of the switching
network. The preferred embodiment for LH networks belongs to the most general hypercubic-like networks, with uniform number of external (E) and topological (in) ports

https://patents.google.com/patent/CA2872831C/en?q=(high+radix)&oq=high+radix

8/31

2/14/26, 1:58 PM

CA2872831C - Flexible radix switch - Google Patents

per switch (where E+m=R='switch radix'), which retain the vertex and edge symmetries of the regular d-cube Qd. The resulting LH network with n=2d switches in that case
is a Cayley graph of type Cay(g, S.) with n-1 > m> d+1 (these restiction on in exclude well known networks such as d-cube Qd which has in = d, folded d-cube Fl2d with m
=
d+1, as well as fully meshed network nt=n and m=n-1). It will become evident that the construction method shown on zg example applies directly to the general group Z1
with q> 2. For q >2, the resulting Cay(g, S.) is the most general LH type construction of ad-dimensional hyper-torus-like or flattened butterfly-like networks of extent q
(which is equivalent to a hyper-mesh-like network with cyclic boundary conditions). The preferred embodiment will use q =2, since zg is the most optimal choice from
practical perspective due to the shortest latency (average and max), highest symmetry, simplest forwarding and routing, simplest job partitioning (e.g. for multi-processor
clusters), easiest and most economical wiring in the VI class.
[00121] Following the overall task breakdown in section 3, the LH
construction proceeds in two main phases:
(i) Constructing a method for efficient computation of the exact bisection B
(ii) Computing the optimal set of m links (hops) S. per node maximizing this [00122] For the sake of clarity, the main phases are split further into smaller subtasks, each
described in the sections that follow.
Generators and Adjacency Matrix [00123] Network built on Cay(Z1 , S.) graph has n = q d vertices (syn.
nodes), and for q =2 which is the preferred embodiment, n = 2d nodes. These it nodes make the it element vertex set V={ vo,v2,... v..1}. We are using 0-based subscripts
since we need to do modular arithmetic with them.
Node labels and group operation table [00124] The nodes vi are labeled using d-tuples in alphabet of size q:
v1 i E
{0,1,... n-1} expressed as d-digit integers in base q. The group operation, denoted as (f), is not the same as integer addition mod n but rather it is the component-wise
addition modulo q done on d components separately. For q =2, this is equivalent to a bitwise XOR operation between the d-tuples, as illustrated in Table 2.1 (Appendix A)
which shows the full g group operation table for d = 4.
1001251 Table 4.1 illustrates analogous zg group operation table for d=2 and q=3, hence there are n=32=9 group elements and the operations table has nxn =
9x9 =
81 entries. The 2-digit entries have digits which are from alphabet {0,1,21.
The n rows and n columns are labeled using 2-digit node labels. Table entry at row r and column c contains result of rec (component-wise addition mod T=3). For
example, the 3rd row labeled 02, and the 6-th column labeled 12, yield table entry 02E1)12 ¨
(0+0%3, (2+2)%3 =1,1 = 11.
liNVOKARRCWIERMILP, =
00, 00 01 02 10 11 12 20 21 22 44 :OA 01 02 00 11 12 10 21 22 20 :t4V
.0p,1 02 00 01 12 10 22 20 21 r 1,40 10 11 12 20 21 22 00 01 02 4, :**A. 12 10 11 22 20 21 02 00 01 Z c 20 21 22 00 01 02 10 11 12 i*
40 21 22 20 01 02 00 11 12 10 at fA'e .2g.t 22 20 21 02 00 01 12 10 11 !;'ig:,A
.1i*Iiteliraultotetruka Table 4.1 1001261 It can be noted in Table 4.1 for Z1 and in Table 2.1 (Appendix A) for zg that each row r and column c contains all n group
elements, but in a unique order.
The 0-th row or 0-th column contain the unmodified r and c values since the 'identity element' is 10=0. Both tables are symmetrical since the operation rec = cer is
symmetrical (which is a characteristic of the abelian group Z1, used in the example).
Construction of adjacency matrix [Al 001271 Generator set S. contains m "hops" h1, h2,... h. (they are also elements of the group Gr, in Cay(Gõ, S.)), which can be viewed
as the labels of the in nodes to which the "root" node, ve=-0 is connected. Hence, the row r-Al of the adjacency matrix [A] has m ones, at columns A(0,h) for m hops h E
STõ, and 0 elsewhere. Similarly, the column c=0 has m ones at rows A(h,O) for m hops h E
Sõ, and 0 elsewhere. In a general case, some row r--y has m ones at columns A(y,y(Bh) for h E ST,, and 0 elsewhere. Similarly a column c=x has m ones at rows A(xelh,x)
for h E Sm and 0 elsewhere. Denoting contributions of a single generator h E Sm to the adjacency matrix [A] as a matrix T(h), these conclusions can be written more
compactly via Iverson brackets and bitwise OR operator'[ as:
T(a)i [iEDa =111 [j(Ela = i] a E Gi, (4.1) [A] = ZhEsm T(h) Es=1T(h) (4.2) [00128] Note that eq. (4.1) defines T(a) for any element a (or vertex) of the group G. Since the right
hand side expression in eq. (4.1) is symmetric in i and j it follows that T(a) is a symmetric matrix, hence it has real, complete eigenbasis:
T(a)ii = T(a)j,i (4.3) [00129] For the group Gõ= VI, the group operator e becomes regular XOR
operation, simplifying eq. (4.1) to:
T(a)i [i^j = a], a E Z (4.4) [00130] Table 4.2 illustrates the T(a) matrices for q=2,d=3, n=8 and all group elements a = 0..7. For given a=0..7, value 1 is placed on row r and
column c if rAc =
a, and 0 otherwise (Os are shown as '-`).
0=0 a=1 a=2 a=3 a=4 a=5 a=6 a=7 Table 4.2 1001311 Table 4.3 (a) shows the 8 x8 adjacency matrix [A] obtained for the generator set S4 -='=. {1, 2,4, 7}hex {001, 010, 100,
111}bin by adding the 4 generators from Table 4.2: [Al = T(1)+T(2)+T(4)+T(7), via eq. (4.2). For pattern clarity, values 0 are shown as `-`. Table 4.3 (b) shows the indices of
the 4 generators (1, 2, 3, 4) which contributed 1 to a given element of [A] in Table 4.3 (a).
(a) (b) Table 4.3 1001321 Fig. 10 shows the resulting 8-node network (folded 3-cube, FQ3).
Actions (bitwise XOR) of the 4 generators T(a)E {001, 010, 100, 1111b,,, on the node 000 are indicated by the arrows pointing to the target vertex. All other links are shown
without arrows. The total number of links is C=n1n/2=8.4/2=16, which can be observed directly in the figure.
Eigenvectors of T(a) and [A]
{001331 To solve the eigen-problem of [A], couple additional properties of T(a) are derived from eq. (4.4) (using x^x=0 and x^y=-3/^x):
n-1 n-1 (T(a)T(b)).. T(a)i,kT(b)k,i 0 O = [k = a] [k = j 1)] =
k=0 k =
= jAbi [iAj (Ob] = T(a^b)i T(a)T(b) = T(a^b) (4.5) T(a)T(b) = T(a^b) = T(b^a) = T(b)T(a) (4.6) 1001341 Eq. (4.5) shows that T(a) matrices are a representation of the group Gõ
and eq. (4.6) that they commute with each other. Since via eq. (4.2), [A] is the sum of T(a) matrices, then [A] commutes with all T(a) matrices as well. Therefore, since
they are all also symmetric matrices, the entire set { [A], T(a) Va} has a common eigenbasis (via result (M4) in section 2.F). The next sequence of equations shows that
Walsh functions viewed as n-dimensional vectors lUk) are the eigenvectors for T(a) matrices. Using eq. (4.4) for the matrix elements of the T(a), the action of T(a) on
Walsh ket vector lUk) yields for the i-th component of the resulting vector:
n-1 n-1 (T(a)IUk))i = T(a)id Uk(j) = = Oa] Uk(j) = Uk(i^a) (4.7) i=o j=c) 1001351 The result Uk(i^a) is transformed via eq. (2.5) for the general function values of Uk(x):
ijk (0a) = (_1)44:1 kit OA = kp ap+EP, kp fp =
= (_ i)EP) kp ap (_1)4:1, k uk(a)u k(i) = U k (a) aU k)) (4.8) [00136] Collecting all n components of the left side of eq. (4.7) and right side of eq. (4.8) yields in vector form:
T(a)IUk) = Uk(a)IUk) (4.9) [00137] Hence, the orthogonal basis set ( RA), k=0..n-1) is the common eigenbasis for all T(a) matrices and for the adjacency matrix [A]. The n
eigenvalues for T(a) are Walsh function values Uk(a), k=0 ..n-1. The eigenvalues for [A]
are obtained by applying eq.(4.9) to the expansion of [A] via T(h), eq. (4.2):

https://patents.google.com/patent/CA2872831C/en?q=(high+radix)&oq=high+radix

9/31

2/14/26, 1:58 PM

CA2872831C - Flexible radix switch - Google Patents

[A]lUk) = T(h) lUk) = Uk(hs) = lUk) AklUk) (4.10) s=1 s=1 where: A k Uk (Its) (4.11) s=1.
[00138] Since U0(x)=1 is constant (for x=0..n-1), the eigenvalue 20 of [A] for the eigenvectorlUo) is:
m A k (4.12) [001391 From eq. (4.11) it also follows that 20> Ak for k--1..n-1 since the sum in eq. (4.11) may contain one or more negative addends Uk(its)=-1 for k>0,
while for the k=0 case all addends are equal to +1.
Computing Bisection Cuts from adjacency matrix and partition vector [00140] The bisection B is computed by finding the minimum cut C(X) in the set E={X} of all
possible equipartitions X=S1+S2 of the set of n vertices. An equipartition X can be represented by an n-dimensional vector IX) E V,, containing n/2 values +1 selecting
nodes of group SI, and n/2 values -1 selecting the nodes of group S2. Since the cut value of a given equipartition X does not depend on particular +1/-1 labeling
convention (e.g. changing sign of all elements xi defmes the same graph partition), all vectors IX) will have by convention the 1st component set to 1 and only the
remaining n-1 components need to be varied (permuted) to obtain all possible distinct equipartitions from E. Hence, the equipartitions set E consists of all vectors X =
(xo, xli= = = x0-1), where x0=1, xiE f+1 ,-11 and rizfol xi = 0.
[00141] The cut value C(X) for a given partition X = (xo, xi,..1) is obtained as the count of links which cross between nodes in S1 and S2. Such links can be easily identified
via E and adjacency matrix [A], since [A], is 1 if nodes i and j are connected and 0 if they are not connected. The group membership of some node i is stored in the
component x, of the partition X. Therefore, the links (i,j) that are counted have [A], J=1, i.e. nodes i and) must be connected, and they must be in opposite partitions i.e. xi
xi. Recalling that x, and xi have values +1 or -1, the "xi xj" is equivalent to "(xi=xi)=-1". To express that condition as a contribution +1 when xi xj and a contribution 0 when xi
= xj, expression (1- xi. xj)/2 is constructed which yields precisely the desired contributions +1 and 0 for any xi, xj = 1.
Hence, the values added to the link count can be written as Co(l-x xi).[A]ii/2 since Co=1 if nodes i and) are connected ([A]o=1) and they are in different groups (xexi=-1).
Otherwise Co is 0, thus adding no contribution to the C(X).
1001421 A counting detail that needs a bit of care arises when adding Co terms for all Namely, if the contribution of e.g. C3,5 for nodes 3 and 5 is 1, because [A]3,5=1
(3,5 linked), x3=-1 and x5=+1, then the contribution of the same link will contribute also via C5,3 term since [A]5,3=1, x5=+1, x3=-1. Hence the sum of Co for all counts the
contribution for each link twice. Therefore, to compute the cut value C(X) for some partition X, the sum of Cj terms must be divided by 2.
Noting also that for any vector XEE (XIX) = Erfol xixi = n and EZ;11-0[A]i =
Errg m = n = m, yields for the cut C(X):
n-1 71-1 I! nm 1 C(X)= (1 ¨ xixi) [A] " = =
¨ ¨ ¨ xixj [AL.' i,i=o cf.
n (XIAIX)) (XIX) = ¨ m _______________________ (4.14) [00143] To illustrate operation of the formula (4.14), the Table 4.5 shows adjacency matrix [A] for Cay(Z1,S5), which
reproduces FQ4 (folded 4-cube), with d=4, n=2d=24=16 nodes, m=5 links per node, produced by the generator set S5=
1, 2, 4, 8, Flhex={0001, 0010, 0100, 1000, 1111}bin. The row and column headers show the sign pattern of the example partition X=(1,1,1,1, -1,-1,-1,-1, 1,1,1,1, -1,-1,-1,-1)
and the shaded areas indicate the blocks of [A] in which eq. (4.14) counts ones ¨
elements of [A] where row r and column c have opposite signs of the X components xr and xc.
The cut is computed as C(X) = 1/2 (sum of ones in shaded blocks) = 1/2*(4*8) =
which is the correct B for FQ4 (2*n/2=2*8=16). Note that the zeros (they don't contribute to C(X)) in the matrix [A] are shown as `-` symbol.
X 11=1111111111 +
N;;1`.```,*4 1*"-+ 10,401a + _ 1 õam_ _ _ 1,7 7 _4ativtl _ 11 _ 1 _ _ _ _ _ _ 3. - - 1 õ Anna_ 1 1 vktwo - ¨
- - *OWL 1 Mitil - - - -Ma.ft - - - - -- - - - - -Table 4.5 Finding the minimum cut (bisection) 1001441 Bisection B is computed as the minimum cut C(X) for all XEE, which via eq.
(4.14) yields:
B = min {: (m ("IX)\ nm n (XIAIX)1 nm n XEE -r (XIX) )¨ licei 4 4 (XIX) J 4 Mc (4.15) 4 -(XIAIX)) where: ME -17 IPCEIC (XIX) (4.16) [001451 Despite the apparent similarity
between the max{} term ME in eq.
(4.16) to the max{ } term Mv in eq. (2.46), the Rayleigh-Ritz eqs. (2.45)-(2.46) do not directly apply to min{} and max{ } expressions in eq. (4.15). Namely, the latter
extrema are constrained to the set E of equipartitions, which is a proper subset of the full vector space Vr, to which the Rayleigh-Ritz applies. The ME max{ } in eq.
(4.16) can be smaller than the My max{ } computed by eq. (2.46) since the result My can be a vector from VT, which doesn't belong to E (the set containing only the
equipartition vectors X) i.e. if Mv is solved only by some vectors Y which do not consist of exactly n/2 elements +1 and n/2 elements -1.
[001461 As an illustration of the problem, ME is analogous to the "tallest programmer in the world" while Mv is analogous to the "tallest person in the world."
Since the set of "all persons in the world" (analogous to Vn) includes as a proper subset the set of "all programmers in the world" (analogous to E) the tallest programmer
may be shorter than the tallest person (e.g. the latter might be a non-programmer). Hence in general case the relation between the two extrema is ME
<
M. The equality holds only if at least one solution from Mv belongs also to ME, or in the analogy, if at least one person among the "tallest person in the world" is also a
programmer. Otherwise, strict inequality holds ME < MV-[00147] In order to evaluate ME ai max.() in eq. (4.16), the n-dimensional vector space V, (the space to which
vectors IX) belong) is decomposed into a direct sum of two mutually orthogonal subspaces:
Vn = VoEDVE (4.17) [00148] Subspace Vo is one dimensional space spanned by a single 'vector of all ones' (11 defined as:
(11 (1,1,1, ... ,1) (4.18) while VE is the (n-1) dimensional orthogonal complement of Vo within Võ, i.e.
VE is spanned by some basis of n-1 vectors which are orthogonal to (11. Using the eq.
(2.6) for Walsh function Uo(x), it follows:
(1,1,1, ... ,1) = (U01 (4.19) [00149] Hence, VE is spanned by the remaining orthogonal set of n-1 Walsh functions Uk), k=1..n-1. For convenience the latter subset of Walsh
functions is labeled as set (I) below:
(13 flUk): k=1..n-11 (4.20) [00150] Since all vectors XEE contain nI2 components equal +1 and n/2 components equal -1, then via (4.18):
(11X) = 1 xi = 0, VX E E (4.21) i.e. (11 is orthogonal to all equipartion vectors X from E, hence the entire set E
is a proper subset of VE (which is the set of all vectors E V, orthogonal to (11). Using ME in eq. (4.16) and eq. (2.46) results in:
((XIA1X)) f(XIAIX)) MErnaXt (XIX) I 5 Mv EVE t (XIX) = Amax XE (4.22) [00151] The Mv in eq. (4.22) is solved by an eigenvectorlY) of [A] for which [A]lY)=Inun,IY> since:
(YIAIY) (Yiiimax1Y) Amax(YIY) = (4.23) Amax (YIY) (Yri) [00152] Recalling, via eq. (4.10), that the eigenbasis of the adjacency matrix [A] in eq. (4.22) is the set of Walsh
functions Ilk), and that VE in which the Mv=max {}is searched for, is spanned by the n-1 Walsh functions lUk> E (1), it follows that the eigenvector IY) of [A] in eq. (4.23)
can be selected to be one of these n-1 Walsh functions from (I) (since they form a complete eigenbasis of [A] in YE) i.e.:
IY) E (I) tlUk): k=1..n-1} (4.24) [00153] The equality in (4.22) holds if at least one solution IY) E VE
is also a vector from the set E. In terms of the earlier analogy, this can be stated as:
in the statement "the tallest student" < "the tallest person", the equality holds if at least one among the "tallest person" happens to be a "programmer."
1001541 Since IY) is one of the Walsh functions from (I) and since all IUk) E (I) have, via eqs. (2.5) and (2.7), exactly nI2 components equal +1 and nI2 components equal
-1, IY) belongs to the set E. Hence the exact solution for ME in eq.
(4.22) is the Walsh functions lUk) E (1) with the largest eigenvalue A. Returning to the original bisection eq. (4.15), where ME is the second term, it follows that B is solved
exactly by this same solution 111)=1Uk) E (1). Combining thus eq. (4.15) with equality case for ME in eq. (4.22) yields:

https://patents.google.com/patent/CA2872831C/en?q=(high+radix)&oq=high+radix

10/31

2/14/26, 1:58 PM

CA2872831C - Flexible radix switch - Google Patents

nm n B = ¨4 ¨ ¨4 ME = ¨4 (m ¨ Amax) = (m. ¨ kEm[1ax )fAk)) (4.25) 4 ,n [00155] Therefore, the computation of B is reduced to evaluating n-1 eigenvalues Ak of [A] for
k=1..n-1 and finding a t (k with the largest AO i.e.
a t such that At?: .4 for k=1..n-1. The corresponding Walsh function Ut provides the equipartition which achieves this bisection B (the exact minimum cut). The evaluation
of Ak in eq. (4.25) can be written in terms of them generators h, E Sin via eq. (4.11) as:
B = ¨n, (m ¨ max kE) {I Uk(iis)1) (4.26) 4 [1,n s=1 [00156] Although the function values Uk(x) above can be computed via eq.
(2.5) as Uk = (-1)P(k8a), due to parallelism of binary operation on a regular CPU, it is computationally more efficient to use binary form of Walsh functions, Wk(x). The
binary 4--+ algebraic translations in eqs. (2.8) can be rewritten in vector form for Uk and Wk, with aid of definition of 11) from eq. (4.18), as:
iWk) (I1) ¨ Mk)) (4.27) lUk) = 11) ¨ 2 = liNk) (4.28) [00157] Hence, the B formula (4.26) can be written in terms of Wk via eq.
(4.28) and Wk formula eq. (2.10) as:
B = ¨n, (m ¨ max 1D1 ¨ 2 = Wk(k)))) = (m ¨ max ke[1) im ¨ 2 / Wk (hs)1) it kE[1,n) 4 ,n s=1 s=1 B = kE [1 minn) 11 Wk(lis)1 = 2 kE[1 ¨n minn) II P(k&h.$)) (4.29) z s=1 s=1
[00158] The final expression in (4.29) is particularly convenient since for each k=1..n-1 it merely adds parities of the bitwise AND terms: (k&hs) for all m Cayley graph
generators h, E Sm. The parity function P(x) in eq. (4.29) can be computed efficiently via a short C function ([141 p. 42) as follows:
//-- Parity for 32-bit integers inline int Parity(unsigned int x) (4.30) x^=x 16, x^=x 8, x^=x 4, x^=x 2;
return (xA(x 1))&1;
1001591 Using a P(x) implementation Parity(x), the entire computation of B
via eq. (4.29) can be done by a small C function Bisection(n,hops1],m) as shown in code (4.31).
int Bisection(int n,int *ha,int m) (4.31) {
int cut,b,i,k; // n=2d is # of nodes, m=# of hops for(b=n,k=1; k<n; ++k) // Loop through all n-1 Wk() functions { // Set initial min cut b=n (out of range since m< n)
for(cut=i=0; i<m; ++i) // Loop through all m hops ha[i], add cut+=Parity(ha[i]&k); // +1 if hop[i] coincides with Wk(hop[i]) if (cut<b) b=cut; // Update min cut if count
cut<old_min_cut return b; // Return bisection (min cut) in units n/2 [40160] The inner loop in (4.31) executes m times and the outer loop (n-1) times, yielding total of In.n
steps. Hence, for n-1 values of k, the total computational complexity of B is ¨ 0(m.n2).
"Symmetry Optimization" of B computation 1001611 A significant further speedup can be obtained by taking full advantage of the symmetries of Walsh functions Wk
particularly evident in the recursive definition of Hadamard matrix Hõ in eq. (2.1). The corresponding recursion for the binary Walsh matrix [Wd can be written as:
=
[Wi 0 0 ( 0 1 ) An ] == 1[W] rWni 2 ) k [Wn] Pnl) (4.32) where [IN-n] denotes bitwise complement of matrix [Wn]. For example, in the upper half of [W2n] the left and right
sub-matrices [Wn] are the same, suggesting that after computing in eq. (4.29) the partial sums of Wk(hs) over hs<n and k < n (upper left quadrant of W2.) the remaining n
partial sums fork? n (top right qiindrant of W2.) can be copied from the computed left half. Similarly, in the lower half of [W2n]
the left and right quadrants of sub-matrices are a complement of each other, which replaces the above copying method with subtractions from some constant and
copying (the constant is the number of hops h8> n, i.e. the h, in the lower half of W2õ matrix).
The net result of these two computational short-circuits is a reduction of original computation in half. Since computation inside the halves Wi, are of the same type as the
as those just described for W2, applying the same symmetry method recursively log(n) times to the halved matrices being generated in each stage, the net complexity of
the computation of B is reduced from the earlier 0(nrn2) to 0(nrn.log(n)) i.e. the gain is a speedup factor of nllog(n) over the original method of eq. (4.29).
"Fast Walsh Transform Optimization" of B computation [001621 Analogue of the above 'halving' optimization of B computation can be formulated for the algebraic form
of Walsh functions Uk by defining a function f(x) for x=0,1,... n-1 as:
f(x) [x Sm] {1 if x E Sm (4.33) 0 if x Sm.
where and 0 <x < n and Sm={hi, h2,... km} is the set of m graph generators.
Hence, f(x) is 1 when x is equal to one of the generators h, E S. and 0 elsewhere.
This function can be viewed as a vector If), with components fi= f(i).
Recalling the computation of adjacency matrix [A] via eq. (4.2), vector If ) can also be recognized as the 0-th column of [A] i.e.fi = [A]0,1. With this notation, the eq. (4.26)
for B
becomes:
n B = (m ¨ max II Uk (hs)}) = ¨ (M ¨ max {(Uk If)}) F. ¨4 (m ¨ max {Fk}) (4.34) kE[1,n) 4 kE[1,n) kE[1,n) s=1 where: Fk (U k I f) (4.35) [00163]
Therefore, the B computation consists of finding the largest element in the set {Fk} of n-1 elements. Using the orthogonality and completeness of the n vectors Uk), (Ui I
Uk) = n = 8j,k from eq. (2.3), important property of the set {Fk}
follows:
n-1 n-1 n-1 -1FklUk) = -1 lUk)(Uk I f) = (-1I1uk)(uk1)If) = la) = If) (4.36) k=0 k=0 k=0 [00164] The eqs. (4.35),(4.36) can be recognized as the Walsh transform (NJ
chap. 23) of fimetion f(x), with n coefficients Fkln as the transform coefficients.
Hence, evaluation of all n coefficients Fk, which in direct (4.35) computation requires 0(n2) steps, can be done via Fast Walsh Transform (FWT) in 0(n10g(n)). Note that
FWT will produce n coefficients Fk , including Fo, even though we don't need Fo i.e.
according to eq. (4.34), we still need to look for the max{ } in the set {F1, F2,- Fn-1}.
Since each step involves adding of m points, the net complexity of the B
computation via (4.34) using FWT is 0(m=n1og(n)), which is the same as the "symmetry optimization" result in the previous section.
[001651 Although both methods above achieve a speedup by a factor n/log(n) over the direct use of eqs. (4.26) and (4.29), the far greater saving has already occurred in
the original eq. (4.26). Namely, the eq. (4.26) computes B by computing only the n-1 cuts for equipartitions Uk E SI, instead of computing the cuts for all equipartitions in
the set E of all possible equipartitions. The size of the full set E of "all possible equipartitions" is (factor Y2 is due to convention that all partitions in E
have +1 as the 1st component):
1 n 2n-1 /
I El =1n/2) 2\hr __ .7.1/2 (4.37) [00166] To appreciate the savings by eq. (4.26) alone, consider a very small network of merely n=32 nodes. To obtain the exact B for this
network the LH
method needs to compute n-l= 31 cuts, while the exact enumeration would need to compute 1E1= 0.5=C(32,16) = 300,540,195 cuts i.e. 9,694,845 times greater number
of cuts.
Further, this ratio via eq. (2.37) grows exponentially in the size of the network n, nearly doubling for each new node added.
Optimizing Bisection [00167] With the couple 0(m-n-log(n)) complexity methods for computation of bisection B for a given set of generators S. described in the previous
sections, the next task identified is the optimization of the generator set Sm ={hb h2, ...hm} i.e. the finding of the S. with the largest B. The individual hops h, are
constrained to n-1 values: 1,2,... n-1 (0 is eliminated since no node is connected to itself), i.e. S. is an m element subset of the integer sequence 1..n-1. For convenience,
this set of all m-subsets of integer sequence 1..n-1 is labeled as follows:
1/(n, m) E- Sin E tSm: (Sm = fit1,112, ...,hm)) and (0 < h, <n)) (4.40) =7" In(n,m)1 = (n ¨ 1) = 0(nm) (4.41) [00168] With this notation and using the binary formula for B, eq.
(4.29), the B
optimization task is:
m b ¨ = max min Wk(its)11 (4.42) n/2 s. E fl kE[1,n) ts=1 [00169] For convenience, eq. (4.42) also defines a quantity b which is the bisection in units nI2. The worst case
computational complexity the B

https://patents.google.com/patent/CA2872831C/en?q=(high+radix)&oq=high+radix

11/31

2/14/26, 1:58 PM

CA2872831C - Flexible radix switch - Google Patents

optimization is thus 0((m=n1og(n))m), which is polynomial in n, hence, at least in principle, it is a computationally tractable problem as n increases. (The actual exponent
m would be (m - log(n) ¨ 1), not m, since the Cayley graphs are highly symmetrical and one would not have to search over the symmetrically equivalent subsets S.. Note
that m is typically a hardware characteristics of the network components, such as switches, which usually don't get replaced often as network size n increases.
[00170] Since for large enough n, even a low power polynomial can render 'an in principle tractable' problem practically intractable, approximate methods for the max{ }
part of the computation (4.42) would be used in practice. Particularly attractive for this purpose would be genetic algorithms and simulated annealing techniques used in
[1.2] (albeit for the task of computing B, which the methods of this invention solve efficiently and exactly). Some of the earlier implementations of this inventions have
used fast greedy algorithms, which work fairly well. The 'preferred embodiment' for the invention which is described next does not perform any such direct optimization
of eq. (4.42), but uses a more effective method instead.
Bisection B optimization via EC Codes [00171] In order to describe this method, the inner-most term within the nested max{min{}} expression in the eq. (4.42) is identified
and examined in more detail.
For convenience, this term, which has a meaning of a cut for a partition defined via the pattern of ones in the Walsh function Wk(x), is labeled as:
m m Wk(hs) = P(k&hs) (4.43) s=i s=1 {001721 Eq. (4.43) also expresses Wk(x) in terms of parity function P(x) via eq.
(2.10). The function P(x) for some d-bit integer x --(xd..i... xt xo)bmary is defined as:
d-1 P(x) ---a= ( 1 x mod 2 =
=0 It (X0AX1 A " ' Axd_i) (4.44) [00173] The last expression in eq. (4.44) shows that P(x) a: P(xd..1...
xl xo) is a "linear combination" in terms of the selected field GF(2)d, of the field elements provided in the argument. The eq. (4.43) contains a modified argument of type
P(k&h), for h E Sm, which can be reinterpreted as: the 'ones' from the integer k are selecting a subset of bits from the d-bit integer h, then P(x) performs the linear
combination of the selected subset of bits of h. For example, if k=11dec=1011bm than the action of W loil(h)----P(1011&h) is to compute linear combination of the bits
bit-0,1 and 3 of h (bit numbering is zero based, from low/right to high/left significance).
Since eq. (4.43) performs the above "linear combination via ones in k" action of Wk on a series of d-bit integers hõ s=1..m, the Wk "action" on such series of integers is
interpreted as the parallel linear combination on the bit-columns of the list of h, as shown in the Table 4.6, for k=1011 and W1011 acting on a set of generators S5={
0001, 0010, 0100, 1101}. The 3 bit-columns V3. V1 and Vo selected by ones in k are combined via XOR into the resulting bit-column V: 1V3)(DIVOIEDIVo)=1V).
k 1 0 1 1 Resulting column 1 1 1 v hi-0 0,0 1 1 h2-0 0 1 0 1 h3-40 1 0 0 4> 0 h4-1 0 0 0 1 h5-01 1 0 1 .. 0 Columns: V3 e vi . yo ..........4 Table 4.6 [001741 Therefore, the
action of a Wk on the generator set Sffi= {hi, h2, ...h,,,}
can be seen as a "linear combination" of the length-m columns of digits (columns selected by ones ink from Wk) formed by them generators !h. If instead of the g used in
the example of Table 4.6, there was a more general Cayley graph group, such as Z1, instead of the bit-columns there would have been length-m columns made of digits in
alphabet of size q (i.e. integers 0..q-1) and the XOR would have been replaced with the appropriate GF(q) field arithmetic e.g. addition modulo q on m-tuples for Z1 as
illustrated in an earlier example in Table 4.1. The construction of column vectors IVO of Table 4.6 can be expressed more precisely via an mxd matrix Rtm,c11 defined as:
<h1 I hl,d-1 h1,_2 ...
[R7714] Ei Iles)(hsl =
s=i (hm = hm-4-2*=====
7-2 (1Vd--1), IV-2), (4.45) where: (IV)), hs.,4 = ((hi) ,i for p = 0..d -1, s=1..m (4.46) [00175] Hence the m rows of matrix [Rd] are m generators (hsI E Sff, and its d columns are
d column vectors IV). The above 'linear combination of columns via ones in k' becomes in this notation:
d-1 IV(k))lkitiVii) where k k 2g (4.47) =0 g=0 where the linear combination of kiiIV4) is performed in GF(q) i.e. mod q on each component of m-tuples kplVp). The sum
computing the cut Ck in eq. (4.43) is then simply adding (without mod q) all components of the vector V (k)) from eq.
(4.47). Recalling the definition of Hamming weight as the number of non-zero digits, this cut Ck is recognizable as the Hamming weight of the vector IV(k)):
Ck = (V(k)) (4.48) [001761 The next step is to propagate the new "linear combination"
interpretation of Wk action back one more level, to the original optimization problem in eq. (4.42), in which the cut Ck was only the innermost term. The min{ }
block of eq. (4.42), seeks a minimum value of Ck for all k=1..n-1. The set of n vectorsIV(k)) obtained via eq. (4.47) when k runs through all possible integers 0..n-1 is a ddimensional vector space, a linear span (subspace of m-tuples vector space V.), which is denoted as S(d , m, q) fIV(k)): k = O.. n ¨ 11 (4.49) [00177] Therefore, the min{}
level optimization in eq. (4.42) computing bisection b, seeks a non-zero vector1V(k)) from the linear span S(d,m,q) with the smallest Hamming weight (V (k)):
b = mint(V(k)): (V(k) E (d, m, q)) and (V (k) # 0)1 (4.50) [00178] While Hamming weight can be used in some embodiments of the invenition, any other weight, such as Lee
weight, which would correspond to other Cayley graph groups Gn and generator sets Sm, can also be used.
[00179] But b in eq. (4.50) is precisely the definition eq. (2.25) of the minimum weight wmin in the codeword space (linear span) S(_k,_n,q) of non-zero codewords Y.
Note: In order to avoid the mix up in the notation between the two fields, the overlapping symbols [n, k] which have a different meaning in ECC, will in this section have an
underscore prefix, i.e. the linear code [n, k] is relabeled as Ln, _k].
[00180] The mapping between the ECC quantities and LH quantities is then:
wmm <=> b, k<=> d, _n <=> m, _k vectors (gil spanning linear space SCk,_n,q) of n-tuples and constructing code generator matrix [G] (eq. (2.20)) <=> d columns IVO
for It=0..d-1 spanning linear space S(d,m,q) of m-tuples (digit-columns in the generator list). Since, via eq. (2.26) the minimum weight of the code wnlin is same as the
minimum distance A between the codewords Y, it follows that the bisection b is also the same quantity as the ECC A (even numerically). Table 4.7 lists some of the
elements of this mapping.
Linear EC codes _k _n SCk,_n,q) A _k rows of _n-tuples (g,I q for GF(q) LH Networks d m S(d,m,q) b d columns of m-tuples q for Zfil, Table 4.7 [00181] The optimization of
linear code Ln, _k, Al that maximizes A is thus the same optimization as the outermost level of the LH optimization, max{ ) level in eq. (4.42) that seeks the Cayley graph
generator set Sm with the largest bisection b ¨
other than difference in labeling conventions, both optimizations seek the d-dimensional subspace S(d,m,q) of some vectors space Vm which maximizes the minimum
non-zero weight wmin<=t,b of the subspace S. The two problems are mathematically one and the same.
[00182] Therefore, the vast numbers of good/optimal linear ECC codes computed over the last six decades (such as EC code tables [11] and [22]) are immediately
available as good/optimal solutions for the b optimization problem of the LH networks, such as eq. (4.42) for Cayley graph group G=2. Similarly any techniques,
algorithms and computer programs (e.g. MAGMA ECC module http://magma.maths.usyd.edu.au/magma/handbook/linear codes_over Jmite fields) used for
constructing and combining of good/optimum linear EC codes, such as quadratic residue codes, Goppa, Justesen, BCH, cyclic codes, Reed-Muller codes,...
[15],[16], via translation Table 4.7, automatically become techniques and algorithms for constructing good/optimum LH networks.
[00183] As an illustration of the above translation procedure, a simple parity check EC code [4,3,112 with generator matrix [G3,4] is shown in Table 4.8. The codeword has
1 parity bit followed by 3 message bits and is capable of detecting all single bit errors. The translation to the optimum network shown on the right, is obtained by rotating
900 counter-clockwise (5 the 3 x4 generator matrix [G3,4]. The obtained block of 4 rows with 3 bits per row is interpreted as 4 generators hõ
each 3 bits wide, for the Cay(Z1,C4) graph. The resulting network thus has d=3, n=23=8 nodes and in--4 links/node. The actual network is a folded 3-cube shown within an
earlier example in Table 4.4. Its bisection is: b2 and B=b=n/2=8 links.
hi = 001 = 1 (1 1 0 0 [G3,4] = 1 0 1 0) C4 =
h3= 100 = 4 h4 = 111 = 7 Table 4.8 1001841 A slightly larger and denser network using EC code [7,4,312 from Table 2.4 (Appendix A), is converted into an optimum
solution, a graph Cay(Z1,C7), with d=4, n=16 nodes and n2=7 linkJnode as shown in Table 4.9.
h1= 0001 = 1 h2= 0010 = 2 ( 1 1 0 1 0 0 0 h3= 0100 = 4 [G4,7] = 1 11. 11. O 01 1 O C7 = h4 = 1000 = 8 0 1 0 0 0 1 h5= 0111 = 7 h6= 1110 = E
h7= 1011 = B
Table 4.9 [001851 The 4 row, 7 column generator matrix [G4,7] of the linear EC

https://patents.google.com/patent/CA2872831C/en?q=(high+radix)&oq=high+radix

12/31

2/14/26, 1:58 PM

CA2872831C - Flexible radix switch - Google Patents

code [7,4,312 on the left side was rotated 90 counter-clockwise and the resulting 7 rows of 4 digits are binary values for the 7 generators hs (also shown in hex) of the 16
node Cayley graph. The resulting n=16 node network has relative bisection (in n12 units) 11)=A=3 and absolute bisection (in # of links) of: B = b-n/2 = 3-16/2 = 24 links.
Since the network is a non-planar 4-dimensional cube with total min/2=16-7/2=56 links it is not drawn.
1001861 The above examples are captured by the following simple, direct translation recipe:
EC code Lnt_k, Alq ---> LH Cay(g,S.) (4.45) (i) Take EC code generator matrix [Gd and rotate it 90 (in either direction -direction of rotation merely selects order of
generators in the list, which is an arbitrary convention.) (ii) The result is m =--__n row by d --tk column matrix [R,,,,d] of GF(q)-digits 0..q-1 (iii) Read m rows of d-tuples in
base q from [Rd] as m generators hs E S. c (iv) Compute Cayley graph LH=Cay(Z1,S.) from the obtained generators Sm=fiti, h2,.. km) (v) LH: n=qd nodes, m links/node,
bisection: relative b=6" absolute B=A-n/2 links.
1001871 The methods of determining the bisection B can be implemented using a computer program or set of computer programs organized to perform the various
steps described herein. The computer can include one or more processors and associate member, including volatile and non-volatile memory to store the programs and
data. For example, a conventional IBM compatible computer running the Windows or Linix operating system or an Apple computer system can be used and the programs
can be written, for example, the in C programming language.
Implementation Notes N-1. Equivalent LH networks 1001881 Order of elements in a generator set S. = ( hi, h2,... h.) is clearly a matter of convention and network
performance characteristics don't depend on particular ordering. Similarly, the subspace S(d,m,q) of the column vectors can be generated using any linearly independent
set of d vectors from S(d,m,q) instead of the original subset {V. All these transformation of a given network yield equivalent networks, differing only in labeling convention
but all with the same distribution of cuts (including min-cut and max-cut) and the same network paths distribution (e.g.
same average and max paths). This equivalence is used to compute specific generators optimized for some other objective, beyond the cuts and paths. Some of these
other objectives are listed in the notes below.
N-2. Minimum change network expansion [001891 During expansion of the network, it is useful that the next larger network is produced with the minimum change from
the previous configuration e.g.
requiring the fewest cables to be reconnected to other switches or ports. The equivalence transforms of N-1 are used to "morph" the two configuration, initial and final
toward each other, using the number of different links in S. as the cost function being minimized. Techniques and algorithms of "Compressive Sensing" [CS] (see [_21])
are particularly useful as the source for the efficient "morphing"
algorithms.
N-3. Diagonalization [001901 It is often useful, especially in physical wiring, discovery and routing, to have a Zfie based network in which (usually first) d hops from S. are
powers of q.
This property of generator set S. corresponds to systematic generator matrix [G]
for linear codes and can be recognized by the presence of identity matrix Id within [G ] (possibly with permuted columns). The two previous examples, Tables 4.8 and 4.9
were of this type (the digits of Id sub-matrix were in bold).
1001911 A simple, efficient method for computing a "systematic generator"
from non-systematic one is to select for each column c = 0..d-1 a row r(c)=1..m that contains a digit 1 in column c. If row r(c) doesn't contain any other ones, then we
have one column with desired property (the Ih(c) is a power of 2). If there are any other columns, such as c' which contain ones in row r(c), the column V, is XOR-ed into
these columns clearing the excessive ones in r(c). Finally, when there is a single 1 in row r(c) and column c, the hop hl(C) is swapped with hop h,+1 so that the resulting
matrix contains generator h,+1=2c . The process is repeated for the remaining columns c < d.
1001921 The number of XOR operations between columns needed to reduce some row r(c) to a single 1 in column c, is (h,(0)-1. Therefore, to reduce number of required
XOR-s (columns are m bits long which can be much larger than the machine word), for each new c to diagonalize, algorithm picks the row which has the smallest weight,
minf<h1.(0)).
N-4. Digital or (t,m,$) nets (or designs, orthogonal arrays) [00193] This research field closely related to design of optimal linear codes Ln,_k,A]q (cf. [21],[22]). The basic
problem in the field of 'digital nets' is to find distribution of points on s-dimensional hypercubic (fish-) net with "binary intervals"
layout of 'net eyes' (or generally analogous b-ary intervals via powers of any base b, not only for b=2) which places the same number of points into each net eye.
There is a mapping between (t,m,$)b digital nets and Ln,_k]q codes via identities: _n=s, _k=s-_m, q=b. A large database of optimal (t,_m,$) nets, which includes linear code
translations is available via a web site [22]. Therefore, the solutions, algorithms and computer programs for constructing good/optimal (t,_m,$) nets are immediately
portable to construction of good/optimal LH networks via this mapping followed by the Ln,k],i -4 LH mapping in Table 4.7.
N-5. Non-binary codes [00194] The linear codes with q>2 generate hyper-torus/-mesh type of networks of extent q when the A metrics of the code is Lee distance. When
Hamming distance is used for q>2 codes, the networks are of generalized hypercube/flattened butterfly type [3]. For q=2, which is the binary code, the two types of
distance metrics are one and the same.
N-6. Non-binary Walsh functions [00195] Walsh functions readily generalize to other groups, besides cyclic group Z1 used here (cf. []). A simple generalization to base
q>2 for groups Z1, for any integer q is based on defining function values via q-th primitive root of unity to:
Uq,k(x) = (a/PcikPxm for x,k <n qd (4.50) where: w e2nqq (4.51) [00196] For q=2, eq. (4.51) yields to=(-1), which reduces Uq,k(x) from eq.
(4.50) to the regular Walsh functions Uk(x), eq. (2.5). The q discrete values of Uct,k(x) can be also mapped into integers in [0,q) interval to obtain integer-valued Walsh
functions Wq,k(x) (analogue of binary form Wk(x)), which is useful for efficient computer implementation, via analogous mapping to the binary case e.g. via mapping a =
cob for integer b=0..n-1, where b:integer, a:algebraic value, as in eq.
(2.8) where this same mapping (expressed differently) was used for q=2.
[001971 The non-binary Walsh functions Uq,k can also be used to define graph partition into f parts where f is any divisor of q (including q). For even q, this allows for
efficient computation of bisection. The method is a direct generalization of the binary case: the q distinct function values of Uq,k(x) define partitions arrays Xk[x]F-Uq,k(x)
containing n=qd elements indexed by x-4)..n-1. Each of q values of Uq,k(x) indicates a node x belongs to one of the q parts. The partitions X16 for k=1...n-1 are examined
and cuts computed using the adjacency matrix [A] for Cay(Z1,S.) graph, as in eq. (4.14) for q=-2. The generators T (a) and adjacency matrix [A] are computed via general
eqs. (4.1),(4.2), where 19 operator is GF(q) addition (mod q).
1001981 The algorithmic speed optimizations via "symmetry optimization"
and "Fast Walsh Transform optimization" apply here as well (see [14] pp. 465-468 on fast transforms for multi-valued Walsh functions).
N-7. Secondary 0ptimi7ati0ns [00199] Once the optimum solution for (4.42) is obtained (via ECC, Digital nets, or via direct optimization), secondary optimizations, such
as seeking the minimum diameter (max distance) or minimum average distance or largest max-cut, can be performed on the solution via local, greedy algorithms. Such
algorithms were used in construction of our data solutions data base, where each set of parameters (d, m, q) has alternate solutions optimized for some other criteria
(usually diameter, then average distance).
[00200] The basic algorithm attempts replacement of typically 1 or 2 generators h, E S., and for each new configuration it evaluates (incrementally) the target utility
function, such as diameter, average distance or max-cut (or some hierarchy of these, used for tie-breaking rules). The number of simultaneous replacements depends on
n, m and available computing resources. Namely, there are ¨ nr possible simultaneous deletions and insertions (assuming the "best deletion" is followed by "best"
insertion).
The utility function also uses indirect measures (analogous to sub-goals) as a tie-breaking selection criterion e.g. when minimizing diameter, it was found that an
effective indirect measure is the number of nodes #F in the farthest (from node 0) group of nodes. The indirect objective in this case would be to minimize the #F of such
nodes, whenever the examined change (swap of 1 or two generators) leaves the diameter unchanged.
[00201] In addition to incremental updates to the networks after each evaluated generators replacement, these algorithms rely on vertex symmetry of Cayley graphs to
further reduce computations. E.g. all distance tables are only maintained and updated for n-1 distances from node 0 ("root"), since the table is the same for all nodes

https://patents.google.com/patent/CA2872831C/en?q=(high+radix)&oq=high+radix

13/31

2/14/26, 1:58 PM

CA2872831C - Flexible radix switch - Google Patents

(with mere permutation of indices, obtainable via T (a) representation of Gi, if needed).
[00202] Depending on network application, the bisection b can be maintained fixed for all replacements (e.g. if bisection is the highest valued objective), or one can allow
b to drop by some value, if the secondary gains are sufficiently valuable.
[00203] After generating and evaluating all replacements to a given depth (e.g.
replacement of 1 or 2 generators), the "best" one is picked (according to the utility/cost function) and replacement is performed. Then the outer iteration loop would
continue, examining another set of replacements seeking the best one, etc. until no more improvements to the utility/cost function can be obtained in the last iteration.
Specialized solution [00204] This section describes several optimum LH solutions with particularly useful parameters or simple construction patterns.
S-1. High Density LH Networks for modular switches (LH-HD) [00205] This is a special case of LH networks with high topological link density, suitable for combining
smaller number of high radix switches into a single large radix modular switch. This is a specialized domain of network parameters where the 2-layer Fat Tree (FT-2)
networks are currently used since they achieve the yield of E=R/3 external ports/switch, which is the maximum mathematically possible for the worst case traffic
patterns. The 'high density' LH networks (LH-HD) match the in this optimum E=R/3 external ports/switch yield for the worst case traffic patterns, while achieving
substantially lower average latency and the cost in Gb/s of throughput on random or 'benign' (non-worst case) traffic.
[00206] In our preferred embodiment using Cay(g,S.) graph, the network size is n=2d switches and the number of links per node m is one of the numbers:
nI2, nI2+n14, n12+n14+n18,... , n12+n14+n18+...+1, then the optimum m generators for LH-HD are constructed as follows:
(i) h1=n-1, h2=n-2, h3=n-3,... hin=n-m (ii) Optionally diagonalize and sort Sn, via procedure (N-3) (Of course, there are a large number of equivalent configurations obtained
via equivalence transforms N-1.) [00207] The resulting bisection is: b4(m+1)/2i or B=b=n/2, diameter is 2 and average hops is 2-m/n. The largest LH-HD m =
n12+n14+n18+...+1 = n-1 has b=n/2 and corresponds to a fully meshed network.
[00208] Table 4.10 shows an example of LH-HD generators for n=26=64 nodes and nr=n/2=32 hops/node, with the hops shown in hex and binary (binary Os are shown as
`-` character). Table 4.10(a) shows the non-diagonalized hops after the step (i), and Table 4.10(b) shows the equivalent network with m=32 hops after diagonalization in
step (ii) and sorting. Other possible LH-HD m values for the same n=64 node network are nr=32+16=48, m=48+8=56, m=56+4=60, ttr=60+2=62 and m=61+1=63 hops.
[00209] Additional modified LH-HD networks are obtained from any of the above LH-HD networks via removal of any one or two generators, which yields networks LHHD1 with mi = m-1 and LH-HD2 with m2-m-2 generators. Their respective bisections are b1=b-1 and b2=b-2. These two modified networks may be useful when an
additional one or two server ports are needed on each switch compared to the unmodified LH-I-ID network.
[00210] These three types of high density LH networks are useful for building modular switches, networks on a chip in multi-core or multi-processor systems, flash
memory/storage network designs, or generally any of the applications requiring very high bisection from a small number of high radix components and where FT-2 (two
level Fat Tree) is presently used. In all such cases, LH-HD will achieve the same bisections at a lower latency and lower cost for Gb/s of throughput.
1. 3F 111111 1. 1 1 2. 3E 11111. 2.
3. 3D 1111.1 3.
4. 3C 1111.. 4.
5. 38 111.11 5. 10 .1....
6. 3A 111.1. 6. 20 1
7. 39 111..1 7. 7 .. .111
8. 38 111... 8. B ..1.11
9. 37 11.111 9. D ..11.1
10. 36 11.11. 10. E ..111.
11. 35 11.1.1 11. 13 .1..11
12. 34 11.1.. 12. 15 .1.1.1
13. 33 11..11 13. 16 .1.11.
14. 32 11..1. 14. 19 .11..1
15. 31 11...1 15. LA .11.1.
16. 30 11.... 16. 1C .111..
17. 2F 1.1111 17. 1F .11111
18. 2E 1.111. 18. 23 1...11
19. 2D 1.11.1 19. 25 1..1.1
20. 2C 1.11.. 20. 26 1..11.
21. 26 1.1.11 21. 29 1.1..1
22. 2A 1.1.1. 22. 2A 1.1.1.
23. 29 1.1..1 23. 2C 1.11..
24. 28 1.1... 24. 2F 1.1111
25. 27 1..111 25. 31 11...1
26. 26 1..11. 26. 32 11..1.
27. 25 1..1.1 27. 34 11.1..
28. 24 1..1.. 28. 37 11.111
29. 23 1...11 29. 38 111...
30. 22 1...1. 30. 38 111.11
31. 21 1....1 31. 30 1111.1

https://patents.google.com/patent/CA2872831C/en?q=(high+radix)&oq=high+radix

14/31

2/14/26, 1:58 PM

CA2872831C - Flexible radix switch - Google Patents

32. 20 1 32. 3E 11111.
(a) (b) Table 4.10 S-2. Low Density LH networks with b=3 [00211] This subset of LH networks is characterized by comparatively low link density and low bisection b=3 i.e.
B=3n/2 links. They are constructed as a direct augmentation of regular hypercubic networks which have bisection b=1. The method is illustrated in Table 4.11 using
augmentation of the 4-cube.
hi -4. 0 0 0 1 h2 -4 0 0 10 h3 1 0 0 h 4 -410 0 0 hs -4 0 1 1 T.' h6-= 1 1 1 0 h1 1 0 11 Ci C2 C3 C4 Table 4.11 1002121 The d=4 hops h1, h2, 113 and 1:4 for the regular 4-cube
are enclosed in a 4x4 box on the top. The augmentation consists of 3 additional hops 1/5, 116 and 1/7 added in the form of 4 columns C1, C2, C3 and C4, where each
column Cg ( =1..d) has length of L=3 bits. The resulting network has n =16 nodes with 7 links per node and it is identical to an earlier example in Table 4.9 with b=3
obtained there via translation from a [7,4,312 EC code into the LH network. General direct construction of the b=3 LH network from a d-cube is done by appending d
columns CIL
(11=1..(1) of length L bits, such that each bit column has at least 2 ones and L is the smallest integer satisfying inequality:
2L ¨L¨ 1 > d (4.60) [00213] The condition in eq. (4.60) expresses the requirement that d columns Cg must have at least 2 ones. Namely, there are total of 21' distinct bit
patterns of length L. Among all 21' possible L-bit patterns, 1 pattern has 0 ones (00..0) and L
patterns have a single one. By removing these two types, with 0 or single one, there are 2L-(L+1) remaining L-bit patterns with two or more ones, which is the left hand
side of eq. (4.60). Any subset of d distinct patterns out of these 2L-(L+1) remaining patterns can be chosen for the above augmentation. The Table 4.12 shows values L
(number of added hops to a d-cube) satisfying eq. (4.60) for dimensions d of practical interest.
dram dmax L
Table 4.12 S-3. Augmentation of LH networks with b=odd integer 1002141 This is a very simple, yet optimal, augmentation of an LH
network which has m links per node and bisection b=odd integer into LH network with bisection b1=b+1 and mi=m+1 links per node. The method is illustrated in Table
4.14 using the augmented 4-cube (d=4, n=16 nodes) with m=7 links per node and bisection b=3, which was used in earlier examples in Tables 4.9 and 4.12.
hi-40001 h2 0 0 10 h3 ¨O 1 0 0 h4 1 0 0 0 h5 -4 0 1 1 1 h6¨. 1 1 1 0 h7 1 0 11 XOR ED ED
its 1 0 1 Table 4.14 1002151 A single augmenting link lig = h1^h2^...^117 (bitwise XOR of the list) is added to the network which increases bisection from b=3 to b=4 i.e. it
increases the absolute bisection B by n/2=16/2=8 links. The general method for Cay(Z1 ,Sin) with b='odd integer' consists of adding the link hin+i=h1^112^...^h. (the
bitwise XOR of the previous m hops) to the generator set S.. The resulting LH network Cay(ZI,Sm+1) has bisection b1=b+1.
[00216] The only case which requires additional computation, beyond merely XOR-ing the hop list, is the case in which the resulting hop kw-I happens to come out as 0
(which is an invalid hop value, a self-link of node 0 to itself). In such case, it is always possible to perform a single hop substitution in the original list Sn, which will
produce the new list with the same b value but a non-zero value for the list XOR
result hm+i=
LH construction for a target network [00217] In practice one would often need to construct a network satisfying requirements expressed in terms of some target number
of external ports P
having oversubscription 4), obtained using switches of radix R. The resulting construction would compute the number n of radix-R switches needed, as well as the list for
detailed wiring between switches. For concreteness, each radix-R switch will be assumed to have R ports labeled as port #1, #2,... #R. Each switch will be connected to m
other switches using ports #1, #2,... #m (these are topological ports or links) and leave E R-m ports: #m+1, #R as "external ports" per switch available to the network
users for servers, routers, storage,... etc. Hence, the requirement of having total of P external ports is expressed in terms of E and number of switches n as:
E = P/n (4.70) [00218] The oversubscription eq. (3.1) is then expressed via definition of bisection b in eq. (4.42) as:
P/2 E = n/2 E E R ¨ m = = B b b (4.71) n/2 [00219] The illustrative construction below will use non-oversubscribed networks, 4)=1, simplifying eq. (4.71):
E=b=R¨m (4.72) i.e. for non-oversubscribed networks, the number of external ports/switch E
must be equal to the relative bisection b (this the bisection in units n/2), or equivalently, the number of links/switch: m = R - b.
[00220] In order to find appropriate n=21 and m parameters, LH solutions database, obtained by translating optimum EC code tables [E] and [22] via recipe (4.45), groups
solutions by network dimension d into record sets Dd, where d=3,4,...
24. These dimensions cover the range of network sizes n=2d that are of practical interest, from n = 23 = 8 to tt= 224 16 million switches. Each record set Dd contains
solution records form = d, d+1,... mt. links/switch, where the present database has mmax=256 links/switch. Each solution record contains, among others, the value m,
bisection b and the hop list hi, h2,...
[002211 For given P, R and 4), LH constructor scans record sets pd, for and in each set, inspects the records for nr=d, d+1, ... computing for each (d,m) record values
E(d,m)=R-m ports/switch, total ports P(d,m)= n= E(d,m)= 2d .(R-m) and oversubscription 4)(d,m)--E(d,m)/b (value b is in each (d,m) record).
The relative errors 6P = IP(d,m)-PI/P and 64 =14)(d,m)- 4)1/4) are computed and the best match (record (d,m) with the lowest combined error) is selected as the solution
to use.
If the requirement is "at least P ports" then the constraint P(d,m)-P>0 is imposed for the admissible comparisons. The requirements can also prioritize 6P and 6. via
weights for each (e.g. 0.7.6P + 0.3-4 for total error). After finding the best matching (d,m) record, the hop list hi, h2,... hm is retrieved from the record and the set of links
L(v) is computed for each node v, where v = 0, 1, ... n-1, as: L(v) = { vAhs for s=1..m}. Given n such sets of links, L(0), L(1),..., L(n-1), the complete wiring for the network is
specified. The examples below illustrate the described construction procedure.
Example 1. Small network with P=96 ports at 4)=1, using switches with radix R=12 1002221 The LH database search fmds the exact match (6P=0, 64=0) for the record
d=5, m=9, hence requiring n=2d=25=32 switches of radix R=12. The bisection b=3 and the hop list (in hex base) for the record is: S9 = {1, 2, 4, 8, 10, E, F, 14, 19}nex. The
number of external ports per switch is E=b=3, combined with nr=9 topological ports/switch, results in radix R=3+9=12 total ports/switch as specified.
The total number of external ports is P = En = 3.32 = 96 as required. Diameter (max hops) for the network is D=3 hops, and the average hops (latency) is Avg=1.6875
hops. Table 4.15 shows complete connection map for the network for 32 switches, stacked in a 32-row rack one below the other, labeled in leftmost column "Sw"
as 0, 1,... 1F (in hex). Switch 5 is outlined with connections shown for its ports #1,#2,...
#9 to switches (in hex) 04, 07, 01, OD, 15, OB, OA, 11 and 1C. These 9 numbers are computed by XOR-ing 5 with the 9 generators (row 0): 01, 02, 04, 08, 10, OE, OF, 14, 19.
The free ports are #10, #11 and #12.
sw/Pt : #1. #2 #3 #4 #5 #6 #7 #8 #9 #10 #11 #12 0: 01 02 04 08 10 OE OF 14 19 ** ** **
1: 00 03 05 09 11 OF OE 15 18 ** ** **
2: 03 00 06 OA 12 OC OD 16 1B ** ** **
3: 02 01 07 OB 13 OD OC 17 1A ** ** **
4: 05 06 00 OC 14 OA OB 10 10 ** ** **
5: 04 07 84..)0D 15 OB OA 11 1C ** ** **
6: 07 04 02 OE 16 08 09 12 1F ** ** **
7: 06 05 003F 17 09 08 13 1E ** ** **
8: 09 OA OC 00 18 06 07 1C 11 ** ** **
9: 08 OB 00 01 19 07 06 10 10 ** ** **
A: OB 08 OE 02 lA 04 05 1E 13 ** ** **

https://patents.google.com/patent/CA2872831C/en?q=(high+radix)&oq=high+radix

15/31

2/14/26, 1:58 PM

CA2872831C - Flexible radix switch - Google Patents

B: OA 09 OF 03 1B 05 04 1F 12 ** ** **
C: OD OE 08 04 1C 02 03 18 15 ** ** **
0: OC OF 09 05 10 03 02 19 14 ** ** **
E: OF OC 0A 06 1E 00 01 1A 17 ** ** **
F: 0E 00 OB 07 1F 01 00 1B 16 ** ** **
10: 11 12 14 18 00 1E 1F 04 09 *4 ** **
11: 10 13 15 19 01 1F 1E 05 08 ** ** **
12: 13 10 16 lA 02 1C 10 06 013 ** ** **
13: 12 11 17 1B 03 10 1C 07 0A ** ** **
14: 15 16 10 1C 04 1A 1B 00 OD ** ** **
15: 14 17 11 1D 05 1B 1A 01 OC ** ** **
16: 17 14 12 1E 06 18 19 02 OF ** ** **
17: 16 15 13 1F 07 19 18 03 OE ** ** **
18: 19 1A 1C 10 08 16 17 OC 01 ** ** **
19: 18 1B 10 11 09 17 16 OD 00 ** ** **
1A: 1B 18 1E 12 OA 14 15 OE 03 ** ** **
1B: lA 19 1F 13 OB 15 14 OF 02 ** ** **
1C: 10 1E 18 14 0C 12 13 08 05 ** ** **
10: 1C 1F 19 15 OD 13 12 09 04 ** ** **
1E: 1F 1C LA 16 OE 10 11 0A 07 ** ** **
1F: 1E 10 1B 17 OF 11 10 OB 06 ** ** *4 Table 4.15 1002231 To illustrate the interpretation of the links via numbers, the outlined switch "5:" indicates on its port #2 a
connection to switch 7 (the encircled number 07 in the row 5:). In the row 7:, labeled as switch "7:", there is an encircled number 05 at its port #2 (column #2), which
refers back to this same connection between the switch and the switch 7 via port #2 on each switch. The same pattern can be observed between any pair of connected
switches and ports.
Example 2. Small network with P=1536 (1.5K) ports at 4)=1, using switches with radix R=24.
(002241 The LH
solutions database search finds an exact match for d =8, n =
256 switches of radix R=24 and nr=-18 topological ports/switch. Diameter (max hops) of the network is D=3 hops, and average latency is Avg=2.2851562 hops. The
bisection is b=6, providing thus E=6 free ports per switch at 4)=1. The total number of ports provided is E-n=6-256=1536 as required. The set of 18 generators is:
S18 =
{01, 02, 04, 08, 10, 20, 40, 80, 1A, 2D, 47, 78, 7E, 8E, 9D, B2, D1, F13}
hex. Note that the first 8 links are regular 8-cube links (power of 2), while the remaining 10 are LH
augmentation links. These generators specify the target switches (as index 00..FFnex) connected to switch 00 via ports #1, #2,... #18 (switches on both ends of a link
use the same port number for mutual connections). To compute the 18 links (to 18 target switches) for some other switch x t 00, one would simply XOR number x with
the generators. Table 4.16 shows the connection table only for the first 16 switches of the resulting network, illustrating this computation of the links. For example,
switch 1 (row `1:') has on its port #4 target switch 09, which is computed as 1^8=9, where 8 was the generator in row '0:' for port #4. Checking then switch 9 (in row `9:'),
on its port #4 is switch 01 (since 9A8=1), i.e. switches 1 and 9 are connected via port #4 on each. The table also shows that each switch has 6 ports #19, #20,... #24 free.
Sw/Pt #1 #2 #3 #4 #5 #6 #7 #8 #9 #10 #11 #12 #13 #14 #15 #16 #17 #18 #19 #20 #21 #22 #23 #24 0: 01 02 04 08 10 20 40 80 1A 2D 47 78 7E 8E 90 B2 D1 FB ** ** **
** **
**
1: oe 03 05 09 11 21 41 81 18 2C 46 79 7F 8F 9C 83 DO FA ** ** ** ** 4*
**
2: 03 00 06 OA 12 22 42 82 18 2F 45 7A 7C 8C 9F BO D3 F9 ** ** ** ** **
**
3: 02 01 07 GB 13 23 43 83 19 2E 44 78 7D 80 9E 81 02 F8 ** ** ** ** **
**
4: 05 06 08 OC 14 24 44 84 1E 29 43 7C 7A 8A 99 86 05 FE ** ** ** ** *4 **
5: 04 07 01 OD 15 25 45 85 1F 28 42 70 78 8B 98 97 D4 FE ** ** ** ** **
**
6: 07, 04 02 OE 16 26 46 86 1C 2B 41 7E 78 88 9B 84 D7 FD ** ** ** ** **
**
7: 06 05 03 OF 17 27 47 87 1D 2A 40 7F 79 89 9A 85 06 FC ** ** ** ** **
**
8: 09 OA OC 00 18 28 48 88 12 25 4F 70 76 86 95 BA D9 F3 ** ** ** ** **
**
9: 08 AB OD 01 19 29 49 89 13 24 4E 71 77 87 94 BB 06 F2 ** *4 *4 4* *4 **
A: 08 08 GE 02 1A 2A 4A 8A 10 27 41) 72 74 84 97 88 DO F1 ** ** ** ** **
**
8: OA 09 OF 03 18 28 48 88 11 26 4C 73 75 85 96 89 DA FO ** ** ** 4*
**
C: OD OE 08 04 1C 2C 4C 8C 16 21 48 74 72 82 91 BE DO F7 *4 ** ** ** 4*
4*
D: OC OF 09 05 1D 20 41) 80 17 20 4A 75 73 83 90 BF DC F6 ** ** *4 ** **
4*
E: OF OC OA 06 1E 2E 4E 8E 14 23 49 76 70 80 93 BC DE F5 ** ** *4 ** 4*
4*
F: OE OD OB 07 1F 2F 4F 8F 15 22 48 77 71 81 92 BD DE F4 ** ** 4* ** *4 *4 10: -Table 4.16 Example 3. Large network with P=655,360 (640K) ports at 4)=1, using
switches with radix R=48.
[00225] The database lookup finds the exact match using d=16, n=216=
65,536 = 64K switches of radix R=48. Each switch uses n38 ports for connections with other switches leaving E=48-38=10 ports/switch free, yielding total of P= En =

https://patents.google.com/patent/CA2872831C/en?q=(high+radix)&oq=high+radix

16/31

2/14/26, 1:58 PM

CA2872831C - Flexible radix switch - Google Patents

10-64K=-640K available ports as required. Bisection is b=10 resulting in (I)=E/b=1.
The list of m=38 generators S38 = {hi, h2,... h38} is shown in Table 4.17 in hex and binary base. The 38 links for some switch x (where x: 0..FFFF) are computed as {x^hi,
x"h38}. Diameter (max hops) of the network is D=5 hops, and the average latency is Avg=4.061691 hops.
1. 1 1 2. 2 1 3. 4 1 4. 8 1 5. 10 1 6. 20 1 7. 40 1 8. 80 1 9. 100 1 10. 200 1 11. 400 1 12. 800 1 13. 1000 1 14. 2000 1 15. 4000 .1 16. 8000 1 17. 6F2 11.1111..1.
18. 1800 ...11.1111.1....
19. 1F3D ...11111..1111.1 20. 3072 ..1111.1.111..1.
21. 6864 .11.1.11.11..1..
22. 775C .111.111.1.111..
23. 8934 1...1..1..111.1.
24. 8681 1...1.111 1 25. 9914 1..11..1...1.1..
26. 44C2 1.1..1..11....1.
27. 4750 1.1..111.1.1....
28. 870E 1.11.111....111.
29. BFF1 1.1111111111...1 30. C57D 11...1.1.11111.1 31. 0046 11.1....1.1..11.
32. D1CA 11.1...111..1.1.
33. E6135 111..11.1.11.1.1
34. EAB9 111.1.1.1.111..1
35. F2E8 1111..1.111.1...
36. F313 1111..11...1..11
37. F9BF 11111..11.111111
38. FC31 111111....11...1 Table 4.17 LH performance comparisons [00226] The LH solutions database was used to compare LH networks against several leading
alternatives from industry and research across broader spectrum of parameters. The resulting spreadsheet charts are shown in Figures 11 - 15. The metrics used for
evaluation were Ports/Switch yield (ratio P/n, higher is better) and the cables consumption as Cables/Port (ratio: # of topological cables/P, lower is better). In order to
maximize the fairness of the comparisons, the alternative networks were set up to generate some number of ports P using switches of radix R, which are optimal
parameters values for a given alternative network (each network type has its own "natural" parameter values at which it produces the most efficient networks).
Only then the LH network was constructed to match the given number of external ports P using switches of radix R (as a rule, these are not the optimal or "natural"
parameters for LH networks).
[00227] In Figs. 11 - 15, the Ports/Switch chart for each alternative network shows Ports/Switch yields for the LH network _ _ _ and the alternative network .... , along with
the ratio LH/alternative with numbers on the right axis (e.g. a ratio 3 means that LH yields 3 times more Ports/Switch than the alternative).
The second chart for each alternative network shows the Cables/Port consumption for the LH and the alternative, along with the ratio: alternative/LH on the right axis
(e.g. a ratio 3 means that LH consumes 3 times fewer cables per port produced than the alternative). All networks are non-oversubscribed i.e. 4)=1.
[00228] For example, the Ports/Switch chart in Fig. 11 shows yield for hypercube (TIC), for network sizes from n=28 to 224 switches of radix 11=64.
The Ports/Switch for LH network yielding the same total number of ports P is shown, along with the ratio LH/HC, which shows (on the right axis scale) that LH
produces 2.6 to 5.8 times greater Ports/Switch yield than hypercube, hence it uses 2.6-5.8 times fewer switches than HC to produce the same number of ports P as HC
at the same throughput. The second chart in Fig. 11 shows similarly the Cables/Port consumption for HC and LH, and the ratio HC/LH of the two (right axis scale),
showing that LH
consumes 3.5 to 7 times fewer cables to produce the same number of ports P as HC at the same throughput. The remaining charts in Figs. 12 - 14show the same type of
comparisons for the other four alternatives.
Performance Measurement [00229] It is desirable to maximize X since )4. quantifies the external port yield of each switch. Namely if each switch's port count (radix) is R,
then R=E+T
(where E
is the number of external ports and T number of topological ports) and the E-port yield per IPA port is: Yield E/R=X/(X+1), i.e. increasing Xincreases the Yield. But
increasing X for a given N also lowers the bisection for that N, hence in practical applications, data center administrators need to select a balance of Yield vs.
bisection and N suitable for the usage patterns in the data center. The centralized control and management software provides modeling tools for such evaluations.
1002301 Denoting the number of external ports and topology ports per switch as E and T, the radix (number of ports) R of a switch is R=E+T. The topology ports in turn
consist of the d ports needed to connect a d-dimensional hypercube HCd and of h long hop ports used for trunking, so T=d+h. If the number of switches is N, the N=2d or
d=log(/V), where log(x)is the logarithm base 2 of x i.e. log(x) =
hi(x)/1n(2) cz-4.443.1n(x). In order to relate formally the invention's long hops to terminology used with conventional trunlcing (where each of the d HCd cables is
replaced with q cables, a trunk quantum), defme q---T1d, i.e. T=q.d. Hence q and h are related as: q=
l+hld and h= d(q-1). Using the ratio: k-=EIT, E and T is expressed as T=RI(1+2.) and E=X=RI(141). Restating the bisection formula:
Br---B(/V) = (N12)-q=C= N/2-(1+h1d)-C (5) [00231] Where C is a single IPA switch port capacity (2-<Port Bit Rate>
for duplex ports). Bisection B is the smallest total capacity of links connecting two halves of the network (i.e. it's the minimum for all possible network cuts into halves).
Consider two network halves with N/2 switches each and E external ports per switch, there are EN/2 external ports in each half. If these two sets of eternal ports were to
transmit to each other at full port capacity C, the total capacity needed to support it is E.(NI2).C. Since bisection limits the worst case capacity between halves to B, the
oversubscription 4) is defined as the ratio between the capacity needed E(N12)-C and the capacity for the job available via B:
Es E-(N/2)-C / B = E/q = = X-log(N) (6) 1002321 Eq. (6) shows in what ratio X=E/T ports must be divided in order to obtain oversubscription 4) using N switches:
X=4)/log(N). The quantity most often of interest is the total number of external ports provided by the network, P =
NE, which in terms of other quantities typically given as constraints (4), N and radix R), and recalling that EA=R/(1+4 is then:
0./2.N
P = 404-log(N) (7) [002331 Although Eq. (7) doesn't yield a closed form expression for N, it does allow computation of the number of IPA switches N needed to get some
target number of total network ports Pat IB-oversubscription 4), knowing the radix R
of the switches being used. Qualitatively, the number of total network ports P
increases slightly slower than linearly in N (when 4) is kept fixed) due to the denominator D,---(4)-Flog(N)) which also increases with N. Its effects diminish as N
increases (or if 4) is large or grows with N), since doubling of N increments D by +1 (which is only by ¨5% for N=64K and 4)=4). Within the log(log(P)) error margin, the N
above grows as N P=log(P), which is an unavoidable mathematical limit on performance of larger switches combined from N smaller switches at fixed 4).
[002341 Figure 16 (computed for the commercially available Pronto 3780 switch) shows the resulting network capacity based on a simple un-optimized configuration

https://patents.google.com/patent/CA2872831C/en?q=(high+radix)&oq=high+radix

17/31

2/14/26, 1:58 PM

CA2872831C - Flexible radix switch - Google Patents

(for the lowest commonly used fixed IB-oversubscription (f)=4, other values of interest (1)=1, 2, 5, 10, 15 and 20 are shown later). The slight log(N) nonlinearity when
using fixed cp can be seen in the price per port ¨ while N
increased by a factor 128K, the price per 10G port increased only 3.4 times (i.e. the cost per 10G port grew over 38,000 times slower than the network size and capacity,
which is why the slight non-linearity can be ignored in practice). If instead of using fixed gal the fixed k(E/P ratio) is used, then via Eq. (3): p p(N,X) =X=log(N), the port Eq.
(6) becomes linear in N:
A=R=N
P= ______________________________________ 1 + A
i.e. we get a fixed cost and power per port as N grows. In this case the tradeoff is that it is 9 which now grows as A,-log(N) as N grows. Recalling that typical aggregate
oversubscriptions on core switches and routers are ¨200+ in the current data centers, log(N) is quite moderate in comparison. The network bandwidth properties for X=1
are shown in Figure 17 where the cost per 10G port remains fixed at $500 (or $104 per 1G port) and power at 14.6W. Results for some values of are shown later.
Elimination of CAM Tables [00235] By using mathematically convenient topologies such as an enhanced hypercube connection pattern or its hierarchical variants, the
switch forwarding port can be computed on the fly via simple hardware performing a few bitwise logical operations on the destination address field, without any
expensive and slow forwarding Content Addressable Memory (CAM) tables being required. Hence, for customized switches, price and power use advantages can be
gained by removing CAM hardware entirely.
Exception and Fault Handling Using CAM
[00236] Although the most favorable embodiments of the invention can eliminate CAMs completely, a much smaller (by at least 3 orders of magnitude smaller) CAM
hardware can still be useful to maintain forwarding exceptions arising from faults or congestion. Since the enhanced hypercubic topology allows for forwarding via
simple, small logic circuits (in the ideal, exception free case), the only complication arises when some port P is faulty due to a fault at the port or failure/congestion at the
nearest neighbor switch connected to it. Since number of such exceptions is limited by the radix R of the switch, the necessary exception table needs a space for at most
R small entries (typical R=24..128, entry size 5-7 bits). A
match of a computed output port with an entry in the reduced CAM overrides the routine forwarding decision based on the Jump Vector computed by the logic circuit.
Such a tiny table can be implemented in the substantially reduced residual CAMs, or even within the address decoding logic used in forwarding port computation.
This exception table can also be used to override the routine forwarding decisions for local and global traffic management and load balancing.
Improved Trunking and Link Aggregation 1002371 In order to increase the pipe capacity along overloaded paths, while under the tree topology constraints, the
conventional data center solution is trunking(or link aggregation in the IEEE 802.1AX standard, or Cisco's commercial EtherChannel product), which amounts to cloning
the link between two switches, resulting in multiple parallel links between the two switches using additional pairs of ports. The invention shows a better version of
thinking for increasing the bisection with a fixed number of switches.
1002381 With the invention, this problem arises when number of switches in a network is fixed for some reason so bisection cannot be increased by increasing N.
Generally, this restriction arises when the building block switches are a smaller number of high radix switches (such as the Arista 7500) rather than the larger number of
low radix switches that allow the desirable high bisection bandwidth as provided by the invention. Data centers making use of the invention can use conventional trunking
by building hypercubes using multiple parallel cables per hypercube dimension. While that will increase the bisection as it does for regular tree based data center
networks, there are better approaches that can be used.
The procedure is basically the opposite of the approach used for traditional trunking. By adding a link from some switch A, instead of picking the target switch B
from those closest to A, B is picked such that it is the farthest switch from A. Since the invention's topologies maintain uniform bisection across the network, any target
switch will be equally good from the bisection perspective, which is not true for conventional trees or fat trees. By taking advantage of this uniformity, picking the farthest
switch B also maximally reduces the longest and the average hop counts across the network. For example, with a hypercube topology, the farthest switch from any
switch A is the switch B which is on the long diagonal from A. Adding that one link to A cuts its longest path by half, and reduce the average path by at least 1 hop.
When the long hops are added uniformly to all switches (hence N/2 wires are added per new long hop), the resulting topology is called enhanced hypercube. Figure
shows the reductions in the maximum and average hops due to adding from 1 to long hops. In Figure 18, LH shows hex bitmasks of the long hop, i.e. the index of the
farthest switch chosen.
[00239] The table was obtained by a simple 'brute force' counting and updates of distance tables as the new long hops were added. At each stage, the farthest node from
the origin is used as a new link (a variety of tiebreaking rules were explored to provide a pick when multiple 'farthest' nodes are equally far, which is the common
occurrence). After each link is added the distance table is updated. For Dim=4, N=16, adding long hops beyond 11 doesn't have an effect since the small network
becomes fully meshed (when total number of links is N-1), hence all distances become 1 hop.
Optimizing Wiring Using Port Dimension Mapping [00240] In some embodiments of the invention, systems being implemented via a set of switches in a data center, (e.g.
available as line cards in a rack), wiring such dense networks can easily become very complex, error prone and inefficient.
With (d!)/v topologically equally correct mappings between ports and dimensions for a d-dimensional hypercube using N=2" switches, d ports per switch, there are lots of
ways to create an unmanageable, error prone, wasteful tangle. The invention optimizes the mapping between the ports and HC/FB dimensions using the following rules:
(i) The same dimensions are mapped to the same ports on all switches (ii) Consecutive dimensions (0,1,... d-1) are mapped onto consecutive ports (a, a+1,... a+d-1) The
resulting wiring pattern shown in Figure 19 has the following advantages over a general topologically correct mapping:
a) All cables belonging to the same dimension have the same length b) All cables have the same port number on both ends (cables strictly vertical) c) All cables in the
same vertical column (dimension) have the same lengths [00241] Provided the cables and corresponding port connectors in the same column are color coded using
matching colors (properties (b) and (c) makes such coding possible), and the cables are of the minimum length necessary in each vertical column, this port-dimension
mapping makes the wiring of a rack of switches easy to learn, easy to connect and virtually error proof (any errors can be spotted at a glance).
The total length of cables is also the minimum possible (requiring no slack) and it has the fewest number of distinct cable lengths allowed by the topology. In addition to
economizing the quantity and complexity of the wiring, the shortening and uniformity of cables reduces the power needed to drive the signals between the ports, a factor
identified as having commercial relevance in industry research.
Details of Connecting 64=26 switches -4 6-D hypercube [00242] In Figure 19, the column headers show 6 color coded port numbers 0=red, 1=blue, 2=orange, 3=purple,
4=green and 5=-cyan. The 64 switches are line cards mounted in a rack one below the other and they are depicted as 64 separate rows 0, 1, 2,...63. The 6 ports/switch
used for wiring these switches into a 6-D
hypercube, line up into 6 columns (the wire colors match the port colors in each column).
[00243] The 6 numbers inside some row #k show the 6 switches connected to the 6 ports of the switch #k. E.g. row #7 shows that switch #7 is connected to switches
#6, 5, 3, 15, 23, 39 on its ports 0, 1, 2,... 5. Picking now say, port (column) #4 for switch (row) #7, it connects on port 4 to switch #23. Looking down to switch (row) #23,
its port (column) #4 it connects back to switch #7 i.e. switch 7 and switch 23 are connected to each other's port #4. This simple rule ¨ two switches always connect on
the same port # with each other¨ holds generally for hypercubes. This leads to the proposed port and cable color coding scheme. E.g. green: 4 cables connect green
ports #4 on some pair of switches, red: 0 cables connect red ports #0 on some other pair of switches, blue:1 cables connect blue ports #1, etc.
[00244] The wiring pattern is as simple. All wires of the same color have the same length 1_,---2Nft#, e.g. orange: 2 wire (connecting always ports #2, orange:2 ports) has
length 22=4, green:4 24=16, red: 0 2 =1, etc. Hence switch pairs connected on their port #2 with each other are 4 rows apart, e.g. switch (row) 0 connects on its port #2 to
switch 4 on its port #2 and they use orange:2 wire (the color of port #2).
This connection is shown as the top orange: 2 arc connecting numbers 4 and 0.

https://patents.google.com/patent/CA2872831C/en?q=(high+radix)&oq=high+radix

18/31

2/14/26, 1:58 PM

CA2872831C - Flexible radix switch - Google Patents

The next orange:2 (port #2) wire start at the next unconnected row, which is row #1 (switch #1), and connects to row 1+4=5 (switch #5), and so on until the first row
already connected on port #2 is reached, which is row #4 (Step 1-4). At that point 8 top rows on port #2 are connected. Then proceed down to the next row with free port
#2, which is row 8. That port #2 is now connected with the port #2 down 4 rows, i.e.
with row 8+4=12, which is shown with orange: 2 wire linking numbers 12 and 8.
Now the next two rows (orange: 2 arc connecting numbers 13 and 9), etc, until column (port) #2 is connected on all switches. Then follows purple: 3 port #3, using
purple: 3 wires 23=8 slots long, and repeat the same procedure, except with longer wires... etc.
Containers for Prewired Internal Topology [00245] While the above wiring of a 64-switch hypercube H611(64) is not difficult since errors are unlikely because starting at
the top row and going down, any new wire can go into just one port of the matching color, the pattern above suggests a simple way to design easily connectable
internally prewired containers, which eliminate much of the tedium and expense of this kind of dense manual wiring.
[00246] Consider the above 11(64) as being composed of two prewired H(32) boxes A and B (separated by the dotted horizontal line at 32/32). The first 5 dimensions,
ports 0,1,...4, of each H(32) are already fully wired and the only missing connections are the 32 wires connecting ports #5 on the 32 switches from one to the other
container, in perfectly orderly manner (row 0 of container A to row 0 of container B, row 1 from A to row 1 from B,... etc). Hence, instead of wiring 32x6=192 wires for
11(64), two prewired containers and 32 wires now connect between them in a simple 1,2,3... order. The job is made even easier with a bundled, thick cable with these 32
lines and a larger connector on each box, requiring thus only one cable to be connected.
[00247] Looking further at the wiring relation between port #4 and port #5, it is obvious that these thick cables (each carrying e.g. 64 or 128 Cat 5 cables) follow the
exact pattern as ports #1 and #2, except with cable bundles and big connectors instead of single Cat 5 cables and individual ports. Hence, if one had a row of internally
prewired (e.g. via ASIC) 128-switch containers (e.g. one rack 64RU tall, 2 line cards per slot), each container having 8 color coded big connectors lined up vertically on its
back panel, matching color thick cables may be used that repeat the above exact wiring pattern between these 28=256 containers (except it goes horizontally) to create a
network with 27+8= 32K IPA switches (for only $393 million), providing 786,432 x 10Gports (1 port per 10G virtual server with 32 virtual machines (VMs), totaling
25,165,824 VMs; i.e. switching cost < $16/VM). For large setups a single frame may be used where any newly added container can just be snapped into the frame
(without any cables), that has built in frame-based connectors (with all the inter-container thick cabling prewired inside the frame base).
[00248] The ultimate streamlining of the wiring (and of a lot more) is achieved by using "merchant silicon", where all such dense wiring, along with the connectors and
their supporting hardware on the switch is replaced with ASICs tying together the bare switching fabric chips. This approach not only eliminates the wiring problem, but
also massively reduces the hardware costs and power consumption.
[00249] For ASIC wiring of the Figure 19 pattern, in order to reduce the number of circuit layers the connection order must be reversed, changing all wire intersects into
nestings, allowing for single layer wiring. The resulting hypercube is just another one among the alternate labelings.
Non-Power-of-2 Networks [00250] The above manual wiring scheme can also be used to build a network that has a number of switches N which is not a power of 2 (thus
it cannot form a conventional hypercube). Consider the case of a network where that has 32 switches (d=5, using ports #0..#4, rows 0..31) and now wish to add two
more switches, (rows) #32 and #33. This starts the 6th dimension (port #5, long cyan wires), but only having two of the 32 cyan lines connected on port #6 (the two are
connecting port #6 in rows 04-62 and 14-63 for the 2 new switches #32 and #33). The first 5 ports #0-#4 of the two new switches have no switches to go to, since these
haven't been filled in (these will come later in the rows 34-63).
[00251] The problem with such partial wiring is that it severely restricts forwarding to and from the new switches (just 1 link instead of 6 links), along with reduced
bandwidth and fragility (due to single points of failure. This problem can be eliminated by using port (column) #4 of the new first new switch (row) #32.
The port #32:4 normally connects (via green wire going down to row #48) to switch: port #48:4, but the switch #48 isn't there yet. Switch #48 also connects on port #5
(via dotted cyan wire) back to the existent switch #16:5. Thus, there are two broken links #32:44-448:4 and #48:54-416:5, with missing switch #48 in the middle.
Therefore, the two ends of existing switches can be connected directly to each other, i.e.
#32:44-416:5 as shown by the top dotted green wire (which happens to be just the right length, too). Later, when switch #48 is finally added, the shortcut (green dotted
wire going up) moves down to #48:4 while the #16:5, which becomes free as well (after moving the green wire down), now connects to #48:5 (dotted cyan wire).
The same maneuver applies to switch #33 as shown with the 2nd green dotted wire.
The analogous shortcuts follows for lower ports of #32 and #33 e.g. the broken pairs #32:34-440:3 and #40:54-48:5 are short-circuited via #32:34-48:5 etc, resulting in
full (with natural forwarding) 6-D connectivity for the new switches and their neighbors. The general technique is to first construct correct links for the target topology
(e.g. hypercube), which include the non-existent nodes. Then one extends all shortest paths containing the non-existent nodes until they reach existent nodes on both
ends. The existent nodes terminating such "virtual" shortest paths (made of non-existent nodes on the inner links) are connected directly, using the available ports
(reserved on existent nodes for connections with as yet non-existent ones).
Programmable Connector Panel [002521 Another approach according to embodiments of the invention for interconnecting switches can include building large, software
controlled super-connectors ("C-Switches"), where making any desired connections between the physical connectors can be controlled by software.
[00253] Unlike a standard switch, which forwards packets dynamically based on the destination address in the packet frame header, a C-Switch forwards packets
statically, where the settings for the network of crossbar connections within the C-Switch can be provided by an external program at initialization time. Without any need
for high speed dynamic forwarding and buffering of data packets, the amount of hardware or power used by a C-Switch is several orders of magnitude smaller than a
standard switch with the same number of ports.
1002541 The individual connectors (or per-switch bundles of for example individual circuit cables brought in via trunked thick cables, plugged into a large single
connector), plug into the C-Switch's panel (which can cover 3-5 sides of the C-Switch container), which can include a matrix containing hundreds or thousands of
receptacles. Beyond the simple external physical connection, everything else can be done via software controls. Any desired topology can be selected via an operator
using software to select from a library of topologies or topology modules or topology elements.
[00255] To facilitate physical placement and heat management, C-Switches can be modular, meaning that a single C-Switch module can combine several hundred to
several thousand connectors, and the modules can be connected via single or few cables (or fiber links), depending on the internal switching mechanism used by the CSwitch. In such a modular implementation, the inter-module cabling can be done via the cabling built into the frame where the connections can be established indirectly,
by snapping a new module into the frame.
[00256] There is a great variety of possible ways to implement core functionality of a C-Switch, ranging from telephony style crossbar switches, to arrays of stripped
down, primitive hub or bridge elements, to nanotech optical switches and ASIC/FPGA techniques. Since the internal distances within a C-Switch are several orders of
magnitude smaller than standard Ethernet connections, it is useful (for the heat& power reduction) that the incoming signal power be dovvnscaled by a similar factor
before entering the crossbar logic (the signals can be amplified back to the required levels on the output from the crossbar logic). In other embodiments, for example
using WIEMS based devices, power reduction may not be necessary where optical signals are switched via piezo-electrically controlled nano-mirrors or other purely
optical/photonic techniques such as DLP normally used for projection screens, where such down/up-scaling is implicit in the transceivers.
[00257] The internal topology of the C-Switch can be multi-staged since the complexity of a single, flat crossbar grows as 0(X2) for X external ports. For example, a
arrangable non-blocking hypercubic topology requires a hypercube dimension of d, connecting N=2d smaller crossbars, which is twice the number of external ports p per
smaller crossbar, i.e. d=2p. Hence each small crossbar of radix 3p has a circuit complexity (number of cross points) of 0(9p2). The number of external ports X=N=p=
22Pp determines value p needed for a given X in implicit form where approximately 1/2 log(X) +
0(log(log(X))). Hence, the number of small crossbars is N=2dr4Clog(X). With the small crossbar radix p= 72, the C-Switch hardware scales to X=224;.-46 million ports.
[00258] This kind of software controlled multi-connector has a much wider applicability than data centers, or even than Ethernet LANs, since cabling and connectors are
a major problem in many other settings and at much smaller scales of connectivity.

https://patents.google.com/patent/CA2872831C/en?q=(high+radix)&oq=high+radix

19/31

2/14/26, 1:58 PM

CA2872831C - Flexible radix switch - Google Patents

Use of C-Switches for Layer 2 Network Optimization [00259] The traffic patterns in a data center are generally not uniform all-to-all traffic. Instead, smaller clusters of
servers and storage elements often work together on a common task (e.g. servers and storage belonging to the same client in a server farm). The integrated control
plane of the current invention allows traffic to be monitored, and to identify these types of traffic clusters and reprogram the C-Switch so that the nodes within a cluster
become topologically closer within the enhance hypercube of Ethernet switches. By reducing the path lengths of the more frequent traffic patterns or flows by using a CSwitch, the load on the switching network is reduced since fewer switching operations are needed on average from ingress to egress, hence increasing capacity. The CSwitch is used in this new division of labor between the dynamic switching network of the Layer 2 switches and the crossbar network within the C-Switch, which offloads
and increases the capacity of the more expensive network (switches) by the less expensive network (crossbars). This is a similar kind of streamlining of the switching
network by C-Switch that layer 2 switching networks perform relative to the more expensive router/layer 3 networks.
In both cases, a lower level, more primitive and less expensive form of switching takes over some of the work of the more expensive form of switching.
Wiring Improvements [00260] Although the d-cube wiring is highly regular and can be performed mechanically (a la weaving), the 'long hops' do complicate the simple
pattern enough to make it error prone for brute force manual wiring. Since this problem is shared by many other desirable topologies, a general solution is desirable to
make networks built according to the invention practical in the commercial world.
Computer assisted manual wiring [00261] In this method, the switches are numerically labeled in a hierarchical manner tailored to the packaging and placement system
used, allowing technicians to quickly locate the physical switch. A wiring program displays the wiring instructions in terms of the visible numbers on the switches
(containers, racks, boxes, rooms) and ports. The program seeks to optimi7e localization/clustering of the wiring steps, so that all that is needed in one location is
grouped together and need not be revisited.
C-Box ¨ Prewired crossbar for fixed topologies 1002621 This is a more attainable lower tech variation of the C-Switch in the form of a connector box with pre-wired
topologies, such as enhanced hypercubes, within certain range of sizes. Front panels of the C-Box provide rows of connectors for each switch (with ¨10-20 connectors
per switch) with numbered rows and columns for simple, by the numbers, wiring for the entire rows of rack switches and hosts.
1002631 C-Box is as easy to hook up and functions exactly as the C-Switch (e.g. with a built in processor and a unified control plane per box), except that the topology is
fixed. As with the C-Switch, multiple C-Boxes can be connected via thick cables to form a larger network.
Automated wiring verification 1002641 This facility is useful for the manual wiring methods described above.
Diagnostic software connected to the network can test the topology and connections, then indicates which cables are not connected properly and what corrective
actions need to be taken.
Data Center Application [002651 Figure 20 shows an embodiment of the invention applied to a complete data center. The particular details of this diagram are illustrative
only, and those skilled in the art will be able to see that many other combinations of data center components with various attributes such as number or ports and port
speed may also be used, and connected in various topologies. The cables (vertical arrows) are coded by capacity and named according to their roles: S(erver)-Lines from
server to TORs or transceivers, U(plink)-Lines from edge to network ports, T(opology)-Lines:
internal to the network (aggregate switching fabric via scalable topology &
forwarding) and W(AN)-Lines to routers/L3. The only long lines, thus posing cabling bulk problems, are U-Lines, but these already exist in a standard data center.
The internal switching fabric of the network consists of the fabric from variable number of common off-the-shelf (COTS) switches with firmware extensions, connected
via the Topology Panel (ITP). Depending on size and complexity of topology (which depends on the type of data center), the ITP block may merely symbolize a prescribed
pattern of direct connections between ports (by the number wiring), or it can be realized as a prewired connector panel or as programmable crossbar switch.
[00266] The network spanned by the T-Lines is the network backbone. The encircled "A" above the top-of-rack (TOR) switches represents fabric aggregation for parts of
the TOR fabric which reduces the TOR inefficiencies.
[00267] The control and management software, MMC (Management, Monitoring and Control module), CPX (Control Plane Executive) and IDF (Data Factory), can run on
one or more servers connected to the network switching fabric.
Virtual Machine Motion [00268] In a data center using virtual machine instances, the MMC and CPX
can cooperate to observe and analyze the traffic patterns between virtual machine instances. Upon discovering a high volume of data communication between two
virtual machine instances separated by a large number of physical network hops, the MMC and/or CPX can issue instructions to the virtual machine supervisor that
results in one or more virtual machine instances being moved to physical servers separated by a smaller number of network hops or network hops that are less used by
competing network communication. This function both optimizes the latency between the virtual machines and releases usage of some network links for use by other
communicating entities.
Layer 3+ Protocol Performance Improvement [00269] The most commonly used layer 3 (or higher) reliable communication protocols, such as TCP and HTTP, which have
large communication overheads and non-optimal behaviors in data center environments, can be substantially optimized in managed data center networks with a unified
control plane such as in the current invention.
[00270] The optimization consists of replacing the conventional multi-step sequence of protocol operations (such as three way handshake and later ACKs in TCP, or large
repetitive request/reply headers in http) which have source and destination addresses within the data center, with streamlined, reliable Layer 2 virtual circuits managed by
the central control plane where such circuits fit naturally into the flow-level traffic control. In addition to reducing communication overhead (number of frames sent, or
frame sizes via removal of repetitive, large headers) and short-circuiting the slow error detection and recovery (the problem known as "TCP
incast performance collapse"), this approach also allows for better, direct implementation of the QoS attributes of the connections (e.g. via reservation of the appropriate
network capacity for the circuit). The network-wide circuit allocation provides additional mechanism for global anticipatory traffic management and load balancing that
operates temporally ahead of the traffic in contrast to reactive load balancing. This approach of tightly integrating with the underlying network traffic management is a
considerable advance over current methods of improving layer 3+ protocol performance by locally "spoofing" remote responses without visibility into the network
behavior between the spoofing appliances at the network end points.
[00271] Further, by operating in the network stacks/hypervisor, the virtualized connections cooperate with the Layer 2 flow control, allowing for congestion/fault triggered
buffering to occur at the source of the data (the server memory), where the data is already buffered for transmission, instead of consuming additional and far more
expensive and more limited fast frame buffers in the switches. This offloading of the switch frame buffers further improves the effective network capacity, allowing
switches to handle much greater fluctuations of the remaining traffic without having to drop frames.
Flexible Radix Switch Control Plane Control Plane Capabilities [002721 The FRS Control Plane (FRS-CP) makes use of the advanced routing and traffic management
capabilities of the Infinetics Super Switch (ISS) architecture.
It can also be used to control conventional switches, although some of the capabilities for Quality of Service control congestion control may be limited.
FRS-CP provides:
Performance = Controls the flat fully meshed layer 2 substrate/fabric to maximize effective throughput to near physical limits = Self-configuring, self-balancing, selfhealing dynamic networks = Device and service level bandwidth optimization and QoS guarantees Management = Unified logical management framework for all
networked devices = Hierarchical group-based management to reduce large network complexity = Autonomic, self-healing traffic flow management Security = Single
point of authentication for all points of attachment and services at origin = Group-based networked device isolation throughout physical and virtualized networks Cost
Savings = Far less network infrastructure required; substantial savings on capital expenditures, power, and payroll = Subsumes the functionality of other monolithic
appliances such as load balancers, NATs, firewalls =
Control Plane Architecture [00273] FRS-CP can include a central control system that connects directly to all the switches in the network, which may be replicated for
redundancy and failover.
Each switch can run an identical set of services that discover network topology and forward data packets.

https://patents.google.com/patent/CA2872831C/en?q=(high+radix)&oq=high+radix

20/31

2/14/26, 1:58 PM

CA2872831C - Flexible radix switch - Google Patents

[00274] Switches can be divided into three types based upon their role in the network, as shown in Figure 24:
= Ingress switches = Fabric switches = Egress switches [00275] ARP and broadcast squelching. When a specific machine attempts to locate another machine on the
network in a classic network, it sends out a broadcast ARP (sort of a where are you type message), which will be transmitted across the entire network. This message
needs to be sent to every machine across the network on every segment which significantly lowers the throughput capacity of the network.
We keep a master list(distributed to every switch) of every host on the network, so that any host can find any other host immediately. Also any other broadcast type
packets which would have been sent completely across the network are also blocked.
(** See CPX Controller / Data Factory) Overview Data Factory (IDF) [00276] Fig. 25 shows a system according to one embodiment of the invention.
The Data Factory component can be used to establish the behavior of the IPA
Network. The Control Plane Executive (CPX) uses the data stored in the data factory to configure the network and to set up services such as security and quality
guarantees. Management consoles access this component to modify system behavior and retrieve real time network status.
Control Plane Executive (CPX) [00277] The Data Factory communicates with the Control Plane Executive (CPX) through a service interface using a communication
mechanism such as Thrift or JSON as shown in Fig. 26. Any form of encryption can be supported. In accordance with some embodiments of the invention, a public key
encryption system can be used.
Universal Boundary Manager (UBM) [00278] In accordance with some embodiments of the invention, the UBM
can provide some or all of the following functions:
= Abstracts the physical network to a unified and hierarchical logical group with rights-based inheritance for security and QoS parameters = Controls visibility of hosts
and services = Provides a single "Firewall" around perimeter of entire layer 2 network managing routing decisions for quality assurance and security enforcement for
network access = Scriptable policy management based upon time-of-day, congestion and application type = Data stored in the Data Factory, and read by CPX for
distribution to the switches.
1002791 A UBM entry can describe a name for an organization or a specific service. A UBM entry could be a company name like ReedC0 which would contain all the
machines that the company ReedC0 would use in the data center. A UBM
entry can also be used to describe a service available in that data center. A
UBM
entry has the following attributes:
= Name of node = DNS Name of this node (for DNS lookup) = Port(s) - these are the port(s) that are allowed to the specified machines. If there are no ports, then this is a
container Node which means it is used to store a list of allowed machines.
= QOS information = Parent Node. Each parent can have multiple child Nodes, but each child can only have one parent Node.
= Allow Public Access [00280] To allow external access, a flag can be provided in or associated with the Node definition that indicates that this Node can be accessible
from anybody without restrictions. So a typical company with a Database server, Backup Database server, WWW
server, and Backup server could look like the following:
= COMPCO (Lists all four computers, but no public access) = DB (lists just the Database server) = BACKUPDB (lists just the backup database server) = BACKUP (Lists
just the backup server) = WWW (Lists just the WWW server, but allow public connections) A machine table contains at least the following information:
= MAC Address = IP Address (If the machine is defined as static) = Description of machine 1002811 The firewall rules that are necessary to allow dataflow across the
network can be created from this table. Only flows that are allowed will be sent to the KLM.
UBM Service 1002821 The Universal Boundary Manager service can provide membership services, security services and QoS. There can be two or more types of UBM
groups:
Transparent UBM Group 1002831 A transparent group can be used as an entry point into the IPA
Eco-System. It can be visible and allow standard IP traffic to flow over its interface -UBM Interfaces can be determined by port number ¨ e.g. Port 80. This type of group
can be used to handle legacy IP applications such as Mail and associated Web Services. Since a Web Service can be tied to an IP port, limited security (at the Port Level)
and QoS attributes (such as Load Balancing) can be attributes of the UBM
structure.
,,,,.i*w,;,aa.LaaLLL.iL.LLLLLL'8'ZEEL,aaagFaW:'O"iVVZa.':.:SPigii;k4:4'qZg==a]V
Z'Zt.:'4AP
Visible Limited Security (Port Level) / No Security Allows legacy IP (v4 & v6) Traffic Qos Lite Traffic Sensitive Routing / Most Efficient Path to Destination Layer 2 Ring
Routing (Distributed Hash) Layer 3 Fallback if Layer 2 Ring Routing Fails Connectionless QoS Tunnel Explicit Congestion Control Notification Opaque UBM Group [00284]
An opaque group can have all the attributes of the Transparent group's attributes, but allows for the extension of pure IPA security, signaling (switch layer) and the ability
to provided guaranteed QoS.
Hidden - group Members only know about group Members Membership Driven Secure (Utilizing Public Key Security or Lattice based cryptography) Polymorphic
Membership Model (The rise of Inheritance) Pure IPA
Guaranteed QoS based upon proprietary meshed network IN:770!%*:V.P.O.WW,41.WIP4.4..FAME;!i!:!M!!i.i!ERNO!:M:E2U':!!!!
!ita All of the transparent groupbenefits Infinetics Certificate Authority Security Sub-System Guaranteed QoS Properties Signaling [002851 The major extensions to the
Opaque group can include the security attributes along with the guaranteed QoS attributes. Multiple opaque or visible groups can be defined from this core set of
attributes.
Firewall [00286] The firewall can be a network-wide mechanism to pre-authorize data flows from host to host. Since every host on the network must be previously
configured by the network administrator before it can be used, no host can successfully transmit or receive data unless it has been authorized in the network.
Furthermore because of the built in security model applied to all devices connected to the network, hosts can only communicate with other authorized hosts. There is no
way a rogue host can successfully communicate with any unauthorized host. The data defined in the UBM can control all access to hosts. The KLM loaded into each
Hypervisor can provide this functionality. Alternatively, this functionality can be provided on each switch for each attached physical host.
[00287] The ingress switch where a data packet from a host first arrives in the network can use the following rules to determine whether the data packet will be admitted
to the network as shown in Figure 22:
Forward Path Rules Ingress Switch I. Is H2 using the correct Ethernet Address? (Drop point I) I. Use source IP address to fetch triplet, compare addresses II.Can H2 send
to HI on the given destination port? (Drop point 2) I. (Use UBM group rules.) III.Send packet to Si IV.Create "reverse" rule for H1->H2 for given source Port I. Time stamp
and age out rule.
Egress Switch I. Can H2 send to HI on the given destination port? (Drop point 3) II.Create "reverse" rule for H1->H2 for given source Port I. Time stamp and age out rule.
III.Send packet to H1 Reverse Path Rules Ingress Switch I. Is HI using the correct Ethernet Address? (drop point 4) I. Use source IP # to fetch triplet, compare MAC #s
II.Can H1 send to H2 on the given destination port? (drop point 5) I. UseUBM group information III.Send encapsulated packet to S2 Egress Switch I. Can H2 send to HI on
the given destination port? (drop point 6) I. Use reverse rule.
II. Send packet to H1 [00288] This is the opposite way to which traditional firewalls work, where data is allowed to enter the network from any source, the data then
traverses the network and is prevented from reaching a destination host once the data packet has nearly reached its intended destination. This significantly lowers
"backbone"

https://patents.google.com/patent/CA2872831C/en?q=(high+radix)&oq=high+radix

21/31

2/14/26, 1:58 PM

CA2872831C - Flexible radix switch - Google Patents

traffic on the network.
Central Services Data Factory [00289] This is the starting point for full control of the network. All static and dynamic data is stored here, and a user interface is used to
view and modify this data.
CPX Controller [00290] The CPX computer is the Control Plane Executive which controls all switches, and receives and sends data to the switches. This data is what is
necessary to route data, firewall info, etc. It also controls the ICP (Integrated Control Plane) module which determines topology, and controls the IFX (Firmware
eXtensions) which are installed on every switch and hypervisor.
CPX connects to the Data Factory to read all of the configuration data necessary to make the entire network work. It also writes both log data and current configuration
data to the Data Factory for presentation to users.
ICP (Integrated Control Plane) [00291] This module controls each instance of IFX on each switch, and takes that neighbor data from each IFX instance and generates
cluster data which is then sent back to each IFX instance on each switch.
CPX Interaction with ICP
The types of data that will flow through CPX for the data plane are:
= Triplets = Firewall Rules/QoS Data = Topology Information = Logging Data [00292] Triplets (which contain the Host LP Address, Switch ID, and MAC
address of the host) are generated by the Host detector that runs on each switch. The detected triplets are sent through the Host Controller to the CPX controller.
First the triplet's data is validated to make sure that this host MAC address (and IP
address if defined), is a valid one. Once validate, the triplet is enabled in the network.
Optionally, before a host's triplet is added to the database, the host can be forced to validate themselves using various standard methods such as 802.1x.
[00293] The triplets can be sent to the Data Factory for permanent storage, and are also sent to other switches that have previously requested that triplet.
The sends will be timed out, so that if a switch has not requested a specific triplet for a specific time, the CPX will not automatically send it if it changes again unless the
ICP
requests it.
[00294] When a switch needs to route data to a host that it does not have a triplet for, the host controller sends a request for the triplet associated with the specific IP
address. The CPX looks up that triplet and sends it to the IFX
which in turn sends it to the KLM module so that the KLM can route data.
[00295] Firewall rules and Quality of Service (QOS) data travel along the same route as triplets. A switch always receives all the firewall rules involving hosts that are
connected to that switch so that quick decisions can be made by the KLM
module.
If a firewall rule changes, then it is sent to the IFX which sends it to the KLM module.
In cases where there are firewall rules with schedules or other "trigger points", the firewall rules are sent to the IFX and IFX sends them to the KLM module at the
appropriate time.
[00296] Logging Data such as data sent/received, errors, etc is sent from the KLM (or some other module) to IFX, and then to CPX which sends it to the Data Factory.
ICP Interaction with IFX on Switches [00297] CPX controls ICP which then controls each instance of IFX on each switch through ICP, telling it to send "discover" packets,
and return back neighbor topology data to ICP. All this data is stored in the Data Factory for permanent storage, and for presentation to users. This topology data is used
by IFX to generate routes.
When link states change, the IFX module notifies ICP, and a new routing table will be generated by IFX. Initially IFX will reroute the data around the affected path.
CPX Interaction with Data Factory CPX reads the following data from the Data Factory:
= Host information to validate the host being allowed, including authorization keys, etc.
= Firewall Rules and QoS for inter-host interaction = Triplets that have been previously deposited into the Factory = Routing and topology data CPX writes the following
data into the Data Factory:
= Triplet information determined by host detectors = Topology and routing data determined by CPX and IFX
= Log information about changes in network infrastructure, including routing, host, and other data ICP Data Factory The following information is needed by ICP.
[00298] This can happen at a very high rate upon startup, and can reoccur on a regular basis very slowly = Switch Information Key Value will either be MAC or IP address
Data Returned will be information necessary for to calculate topology, and identify switches.
= Topology information previously written by CPX before. This will be used as "hints" to restart routing in case of a failed switch for example = Routing information
necessary to route data between switches. This will need to be updated on all affected switches whenever the ICPupdates the Datastore Factory.
The following information will be written by ICP.
[00299] This can happen on a very regular basis (e.g., at least 1 per second and can occur more often), but the writes can be buffered and delayed for writing if need be.
The data will not be read on a regular basis, except for startup, but will need to be updated on all other switches. Of course the data will be read by the User for network
status monitoring.
= Switch Status - Current Status of each switch, including port status = Topology information - links between switches including metadata about each link = Routing
information. Calculated "best" routes between switches ICP Data needed for Switches The following information will be written by the switches Triplets from switches for
hosts. These will be written whenever a new host comes online, or a host goes away. They can happen anywhere from once every few seconds, to much more often as
hosts come online. There needs to be some sort of acknowledgement that the specific host being added already exists in the UBM so that we can route to that host. If
the host does not exist we need to flag that host's information so that the user can see that a undefined host has been activated on the network, and allow the user to
add it to the UBM.
The following information will be read by the switches.
1003001 All of these reads can occur as fast as possible. Any slowness in these reads may slow down the data path.
= Triplets for hosts. This can happen quite often, and needs to be as fast as possible.
= UBM data that allows all the data necessary to create the firewall/QOS
rules, multi-server data, and everything else necessary to route to that host.
= The data that will be delivered to the switches from the UBM is:
Firewall Rules with QOS information Multi-server data. This is all the servers of an equivalent type.
Switch Services The following services can run on all switches in the network.
IFX (Firmware eXtensions) 1003011 This module runs on each switch and is responsible for determining the topology of the neighbors. It sends data back to the ICP
module about its local physical connectivity, and also receives topology data from ICP. It supports multiple simultaneous network logical topologies, including n-cube,
butterfly, torus, etc as shown in Figure 23. It uses a raw Ethernet frame to probe the devices attached to this switch only. It also takes the topology data from ICP, and the
cluster data from ICP
and calculates forwarding tables.
IFXS (Firmware eXtensions for Servers) [00302] This module runs on each hypervisor and interact s with the Hypervisor/KLM module to control the KLM. Flow data
related to how many bytes of data flowing from this hypervisor to various destinations is accepted by this module and used to calculate forwarding tables.
Hvnervisor Controller This can include a Linux kernel loadable module (KLM) that implements the Data plane. It can be controlled by the Switch Controller.
The input to this module are:

https://patents.google.com/patent/CA2872831C/en?q=(high+radix)&oq=high+radix

22/31

2/14/26, 1:58 PM

CA2872831C - Flexible radix switch - Google Patents

= Triplets from this and other switches = Firewall Rules and QoS Associated data = Routes from IFX
[00303] The KLM can route packets from hosts to either other hosts, or to outside the network if needed (and allowed by rules). All packets sent across the "backbone"
can be encrypted, if privacy is required.
1003041 The KLM switch module can have access to caches of the following data: triplets (they map IPv4 addresses into (Egress Switch ID, host Ethernet Address
pairs); routes (they define the outbound interfaces, and next hop Ethernet Address to use to reach a given Egress Switch); and firewall rules (they defme which IPv4 flows
are legal, and how much bandwidth they may utilize).
[00305] The KLM can eavesdrop on all IP traffic that flows from VM
instances (that are supported by the local hypervisor). It can, for example, use functionality (defined in the Linux netfilter library) to S ___________________ l'EAL, DROP, or
ACCEPT individual IP
datagrams that are transmitted by any VM.
[00306] When a datagram is transmitted by a VM, the KLM switch can intercepts (STEALs) it and determines if firewall rules classify the corresponding flow to be legal. If
it's illegal, the packet is dropped. If the flow is legal and it's destination is local to the hypervisor, it's made to obey QoS rules, and delivered. If the flow is legal and
exogenous, the local triplet cache is consulted with the destination IP address as an index. If a triplet exists, it determines the Egress Switch ID (which is just a six-byte
Ethernet address). If a route also exists to the Egress switch, then the packet will be forwarded with the destination switch Topological MAC address put into the Ethernet
frame.
[00307] The KLM can use a dedicated Ethernet frame type to make it impossible for any backbone switch or rogue host to send a received frame up its protocol stack.
[00308] When a frame arrives at a hypervisor, it can be intercepted by its kernel's protocol handler (functionality inside the KLM) for Ethernet frame type defmed. The
protocol handler can examine the IP datagam, extract the destination IP
address, and then index it into it's triplet cache to extract the Ethernet address of the local VM. If no triplet exists, the frame can dropped. The socket buffer's protocol
type can switched from 0xbee5 to 0x0800, and the packet can be made to obey QoS
rules before it is queued for transmission to the local host.
[00309] The KLM can use IFXS, for example, as its method to talk with CPX
to access the data factory.
Examples [00310] Figure 24 shows a typical use case where switching systems according to various embodiments of the invention can be used within a data center.
[00311] Figure 15 shows one embodiment of the invention where the FRS is used alone to provide an ultra-high bisection bandwidth connection between multiple CPU
cores and a large array of flash memory modules. The prior art approach for having CPU cores transfer data to and from flash memory treats the flash memory modules
as an emulated disk drive where data is transferred serially from a single "location". The invention allows large numbers of CPUs or other consumers or generators of
data communicate in parallel to multiple different flash memory storage modules. In this embodiment of the invention, the ISS network can be designed using the
physical constraints of the various methods that semiconductor devices are packaged and interconnected. This embodiment results in a network that has a different
connection pattern than would be used in a data center, but still provides extremely high bisection bandwidth for the available physical connections within and between
semiconductor devices and modules.
[00312] Additional supporting information relating to the construction of Long Hop networks is provided in attached Appendix A.
[00313] Those skilled in the art will realize that the methods of the invention may be used to develop networks than interconnect devices or nodes with arbitrary
functionality and with arbitrary types of information being exchanged between the nodes. For example, nodes may implement any combination of storage, processing or
message forwarding functions, and the nodes within a network may be of different types with different behaviors and types of information exchanged with other nodes in
the network or devices connected to the network.
1. Introduction Rapid proliferation of large Data Center and storage networks in recent years has spurred great deal of interest from industry and academia in
optimization of network topologies [1]-j121. The urgency of these efforts is further motivated by the inefficiencies and costs of the presently deployed large Data Center
networks which are largely based on non-scalable tree topology.
There are two main types of network topologies proposed as scalable alternatives to the non-scalable tree topology of the conventional Data Center:
= Fat Tree (FT) (syn. folded Clos) based networks, a class of "indirect networks"
= Hypercubic (HC) networks, a class of "direct networks" using Cartesian product construction recipe. This class includes plain hypercube variants (BCube, MDCube),
Folded Hypercube (FC), Flattened Butterfly (FB), HyperX (HX), hyper-mesh, hyper-torus, Dragonfly (DF),... etc.
While the HC networks are overall the more economical of the two types, providing the same capacity for random traffic as FT with fewer switches and fewer cables, the
FT
is more economical on the worst case traffic, specifically on the task of routing the worst case 1-1 pairs permutation.
The Long Hop (LH) networks stand above this dichotomy by being simultaneously the most optimal for the common random traffic and for the worst case traffic. The LH
optimality is result of the new approach to network construction which is fundamentally different from the techniques used to construct all the leading alternatives.
Namely, while the alternative techniques build the network via simple mechanical, repetitive design patterns which are not directly related to the network performance
metrics such as throughput, the LH
networks are constructed via an exact combinatorial optimization of the target metrics.
Although there have been some previous attempts to optimize the network throughput directly, such as the "entangled networks" described in [2_] and RA, these
techniques sought to optimize general random networks. Since such optimization is computationally intractable for general graphs (it is an NP-complete problem), the
computations of both, the network performance and the search for its improvements, are by necessity very approximate (simulated annealing) and still, they become
prohibitively expensive as the network size if Appendix A -increases beyond few thousand nodes. For example, the largest computed size in [12] had n=2000 nodes.
Further, since the resulting approximate solutions have variable node degree and random connectivity, appearing to a network technician as massive, incoherent tangles
of wires without any pattern or logic, the "entangled networks" are in practice virtually impossible to wire and troubleshoot. Finally, the node degree irregularity and the
complete lack of symmetry of such networks compound their impracticality due to complicated, resource hungry routing algorithms and forwarding tables.
In contrast, the LH construction method optimizes the highly symmetrical and, from practical perspective, the most desirable subset of general networks, Cayley graphs
all As result of its more focused and more careful identification of the target domain, the LH
networks are optimal regarding throughput and latency within that domain, practical to compute and discover, simple and economical to wire and troubleshoot and highly
efficient in routing and forwarding resources ("self-routing" networks).
Appendix A
2. Mathematical Tools and Notation = A B equality defining expression A via expression B
(tautology) = A <=> B expression or statement "A is equivalent to B"
= A B "A implies B"
= V a iterator or a set defined by the statement "for all a"
= if "if and only if"
= IS sets: size of set S (number of elements in S), numbers:
absolute value of S

https://patents.google.com/patent/CA2872831C/en?q=(high+radix)&oq=high+radix

23/31

2/14/26, 1:58 PM

CA2872831C - Flexible radix switch - Google Patents

= Lai floor(a): the largest integer < a = Vn n-dimensional vector space (over some implicit field Fq) = S(k,n,q) k-dimensional subspace of Vn (linear span) over field Fq =
(xly) scalar (dot) product of real vectors x and y: (xly) VxL Yi = 11x11 norm (length) of vector x: 114 2V(xlx) = a. .b integer sequence a, a+1, b for some integers a < b = [a,
b) half-open interval: contains all x satisfying a < x <b = [a, b] closed interval: contains all x satisfying a <x < b = {al, a2, a3} set of elements al, a2 and a3 = {x: E(x)} set of
elements x for which Boolean expression E(x) is true = minEfset) minimum element of a {set} under condition E;
analogously for max { set}
= a % b "a mod b" or "a modulo b" (remainder in integer division a / b) = bitwise operation on bit strings done separately in each bit position = -a or at NOT a (bitwise
complement, toggles each bit of a) = a & b bitwise AND (bitwise a- b) = a I b bitwise OR (bitwise a + b - a-b) = a A b XOR, exclusive OR (bitwise: (a + b) mod 2, also a + b
-2.a.b) = a ED b modular addition in ring (Ziq)d: component-wise (a +
b) mod q = a e b synonym for a (1) (-b); for aeb <=> a(Bb <=t, a^b (bitwise XOR) = V=V091/2 Vector space V is direct sum of vector spaces 1/1 and V2 = A0B=B0A Objects
(matrices, group elements, etc.) commute for operation '0' Appendix A
= [E] Iverson bracket (E is a Boolean expression): E true (false) [E],s1 (0) = ,31,1 Kronecker delta: 6, j [i=jj i.e. Si j is 1 if i =j and 0 if i = Si Dirac integer delta: 8i3O i.e. Si is 1 if
i = 0 and 0 if i 0 = B =AT A matrix B is a transpose of matrix A i.e. elements = A 0 B Kronecker product of matrices A and B
= Ace Kronecker n-th power of matrix A: Ace A0A0...0A (n times) = AxB Cartesian product of sets or groups A and B
= A' Cartesian n-th power of a set or group A
= C(n,k) Binomial coefficient C(n,k)F-i n!l[k!(n-k)!)]=() = 0(N) Big 0 notation characterizes the growth rate and complexity.
Binary expansion of a d-bit integer X <=> X = V' where xi, is the " -th bit of X"
(bits x. have values 0 or 1). Bit-string form of the binary expansion of integer X is denoted as: X = = xi xo=
Parity of a d-bit integer X = ... Xi xo is: P(X) (x0+x1+...+xd.1) mod 2 =
xo ^ ^. = =A xd-i=
Hamming weight (X) or A(X) of n-tuple X xi x2... x, where xi E [0,q), is the number of non-zero symbols in X. Hamming distance A(X,Y) between n-tuples X and Y is the
number of positions i where xi For vectors X
and Y this is equivalent to A(X,Y) = (X ¨ Y) A(X ¨ Y) i.e. to Hamming weight of (X-Y). For binary strings this yields A(X,Y)= (X^Y) i.e.
the Hamming weight of XAY.
Lee distance is A(X, Y) -a-E mintlxi ¨ yil,q ¨ - yip. Lee weight is: A(X) a--A(X, 0).
Binary intervals (or binary tiles) are intervals of size 2k (fork = 1,2,...) such that each "tile"
of size 2k starts on an integer multiple of 2k e.g. [m.2k, (m+1) .2k) for any integer m are "binary intervals" of size 2k.
Cyclic group Z.: set of integers (0,1,... n-1) with integer addition modulo is as the group operation. Note that Z2 group operation is equivalent to a single bit XOR
operation Appendix A
(1"0=0^1=1, 0^0=1 ^1=0). The same symbol 4 is also used for commutative ring with integer additions and multiplication performed mod n.
Product group zfit E Zq X Zq X = == X Zq (d times): extension of Zq into a d-tuple. As with Zn, Z1 also denotes a commutative ring in which the Zq operations (integer +,*
mod q) are done component-wise.
Finite Dyadic group Dd of order n=2d is abelian group consisting of all d-bit integers 0..n-1 using bitwise XOR (^) as the group operation. Notes: (i) for n=2d and d >2 4 Dd;
(ii) Dd is an instance of Z.
0: 0 1 2 3 4 5 6 7 8 9 ABCDEF :0 1: 1 0 3 2 5 4 7 6 9 8 BADCF E :1 2: 2 3 0 1 6 7 4 5 AB 8 9 EFCD:2 3: 3 2 1 0 7 6 5 4BA 9 8 FEDC:3 4: 4 5 6 7 0 1 2 3 CDEF 8 9 AB:4 5: 5 4 7 6
1 0 3 2 DCF E 9 8 BA:5 6: 6 7 4 5 2 3 0 1 EFCDAB 8 9 7: 7 6 5 4 3 2 1 OF EDCBA 9 8 :7 8: 8 9 ABCDEF 0 1 2 3 4 5 6 7 :8 9: 9 8 BADCFE 1 0 3 2 5 4 7 6 :9 A: AB 8 9 EFCD 2 3
0 1 6 7 4 5 :A
B: BA 9 8 FEDC 3 2 1 0 7 6 5 4:8 C: CDEF 8 9 AB4 5 6 7 0 1 2 3 :C
D: DCF E 9 8 BA 5 4 7 6 1 0 3 2 :0-E: EFCDAB 8 9 6 7 4 5 2 3 0 1 :E
F: F EDCBA 9 8 7 6 5 4 3 2 1 0:F
Table 2.1 Table 2.1 illustrates the group operation table for group D4 with n = 24 =16 elements 0, 1, 2,... F (all numbers are in base 16). Table entry in row Y and column X
is the result of bitwise XAY operation.
Appendix A
A. Matrices and Vectors in Dirac Notation Dirac notation (also called "bra-ket" notation, [13]) is a mnemonic notation which encapsulates common matrix operations and
properties in a streamlined, visually intuitive form.
Matrix [A,,c] (also: [A] or just A) is a rectangular table with r rows and c columns of "matrix elements". An element on i-th row and j-th column of a matrix [A] is denoted as
[A]; j.
Identity matrix nxn is denoted as If, or!. Matrices with r = 1 or c = 1, row or column vectors, are denoted as follows:
fYi\
Row vector (bra): (XI = (xi. x2 ' = Xn) Column vector (ket):
I Y) \Yn/
Inner (scalar) product: (XIV) (xi x2 = = = xn) = Y:2 ri xi yi = "number"
Yi Y1X2 === Yixc Outer product: I Y) (XI Y.2 (x1 x2 xc) Y2 xi Y2 x2 Y2Xc = "matrix"
Yr Yr xi Yr x2 Yrxc Translation bra 4-+ ket x real matrix A: lu) = AI v) 4=> (ul = (VIAT
i-th "canonical basis" bra vector: (ei a- (0102 === ot_i i 0i4.1 === On) General "orthonormal basis" (B) tIbi): i = 1.. n): (bilbj) =
Orthogonal matrix U: UUT=In, orthonormal bases {B},{C}: U = Eil bi)(ci I
Projector (matrix) onto the i-th canonical axis: Pi E. lei) (ei Projector (matrix) onto any normalized ((ulu) = 1) vector the lu): P1 lu) (ul Component (vector) of (XI along axis
(eil: (XI P = (Xlei) (eil = (eil xi "Resolution of identity" in any basis {B} : = I bi) (bi I
The above examples illustiate a rationale for Dirac notation: product expressions of the form with two "pointy" ends such as <...> are always scalars (numbers), while
products of the form with two flat ends are always matrices. Mixed ends products (those with one Appendix A
pointy and one flat end) such as or I...> are always row or column vectors.
Due to associativity of matrix products, these "object type rules" are valid however many other matrix or vector factors may be inside and outside of the selected subproduct of a given type.
Also, the "resolution of identity" sums E I bi)(b, I can be freely inserted between any two adjacent bars ('flat ends') within a large product, further aiding in the breakup of
longer chains of matrices into scalars. Such rules of thumb often suggest, purely visually, quick, mistake-proof simplifications e.g. any scalars spotted as ...<...>... pattern
can be immediately factored out.
B. Hadamard Matrices and Walsh Functions Hadamard matrix 11. (or H) is a square nxn matrix defined by equation HõHõT =

https://patents.google.com/patent/CA2872831C/en?q=(high+radix)&oq=high+radix

24/31

2/14/26, 1:58 PM

CA2872831C - Flexible radix switch - Google Patents

nlii. Of interest here are the Sylvester type of Hõ matrices characterized by the size constraint n -7= 2d.
Under this constraint the Hõ matrices can be constructed recursively (equivalent to Kronecker products of H2) as follows [14]:
H2 = I 1 1 ¨
IFI2n = n n H2 Hn. H?(d+1) (2.1) 1 ¨1 H n H
The pattern of 1132 (d=5) is shown in Table 2.2 with '4' elements shown as `-`
and coordinates in base 16.
Appendix A
00 02 04 06 08 0A OC OE 10 12 14 16 18 1A 1C lE
0:00 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 00 1:01 1 - 1 - 1 - 1 - 1 - 1 - 1 - 1 - 1 - 1 - 1 - 1 - 1 - 1 - 1 - 1 - 01 2:02 1 1 - - 1 1 - - 1 1 - - 1 1 - - 1 1 - - 1 1 - - 1 1 - 1 1 - - 02 3:03 1 - - 1 1 - - 1 1 - - 1 1 - - 1 1 - - 1 1 - - 1 1 - - 1 1 - - 1 03 4:04 1 1 1 1 - - - - 1 1 1 1 - - - - 1 1 1 1 - - - - 1 1 1 1 - - - - 04 5:05 1 - 1 - - 1 - 1 1 - 1 - - 1 - 1 1 - 1 - - 1 - 1 1 - 1
- - 1 - 1 05 6:06 1 1 - - - - 1 1 1 1 - - - - 1 1 1 1 - - - - 1 1 1 1 - - - - 1 1 06 7:07 1 - - 1 - 1 1 - 1 - - 1 - 1 1 - 1 - - 1 - 1 1 - 1 - - 1 - 1 1 - 07 8:08 1 1 1 1 1 1 1 1 ----------- 1 1 1 1 1 1 1 1 -------- 08 9:09 1 - 1 - 1 - 1 - - 1 - 1 - 1 - 1 1 - 1 - 1 - 1 - - 1 - 1 - 1 - 1 09 10:0A 1 1 - -11- - - - 1 1 - - 1 1 1 1 - -11- - - - 1 1 - -11 0A
11:08 1 - -11- - 1 - 1 1 - -11-1- -11- - 1 - 1 1 - -11- OB
12:0C 1 1 1 1 ------------ 1 1 1 1 1 1 1 1 --------------- 1 1 1 1 OC
13:0D 1 - 1 - - 1 - 1 - 1 - 1 1 - 1 - 1 - 1 - - 1 - 1 - 1 - 1 1 - 1 - OD
14:0E 1 1 - - - - 1 1 - - 1 1 1 1 - - 1 1 - - - - 1 1 - - 1 1 1 1 - - OE
15: F 1 - -1-11- -11-1- - 1 1 - -1-11- - 1 1 - 1 - -1 OF
16:10 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ---------------------- 10 17:11 1 - 1 - 1 - 1 - 1 - 1 - 1 - 1 - - 1 - 1 - 1 - 1 - 1 - 1 - 1 - 1 11 18:12 1 1 - -11- -11- -11- - - - 1 1 - -11- -11- -11 12 19:13 1 -11- -11- - 1 1 - -1-11- - 1 1 - -11- - 1 1 - 13 20:14 1 1 1 1 - - - - 1 1 1 1 ------------------------------ 1 1 1 1 - - - -21:15 1 - 1 - - 1 - 1 1 - 1 - - 1 - 1 - 1 - 1 1 - 1 - - 1 - 1 1 - 1 - 15 22:16 1 1 - - - 1 1 1 1 - - - - 1 1 - - 1 1 1 1 - - - - 1 1 1 1 - - 16 23:17 1 - - 1 - 1 1 - 1 - - 1 - 1 1 - - 1 1 - 1 - - 1 - 1 1 - 1 - - 1 17 24:18 1 1 1 1 1 1 1 1 -------------------------------------- 1 1 1 1 1 1 1 1 25:19 1 1 - 1 - 1 - - 1 - 1 - 1 - 1 - 1 - 1 - 1 - 1 1 - 1 - 1 - 1 - 19 26:1A 1 1 - - 1 1 - - - - 1 1 - - 1 1 - - 1 1 - - 1 1 1 1 - - 1 1 - - lA
27:18 1 - -11- -1-11- - 1 1 - - 1 1 - -11-1- -11- -1 18 28:1C 1 1 1 1 ---------------------------------------------- 1 1 1 1 - - - -29:10 1 - 1 - - 1 - 1 - 1 - 1 1 - 1 - - 1 - 1 1 - 1 - 1 - 1 - - 1 - 1 1D
30:1E 1 1 - - - - 1 1 - - 1 1 1 1 - - - - 1 1 1 1 - - 1 1 - - - - 1 1 1E
31:1F 1 - - 1 - 1 1 - - 1 1 - 1 - - 1 - 1 1 - 1 - - 1 1 - - 1 - 1 1 - 1F
00 02 04 06 08 04 OC OE 10 12 14 16 18 lA 1C 1E
Table 2.2 Appendix A
From the construction eq. (2.1) of 11,1 (where n E 2d) it follows that HE, is a symmetric matrix:
Symmetry: Ho = H1,1 (2.2) Walsh function Uk(x) for k=0..n-1, x=0..n-1, is defined as the k-th row of H.
By virtue of lin symmetry, eq. (2.2), the k-th column oflin is also equal to Uk(x). The row and column forms of Uk(x) can also be used as the n-dimensional bra/ket or
row/column vectors (UklandlUO.
Some properties of Uk(x) are:
in for j = k Orthogonality: (11j1Uk) = n = Spt (2.3) tO for j # k Symmetry: Uk(x) = U( k) (2.4) Function values: Uk(x) (2.5) = (-1)4.14kpx# (_1)P(k&x) U0 (x) = (-1)4;1 =
(-1)E4/1;110. = (-1) = 1, V (2.6) n-i Uk(X) = 0 for k = 1..n ¨1 (2.7) x=o The exponent Zdi,1k xi, in eq. (2.5) uses binary digits kg and xv, of d-bit integers k and x.
When this sum is even number Uk(x) is 1 and when the sum is odd number Uk(x) is -1. The second equality in eq. (2.5) expresses the same results via parity function
P(k&x), where k&x is a bitwise AND of integers k and x. For example U14(15)=(-1) from the table Fig. 1. Binary forms for k and x are: k=14=01110 and x=15=01111. The
sum in the exponent is = 0.0+14+14+1.1+0-1 = 3 U14(15) =
(-1)3= (-1)1= -1. The parity approach uses k &x=
01110 & 01111 = 01110 yielding exponent P(01110) 0= AriAIA.,, u= 1 and U14(15)=(-1)1 = -1 i.e. the same result as the one obtained via the sum formula.
For efficiency, the LH network computations use mostly binary (also called boolean) form of Uk and H,, denoted respectively as Wk and Md. When both forms are used in
the same context, the Uk and H,, forms are referred to as algebraic forms. Binary form is obtained from the algebraic form via mappings 1¨* 0 and -1 1.
Denoting algebraic values as a and binary values as b, the translations between the two are:
Algebraic a -1 1 Appendix A
Binary b 1 0 1-a b ¨2 and a = 1 ¨ 2b (2.8) The symmetry eq. (2.4) and function values eq. (2.5) become for the binary form Wk(x):
Symmetry: Wk(x) = W( k) (2.9) Function values: Wk(x) = 15, =
IP(k&x) (2.10) Binary Walsh functions Wk(x) are often treated as length n bit strings, which for k=1. .n-1 have exactly n/2 zeros and n/2 ones. In the bit string form one
can perform bitwise Boolean operations on Wk as length n bit strings. Their XOR property will be useful for the LH
computations:
WiAWk = Wk (2.11) i.e. the set {Wk} {Wk: k=0..n-1} is closed with respect to bitvvise XOR
(denoted as ^) operation and it forms a group of n-bit strings isomorphic to the dyadic group Dd of their indices k (d-bit strings).
Table 2.3 below shows the binary form of Hadamard (also called Walsh) matrix [W32]
obtained via mapping eq. (2.8) from H32 in Table 2.2 (binary O's are shown as `-`).
Appendix A
0:00 ------------------------------------------------------ 00 1:01 - 1 - 1 - 1 - 1 - 1 - 1 - 1 - 1 - 1 - 1 - 1 - 1 - 1 - 1 - 1 - 1 01 2:02 - - 1 1 - - 1 1 - - 1 1 - -11- -11- - 1 1 - -11- -11 02 3:03 - 1 1 - - 1 1 - - 1
1 - - 1 1 - - 1 1 - - 1 1 - - 1 1 - - 1 1 - 03 4:04 - - - - 1 1 1 1 - - - - 1 1 1 1 - - - - 1 1 1 1 - - - -1111 04 5:05 - 1 - 1 1 - 1 - - 1 - 1 1 - 1 - -1- 1 1 - 1 - - 1 -11-1- 05 6:06 - - 1 1 1 1 - - - - 1 1 1 1
- - - - 1 1 1 1 - - - -1111- - 06 7:07 -11-1--1-11-1--1-11-1--1-11-1--1 07 8:08 -------------- 1 1 1 1 1 1 1 1 --------------------- 1 1 1 1 1 1 1 1 9:09 - 1 - 1 - 1 - 1 1 - 1 - 1 - 1 - - 1 - 1 - 1 - 1 1 - 1
- 1 - 1 - 09 10:0A - - 1 1 - - 1 1 1 1 - - 1 1 - - - - 1 1 - - 1 1 1 1 - - 1 1 - - 0A
11:08 -11--11-1--11--1-11--11-1--11--1 OB
12:0C - - - - 1 1 1 1 1 1 1 1 ----------------------------- 1 1 1 1 1 1 1 1 -- - - OC
13:0D -1-11-1- 1 - 1 - - 1 - 1 - 1 - 1 1 - 1 - 1 - 1 - - 1 - 1 OD
14:0E - - 1 1 1 1 - -11- - - - 1 1 - - 1 1 1 1 - -11- - - - 1 1 OE
15:OF - 1 1 - 1 - - 1 1 - - 1 - 1 1 - - 1 1 - 1 - - 1 1 - - 1 - 1 1 - OF
16:10 ----------------------------------------------------- 1 1 1 1 1 1 1 1 17:11 - 1 - 1 - 1 - 1 - 1 - 1 - 1 - 1 1 - 1 - 1 - 1 - 1 - 1 - 1 - 1 - 11 18:12 - - 1 1 - - 1 1 - - 1 1 - - 1 1 1 1 - - 1 1 - - 1 1 - - 1 1 - - 12
19:13 - 1 1 - - 1 1 - - 1 1 - - 1 1 - 1 - - 1 1 - - 1 1 - - 1 1 - - 1 13 20:14 - - - - 1 1 1 1 - - - - 1 1 1 1 1 1 1 1 - - - - 1 1 1 1 - - - - 14 21:15 -1-11-1--1-11-1-1-1--1-11-1--1-1 15 22:16 - - 1 1 1
1 - - - - 1 1 1 1 - - 1 1 - - - - 1 1 1 1 - - - - 1 1 16 23:17 - 1 1 - 1 - - 1 - 1 1 - 1 - - 1 1 - - 1 - 1 1 - 1 - - 1 - 1 1 - 17 24:18 ------------- 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ----- 18 25:19 - 1 - 1 - 1 1 1 - 1 - 1 - 1 - 1 - 1 - 1 - 1 - - 1 - 1 - 1 - 1 19 26:1A - - 1 1 - - 1 1 1 1 - - 1 1 - - 1 1 - - 1 1 - - - - 1 1 - - 1 1 lA
27:10 - 1 1 - - 1 1 - 1 - -11- - 1 1 - - 1 1 - -1-11- - 1 1 - 18 28:1C - - - - 1 1 1 1 1 1 1 1 - - - - 1 1 1 1 ------------- 1 1 1 1 1C
29:1D -1-11-1-1-1--1-11-1--1-1-1-11-1- 1D
30:1E - - 1 1 1 1 - - 1 1 - - - - 1 1 1 1 - - - - 1 1 - - 1 1 1 1 - - lE
31:1F - 1 1 - 1 - - 1 1 - - 1 - 1 1 - 1 - - 1 - 1 1 - - 1 1 - 1 - - 1 1F

https://patents.google.com/patent/CA2872831C/en?q=(high+radix)&oq=high+radix

25/31

2/14/26, 1:58 PM

CA2872831C - Flexible radix switch - Google Patents

00 02 04 06 08 0A OC OE 10 12 14 16 18 lA 1C 1E
Table 2.3 Appendix A
C. Error Correcting Codes Error correcting coding (ECC) is a large variety of techniques for adding redundancy to messages in order to detect or correct errors in the
decoding phase. Of interest for the LH
network construction are the linear EC codes, which are the most developed and in practice the most important type of ECC [11],[16].
Message X is a sequence of k symbols xi, x2,..., xk from alphabet A of size q?
2 i.e. x, can be taken to be integers with values in interval [0,q). EC code for X is a codeword Y which is a sequence yi, of n > k symbols from A. The encoding procedure
translates all messages from some set {X} of all possible messages into codewords from some set {Y}.
For block codes the sizes of the sets {X) and {Y} are qk i.e. messages are arbitrary k-symbol sequences. The excess symbols n-k > 0 in Y represent coding redundancy or
"check bits" that support detection or correction of errors during decoding of Y into X.
For ECC algorithmic purposes, the set A is augmented with additional mathematical structure, beyond merely that of a bare set of q elements A. The common
augmentation is to consider symbols xi and y1 to be elements of a Galois field GF(q) where q npm for some prime p and some integer m>1 (this condition on q is a
necessary condition in order to augment a bare set A into a finite field Fq). Codewords Y are then a subset of all n-tuples over the field GF(q). The GF(q) field arithmetic
(i.e. the + and scalar .) for the n-tuples F41 is done component-wise i.e. F42. is n-dimensional vector space 118 rover GF(q).
Linear EC codes are a special case of the above n-tuple F structure of codewords, in which the set IYI of all codewords is a k-dimensional vector subspace (or span)
S(k,n,q) of lin.
Hence, if two n-tuples Y1 and V2 are codewords, then the n-tuple Y3=Y1+Y2 is also a codeword. The number of distinct codewords Y in S(k,n,q) is S(k,n,q)l=qk. This linear
code is denoted in ECC convention as [n,k]q code, or just [n,k] code when q is understood from the context or otherwise unimportant in a context.
A particular [n, k] code can be defmed by specifying k linearly independent n-dimensional row vectors (gil = (g1,1 g ... go) for V-1..k, which are used to defme the kxn
"generator matrix" [G] of the [n,k] code as follows (LW p. 84):
More generally message X and codeword Y can use different alphabets, but this generality merely complicates exposition without adding anything useful for the LH
construction.
Appendix A -(fill) (91,1 91,2 =-- 91,n) [Gi EIlei)(9i1 *** (2.20) 1=1 (g kl g k,1 fl k,2 g k,n Encoding of a message X (XI (xi, x2, ..., xk) into the codeword Y (Y1 E 071, Y2, ¨, Yn)
is:
Of I E (XI [G.] = J
x-(e .1 lei)(9 it) = xj6j,i(9 it <gil (2.21) j=1 1=1 i,j=1 i=1 Individual component (symbol) y, (where s=1..n) of the codeword Y is then via eqs. (2.20)-(2.21):
Ys E (Iles) = xi(gi les) = /xigi,s (2.22) The kxn matrix [Gk,n] is called systematic generator if the original message X
=x1, x2,= =
xk occurs as a substring of the output codeword V. The systematic generators [G] combine a kxk identity matrix Ik as a sub-matrix of [G] i.e. [G] typically has a form Ilk
IAk,õ..k] or [Ak Ik ], yielding unmodified substring X as a prefix or a suffix of Y, which simplifies encoding and decoding operations. The remaining n-k symbols of Y are
then called parity check symbols.
The choice of vectors (gi I used to construct [G] depends on type of errors that the [n,kj code is supposed to detect or correct. For the most common assumption in ECC
theory, the independent random errors for symbols of codeword Y, the best choice of (,gj are those that maximize the minimum Hamming distance A(YI,Y2) among all
pairs (YI,Y2) of codewords.
Defining minimum codeword distance via:
A mintgYi, 1'2) I V Y1, Y2 E (k, n, q) and Y1 # Y2} (2.24) the [n,k]q code is often denoted as [n,k,A]q or [n,k,A] code. The optimum choice for vectors (11 i I maximizes A for
given n, k and q. The tables of optimum and near optimum [n,k,d]q codes have been computed over decades for wide ranges of free parameters n, k and q (e.g.
see web repository [11]).
Table 2.4 ([16] p. 34) illustrates optimum [7,4,312 code i.e. a systematic binary code with n =
7 bit codewords each containing 3 parity check bits, I4 message bits (appearing as suffix in Appendix A
the codeword Y), with minimum distance A----.3, thus capable of correcting all 1-bit errors and detecting all 2-bit errors.
( [ ) G4,71 = 1 11 11 00 01 10 00 Table 2.4 Quantity closely related to A, and of importance for LH construction, is the minimum non-zero codeword weight wõõõ defined
via Hamming weight (Y) (the number of non-zero symbols in Y) as follows:
wmii, E mint(Y): (Y E S(k, n, q)) and (Y # 0)1 (2.25) The property of wõ,,õ (cf. Theorem 3.1, p. 83 in [16]) of interest is that for any linear code [n,k,A]q:
wmin = A (2.26) Hence, the construction of optimal [n,k,d]q codes (maximizing A) is a problem of finding k-dimensional subspace S(k,n,q) of an n-dimensional space F7qt
which maximizes wriiin Note also that since any set of k linearly independent vectors (gi I (a basis) from S(k,n,q) generates (spans) the same space S(k,n,q) of qk vectors
Y, wõõõ and A are independent of the choke of the basis {(gil: i = 1.. k). Namely by virtue of uniqueness of expansion of all qk vectors Y E
S(k,n,q) in any basis and pigeonhole principle, the change of basis merely permutes the mapping X¨>Y, retaining exactly the same set of qk vectors of S(k,n,q).
Appendix A
D. Graphs: Terms and Notation = F(V,E) Graph r with vertices V={vi,v2,... vn} and edges E=
lei, 62,... el = degree of v Number of edges (links) connected to node v = FIDF2 Cartesian product of graphs Fl and F2 (syn. "product graph") = co n (Cartesian) n-th power
of graph F
= & k = (VI,Vj) Edge c k connects vertices v, and vi = v, vi Vertices vi and vi are connected = 1,, Vertices vi and vi are not connected = [A] Adjacency matrix of a graph: [A4--.. [vi vi]: 1 if v v, 0 if vi v.
Number of ones on a row r (or column c) is the degree of node r (or c) = A(ij)=A(j,i) Symmetry property of [A] (for undirected graphs) = Cr, Cycle graph: A ring with n
vertices (syn. n-ring) = Pn Path graph: n-ring with one link broken i.e. a line with n vertices (syn.
n-path) = Qd d-dimensional hypercube (syn. d-cube): (P2) d P212 P20... 0 P2 (d times) = FQd Folded d-cube: d-cube with extra link on each long diagonal (see Table 4.4)
Cayley Graph Cay(Gn, Sm), where: Gn is a group with n elements { guI, g2,...
gn } and Sm, called generator set, is a subset of Gn with m elements: S. =h2,...{ hi, h2,.
hm} such that (cf.
am chap. 5):
(i) for any h E Sm E Sm (i.e. S. contains inverse of any of its elements) S, does not contain identity element (denoted as lo) gi of Gna Construction: Vertex set V of Cay(Gn,
Sm) is vat gi, g,.. .g 1 and the edge set is E (gõ
Oh), V i, s}. In words, each vertex g, is connected to m vertices g=Its for s=1..m.
Generating elements h, are called here "hops" since for identity element glFalo ("root node") their group action is precisely the single hop transition from the root node gi
to its 1-hop neighbors h1, h2,... hm E V(G).
* The requirement for inverse to be in S. applies to undirected Cayley graphs, not to directed graphs. The exclusion of identity S. applies to graphs that have no self-loops
of a node to itself (i.e. a vertex v v). These restrictions are not essential but mere conveniences of the 'preferred embodiment'.
Appendix A

https://patents.google.com/patent/CA2872831C/en?q=(high+radix)&oq=high+radix

26/31

2/14/26, 1:58 PM

CA2872831C - Flexible radix switch - Google Patents

The construction of Q3=Cay(D3,S3) is illustrated in Fig. 10. Group is the 8 element Dyadic group D3 and the 3 generators h1=001, h2=010 and h3=100 are shown with
arrows indicating the group action (X0Rs node labels with generators; all labels are in binary) on vertex v1=000. The resulting graph is a 3-cube.
Appendix A 107 E. Properties of Matrices This section lists several results about matrices (cf. [ID needed in LH
construction. All matrices below will be assumed to be real (rather than complex valued matrices).
Mt) Square nxn real matrix A is called normal matrix ([12] p. 100) if it satisfies relation:
AAT ATA
(2.40) This implies that any symmetrical (real) matrix S is normal matrix (since S=ST, hence SST=S2=STS).
M2) Any real, symmetrical nxn matrix [S] has n real eigenvalues At (i=1..n) and then corresponding orthonormal eigenvectors: vi) for i=1..n (cf. [n] p.101):
[S]Iv) = Ailvi) for i = 1..n (2.41) (vi Iv!) = j (2.42) M3) Since set {Ivi)} is a complete orthonormal set of vectors (a basis in Vi,), any [S] from (M2) can be diagonalized via an
orthogonal nxn matrix" [U] (orthogonal matrix is defined via condition [U][C]=In) which can be constructed as follows (applying eqs. (2.41)-(2.42)):
[U] I ei) (vi I (2.43) i=1 n [U] [S] [UT] ¨ I ei) (vi I ) [S] vi) (ei I = I
er.)(ei AA,' 1=1 j=1 = A1IeXetI (2.44) The final sum in (2.44) is a diagonalized form of [S], with Ar's along main diagonal and O's elsewhere.
M4) A set of m symmetric, pairwise commuting matrices .Fm { Sr: Sr St = St Sr for t, r = Lit}
is called commuting family (cf. [19] p. 51). For each commuting family Fin there is an Appendix A
orthonormal set of n vectors (eigenbasis in 14) { I v1)} which are simultaneously eigenvectors of all ST E Fm (cf. [19] p. 52).
M5) Labeling the n eigenvalues of the symmetric matrix S from (MI) as: Amin Ai <22 <=== <
An =='- A max, then the following equalities hold (Rayleigh-Ritz theorem, [19]
p. 176):
f(XISIX) Amin E Al = min _________ , for IX) E Vn and IX)* 01 (2.45) f(XISIX) 2-max = max _____ , for IX) E Vn and IX)* 0} (2.46) (X IX) Appendix A
References 1. Taming the Flying Cable Monster: A Topology Design and Optimization Framework for Data-Center Networks J. Mudigonda, P. Yalagandula, J.C. Mogul
(HP), (slides) USENIX ATC-11, June 14.2011, pp. 101-114 2. Network Topology Analysis D. S. Lee, J. L. Kalb (Sandia National Laboratories) Sandia Report SAND20080069, Jan 2008 3. Flattened butterfly: a cost-efficient topology for high-radix networks J. Kim, W. J. Daily, D. Abts (Stanford-Google), Proc. ISCA`07, May 2007, pp. 126-137
High-Radix Interconnection Networks J. Kim, PhD thesis, Stanford University, 2008.
4. High Performance Datacenter Networks: Architectures, Algorithms, and Opportunities D. Abts, J. Kim (Stanford-Google) Synthesis Lectures on Computer Architecture
#14, M & C Pub., 2011.
5. Energy Proportional Datacenter Networks D. Abts , M. Marty, P. Wells, P. Klausler, H. Liu (Google), Proc. ISCA`10, June 2010, pp. 338-347 6. BCube: A High Performance,
Server-centric Network Architecture for Modular Data Centers C. Guo, G. Lu, D. Li, H. Wu, X. Zhang, Y. Shi, C. Tian, Y. Zhang, and S. Lu (Microsoft) Proc. SIGCOMM, 2009,
pp. 63-74 7. MDCube: A High Performance Network Structure for Modular Data Center Interconnection H. Wu, G. Lu, D. Li, C. Guo, Y. Zhang (Microsoft) Proc. SIGCOMM,
2009, pp. 25-36 8. DCell: A Scalable and Fault-Tolerant Network Structure for Data Centers C. Guo, H. Wu, K. Tan, L. Shi, Y. Zhang, S. Lu (Microsoft) Proc. SIGCOMM, 2008,
pp. 75-86 9. HyperX: Topology, Routing, and Packaging of Efficient Large-Scale Networks J. H. Alm, N. Binkert, M. M. L. Al Davis, R. S. Schreiber (HP) SCO9 Nov 2009, pp.
14-20 10. Technology-Driven, Highly-Scalable Dragonfly Topology J. Kim, W. J. Dally, S. Scott, D. Abts (Stanford-Google) Proc. ISCA'08, 2008, pp. 77-88 Appendix A
11. A Group-Theoretic Model for Symmetric Interconnection Networks S. B. Akers, B. Krislmamurthy IEEE Transactions on Computers, pp. 555-566, April, 1989 12.
Optimal network topologies: Expanders, Cages, Ramanujan graphs, Entangled networks and all that L. Donetti, F. Neri, M. A. Munoz May 2006, arXiv:cond-mat/0605565v2
[cond-mat.other]
url: http://arxiv.org/abs/cond-mat/0605565 13. Bra-Ket notation Wikipedia article (includes a link to the full text of Berkeley lecture notes), March 2012 url:
http://en.wikipedia.org/wiki/Bra-ket notation url: http://bohr.physics.berkele_y.edu/classes/221/1112/notes/hilbert.pdf 14. Matters Computational (Algorithms for
Programmers) Jorg Arndt (c) 2011, Springer, ISBN 978-3-642-14763-0 Dec 2010 online edition, url: http://www.jjj.de/fxt/fxtbook.pdf 15. The Theory of Error-Correcting
Codes F. J. MacWilliams, N. J. A. Sloane (c) 1977 by North-Holland Publishing Co., ISBN: 0-444-85009-0 16. Error Correction Coding, Mathematical Methods and
Algorithms T. K. Moon (c) 2005 by John Wiley & Sons, Inc., ISBN 0-471-64800-0 17. Code Tables A. E. Brouwer, M. Grassi Web repository 2012, url:
http://www.codetables.de/
18. Representation Theory of Finite Groups B. Steinberg (c) 2011, Springer, ISBN 978-1-4614-0775-1 19. Matrix Analysis R. A. Horn, C. R. Johnson (c) 1985 Cambridge
Univ. Press, 1990 edition, ISBN 0-521-30586-1 20. Compressive Sensing Resources C.S. web repository by Rice university http://dsp.rice.eduks 21. Ordered Orthogonal
Arrays and Where to Find Them R. Scharer University of Salzburg PhD thesis 2006 http://mint.sbg.ac.at/rudilprojects/corrected diss.pdf Appendix A
22. MinT Database (Digital Nets, Orthogonal Arrays and Linear Codes) W. C. Schinid, R. Schtirer url: http://mint.sbg.ac.at/index.php 23. Walsh Transforms, Balanced Sum
Theorems and Partition Coefficients over Multary Alphabets M. T. Iglesias, A. Verschoren, B. Naudts, C. Vidal Proc. GECCO '05 (Genetic and evolutionary computation),
2005 Appendix A

Patent Citations (11)
Publication number

Priority date

Publication date

Assignee

Title

WO1989012861A1 *

1988-06-20

1989-12-28

United States Department Of
Energy

Interconnection networks

US5684959A *

1995-04-19

1997-11-04

Hewlett-Packard Company

Method for determining topology of a network

CA2190425A1 *

1995-11-16

1997-05-17

Nicholas W. Dawes

Method of determining the topology of a network of objects

US6046988A *

1995-11-16

2000-04-04

Loran Network Systems Llc

Method of determining the topology of a network of objects

US5844887A *

1995-11-30

1998-12-01

Scorpio Communications
Ltd.

ATM switching fabric

US5793975A *

1996-03-01

1998-08-11

Bay Networks Group, Inc.

Ethernet topology change notification and nearest neighbor determination

US6697338B1 *

1999-10-28

2004-02-24

Lucent Technologies Inc.

Determination of physical topology of a communication network

JP4163023B2 *

2003-02-28

2008-10-08

三菱電機株式会社

Parity check matrix generation method and parity check matrix generation
apparatus

Family To Family Citations

https://patents.google.com/patent/CA2872831C/en?q=(high+radix)&oq=high+radix

27/31

2/14/26, 1:58 PM

CA2872831C - Flexible radix switch - Google Patents

US7369513B1 *

2003-05-16

2008-05-06

Cisco Technology, Inc.

Method and apparatus for determining a network topology based on
Spanning-tree-Algorithm-designated ports

DE60332501D1 *

2003-05-28

2010-06-17

Mitsubishi Electric Corp

NEW TRANSFER CONTROL METHOD AND COMMUNICATION DEVICE

US9109904B2 *

2007-06-28

2015-08-18

Apple Inc.

Integration of map services and user applications in a mobile device

Priority date

Publication date

Assignee

Title

US8300798B1

2006-04-03

2012-10-30

Wai Wu

Intelligent communication routing system and method

US9008510B1

2011-05-12

2015-04-14

Google Inc.

Implementation of a large-scale multi-stage non-blocking optical circuit switch

JP5790312B2 *

2011-08-25

2015-10-07

富士通株式会社

COMMUNICATION METHOD, COMMUNICATION DEVICE, AND
COMMUNICATION PROGRAM

KR20140088069A *

2011-10-26

2014-07-09

인터내셔널 비지네스 머신
즈 코포레이션

Optimising data transmission in a hypercube network

US9515920B2 *

2012-04-20

2016-12-06

Futurewei Technologies,
Inc.

Name-based neighbor discovery and multi-hop service discovery in
information-centric networks

US9473422B1 *

2012-05-09

2016-10-18

Google Inc.

Multi-stage switching topology

US8983816B2 *

2012-06-18

2015-03-17

International Business
Machines Corporation

Efficient evaluation of network robustness with a graph

US9197509B1

2013-03-15

2015-11-24

Google Inc.

Logical topology in a dynamic data center network

US9817933B2 *

2013-03-15

2017-11-14

The Regents Of The
University Of California

Systems and methods for switching using hierarchical networks

US9363204B2

2013-04-22

2016-06-07

Nant Holdings Ip, Llc

Harmonized control planes, systems and methods

US9246760B1

2013-05-29

2016-01-26

Google Inc.

System and method for reducing throughput loss responsive to network
expansion

CN103516613A *

2013-09-25

2014-01-15

汉柏科技有限公司

Quick message forwarding method

US9548960B2

2013-10-06

2017-01-17

Mellanox Technologies
Ltd.

Simplified packet routing

US9166692B1 *

2014-01-28

2015-10-20

Google Inc.

Network fabric reconfiguration

US10218538B1 *

2014-01-30

2019-02-26

Google Llc

Hybrid Clos-multidimensional topology for data center networks

US9678800B2

2014-01-30

2017-06-13

International Business
Machines Corporation

Optimum design method for configuration of servers in a data center
environment

NO2776466T3

2014-02-13

2018-01-20

CN105141434B

2014-05-26

2019-03-26

华为技术有限公司

Service chain fault detection method and device

US9729473B2

2014-06-23

2017-08-08

Mellanox Technologies,
Ltd.

Network high availability using temporary re-routing

US9703738B2 *

2014-06-24

2017-07-11

Palo Alto Research Center
Incorporated

Computing system framework with unified storage, processing, and network
switching fabrics incorporating network switches and method for making and
using the same

US9806994B2

2014-06-24

2017-10-31

Mellanox Technologies,
Ltd.

Routing via multiple paths with efficient traffic distribution

CN105337866B *

2014-06-30

2019-09-20

华为技术有限公司

A flow switching method and device

US9699067B2

2014-07-22

2017-07-04

Mellanox Technologies,
Ltd.

Dragonfly plus: communication over bipartite node groups connected by a
mesh network

CN104156282A *

2014-08-15

2014-11-19

上海斐讯数据通信技术有限
公司

System image file backup system and method

US9690734B2 *

2014-09-10

2017-06-27

Arjun Kapoor

Quasi-optimized interconnection network for, and method of, interconnecting
nodes in large-scale, parallel systems

* Cited by examiner, † Cited by third party

Cited By (73)
Publication number
Family To Family Citations

https://patents.google.com/patent/CA2872831C/en?q=(high+radix)&oq=high+radix

28/31

2/14/26, 1:58 PM

CA2872831C - Flexible radix switch - Google Patents

CN105704180B *

2014-11-27

2019-02-26

英业达科技有限公司

Configuration method and system of data center network

GB2557433B *

2014-12-24

2019-05-01

Airties Kablosuz Iletism
Sanayi Ve Disticaret As

Mesh islands

EP3975429A1

2015-02-22

2022-03-30

Flex Logix Technologies,
Inc.

Mixed-radix and/or mixed-mode switch matrix architecture and integrated
circuit

US9894005B2

2015-03-31

2018-02-13

Mellanox Technologies,
Ltd.

Adaptive routing controlled by source node

US9973435B2

2015-12-16

2018-05-15

Mellanox Technologies Tlv
Ltd.

Loopback-free adaptive routing

US9699078B1 *

2015-12-29

2017-07-04

International Business
Machines Corporation

Multi-planed unified switching topologies

KR102389028B1 *

2016-01-04

2022-04-22

한국전자통신연구원

Apparatus and method for high speed data transfer between virtual desktop

US9893950B2 *

2016-01-27

2018-02-13

International Business
Machines Corporation

Switch-connected HyperX network

US10819621B2

2016-02-23

2020-10-27

Mellanox Technologies Tlv
Ltd.

Unicast forwarding of adaptive-routing notifications

US10225185B2

2016-04-18

2019-03-05

International Business
Machines Corporation

Configuration mechanisms in a switchless network

US10225153B2 *

2016-04-18

2019-03-05

International Business
Machines Corporation

Node discovery mechanisms in a switchless network

US10218601B2

2016-04-18

2019-02-26

International Business
Machines Corporation

Method, system, and computer program product for configuring an attribute for
propagating management datagrams in a switchless network

US10178029B2

2016-05-11

2019-01-08

Mellanox Technologies Tlv
Ltd.

Forwarding of adaptive routing notifications

JP6623939B2 *

2016-06-06

2019-12-25

富士通株式会社

Information processing apparatus, communication procedure determination
method, and communication program

US9780948B1 *

2016-06-15

2017-10-03

ISARA Corporation

Generating integers for cryptographic protocols

CN106126315A *

2016-06-17

2016-11-16

广东工业大学

A kind of virtual machine distribution method in the data center of
minimization communication delay

US10034407B2 *

2016-07-22

2018-07-24

Intel Corporation

Storage sled for a data center

US10225103B2

2016-08-29

2019-03-05

Vmware, Inc.

Method and system for selecting tunnels to send network traffic through

US10681131B2

2016-08-29

2020-06-09

Vmware, Inc.

Source network address translation detection and dynamic tunnel creation

CA3038147A1

2016-09-26

2018-03-29

Nant Holdings Ip, Llc

Virtual circuits in cloud networks

CN106533777B *

2016-11-29

2018-08-10

广东工业大学

Method and system are determined based on the intelligent transformer
substation information flow path of matrix ranks

US10263883B2 *

2016-12-14

2019-04-16

International Business
Machines Corporation

Data flow configuration in hybrid system of silicon and micro-electromechanical-switch (MEMS) elements

US10200294B2

2016-12-22

2019-02-05

Mellanox Technologies Tlv
Ltd.

Adaptive routing based on flow-control credits

US10614055B2 *

2016-12-29

2020-04-07

Emc Ip Holding Cimpany
Llc

Method and system for tree management of trees under multi-version
concurrency control

JP6834771B2 *

2017-05-19

2021-02-24

富士通株式会社

Communication device and communication method

US10862755B2 *

2017-06-30

2020-12-08

Oracle International
Corporation

High-performance data repartitioning for cloud-scale clusters

CN109327409B *

2017-07-31

2020-09-18

华为技术有限公司

Data center network DCN, method and switch for transmitting traffic in DCN

US10855656B2 *

2017-09-15

2020-12-01

Palo Alto Networks, Inc.

Fine-grained firewall policy enforcement using session app ID and endpoint
process ID correlation

US10931637B2 *

2017-09-15

2021-02-23

Palo Alto Networks, Inc.

Outbound/inbound lateral traffic punting based on process risk

FR3076142A1 *

2017-12-21

2019-06-28

Bull Sas

METHOD AND SERVER OF TOPOLOGICAL ADDRESS ALLOCATION TO
NETWORK SWITCHES, COMPUTER PROGRAM AND CLUSTER OF
CORRESPONDING SERVERS

US12063273B2

2018-02-05

2024-08-13

Microsoft Technology
Licensing, Llc.

Server system

https://patents.google.com/patent/CA2872831C/en?q=(high+radix)&oq=high+radix

29/31

2/14/26, 1:58 PM

CA2872831C - Flexible radix switch - Google Patents

US10809926B2

2018-02-05

2020-10-20

Microsoft Technology
Licensing, Llc

Server system

CN110139325B *

2018-02-09

2021-08-13

华为技术有限公司

A kind of network parameter tuning method and device

US10644995B2

2018-02-14

2020-05-05

Mellanox Technologies Tlv
Ltd.

Adaptive routing in a box

US11005724B1

2019-01-06

2021-05-11

Mellanox Technologies,
Ltd.

Network topology having minimal number of long connections among groups
of network elements

JP7437722B2

2019-01-31

2024-02-26

コネクトフリー株式会社

Data transmission method, communication processing method, device, and
communication processing program

CN110719170B *

2019-08-30

2021-04-16

南京航空航天大学

A Bit-Level Image Encryption Method Based on Compressed Sensing and
Optimized Coupled Image Lattice

US11184245B2

2020-03-06

2021-11-23

International Business
Machines Corporation

Configuring computing nodes in a three-dimensional mesh topology

US10812264B1 *

2020-04-30

2020-10-20

ISARA Corporation

Traversing a zigzag path tree topology in a supersingular isogeny-based
cryptosystem

US12380360B2 *

2020-05-26

2025-08-05

Nec Corporation

Interpretable imitation learning via prototypical option discovery for decision
making

US11948077B2 *

2020-07-02

2024-04-02

Dell Products L.P.

Network fabric analysis

US11575594B2

2020-09-10

2023-02-07

Mellanox Technologies,
Ltd.

Deadlock-free rerouting for resolving local link failures using detour paths

US11411911B2

2020-10-26

2022-08-09

Mellanox Technologies,
Ltd.

Routing across multiple subnetworks using address mapping

US11870682B2

2021-06-22

2024-01-09

Mellanox Technologies,
Ltd.

Deadlock-free local rerouting for handling multiple local link failures in
hierarchical network topologies

US11765103B2

2021-12-01

2023-09-19

Mellanox Technologies,
Ltd.

Large-scale network with high port utilization

US12155563B2

2022-09-05

2024-11-26

Mellanox Technologies,
Ltd.

Flexible per-flow multipath managed by sender-side network adapter

US12328251B2

2022-09-08

2025-06-10

Mellano Technologies, Ltd.

Marking of RDMA-over-converged-ethernet (RoCE) traffic eligible for adaptive
routing

US12452194B2 *

2023-07-03

2025-10-21

Amazon Technologies, Inc.

Network architecture with harmonic connections

* Cited by examiner, † Cited by third party, ‡ Family to family citation

Similar Documents
Publication

Publication Date

Title

CA2872831C

2019-10-29

Flexible radix switch

EP2691866B1

2019-07-31

Data transmission system using a network transpose box

CA2812321C

2017-01-24

Transpose box based network scaling

CA2811236C

2015-11-24

Transpose boxes for network interconnection

Lebiednik et al.

2016

A survey and evaluation of data center network topologies

US9363208B1

2016-06-07

Logical switches

Dominicini et al.

2020

Polka: Polynomial key-based architecture for source routing in network fabrics

Zhao et al.

2017

Scalable SDN architecture with distributed placement of controllers for WAN

US20110202682A1

2011-08-18

Network structure for data center unit interconnection

US9166886B1

2015-10-20

Systems and methods for determining physical network topology

CN113938434A

2022-01-14

Large-scale high-performance RoCEv2 network construction method and system

US10218538B1

2019-02-26

Hybrid Clos-multidimensional topology for data center networks

HK1196190A

2014-12-05

Flexible radix switching network

HK1196190B

2021-02-11

Flexible radix switching network

https://patents.google.com/patent/CA2872831C/en?q=(high+radix)&oq=high+radix

30/31

2/14/26, 1:58 PM

CA2872831C - Flexible radix switch - Google Patents

Castillo

2021

A comprehensive DCell network topology model for a data center

Tomic

2013

Network Throughput Optimization via Error Correcting Codes

Huang et al.

2012

SCautz: a high performance and fault-tolerant datacenter network for modular datacenters

Wang

2015

Bandwidth-efficiency-oriented topology optimization for integrated switching systems based on circulant graphs

Csernai

2015

Efficient Cabling in Data Center Networks

US9537714B1

2017-01-03

Randomized rotation striping for direct connect networks

Li

2016

On the design and analysis of cloud data center network architectures

Bruno et al.

2015

Workshop program (SmartVehicles 2015)

Priority And Related Applications
Applications Claiming Priority (5)
Application

Filing date

US201161483687P

2011-05-08

US201161483686P

2011-05-08

US61/483,687

2011-05-08

US61/483,686

2011-05-08

PCT/US2012/036960

2012-05-08

Title

Flexible radix switching network

Legal Events
Date

Code

Title

Description

2017-04-24

EEER

Examination request

Effective date: 20170413

Concepts
machine-extracted

Download

Name

Image

method

claims,abstract,description

87

0.000

matrix material

claims,description

98

0.000

transfer

claims,description

17

0.000

improved effect

abstract,description

12

0.000

data storage

abstract,description

5

0.000

Sections

Count

Filter table

Query match

Show all concepts from the description section

Data provided by IFI CLAIMS Patent Services

About

Send Feedback

Public Datasets

Terms

https://patents.google.com/patent/CA2872831C/en?q=(high+radix)&oq=high+radix

Privacy Policy

Help

31/31

2/14/26, 1:59 PM

CN110708241B - Method for routing packets in a distributed direct interconnect network - Google Patents

Patents
Back to results

13 of 125,048

high radix

(high radix);

Method for routing packets in a distributed direct interconnect network
Abstract

translated from Chinese

CN110708241B
The present invention provides a method and apparatus for routing data packets across a ring or

China

high radix topology with low latency, increased throughput and traffic distribution to avoid hot spot
development. A method of routing packets from a source node to a destination node in a distributed

Download PDF

Find Prior Art

Similar

direct interconnect network is disclosed, the method comprising the steps of: discovering all nodes
and associated ports; updating a database to include the nodes and ports in the network topology ;
compute the shortest path from each output port on each node to every other node in the topology;
segment each packet into flakes at the output port of the source node; as the flakes are
segmentation, using wormhole switching to distribute the flits along the shortest path from each
output port on the source node to the destination node, whereby the packets follow replacements in

Other languages: Chinese
Inventor: D.奥普雷亚, A.卡塔纳, U.纽斯塔特
Current Assignee : Lockport Networks Inc

the network topology The maximum disjoint routes of are distributed; and the packets are
reassembled and reordered at the destination node so that the packets conform to their original
order/form.

Worldwide applications
2012 NO 2015 AU DK US CN EP WO CA BR JP CN KR
2016 IL 2018 US 2020 US

Application CN201910860198.3A events
2015-02-13

Application filed by Lockport Networks Inc

2020-01-17

Publication of CN110708241A

2021-10-08

Application granted

2021-10-08

Publication of CN110708241B

Status

Expired - Fee Related

2035-02-13

Anticipated expiration

Info: Patent citations (43), Cited by (46), Legal events, Similar
documents, Priority and Related Applications
External links: Espacenet, Global Dossier, Discuss

Images (23)

Classifications
H04L45/24 Multipath

View 5 more classifications

Landscapes

Engineering & Computer Science
https://patents.google.com/patent/CN110708241B/en?q=(high+radix)&oq=high+radix&page=1

1/15

2/14/26, 1:59 PM

CN110708241B - Method for routing packets in a distributed direct interconnect network - Google Patents

Computer Networks & Wireless Communication
Show more

Hide Dependent

Claims (11)

translated from Chinese
1. A computer-implemented method of routing packets from a source node to a destination node in a direct interconnect network, the method comprising the steps of: Discover all
nodes in the network topology and all output ports on each node; including nodes and output ports discovered in the network topology in a topology database to allow the nodes
and ports to be included in path routing calculations; computing a path from each output port on each node to each other node in the network topology based on those nodes and
output ports contained in the topology database, wherein each such path does not intersect each other; generating a source routing database at each node containing the shortest
maximally disjoint paths from each output port on each node to all other nodes in the network topology; receiving a packet at the source node; The received packet is sent to a
selected one of the output ports of the source node in a round-robin or weighted round-robin manner, whereby the received packet is thereafter followed along from the selected
output port on the source node. The shortest maximally disjoint path of the output port to the destination node is distributed. 2. The computer-implemented method of claim 1,
wherein the step of computing the path from each output port on each node to each other node in the network topology comprises computing a node having no nodes in common
Disjoint paths. 3. The computer-implemented method of claim 1, wherein computing the path from each output port on each node to each other node in the network topology
comprises computing no links in common disjoint paths. 4. A computer-implemented method of routing packets from a source node to a destination node in a direct interconnect
network, the method comprising the steps of: Discover all nodes in the network topology and all output ports on each node; including nodes and output ports discovered in the
network topology in a topology database to allow the nodes and ports to be included in path routing calculations; computing a path from each output port on each node to each
other node in the network topology based on those nodes and output ports contained in the topology database, wherein each such path does not intersect each other; generating a
source routing database on each node containing a plurality of shortest maximally disjoint paths from each output port on each node to all other nodes in the network topology;
receiving a plurality of packets at the source node; The received plurality of packets are sent to the output port of the source node in a round-robin or weighted round-robin manner,
whereby the received plurality of packets thereafter follow along from the output port on the source node to the output port of the source node. The plurality of shortest maximally
disjoint paths of the destination node are distributed such that the plurality of packets are distributed thereby along a plurality of alternate routes in the network topology. 5. The
computer-implemented method of claim 4, wherein computing the path from each output port on each node to each other node in the network topology comprises computing a
node having no nodes in common Disjoint paths. 6. The computer-implemented method of claim 4, wherein computing the path from each output port on each node to each other
node in the network topology comprises computing no links in common disjoint paths. 7. The computer-implemented method of claim 4, wherein the packets are further reordered
at the destination node such that the packets conform to the original order in which they were received at the source node. 8. A computer-implemented method of routing packets
from a source node to a destination node in a direct interconnect network, the method comprising the steps of: Discover all nodes in the network topology and all output ports on
each node; including nodes and output ports discovered in the network topology in a topology database to allow the nodes and ports to be included in path routing calculations;
computing a path from each output port on each node to each other node in the network topology based on those nodes and output ports contained in the topology database,
wherein each such path does not intersect each other; generating a source routing database at each node containing the shortest maximally disjoint paths from each output port
on each node to all other nodes in the network topology; receiving a packet at the source node; The received packets are sent to the output port of the source node in a round-robin
or weighted round-robin manner, whereby each of the received packets is thereafter segmented into microarrays at the output port of the source node. slices and are distributed
along the shortest maximally disjoint path from the output port on the source node to the destination node, so that the packet thus follows an alternate route in the network
topology and distributed. 9. The computer-implemented method of claim 8, wherein computing the path from each output port on each node to each other node in the network
topology comprises computing no links in common or disjoint paths of nodes. 10. The computer-implemented method of claim 9, wherein the flit is forwarded to the destination
node by using a wormhole exchange. 11. The computer-implemented method of claim 10, wherein the flits are further reassembled into packets and reordered at the destination
node such that the packets conform to their receipt at the source node The initial form and order of .

Description
Method for routing packets in a distributed direct interconnection network
This application is a division of a patent application entitled "method of routing packets in a distributed direct interconnect network" of chinese patent application No.
201580019119.6 filed on 13/2/2015.
Technical Field
The invention relates to a computer network for interconnecting a data center and a cloud data center server. In particular, the present invention relates to a method for
routing packets in a direct interconnection network implemented on a ring (torus) or high radix topology such as a "dragonfly" wiring structure.
Background
The term Data Center (DC) generally refers to a facility for housing large computer systems (often contained on racks housing devices) and their associated components,
all interconnected by a large number of structured cable circuits. Cloud Data Center (CDC) is a term used to refer to a large, often backup, facility that similarly stores
data of an entity.
A network switch is a computer networking device that links network devices for communication/processing purposes. In other words, a switch is a telecommunication
device that is capable of receiving a message from any device connected thereto and sending the message to a specific device to which the message is to be relayed.
Network switches are also commonly referred to as multi-port bridges that process and route data. Here, by port we mean the interface between the switch and the
computer/server/CPU to which it is attached (the egress for the cable or plug).
Today, DC and CDC typically implement data center networking using a set of two-tier switches. A two-layer switch processes and routes data at layer 2 (the data link
layer), which is the protocol layer that transports data between nodes (e.g., servers) on the same local area network or between adjacent nodes in a wide area network.
However, a key issue to solve is how to build a large-capacity computer network that can carry a very large aggregate bandwidth (thousands of TBs) containing a very
large number of ports (thousands), requires minimal structure and space (i.e., minimizes the need for a large room to accommodate a large number of cabinets with card
racks), and is easily scalable and can help to minimize power consumption.
Conventional network topology embodiments are based on completely independent switches organized in a hierarchical tree structure as shown in fig. 1. The core switch
2 is a very high speed low count port with very large switching capacity. The second tier is implemented using aggregation switches 4 (medium capacity switches with a
large number of ports), whereas the third tier is implemented using lower speed, large port count (e.g., forty/forty-eight), low capacity edge switches 6. Typically the edge
switches are layer 2, whereas the aggregation ports are layer 2 and/or layer 3, and the core switches are typically layer 3. This embodiment provides any server 8 to any

https://patents.google.com/patent/CN110708241B/en?q=(high+radix)&oq=high+radix&page=1

2/15

2/14/26, 1:59 PM

CN110708241B - Method for routing packets in a distributed direct interconnect network - Google Patents

server connectivity in up to six hop links (three hops up to the core switch 2 and three hops down to the destination server 8) in the example provided. Such a hierarchy is
also typically duplicated for redundancy reliability purposes. For example, referring to fig. 1, if the rightmost edge switch 6 fails without duplication, there is no
connectivity to the rightmost server 8. At a minimum, core switch 2 is duplicated because failure of core switch 2 will result in total datacenter connectivity failure. For
obvious reasons, this approach has significant limitations in addressing the challenges of future CDC. For example, because each switch is fully self-contained, this adds
complexity, significant floor space utilization, complex cabling that is prone to human error, and manual switch configuration/provisioning, as well as increased energy
costs.
However, many attempts have been made to improve switching scalability, reliability, capacity and latency in data centers. For example, efforts have been made to
implement more complex switching solutions by using a unified control plane (e.g., QFAbic System switches from Juniper Networks; see, e.g., http://
www.juniper.net/us/en/products services/switching/qfabric-system /), but such systems still use and maintain the traditional hierarchical architecture. Furthermore,
given the exponential increase in system users and the number of data to be stored, accessed, and processed, processing power has become the most important factor
when determining the performance requirements of computer network systems. While server performance continues to improve, one server is not powerful enough to
meet demand. This is why the use of parallel processing has become crucial. As a result, what is the dominant north american traffic now has become primarily eastwest traffic in many cases up to 80%. Despite this change in traffic flow, the network architecture has not evolved to be optimal for this model. It is thus still the topology
of the communication network (which interconnects the compute nodes (servers)) that determines the speed of interaction between CPUs during parallel processing
communications.
This need for increased east-west traffic has led to the creation of newer flatter network architectures (e.g., torus/torus networks). A ring interconnect system is a
network topology for connecting network nodes (servers) in a mesh fashion in a parallel computer system. The ring topology can have nodes arranged in 2, 3, or more (N)
dimensions that can be virtualized as an array, where processors/servers are connected to their nearest neighbor processors/servers, and where processors/servers on
opposite edges of the array are connected. In this way, each node has 2N connections in an N-dimensional ring configuration (fig. 2 provides an example of a 3-D ring
interconnect). Because each node in the ring topology is connected to neighboring nodes via short cabling, there is low network latency during parallel processing. In
fact, the ring topology provides access to any node (server) with a minimum number of hops. For example, a four-dimensional torus implementing a 3x 3x 3x4 structure
(108 nodes) requires an average of 2.5 hops in order to provide arbitrary-to-arbitrary connectivity. Fig. 4 provides an example of a 6x 62-D ring showing the minimum
number of hops required to go from corner node 1.6 to all the other 35 nodes. As shown, the number of hops required to reach any destination from node 1.6 can be
plotted as a bell curve with a peak at 3 hops (10 nodes) and typically a tail at 5 hops (4 nodes) and 1 hop (4 nodes), respectively.
Unfortunately, large ring network embodiments have not been practical for commercial deployment of DC or CDC, as large embodiments may take months to build,
cabling may be complex (2N connections for each node), and their modification may be costly and cumbersome if expansion is necessary. However, the implementation
of ring topologies in supercomputers has been very successful, given the need for processing power has outweighed the commercial drawbacks. In this regard, IBM's
blue-gene supercomputer provides an example of a 3-D ring interconnect network, where 64cabinets hold 65,536nodes (131,072 CPUs) to provide billions of floating
point operations per second (petaflops) processing capability (see fig. 3 of the inset), whereas fuji's primhpc FX10 supercomputer system is an example of a 6-D ring
interconnect that is held in 1,024 racks that include 98,304 nodes. While the above examples deal with ring topologies, they are equally applicable to other flat network
topologies.
The present invention deals more specifically with the important issue of data packet traversal and routing from node to node in a ring or high radix network structure. In
this regard, it is a route that determines the actual path taken by a packet of data in the network to go from a source to a destination. For the purposes of this document,
delay refers to the time it takes for a packet to reach a destination in a network, and is typically measured from when the header reaches the input of a source node to
when it reaches the input of the destination node. Hop count refers to the number of links or nodes traversed (passing) between the source and destination and
represents an approximation for determining delay. Throughput is the data rate accepted per input port/node of the network measured in bits per second (bits/sec).
A useful goal when routing is to distribute traffic evenly among the nodes (load balancing) in order to avoid hot spot development (paths or node areas where
usage/demand has exceeded a desired or acceptable threshold) and to minimize contention (when two or more nodes attempt to send messages or packets on the
same wire or path at the same time), thereby improving network latency and throughput. The route chosen for this affects the number of hops from node to node and
may potentially even thereby affect energy consumption when the route is not optimized.
The topology of the network operation also clearly affects the delay, as the topology affects the average minimum hop count and distance between nodes. For example,
in a ring, there are not only several paths that a packet can take in order to reach a destination (i.e., in a ring where there is "path diversity"), but also multiple minimum
length paths between any source and destination pair. By way of example, FIG. 5 shows an example of three minimum routes (3 hops) that a packet can take in order to
go from node S11 to node D12 in a 2-D circular mesh, while also showing a longer fourth route of 5 hops. Path computation by routing is done essentially based only on
topology-source routing is dynamically based on packet source-destination pairs on a hop-by-hop basis.
Routing methodologies that utilize path diversity have better fault tolerance and better load balancing in the network. However, routing methodologies do not always
achieve such a goal and can be generally divided into three categories: deterministic, irrelevant and adaptive. Deterministic routing refers to the fact that the route
between a given pair of nodes is determined in advance, regardless of the current state of the network (i.e., regardless of network traffic). Dimension Order Routing (DOR)
is an example of deterministic routing, where all messages from node a to node B will always traverse the same path. In particular, messages traverse dimension-bydimension (X-Y routing) to reach a vertical coordinate matching their destination in one dimension before switching to the next dimension. As one example, fig. 6 can be
used to illustrate DOR, where a packet first travels along a first dimension (X) as far as needed from nodes 1 to 5 to 9, followed by a second dimension (Y) to destination
node 10. Although such routing is generally easy to implement and deadlock free (deadlock refers to a situation where there is a dead cycle along the path from the
source to the destination), path diversity is not exploited and therefore load balancing is poor.
"independent" routing algorithms are those in which routing decisions are made randomly, regardless of the current state of the network (deterministic routes are a
subset of independent routes). Although this means that the unrelated route may be simple to implement, it is not suitable for traffic and network situations. An example
of a well-known method of independent routing is the Valiant algorithm (known to those skilled in the art). In this method, a packet sent from node a to node B is first
sent from a to a randomly selected intermediate node X (one hop), and then from X to B. Referring again to fig. 6, the Valiant algorithm can randomly select node 2 as an
intermediate node from source node 1 to destination node 10, meaning that path 1-2-3-7-11-10 can be used for routing purposes, for example. This typically randomizes
any traffic pattern and because all patterns appear to be uniformly random, the network load is quite balanced. In fact, the Valiant algorithm has been considered to be
able to load balance for any traffic pattern, typically on almost any topology. However, one problem is that Valiant routes are typically non-minimal (the minimal route is
the path that requires the minimum number of hops between the source and destination), which often results in significant hop count increases, and this further
increases network latency and can potentially increase energy consumption. However, there are exceptions, such as in the least congested networks. Non-minimal routes
may significantly increase latency and may increase power consumption as additional nodes are traversed; on the other hand, in networks experiencing congestion, nonminimal routes may actually help avoid nodes or hotspots, and thus actually result in lower latency.
If a minimum route is desired or necessary, the Valiant algorithm can be modified to limit its random decision to the minimum route/shortest path by specifying that the
intermediate node must be located within the minimum quadrant. As an example, referring to fig. 6, the smallest quadrant of node 1 (with the destination of node 10) will
contain nodes 2, 5, and 0, and will result in a common hop count of 3.

https://patents.google.com/patent/CN110708241B/en?q=(high+radix)&oq=high+radix&page=1

3/15

2/14/26, 1:59 PM

CN110708241B - Method for routing packets in a distributed direct interconnect network - Google Patents

The Valiant algorithm provides some level of path selection randomization that will reduce the likelihood of hotspot development. However, there is another difficulty in
affecting these efficiencies-Valiant routing is deadlock free only when used in conjunction with DOR routing, which itself fails in load balancing and hot spot avoidance.
Adaptive routing algorithms are those in which the routing decision is based on the state of the network or network traffic. They generally involve flow control
mechanisms and buffer occupancy is often used in this regard. Adaptive routing can employ global node information (which is cost-effective) or can use information
from only local nodes, including, for example, queue occupancy for measuring network congestion. The problem with using information from the local node alone is that
this can sometimes lead to sub-optimal choices. Adaptive routing may also be limited to the smallest path, or it can be fully adaptive (i.e., no restriction when taking the
shortest path) by taking a non-smallest path with the potential of livelock (i.e., a situation similar to a deadlock in which packet travel is not going to the destination; often
a result of a lack of resources). This can sometimes be overcome by allowing a certain number of misrouted packets per packet and by giving higher priority to packets
that are misrouted many times. Another problem with adaptive routing is that it may pose a problem for preserving data packet ordering-packets need to arrive at the
destination in the same order or else you need to implement a packet reordering mechanism.
Finally, it is important to mention that routing can be achieved by the source table or the local table. With the source table, the entire route is specified at the source and
can be embedded in the packet header. Delays are minimized because routes do not have to be looked up at each node or routed hop-by-hop. It is also possible to make
the source table specify a plurality of routes per destination to be able to manage failures, and to improve load balancing when routes are randomly selected (i.e.,
unrelated routes). With the local node table, on the other hand, a smaller routing table is employed. However, the next step the packet will take is determined at each
node, and this increases the per-hop delay.
The present invention seeks to overcome the deficiencies in the prior art and to improve upon the known methods of routing packets in a ring or high radix network
topology.
Disclosure of Invention
In one aspect, the present invention provides a novel method for routing data packets on a ring grid or high radix topology to provide low latency, increased throughput,
and avoidance of hotspots and deadlocks.
In yet another aspect, the present invention provides a method for achieving traffic decorrelation (randomization), increased efficiency in bandwidth allocation, load
balancing, and traffic engineering (traffic engineering) in a ring grid or high-radix architecture.
In one embodiment, the present invention provides a computer-implemented method of routing a packet from a source node to a destination node in a direct
interconnect network, the method comprising the steps of: discovering all nodes and all output ports on each node in the network topology; including nodes and output
ports discovered in the network topology in a topology database to allow the nodes and ports to be included in a shortest path routing calculation; calculating shortest
paths from each output port on each node to each other node in the network topology based on those nodes and output ports contained in the topology database;
generating, on each node, a source routing database containing shortest paths from each output port on said each node to all other nodes in said network topology;
receiving a packet at the source node; sending the received packets to an output port of the source node in a round-robin or weighted round-robin fashion, whereby each
of the received packets is thereafter fragmented into flits (flits) at the output port of the source node and distributed along a plurality of shortest paths from the output
port on the source node to the destination node such that the packets are thereby distributed along a plurality of alternate routes in the network topology; and reassembling and re-ordering the packets at the destination node such that the packets conform to their original form and order.
In another embodiment, the present invention provides a method of routing packets in which flits are forwarded to a destination node using wormhole (wormhole)
switching.
Drawings
Embodiments of the invention will now be described, by way of example, with reference to the accompanying drawings, in which:
FIG. 1 is a high-level view of a conventional data center network embodiment;
FIG. 2 is a diagram of a 3-dimensional torus interconnect having 8 nodes;
FIG. 3 is a diagram showing a hierarchy of IBM blue Gene processing units in a torus architecture;
FIG. 4 is a high level diagram showing 36nodes in a 6x 6 2-D folded ring of minimum number of hops required to reach any of 35 nodes starting from corner node (1.6);
FIG. 5 is a diagram of a 4x 42-D ring embodiment showing several routes from node S to node D on each port (x +, x-, y +, y-) of node S;
FIG. 6 is a diagram of a simple network topology to help illustrate how various routing algorithms can route packets from node to node;
FIG. 7 is a high level diagram of a high speed network interface card with all ports in accordance with the system of the present invention;
FIG. 8 is a high level diagram of a Host Network Interface (HNI) card showing the key functional blocks of the system in accordance with the present invention;
FIG. 9 is a high-level view of packet segmentation required to implement packet transfer across a ring mesh;
FIG. 10 shows a graph of routes and hop counts starting from each port on node S (2.4) to each port on node D (5.2) in a 36node 2-D ring structure;
FIG. 11 depicts a high level diagram of the presentation software components TD, SR, WS, FC in accordance with the present invention;
FIG. 12 is a flow chart illustrating a process of packet segmentation in accordance with the present invention;
FIG. 13 shows pseudo code for flit generation by packet segmentation;
FIG. 14 is a flowchart showing the process steps for preparing a source route from each port in accordance with the present invention;
FIG. 15 shows pseudo code implementing path computation according to the present invention;
FIG. 16 illustrates a source routing table format generated by path computation in accordance with the present invention;
FIG. 17 shows a flow chart of a process of transferring "flits" on each port in accordance with the invention;
FIG. 18 shows a wormhole exchange process for transferring "flits" on each port in accordance with the present invention;
FIG. 19 shows pseudo code implementing flit transfer as described in the flow diagram of FIG. 17 in accordance with the invention;

https://patents.google.com/patent/CN110708241B/en?q=(high+radix)&oq=high+radix&page=1

4/15

2/14/26, 1:59 PM

CN110708241B - Method for routing packets in a distributed direct interconnect network - Google Patents

FIG. 20 shows a flow diagram for discovering nodes or links that trigger a topology discovery process for topology updates;
FIG. 21 shows a flow chart for describing topology database updates;
FIG. 22 shows a flow chart of a process for node and/or link failure detection and recovery and a process for topology update; and
FIG. 23 illustrates pseudo code for implementing link and/or node failure detection in accordance with the present invention;
Detailed Description
The present invention utilizes a ring grid or high radix wiring to implement direct interconnect switching for data center applications. In particular, the present invention is
particularly useful with a ring system described in the related PCT patent application No. PCT/CA2014/000652 entitled "Method and Apparatus to Direct Interconnect
Switch with and Growth in Computer Networks", filed on 29/8/2014, the disclosure of which is incorporated herein by reference. Such an architecture can provide high
performance network interconnections with thousands of servers in a single switching domain.
The software controlling the switches follows a software defined network model. The switch control plane (switch OS) preferably runs on a separate server, but interacts
tightly with the distributed processing running on each PCIe card. Referring to fig. 11, key software components include Topology Discovery (TD), packet routing across
topologies (SR), Wormhole Switching (WS), Flow Control (FC) mechanisms, and switch state monitoring and switch management. The TD component is responsible for
discovering substantially the presence (or absence) of nodes in the circular mesh (and thereby allowing possible paths to be updated based on node presence); the SR
component is responsible for source route computation (all paths available from each port of the source node to any destination node); the WS component is
responsible for flit steering across the ring based on the path vector, which is included in the flit header, with minimum delay (pass-through model at each node); the FC
component is responsible for ring link utilization and controls messages to the source to regulate traffic through the input queuing mechanism; the switch state
monitoring component is responsible for traffic balancing, traffic rerouting under fault conditions, and support for switch growth; and the switch management component
is responsible for maintaining a complete image of the switch, including all interconnected servers, all interconnected I/O links to the outside world, and connectivity
status.
The present invention relates generally to the novel topology discovery (TD component) and routing protocol (SR component) of the present invention-particularly useful
for implementing the key functions on the novel switch architecture disclosed in PCT patent application No. PCT/CA 2014/000652. While the conventional
implementation of routing data packets through a ring structure makes the implicit assumption that traffic is distributed randomly across destination nodes, practical
embodiments have actually exhibited switch failure in the case of a single source/single destination continuous packet flow at maximum capacity due to the
development of hotspots that propagate across rings and have the ability to completely disable the ability to transport packets across the ring mesh. The present
invention therefore makes more specific reference to route optimization and implementation for direct interconnect network topologies such as torus and high radix
networks in order to solve performance problems and hot spot development for any type of traffic, whether large flows from any source to any destination (highly
correlated) or traffic to randomly distributed destinations (typical internet traffic). In contrast to known methods of routing packets in a ring as previously described
above, the present invention describes a method of discovering and updating all nodes in a topology, and a method of routing packets to each destination node, where
the shortest path is computed at each node on each output port (e.g., x +, x-, y +, y-, z +, z-, w +, and w-) and whereby packets are distributed (i.e., facilitating multiple
alternate paths) in a round-robin or weighted round-robin fashion along the path from each output port to increase bandwidth and traffic distribution to avoid hot spots.
Flow Control (FC) mechanisms are used to change the output port selection for packet routing from a simple round-robin continuum of source node output port
selections (i.e., first port x +, then x-, then y +, y-, z +, z-, w +, and w-) to a weighted round-robin that is skipped for links identified by FC as busy, or the traffic on those ports
is reduced to a low percentage (e.g., 10%) of the maximum link capacity.
Data packet reordering is implemented at the node destination. Such a method can particularly utilize the novel methods of node (server) discovery discussed in PCT
patent application No. PCT/CA2014/000652 and discussed further herein. The present invention thus addresses the problem of packet routing optimization for direct
interconnect network topologies such as ring or high radix networks to address performance issues and hot spot development for any type of traffic, whether that be
large flows from any source to any destination (highly correlated) or traffic to randomly distributed destinations (typical internet traffic).
First, fig. 7 shows a high-level block diagram of a high-speed network interface card 10 having all of the following ports: an I/O port 13(N-S interface) for accessing the
outside world, a 4D ring interconnect port 14, a PCIe port 15 for accessing computer servers, and controls 19. Those skilled in the art will appreciate that the present
invention is capable of working with HNI cards having fewer or more ports and in association with other ring or high radix topologies (such as "dragonfly," for example).
Fig. 8 shows a high-level block diagram of an HNI card 10 of a system according to the present invention with key functional blocks including a processor 31 with RAM
37 and ROM 38, a switching block 32, physical interface devices 36, ring interconnect ports 14, I/O ports 13, and PCIe ports 15. The physical implementation of this
embodiment is based on a multi-core processor with attached memory and a PHY device for implementing a ring interconnect. The switching function is implemented in
software using the hardware accelerated assist capability of the processor to support multiple queue manipulations. Those skilled in the art will appreciate that the
present invention will also work in situations where the physical implementation is different (i.e., where the switching function is implemented in an FPGA or ASIC), or the
processor need not reside on the same card as that used for the switching function.
Fig. 10 shows a simple 2D ring wiring diagram for use in explaining packet routing according to the present invention. As shown, the structure is a folded 2D loop in
which the length of each connection is equal throughout. Each node in this figure represents a server interconnected via a PCIe switch card (as described above) housed
in the server. The figure shows the shortest maximally disjoint path (no common links or nodes) starting in each dimension (x +, x-, y +, y-) from the port of source node S
(2.4) to destination node D (5.2) as calculated at source node S. In this regard, fig. 10 shows the three calculated paths and routing vectors as having a minimum hop
count of 5 hops (from (S) 2.4: (x +, y-); (y-, x +), from (S) 2.4: (x-, y-, x-), and from (S) 2.4: (x-, y-, x-), whereas the other path is calculated as a non-minimum hop count of 7
hops (from (S) 2.4: (y +, x +, y +)) assuming there is a large traffic volume between the source S node and the D node, the packets/flits will be sent to the destination node
on the shortest path from each source S output port to node D along the path from each output port in a round robin or weighted round robin fashion, thereby on the one
hand using multiple paths to increase bandwidth and distributing traffic to avoid hot spot development (weighted round robin selection of output ports at the source node
will increase the number of packets along a particular path). A second alternate route may also be calculated and stored at each node, whereby, in addition to the first
hop, the route provides an alternate, separate path to help overcome network congestion and hot spots in the event of a link or node failure, if necessary. The method
distributes traffic across a ring or high radix network for load balancing purposes, but is capable of introducing packet reordering. For example, because the path starting
on y + (fig. 10) is 2 hops longer than the minimum path (5 hops), all packets sent on this path will arrive with additional delay that may create packet reordering problems.
According to the invention, the packets must therefore be reordered at the destination node. This is within the skill of the person skilled in the art.
With reference to fig. 9, which shows a high-level view of packet segmentation, it is important to note that packets traversing a direct interconnect ring or high radix are
segmented into smaller sized packets (flits) during routing. Packet segmentation is required across the ring grid in order to avoid "end-of-line blocking" (a common
situation in packet networks using common resources (links and buffers)). The flit size is determined by the traffic type in order to minimize overhead tax, and the actual
value ranges between 64 bytes and 512 bytes. It is important to note that the present invention is capable of handling ethernet packets, Infiniband packets, PCIe flows, or
NVMe transactions, among others. The flits spanning the ring structure carry traffic transparently. The external interface I/O port 13 is specific to the type of traffic and
terminates user traffic, transparently transferring data across the ring via "flits" to the destination PCIe port 15. It is important to note that the present invention is capable

https://patents.google.com/patent/CN110708241B/en?q=(high+radix)&oq=high+radix&page=1

5/15

2/14/26, 1:59 PM

CN110708241B - Method for routing packets in a distributed direct interconnect network - Google Patents

of handling ethernet packets that span a ring (via ethernet flits). A key advantage herein is the variable size of the flits because flits of ethernet packets have a packet
demarcation mechanism. Thus, the flit size is variable from 64 bytes (minimum ethernet packet size) to 512 bytes (payload and header). If the packet is larger than 512
bytes, the system will segment the original packet into flits of 512 bytes: head microchip, body microchip and tail microchip.
FIG. 12 provides a flow chart of a method of presenting packet fragments. Upon receiving a new packet from the host (i.e., PCIe interface) (step 402), the HNI processor
extracts the header in order for the processor to analyze the destination (step 404) and routing path of the packet. At this stage, the packet enters the fragmentation
loop. The byte counter is initialized (step 412) and the end of packet-EOP-parser (step 405) detects the EOP and if the counter is less than or equal to 476 bytes (i.e., the
payload is less than or equal to 476 bytes), the packet is sent to the flit generator (step 407), the header generator (step 408), and the priority decision block (step 409).
The flit is thereafter inserted into an assigned egress queue (step 410) for being sent to the next hop on the circular mesh, and we note whether a new packet has been
received (402). The byte counter is incremented after each individual flit (step 411), the packet fragment is sent to the flit generator (step 407) and the process follows
the steps described above until the last flit in the packet arrives and is sent. As a key observation, the flits on the ring link are preferably 512 bytes followed by 8 "free"
bytes (476 bytes of payload and 36 bytes of header). If the packet is shorter than 476 bytes, the flit (typically the tail flit) will have less than 512 bytes and packet
demarcation will be done using the "free" bytes. FIG. 13 provides pseudo code for flit generation via packet segmentation.
At the destination, the flits are assembled into a packet by stripping the ring header and using a "byte count" for the flit's offset in the original packet. Those skilled in the
art will appreciate that a per-source packet queue is implemented for the destination node, an ordered list of packet fragments is created, and the packets are released
once fully assembled in the original order. Fig. 14 shows a flow chart of the presence route calculation according to the invention. The present invention also provides
pseudo code at fig. 15 for computing all paths (source routes) from a source node to any destination node that are maximally disjoint (no common links and nodes) on
each output port of the source node. The topology upgrade function 412 (which discovers the active servers/nodes and their active ports) will initiate routing/path
computation (414) as soon as an update is detected (i.e., whenever a node has been added/detected or there is a critical problem/failure associated with it). As
described in step 419 (find first destination node in the topology list) followed by steps 420 and 421 (search all destination nodes in the topology list), the Dijkstra
algorithm will be used to calculate the shortest path between the current node from the topology and each destination node according to the present invention (step
415). In step 422, all path costs affected by step 417 are reset to initial values. The computed shortest path from source to destination will be stored in the source routing
database and it will be removed from the further shortest path computation (to provide disjoint paths) by increasing the cost on all links belonging to the stored path to
infinity (417) -this will have the practical effect of removing nodes and links from the topology database to force no longer use for disjoint path selection purposes. This
process continues until all output ports are to be used and all possible paths between the source node and the destination node are stored in the database. The result is
then summarized in the output routing table (418) as a routing vector for each node, where the route will start first from the shortest path. The shortest path computation
itself can be implemented using Dijkstra's algorithm (a well-known method used in routing protocol implementations). It is particularly novel herein that the shortest path
is determined at each output port and used accordingly.
Fig. 16 presents the format of the route output table. Each route has a number of hops representing a particular path and a list/vector of attachment numbers of output
ports to be selected on each hop. In particular, FIG. 16 presents an example snapshot of the output table of routes in a 4-D torus (8x8x8x8) with 8 ports (x +, x-, y +, y-, z +,
z-, w +, and w-) from a source node number 1000to a destination node 0 (the use of node 1000 and node 0 are for example purposes only). It should be noted that the
first output selection in the set of 8 routes (i.e., x +, y +, z +, w +, x-, y-, z-, and w-) in the first set of 8 routes corresponds, for example, to each output port of the source
node. The 8 paths are completely disjoint and have different lengths: 5 hops, 7 hops, or 9 hops. The most important feature is path distribution across the ring enabling
traffic distribution and predictable performance. The second set of 8 paths in fig. 16 represents a second alternative to routing paths. This method can compute many
alternative routes, but the distance (hop count) will become larger and larger. For practical reasons, the method may store the first option of typically only 8 different
paths as needed.
Fig. 17 shows a flow chart representing steps used in packet switching according to the present invention.
As new packets are pushed into the output queue, the packet transfer state machine initiates transfers across the ring. In this regard, the (in query) output port "k" (port 1
to port 8, i.e., the output ports discussed above: x +, y +, z +, w +, x-, y-, z-, and w-) is used to transfer flits following the wormhole model (as shown in FIG. 18): the head flit
and subsequent flits are incremented (body flits) and will be transmitted along the same path from port k until the end of the packet is reached (tail flit) as shown in fig.
17. Once the end of a packet is reached, the counter k is incremented to indicate the next output port and the next packet with the next packet identifier (designated as
the next ordered packet) is transmitted on the ring on a flit-by-flit basis using this wormhole switching model. Once the end of the packet is detected, the port number is
incremented again (i.e., k +1) and the next packet will be transmitted, and so on. This embodiment will distribute packets originating from one node over all output ports
14 of the node in a round robin model in order to de-correlate long data flows from a specific destination and avoid "hot spot" development in the ring grid. Those skilled
in the art will appreciate that this distribution method can be modified as necessary to improve performance for different types of traffic distribution. For example, you
can replace the round robin method with a weighted round robin method or any other user defined method. The method of the present invention has been found to
perform better than Valiant independent distribution and is very consistent and predictable in terms of results. This is very significant from a network performance point
of view. Fig. 19 shows pseudo code implementing flit transfer as described and illustrated in the flow diagram at fig. 17.
FIG. 20 illustrates a flow chart of a method of presenting topology discovery in accordance with the present invention. As noted above, topology discovery components
(TDs) are important for basically discovering the presence (or absence) of nodes (and node ports) in the ring mesh, which nodes/ports can then be used in the above
route distribution. In this regard, TD allows the novel network of the present invention as specifically disclosed in PCT patent application No. PCT/CA2014/000652 to
scale in a simplified and efficient manner. TD is based on the "Hello" protocol and the database exchange protocol. In particular, a "Hello" message is sent and received on
each node and each port. Once the message is received as a "Hello" and the node ID in the "Hello" packet is verified by the management framework, the local node
information is updated to indicate the availability of the new link to the neighbor. The collection of node information is maintained in a neighbor database at each node
and is synchronized with the neighbors through a database exchange protocol. Each node sends its changes to the neighbor database to the neighbor nodes, thereby
propagating the entire topology step by step. After a certain timeout, if a change to the neighbor database occurs, a function call is generated for the path computation.
For a "Hello" message exchange, neighbor-to-neighbor always continues at defined time intervals in order to detect a change in topology.
FIG. 21 shows a flow diagram for presenting DB exchange updates for topology discovery in accordance with the present invention.
This basically involves the TD component responsible for maintaining a complete image of the switch in each node, including all interconnected servers as well as
input/output links and connectivity status.
Fig. 22 provides a flow chart showing how the system of the present invention discovers node/link failure detection and the process of rerouting to avoid failed nodes. As
a link failure is detected 602, the pass-through block is programmed so that traffic destined for the failed link is rerouted to the CPU 603. The traffic source is notified to
send traffic around the failed link 604. This triggers a topology upgrade event 605.
As the link recovery is detected 606, the pass-through block is programmed so that traffic destined for the CPU due to the failed link is rerouted back to the same link
607.
The traffic source is notified to send traffic in the same manner as before the link failure 608. This triggers a topology update event 609. A similar procedure will occur in
the case of link down. FIG. 23 provides pseudo code for link/node failure detection and recovery.

https://patents.google.com/patent/CN110708241B/en?q=(high+radix)&oq=high+radix&page=1

6/15

2/14/26, 1:59 PM

CN110708241B - Method for routing packets in a distributed direct interconnect network - Google Patents

One embodiment of the present invention is a method for new node insertion. To insert a node, the wiring interconnecting existing nodes is interrupted (link down). With
the ring wiring interruption detected 602, the traffic re-routing API is called 604. The link cost is set to infinity and the topology is updated (the shortest (lower cost) path
is computed for all switch nodes). This defines the route that will be used to control the flow of packets to reach any destination. New node insertion triggers neighbor
discovery. The management software checks the new node ID and compares it with the provided database. If the node is valid, the software initiates the process of
topology update and route calculation. The management database will be updated with the new node and the node status will be set to active.
While particular embodiments of the present invention have been described, it will be obvious to those skilled in the art that changes and modifications may be made
thereto without departing from the scope of the following claims.
Description of the figures
FIG. 1 shows a schematic view of a
PRIOR ART Prior Art
FIG. 2
PRIOR ART Prior Art
FIG. 3
PRIOR ART Prior Art
Computer Chip computing Chip
2processors
(compare this with 1988Cray YMP/8at 2.7GF/s (compare this at 2.7GF/s with 1988Cray YMP/8)
Computer Card computing Card
I/O Card
FRU (field replaceable unit)
2nodes (4CPUs) 2nodes
Node Card
16 computer cards 16 computing cards
0-2I/O cards
32nodes of 32nodes
(64CPUs) 64CPUs
Cabinet Cabinet
2midplanes of 2 midambles
1024nodes
2,048CPUs 2,048
System System
64cabinets of 64cabinets
65,536nodes 65,536
131,072CPUs
2.500sq ft. 2.500sq ft
MTBF 6.16Days MTBF 6.16Days
FIG. 4
Number of hops
Number of nodes
Destination node from Source node 1.6 starting from Source node 1.6
FIG. 7
8port NIC Card
FIG. 9
MESSAGE/FLOW MESSAGE/FLOW
PACKET
Seq nb. sequence number

https://patents.google.com/patent/CN110708241B/en?q=(high+radix)&oq=high+radix&page=1

7/15

2/14/26, 1:59 PM

CN110708241B - Method for routing packets in a distributed direct interconnect network - Google Patents

data
Adr Address
FLIT microchip
flit data
FIG. 10 shows a schematic view of a
36Nodes (6x 6) MDP from 2.4to D5.2 36Nodes (6x 6) MDP from S2.4 to D5.2
ROUTE 5(x +, x +, x +, y-, y-)
ROUTE 5(x-, x-, y-, y-, x-)
ROUTE 5(y-, y-, x +, x +, x +)
ROUTE 7(y +, y +, y +, x +, x +, x +, y +) ROUTE 7(y +, y +, y +, x +, x +, y +)
FIG. 11
TOPOLOGY DISCOVERY
PATH VECTOR COMPUTATION Path VECTOR COMPUTATION
HEADER nibble LOOKUP for HEADER NIBLE LOOKUP
CONTROL SENT TO SOURCE CONTROL MICROSLICES TRANSMITTED TO SOURCE
FIG. 12
402 receiving a new packet
404 analyze the destination
Count of 412 bytes is 0
407 creating flits
408 attach ring header
409 analysis priority
410 are inserted in the egress queue
411 byte count +476
405 end of packet?
YES is
NO of
FIG. 14
412 topology update
414 initiate routing path computation
419 destination is initial _ dest
415 calculate the shortest Dijkstra path between the current node and the destination
422 resetting network topology links
416 found a path?
YES is
NO of
417 removing shortest path links from a network topology
418 updating the routing table
420 last destination node?
YES is
NO of
End
FIG. 16
4096(8x8x8x8) MDP (8x8x 8) with 4096(8x8x8x8) MDP from S (1000 to D (0) from S (1000 to D) (0)

https://patents.google.com/patent/CN110708241B/en?q=(high+radix)&oq=high+radix&page=1

8/15

2/14/26, 1:59 PM

CN110708241B - Method for routing packets in a distributed direct interconnect network - Google Patents

ROUTE routing
FIG. 17
421 packet destination "n"
422 packets are in queue?
yes is
no or no
424 increment sequence number
426 routing flit i on port k
End of packet End?
yes is
no or no
FIG. 18
Wormhole Switching Wormhole exchange
Packet
FLIT microchip
FIG. 20
Port UP event
Send Hello @ time interval sends Hello at intervals
The Hello received receives the Hello
YES is
NO of
New Nbr New Nbr
Nbr Valid Nbr
Process Link Master
Start DB Exchange Nbr
Port DOWN or retries exceeded Port disconnection or retry
Decline Link Down declares Link Break
Updating local NodeInfo with Update local NodeInfo
Stop DB _ exchange _ Nbr stops DB _ exchange _ Nbr
DB Exchange Nbr
DB update in progress DB update
Link Master? Is link master?
YES is
NO of
Send DB summary For entity DB sending the summary of the Entire DB
Received DB summary update? Did DB summary update received?
YES is
NO of
Calculate DB delta calculation DB deltas
Send summary update-For old DB elements sending summary updates
Send DB Elements List lower DB Elements List sends DB element List of newer DB Elements
DB elements List received? Received DB element list?
YES is
NO of

https://patents.google.com/patent/CN110708241B/en?q=(high+radix)&oq=high+radix&page=1

9/15

2/14/26, 1:59 PM

CN110708241B - Method for routing packets in a distributed direct interconnect network - Google Patents

DB Update not in progress DB Update
Wait to receive from Nbr DB Element list
For Each DB Element in the list
DB Element List instance? Is the DB element list empty?
YES is
NO of
DB element Old? Is the DB element old?
YES is
NO of
Return own version of DB Element
DB element New? Is the DB element new?
YES is
NO of
Issue of an Issue DB update event by an Issue DB update event
FIG. 21
DB Exchange Process
Wait for DB update event for Wait for DB update event
Any DB _ Exchange _ NBR update progress? Is any DB _ Exchange _ NBR update in progress?
YES is
NO of
Start RouteCalc timer starts RouteCalc timer
For Each DB _ Exchange _ Nbr
DB _ Exchange _ Nbr empty
YES is
NO of
Is Nbr src for update? Nbr src is used for update?
Send new DB elements to Nbr sends new DB elements to Nbr
Updates Ack' ed Updates are acknowledged
YES is
NO of
Nbr still UP? Nbr is still UP?
YES is
NO of
FIG. 22
602 link down
603 program pass-through block to reroute traffic to software
604 informing the service originator node about downlink disconnection
605 triggering a topology update
606 link is restored?
YES is
NO of
607 Programming the pass-through Block to reroute traffic to hardware
608 informing the service originator node about link recovery
609 trigger a topology update

https://patents.google.com/patent/CN110708241B/en?q=(high+radix)&oq=high+radix&page=1

10/15

2/14/26, 1:59 PM

CN110708241B - Method for routing packets in a distributed direct interconnect network - Google Patents

Patent Citations (43)
Publication number

Priority date

Publication date

Assignee

Title

US7373543B1 *

2000-07-31

2008-05-13

Cisco Technology, Inc.

Method and apparatus for uninterrupted packet transfer using replication over
disjoint paths

JP2749725B2 *

1991-03-18

1998-05-13

富士通株式会社

Communication method of parallel computer

US5550815A *

1994-12-30

1996-08-27

Lucent Technologies Inc.

Apparatus and method for reducing data losses in a growable packet switch

US5898826A *

1995-11-22

1999-04-27

Intel Corporation

Method and apparatus for deadlock-free routing around an unusable routing
component in an N-dimensional network

JPH09219702A *

1996-02-14

1997-08-19

Nec Corp

Method for retrieving free route of mesh configuration

JP3860257B2 *

1996-06-28

2006-12-20

富士通株式会社

How to determine the channel

US6285679B1 *

1997-08-22

2001-09-04

Avici Systems, Inc.

Methods and apparatus for event-driven routing

US6947433B2 *

2000-09-21

2005-09-20

Avici Systems, Inc.

System and method for implementing source based and egress based virtual
networks in an interconnection network

JP2002305541A *

2001-04-04

2002-10-18

Kddi Research &
Development
Laboratories Inc

Load balancing method in mesh net

CA2410137C *

2001-11-02

2008-04-15

Nippon Telegraph And
Telephone Corporation

Optical dynamic burst switch

US7457303B2

2003-06-06

2008-11-25

International Business
Machines Corporation

One-bounce network

US7349350B2 *

2003-09-23

2008-03-25

Intel Corporation

Determining two node-disjoint paths using on-demand flooding

US7280755B2 *

2003-09-26

2007-10-09

Minho Kang

Highly utilizable protection mechanism for WDM mesh network

CN100461748C *

2004-10-22

2009-02-11

华为技术有限公司

Method for directly interconnecting switching network routes

WO2007094520A1 *

2006-02-16

2007-08-23

Nec Corporation

Node, network system, frame transfer method, and frame transfer program

US8031614B2 *

2006-10-06

2011-10-04

International Business
Machines Corporation

Method and apparatus for routing data in an inter-nodal communications lattice
of a massively parallel computer system by dynamic global mapping of
contended links

US7765385B2 *

2007-04-18

2010-07-27

International Business
Machines Corporation

Fault recovery on a parallel computer system with a torus network

US8085659B2 *

2007-08-28

2011-12-27

Universidad Politecnica
De Valencia

Method and switch for routing data packets in interconnection networks

US8160063B2 *

2008-06-09

2012-04-17

Microsoft Corporation

Data center interconnect and traffic engineering

CN101651599B *

2008-08-12

2012-05-30

中国移动通信集团公司

Multipath wireless routing method and device

KR20110063819A *

2008-09-04

2011-06-14

파워웨이브 코그니션, 인
크.

Mobile, Broadband Routable Internet Applications

US8065433B2 *

2009-01-09

2011-11-22

Microsoft Corporation

Hybrid butterfly cube architecture for modular data centers

JP5195933B2 *

2009-01-30

2013-05-15

富士通株式会社

Information processing system, information processing apparatus, information
processing apparatus control method, information processing apparatus control
program, and computer-readable recording medium

US8144626B2 *

2009-06-30

2012-03-27

Fujitsu Limited

Determining disjoint paths with an optimized number of regenerators

US8594104B2 *

2009-11-23

2013-11-26

Cisco Technology, Inc.

Providing proxy mobile IP over a communication network

JP5630306B2 *

2011-02-10

2014-11-26

富士通株式会社

Route generation method, relay device, and route generation program

CA2872831C *

2011-05-08

2019-10-29

Infinetics Technologies,
Inc.

Flexible radix switch

US9736054B2 *

2011-10-05

2017-08-15

Cisco Technology, Inc.

Multicast active source discovery and management for layer-2 interconnect
solutions

US9185166B2 *

2012-02-28

2015-11-10

International Business
Machines Corporation

Disjoint multi-pathing for a data center network

EP3720065A1 *

2012-09-28

2020-10-07

Cornell University

System and methods for improved network routing

Family To Family Citations

https://patents.google.com/patent/CN110708241B/en?q=(high+radix)&oq=high+radix&page=1

11/15

2/14/26, 1:59 PM

CN110708241B - Method for routing packets in a distributed direct interconnect network - Google Patents

US8989017B2 *

2012-12-14

2015-03-24

Intel Corporation

Network congestion management by packet circulation

US9401862B2 *

2013-02-07

2016-07-26

Dell Products L.P.

Optimized internet small computer system interface path

CN103327563A *

2013-04-25

2013-09-25

张家港中科港联物联网科
技有限公司

Multi-path unicast method based on position information in wireless selforganization and sensor network

US9311022B2 *

2013-10-07

2016-04-12

Dell Products L.P.

System and method for improved communication in a storage network

NO2776466T3

2014-02-13

2018-01-20

US20160026558A1 *

2014-07-26

2016-01-28

Wipro Limited

Method and system for managing virtual services to optimize operational
efficiency of software testing

CN105871722B *

2015-01-19

2020-02-14

中兴通讯股份有限公司

Label structure and label message forwarding method and device

US10116557B2 *

2015-05-22

2018-10-30

Gray Research LLC

Directional two-dimensional router and interconnection network for field
programmable gate arrays, and other circuits and applications of the router and
network

US10616100B2 *

2016-11-03

2020-04-07

Parallel Wireless, Inc.

Traffic shaping and end-to-end prioritization

US10701126B2 *

2017-08-31

2020-06-30

Wipro Limited

Method and a system to deliver multimedia content in a downstream network

JP6946955B2 *

2017-11-10

2021-10-13

富士通株式会社

Information processing device, arithmetic processing device, and control method
of information processing device

US10476815B2 *

2017-12-11

2019-11-12

Ciena Corporation

Adaptive communication network with cross-point switches

US11677628B2 *

2017-12-12

2023-06-13

Lenovo Enterprise
Solutions (Singapore)
Pte. Ltd.

Topology discovery between compute nodes and interconnect switches

Priority date

Publication date

Assignee

Title

NO2776466T3

2014-02-13

2018-01-20

WO2016159945A1 *

2015-03-28

2016-10-06

Intel Corporation

Distributed routing table system with improved
support for multiple network topologies

US10084860B2 *

2015-04-09

2018-09-25

Electronics And Telecommunications Research Institute

Distributed file system using torus network and
method for configuring and operating
distributed file system using torus network

US20170171084A1 *

2015-12-09

2017-06-15

Alcatel-Lucent Usa, Inc.

Valiant load balanced segment routing

JP2017120542A *

2015-12-28

2017-07-06

富士通株式会社

Parallel information processor, data
transmission method, and data transmission
program

US10572537B2 *

2016-04-13

2020-02-25

International Business Machines Corporation

Efficient graph optimization

US9954611B1 *

2016-12-16

2018-04-24

Futurewei Technologies, Inc.

System and method for abstracting wavelengthswitched optical network traffic engineering
topology in SDN control hierarchy

KR102610984B1 *

2017-01-26

2023-12-08

한국전자통신연구원

Distributed file system using torus network and
method for operating of the distributed file
system using torus network

CN109218176B *

2017-06-30

2020-12-15

华为技术有限公司

Method and device for processing message

US11347774B2 *

2017-08-01

2022-05-31

Salesforce.Com, Inc.

High availability database through distributed
store

CN107733802B *

2017-09-18

2020-11-13

深圳市盛路物联通讯技术有限公司

Node control method and system of distributed
network topology structure

CA2982147A1 *

2017-10-12

2019-04-12

Rockport Networks Inc.

Direct interconnect gateway

US10855581B2

2017-11-10

2020-12-01

Fabriscale Technologies AS

System and method of computing ethernet
routing paths

CN110166310B *

2018-01-30

2020-08-21

山东衡昊信息技术有限公司

A Fast Calculation Method for Concurrent Big
Data Transmission Delay Based on Wormhole
Network

* Cited by examiner, † Cited by third party

Cited By (46)
Publication number
Family To Family Citations

https://patents.google.com/patent/CN110708241B/en?q=(high+radix)&oq=high+radix&page=1

12/15

2/14/26, 1:59 PM

CN110708241B - Method for routing packets in a distributed direct interconnect network - Google Patents

GB201802347D0

2018-02-13

2018-03-28

Nchain Holdings Ltd

Computer-implemented system and method

CN110324249B *

2018-03-28

2023-05-26

清华大学

A dragonfly network architecture and its
multicast routing method

US10944693B2 *

2018-11-13

2021-03-09

Advanced Micro Devices, Inc.

Routing flits in a network-on-chip based on
operating states of routers

JP7167687B2 *

2018-12-18

2022-11-09

富士通株式会社

Information processing device, information
processing method and information processing
program

CN109525518B *

2018-12-25

2021-01-12

北京物芯科技有限责任公司

IP message network address conversion
method and device based on FPGA

CN109768924B *

2019-02-14

2021-06-08

山东省计算中心（国家超级计算济南中心）

A multi-stream coexistence-oriented SDN
network multi-link fault recovery method and
system

US11252034B1 *

2019-03-15

2022-02-15

Juniper Networks, Inc.

Generating candidate links and candidate paths
before selecting links for an optimized optical
network plan

CN114363241B *

2019-05-15

2023-02-28

清华大学

High-dimensional Torus network architecture
and adaptive routing method

US11055191B2 *

2019-05-17

2021-07-06

Citrix Systems, Inc.

Service graph highlights missing nodes and
links

CN112039795B *

2019-06-04

2022-08-26

华为技术有限公司

Load sharing method, device and network
equipment

CN112491672B *

2019-09-11

2022-05-06

杭州海康威视数字技术股份有限公司

A PCIE communication system, communication
configuration parameter backup method and
PCIE switch

US20210111990A1 *

2019-10-14

2021-04-15

Cisco Technology, Inc.

Systems and methods for providing multiple
disjointed paths to core network at first-mile
access

US11121984B2 *

2019-10-18

2021-09-14

Ciena Corporation

Routing tables for forwarding packets between
switches in a data center network

US11775470B2

2019-11-20

2023-10-03

Intel Corporation

Transaction layer packet format

US11212212B2 *

2020-04-15

2021-12-28

Hewlett Packard Enterprise Development Lp

Non-isolated topologies in computing network
environments

CN111970202B *

2020-08-28

2021-09-10

电子科技大学

Network topology discovery method based on
three-way sub-topology measurement

CN113347029B *

2020-09-29

2022-05-31

北京航空航天大学

Torus network fault tolerance method based on
topology reconstruction and path planning

RU2753147C1 *

2020-11-20

2021-08-12

Федеральное государственное бюджетное
учреждение науки Институт проблем управления им.
В.А. Трапезникова Российской академии наук

Method for organizing optimal fault-tolerant
multidimensional tori based on low-port routers
and duplex channel splitters

WO2022162465A1 *

2021-01-28

2022-08-04

Rockport Networks Inc.

Systems and methods for the temporal
monitoring and visualization of network health
of direct interconnect networks

CN113037637B *

2021-03-23

2022-10-14

福建师范大学

A fault-tolerant information transmission path
selection method for human-machine-material
fusion network

WO2022221466A1 *

2021-04-13

2022-10-20

The Regents Of The University Of California

Configurable memory pool system

CA3221912A1 *

2021-06-09

2022-12-15

Alan James Jennings

Method for distributing multipath flows in a
direct interconnect network

US12413503B2 *

2021-06-23

2025-09-09

Rockport Networks Inc.

Deadlock-free multipath routing for direct
interconnect networks

CA3227857A1 *

2021-08-05

2023-02-09

Dan Oprea

Method for signaling link or node failure in a
direct interconnect network

US12346285B2 *

2021-08-30

2025-07-01

Taiwan Semiconductor Manufacturing Company, Ltd.

Diagonal torus network

CN115499271B *

2022-08-30

2023-10-13

西北工业大学

Hybrid network topology structure and routing
method thereof

US12328251B2 *

2022-09-08

2025-06-10

Mellano Technologies, Ltd.

Marking of RDMA-over-converged-ethernet
(RoCE) traffic eligible for adaptive routing

https://patents.google.com/patent/CN110708241B/en?q=(high+radix)&oq=high+radix&page=1

13/15

2/14/26, 1:59 PM

CN110708241B - Method for routing packets in a distributed direct interconnect network - Google Patents

US12418475B2

2022-12-08

2025-09-16

Google Llc

Fault-tolerant routing algorithm for toroidal
network topologies

EP4635147A1 *

2022-12-14

2025-10-22

Advanced Micro Devices, Inc.

Network collective offloading routing
management

US12028241B1 *

2023-07-10

2024-07-02

Cisco Technology, Inc.

Backup path determination for optical networks

CN117155846B *

2023-10-31

2024-02-06

苏州元脑智能科技有限公司

Routing method, device, computer equipment
and storage medium of interconnection network

CN120825529A *

2024-04-12

2025-10-21

成都华为技术有限公司

Message transmission method, device and
computing equipment

* Cited by examiner, † Cited by third party, ‡ Family to family citation

Similar Documents
Publication

Publication Date

Title

CN110708241B

2021-10-08

Method for routing packets in a distributed direct interconnect network

EP2777229B1

2019-06-05

System and method for providing deadlock free routing between switches in a fat-tree topology

US11121984B2

2021-09-14

Routing tables for forwarding packets between switches in a data center network

US12413503B2

2025-09-09

Deadlock-free multipath routing for direct interconnect networks

JP6541768B2

2019-07-10

System and method for supporting efficient load balancing in a high performance computing (HPC) environment

US20190222481A1

2019-07-18

Data center network topology discovery

CN108111410B

2021-08-10

Method and device for constructing deadlock-free route in network with Cartesian topology

US7307995B1

2007-12-11

System and method for linking a plurality of network switches

Bogdanski

2014

Optimized routing for fat-tree topologies

US10855581B2

2020-12-01

System and method of computing ethernet routing paths

HK40013842B

2022-06-17

Method to route packets in a distributed direct interconnect network

HK40013842A

2020-08-14

Method to route packets in a distributed direct interconnect network

HK1227195A1

2017-10-13

Method to route packets in a distributed direct interconnect network

HK1227195B

2020-12-11

Method to route packets in a distributed direct interconnect network

Csernai et al.

2012

Poincare: A hyperbolic data center architecture

KR102853775B1

2025-09-03

Multicast routing method and device of network on chip(NoC) based on irregular topology

Alshahrani

2013

Delay modeling in data center networks: A taxonomy and performance analysis

Nishikawa et al.

2010

A deadlock-free non-minimal fully adaptive routing using virtual cut-through switching

Sem-Jacobsen et al.

0

Dynamic Fault Tolerance in Fat Trees-Research

Priority And Related Applications
Parent Applications (1)
Application

Priority date

Filing date

Relation

Title

CN201580019119.6A

2014-02-13

2015-02-13

Division

Method for routing packets in a distributed direct interconnect network

Applications Claiming Priority (3)
Application

Filing date

US201461939487P

2014-02-13

US61/939,487

2014-02-13

CN201580019119.6A

2015-02-13

Title

Method for routing packets in a distributed direct interconnect network

https://patents.google.com/patent/CN110708241B/en?q=(high+radix)&oq=high+radix&page=1

14/15

2/14/26, 1:59 PM

CN110708241B - Method for routing packets in a distributed direct interconnect network - Google Patents

Legal Events
Date

Code

Title

Description

2020-01-17

PB01

Publication

2020-01-17

PB01

Publication

2020-02-18

SE01

Entry into force of request for substantive examination

2020-02-18

SE01

Entry into force of request for substantive examination

2020-08-14

REG

Reference to a national code

Ref country code: HK
Ref legal event code: DE
Ref document number: 40013842
Country of ref document: HK

2021-10-08

GR01

Patent grant

2021-10-08

GR01

Patent grant

2025-02-14

CF01

Termination of patent right due to non-payment of annual fee

2025-02-14

CF01

Termination of patent right due to non-payment of annual fee

Granted publication date: 20211008

Concepts
machine-extracted

Download

Name

Image

method

title,claims,abstract,description

63

0.000

claims,description

7

0.000

claims

1

0.000

development

abstract,description

8

0.000

developmental process

abstract,description

8

0.000

segmentation

abstract,description

7

0.000

Public Datasets

Terms

calculation method
microarray

Sections

Count

Filter table

Query match

Show all concepts from the description section

About

Send Feedback

Privacy Policy

https://patents.google.com/patent/CN110708241B/en?q=(high+radix)&oq=high+radix&page=1

Help

15/15

2/14/26, 1:54 PM

CN111488133B - High-radix approximate Booth coding method and mixed-radix Booth coding approximate multiplier - Goo…

Patents
Back to results

2 of 125,048

high radix

(high radix);

High-radix approximate Booth coding method and mixed-radix Booth coding approximate multiplier
Abstract

translated from Chinese

CN111488133B
A high radix approximate Booth encoding method and a mixed radix Booth encoding approximate

China

multiplier belong to the technical field of integrated circuits. The high-radix approximate Booth
coding method encodes the m bits with lower weight in the n-bit multiplicand and combines the n-bit

Download PDF

Find Prior Art

Similar

multiplier to obtain two incomplete partial products A and B, and obtains an approximate coding
result after adding them. The mixed radix Booth encoding approximate multiplier combines the
exact Booth encoding module and the high radix approximate Booth encoding module to obtain an
accurate partial product and an approximate partial product respectively, and then combines the
sign extension bits generated by the sign bit extension algorithm proposed by the present invention
to form a partial product The final calculation result of the approximate multiplier is obtained by
compressing and adding the partial product array, and the present invention obtains the precision

Other languages: Chinese
Inventor: 贺雅娟, 衣溪琳, 朱飞宇, 侯博文, 张波
Current Assignee : University of Electronic Science and
Technology of China

index by deriving the error model of the approximate multiplier. The high-radix approximation Booth
encoding ensures high calculation accuracy while reducing the structural complexity of the
multiplier, and the sign extension bit avoids the accumulation of a large number of identical
numbers, which simplifies the hardware design.

Worldwide applications
2020 CN

Application CN202010292881.4A events
2020-04-15

Application filed by University of Electronic
Science and Technology of China

2020-04-15

Priority to CN202010292881.4A

2020-08-04

Publication of CN111488133A

2023-03-28

Application granted

2023-03-28

Publication of CN111488133B

Status

Active

2040-04-15

Anticipated expiration

Info: Patent citations (8), Non-patent citations (3) , Cited by (12),
Legal events, Similar documents, Priority and Related
Applications
External links: Espacenet, Global Dossier, Discuss

Images (7)

Classifications

https://patents.google.com/patent/CN111488133B/en?q=(high+radix)&oq=high+radix

1/32

2/14/26, 1:54 PM

CN111488133B - High-radix approximate Booth coding method and mixed-radix Booth coding approximate multiplier - Goo…

G06F7/5338 Reduction of the number of iteration steps or stages, e.g. using the Booth
algorithm, log-sum, odd-even by using multiple bit scanning, i.e. by decoding groups of successive
multiplier bits in order to select an appropriate precalculated multiple of the multiplicand as a
partial product overlapped, i.e. with successive bitgroups sharing one or more bits being recoded
into signed digit representation, e.g. using the Modified Booth Algorithm each bitgroup having two
new bits, e.g. 2nd order MBA

View 1 more classifications

Landscapes

Physics & Mathematics
General Physics & Mathematics
Show more

Claims (4)

Hide Dependent
translated from Chinese

1. A high cardinality approximate Booth encoding method, characterized in that it comprises the following steps: Step 1, obtain an n-bit multiplier and an n-bit multiplicand, divide
the n-bit multiplicand into two parts according to the weight, select the m-bit and n-bit multipliers with lower weights in the multiplicand as the data to be encoded and perform
high-radix approximate Booth encoding with base -2 m , where m and n are both positive integers and n>m; Step 2: The m bits with lower weight in the multiplicand are y m-1 to y 0 in
order from the highest bit to the lowest bit, select the upper four bits y m-1 to y m-4 of the m bits with lower weight in the multiplicand, and divide all situations of the data to be
encoded into sixteen intervals according to the upper four bits of the m bits with lower weight in the multiplicand, and the grouping signal of each interval is:

Wherein sel 0 to sel 15 are the grouping signals of the first interval to the sixteenth interval respectively; Step 3: Obtain the partial product A and partial product B generated by the
high cardinality approximate Booth coding of the to-be-encoded data according to formula (2):

https://patents.google.com/patent/CN111488133B/en?q=(high+radix)&oq=high+radix

2/32

2/14/26, 1:54 PM

CN111488133B - High-radix approximate Booth coding method and mixed-radix Booth coding approximate multiplier - Goo…

in:

Ai is the i-th bit in the partial product A, Bi is the i-th bit in the partial product B, xi -m+5 is the i-m+5-th bit in the multiplier, xi -m+4 is the i-m+4-th bit in the multiplier, xi -m+3 is the im+3-th bit in the multiplier, xi -m+2 is the i-m+2-th bit in the multiplier, i∈[0,n-1], ck is the sign correction bit; Step 4: Add the partial product A and the partial product B accordingly to
obtain the encoding result of the data to be encoded. 2. A mixed radix Booth coded approximate multiplier, including an encoding module, a sign bit extension module, a tree
compression module and a carry adder module; The encoding module is used to encode the multiplicand to generate a partial product of the multiplier and the multiplicand in the
multiplication operation; The sign bit extension module is used to generate a sign extension bit; The tree compression module receives all partial products generated by the
encoding module and the sign extension bits generated by the sign bit extension module and compresses them to obtain a partial product array after compression and addition of
the sign extension bits; The carry adder module adds all partial products of the partial product array after compression and adding sign extension bits to obtain the final calculation
result of the mixed radix Booth coded approximate multiplier; The encoding module includes an accurate Booth encoding module and a high-radix approximate Booth encoding
module, the input data includes an n-bit multiplier and an n-bit multiplicand, the multiplicand is divided into two parts according to the weight, the n-m-bit and n-bit multipliers with
higher weights in the multiplicand are input into the accurate Booth encoding module, and the accurate Booth encoding module performs encoding to obtain the values of all bits of
the accurate partial product; the m-bit and n-bit multipliers with lower weights in the multiplicand are input into the high-radix approximate Booth encoding module, and the highradix approximate Booth encoding module performs encoding to obtain the values of all bits of the approximate partial product; wherein m and n are both positive integers and n＞
m; The high cardinality approximate Booth coding module includes a grouping unit, a coding unit, and a partial product generating unit. The grouping unit is used to group the input
data received by the high cardinality approximate Booth encoding module into sixteen intervals, and the grouping signal of each interval is:

Wherein sel 0 to sel 15 are grouping signals of the first interval to the sixteenth interval respectively, and y m-1 to y m-4 are the highest bit to the fourth highest bit among the m bits
with lower weights in the multiplicand respectively; The encoding module is used to encode according to the grouping result generated by the grouping unit to obtain the sign

https://patents.google.com/patent/CN111488133B/en?q=(high+radix)&oq=high+radix

3/32

2/14/26, 1:54 PM

CN111488133B - High-radix approximate Booth coding method and mixed-radix Booth coding approximate multiplier - Goo…

correction bit c k and the intermediate result of the encoding:

The partial product generating unit is used to generate partial product A and partial product B according to the sign correction bit c k generated by the encoding module and the
intermediate result of the encoding in combination with the multiplier:

Where Ai is the i-th bit in the partial product A, Bi is the i-th bit in the partial product B, xi -m+5 is the i-m+5-th bit in the multiplier, xi -m+4 is the i-m+4-th bit in the multiplier, xi -m+3 is
the i-m+3-th bit in the multiplier, xi -m+2 is the i-m+2-th bit in the multiplier, i∈[0,n-1]. 3. The mixed radix Booth coded approximate multiplier according to claim 2, wherein the sign bit
extension module uses equation (7) to perform:

Where Sign represents the sum of all extended sign bits, S 0 to

https://patents.google.com/patent/CN111488133B/en?q=(high+radix)&oq=high+radix

4/32

2/14/26, 1:54 PM

CN111488133B - High-radix approximate Booth coding method and mixed-radix Booth coding approximate multiplier - Goo…

is the most significant bit of the partial product from row 0 to row 1 in the partial product array.

The most significant bit of the row partial product; According to formula (7)

The most significant bit S 0 of the 0th row of the partial product of the 2n+m-3 weight bits in the partial product array is inverted, and according to formula (7)

The most significant bit S 1 of the first row of the partial product of the 2n+m-3 weight bits in the partial product array is inverted, ..., according to formula (7)

https://patents.google.com/patent/CN111488133B/en?q=(high+radix)&oq=high+radix

5/32

2/14/26, 1:54 PM
The first 2

2n-2

CN111488133B - High-radix approximate Booth coding method and mixed-radix Booth coding approximate multiplier - Goo…

weighted bits in the partial product array are

The most significant bit of the partial product of the row

Negate, that is, the most significant bit S 0 to 0 of each row of the partial product array

https://patents.google.com/patent/CN111488133B/en?q=(high+radix)&oq=high+radix

6/32

2/14/26, 1:54 PM

CN111488133B - High-radix approximate Booth coding method and mixed-radix Booth coding approximate multiplier - Goo…

All are inverted and correspond one by one to the weight position; According to formula (7)

In the partial product array 2 n+m-2 ,

The weight bits are filled with 1. 4. The mixed radix Booth coded approximate multiplier according to claim 2 or 3, characterized in that the approximate multiplication result
obtained by the mixed radix Booth coded approximate multiplier on the multiplier X and the multiplicand Y is denoted as Z A , the approximate coding result on the multiplicand Y is
denoted as Y A , and the exact multiplication result of the multiplier X and the multiplicand Y is denoted as Z; According to the approximate coding result Y A of the multiplicand Y,
the absolute error ED{Y A } of the multiplicand Y after the approximate coding by the mixed radix Booth coding approximate multiplier can be calculated as |E Y |= |Y A -Y|. The
average error of the multiplicand Y is

The maximum absolute error of the multiplicand Y is EDmax{Y A }=max{ED}=2 m-4 , and the average error distance of the multiplicand Y is

Where P Y is the probability of the multiplicand Y occurring; Further, the average error of the approximate multiplication result Z A of the mixed radix Booth coded approximate
multiplier can be obtained:

The maximum absolute error of the approximate multiplication result Z A of the mixed radix Booth coded approximate multiplier is: EDmax{Z A }＝max{|X|}·EDmax{Y A }＝2 n+m-5

https://patents.google.com/patent/CN111488133B/en?q=(high+radix)&oq=high+radix

7/32

2/14/26, 1:54 PM

CN111488133B - High-radix approximate Booth coding method and mixed-radix Booth coding approximate multiplier - Goo…

The average error distance of the approximate multiplication result Z A of the mixed radix Booth coded approximate multiplier is:

Where P X is the probability of the multiplier X occurring.

Description

translated from Chinese

High radix approximate Booth coding method and mixed radix Booth coding approximate multiplier
Technical Field
The invention belongs to the technical field of integrated circuits, and relates to a high-radix approximate Booth coding method and a mixed-radix Booth coding
approximate multiplier designed based on the high-radix approximate Booth coding method.
Background Art
With the rise of new technologies such as big data, cloud computing, and the Internet of Things, computer systems are increasingly being used to interact with the
physical world. Although semiconductor technology and low-power design technology are also developing, the total energy consumption of computer systems is still
growing at an alarming rate in order to process the increasing amount of information. Nowadays, power consumption and energy consumption issues have become
important factors that must be considered when designing chips. For high-performance computing devices, such as servers and high-performance processors, excessive
power consumption can lead to serious heat dissipation problems and cause circuit damage; for portable devices, the use time of the device is limited by the battery
power, and excessive power consumption will reduce the endurance of the device. At the same time, many application scenarios do not require completely accurate
calculations, such as image processing, multimedia, search engines, neural networks, data mining, etc. Approximate calculation is an emerging circuit design method,
and its calculation result is an approximate value with errors, which allows the circuit structure to be simplified, thereby improving circuit performance, reducing circuit
power consumption, and saving circuit area.
As a commonly used basic operation unit, the multiplier has a great impact on the delay, power consumption and area of the circuit. The Booth coded multiplier is a
typical parallel binary multiplier, which is usually composed of three parts: partial product generation module, partial product compression module and carry propagation
adder. Traditional precise Booth coded multipliers usually use low-radix Booth coding to generate partial products. Although the coding unit is relatively simple, the
number of coding units required is large, and the number of partial products generated is large, which is not conducive to the design of the subsequent partial product
compression module. At the same time, high-radix Booth coding can more significantly reduce the number of partial products, but the coding circuit and partial product
generation circuit are more complicated. If the low-radix Booth coding can be combined with the high-radix Booth coding and the approximate calculation technology can
be used, the hardware performance of the multiplier can be further improved. At the same time, if the error model of the Booth coded multiplier using approximate
calculation technology can be derived, it is easier to obtain various error indicators.
Summary of the invention
Aiming at the problem that the number of coding units and the number of partial products generated by using low-radix Booth coding to generate partial products in the
above-mentioned traditional multiplier are large, and the circuit complexity problem that the partial products are generated by using high-radix Booth coding, the present
invention proposes a high-radix approximate Booth coding method, and designs a mixed-radix Booth coding approximate multiplier using the high-radix approximate
Booth coding method. The mixed-radix design method is adopted, and the accurate radix-4 Booth coding is used in the high-significant bits, and the high-radix
approximate Booth coding proposed by the present invention is used in the low-significant bits, which can significantly reduce the hardware complexity of the coding
circuit and the partial product compression circuit; since there is a large amount of repeated data in the sign extension bit of the mixed-radix Booth coding, the present
invention also redesigns the sign extension bit to avoid data redundancy; in addition, error derivation is also performed on the mixed-radix Booth coding approximate
multiplier of the present invention to verify the accuracy and power consumption of the present invention.
The technical solution of the present invention is
The high cardinality approximate Booth coding method comprises the following steps:
Step 1, obtain an n-bit multiplier and an n-bit multiplicand, divide the n-bit multiplicand into two parts according to the weight, select the m-bit and n-bit multipliers with
lower weights in the multiplicand as the data to be encoded and perform high-radix approximate Booth encoding with base -2 m , where m and n are both positive
integers and n>m;
Step 2: The m bits with lower weight in the multiplicand are y m-1 to y 0 in order from the highest bit to the lowest bit, select the upper four bits y m-1 to y m-4 of the m bits
with lower weight in the multiplicand, and divide all situations of the data to be encoded into sixteen intervals according to the upper four bits of the m bits with lower
weight in the multiplicand, and the grouping signal of each interval is:

https://patents.google.com/patent/CN111488133B/en?q=(high+radix)&oq=high+radix

8/32

2/14/26, 1:54 PM

CN111488133B - High-radix approximate Booth coding method and mixed-radix Booth coding approximate multiplier - Goo…

Wherein sel 0 to sel 15 are the grouping signals of the first interval to the sixteenth interval respectively;
Step 3: Obtain the partial product A and partial product B generated by the high cardinality approximate Booth coding of the to-be-encoded data according to formula (2):

in:

https://patents.google.com/patent/CN111488133B/en?q=(high+radix)&oq=high+radix

9/32

2/14/26, 1:54 PM

CN111488133B - High-radix approximate Booth coding method and mixed-radix Booth coding approximate multiplier - Goo…

Ai is the i-th bit in the partial product A, Bi is the i-th bit in the partial product B, xi -m+5 is the i-m+5-th bit in the multiplier, xi -m+4 is the i-m+4-th bit in the multiplier, xi -m+3
is the i-m+3-th bit in the multiplier, xi -m+2 is the i-m+2-th bit in the multiplier, i∈[0,n-1], ck is the sign correction bit;
Step 4: Add the partial product A and the partial product B accordingly to obtain the encoding result of the data to be encoded.
A mixed radix Booth coded approximate multiplier, comprising an encoding module, a sign bit extension module, a tree compression module and a carry adder module;
The encoding module is used to encode the multiplicand to generate a partial product of the multiplier and the multiplicand in the multiplication operation;
The sign bit extension module is used to generate a sign extension bit;
The tree compression module receives all partial products generated by the encoding module and the sign extension bits generated by the sign bit extension module and
compresses them to obtain a partial product array after compression and addition of the sign extension bits;
The carry adder module adds all partial products of the partial product array after compression and adding sign extension bits to obtain the final calculation result of the
mixed radix Booth coded approximate multiplier;
The encoding module includes an accurate Booth encoding module and a high-cardinality approximate Booth encoding module, the input data includes an n-bit multiplier
and an n-bit multiplicand, the multiplicand is divided into two parts according to the weight, the n-m-bit and n-bit multipliers with higher weights in the multiplicand are
input into the accurate Booth encoding module, and the accurate Booth encoding module performs encoding to obtain the values of all bits of the accurate partial
product; the m-bit and n-bit multipliers with lower weights in the multiplicand are input into the high-cardinality approximate Booth encoding module, and the highcardinality approximate Booth encoding module performs encoding to obtain the values of all bits of the approximate partial product; wherein m and n are both positive
integers and n＞m;
The high cardinality approximate Booth coding module includes a grouping unit, a coding unit, and a partial product generating unit.
The grouping unit is used to group the input data received by the high cardinality approximate Booth encoding module into sixteen intervals, and the grouping signal of
each interval is:

https://patents.google.com/patent/CN111488133B/en?q=(high+radix)&oq=high+radix

10/32

2/14/26, 1:54 PM

CN111488133B - High-radix approximate Booth coding method and mixed-radix Booth coding approximate multiplier - Goo…

Wherein sel 0 to sel 15 are grouping signals of the first interval to the sixteenth interval respectively, and y m-1 to y m-4 are the highest bit to the fourth highest bit among
the m bits with lower weights in the multiplicand respectively;
The encoding module is used to encode according to the grouping result generated by the grouping unit to obtain the sign correction bit c k and the intermediate result of
the encoding:

https://patents.google.com/patent/CN111488133B/en?q=(high+radix)&oq=high+radix

11/32

2/14/26, 1:54 PM

CN111488133B - High-radix approximate Booth coding method and mixed-radix Booth coding approximate multiplier - Goo…

The partial product generating unit is used to generate partial product A and partial product B according to the sign correction bit c k generated by the encoding module
and the intermediate result of the encoding in combination with the multiplier:

Where Ai is the i-th bit in the partial product A, Bi is the i-th bit in the partial product B, xi -m+5 is the i-m+5-th bit in the multiplier, xi -m+4 is the i-m+4-th bit in the multiplier,
xi -m+3 is the i-m+3-th bit in the multiplier, xi -m+2 is the i-m+2-th bit in the multiplier, i∈[0,n-1].
Specifically, the sign bit extension module uses formula (7) to perform:

https://patents.google.com/patent/CN111488133B/en?q=(high+radix)&oq=high+radix

12/32

2/14/26, 1:54 PM

CN111488133B - High-radix approximate Booth coding method and mixed-radix Booth coding approximate multiplier - Goo…

是所述部分积阵列中第0行部分积的最高有效位至第

行部分积的最高有效位；Where Sign represents the sum of all extended sign bits, S 0 to

https://patents.google.com/patent/CN111488133B/en?q=(high+radix)&oq=high+radix

13/32

2/14/26, 1:54 PM

CN111488133B - High-radix approximate Booth coding method and mixed-radix Booth coding approximate multiplier - Goo…

is the most significant bit of the partial product from row 0 to row 1 in the partial product array.

The most significant bit of the row partial product;

将所述部分积阵列中2n+m-3权重位的第0行部分积最高有效位S0取反，根据式(7)中的

将所述部分积阵列中2n+m-3权重位的第1行部分积最高有效位S1取反，……，根据式(7)中的

https://patents.google.com/patent/CN111488133B/en?q=(high+radix)&oq=high+radix

14/32

2/14/26, 1:54 PM
将所述部分积阵列中2

CN111488133B - High-radix approximate Booth coding method and mixed-radix Booth coding approximate multiplier - Goo…
2n-2

权重位的第

行部分积最高有效位

取反，即将所述部分积阵列中每一行部分积的最高有效位S0至

都取反并与所在权重位一一对应；According to formula (7)

https://patents.google.com/patent/CN111488133B/en?q=(high+radix)&oq=high+radix

15/32

2/14/26, 1:54 PM

CN111488133B - High-radix approximate Booth coding method and mixed-radix Booth coding approximate multiplier - Goo…

The most significant bit S 0 of the 0th row of the partial product of the 2n+m-3 weight bits in the partial product array is inverted, and according to formula (7)

The most significant bit S 1 of the first row of the partial product of the 2n+m-3 weight bits in the partial product array is inverted, ..., according to formula (7)

The first 2 2n-2 weighted bits in the partial product array are

The most significant bit of the partial product of the row

https://patents.google.com/patent/CN111488133B/en?q=(high+radix)&oq=high+radix

16/32

2/14/26, 1:54 PM

CN111488133B - High-radix approximate Booth coding method and mixed-radix Booth coding approximate multiplier - Goo…

Negate, that is, the most significant bit S 0 to 0 of each row of the partial product array

All are inverted and correspond one by one to the weight position;

在所述部分积阵列中2n+m-2、

权重位补1。According to formula (7)

In the partial product array 2 n+m-2 ,

https://patents.google.com/patent/CN111488133B/en?q=(high+radix)&oq=high+radix

17/32

2/14/26, 1:54 PM

CN111488133B - High-radix approximate Booth coding method and mixed-radix Booth coding approximate multiplier - Goo…

The weight bits are filled with 1.
Specifically, the approximate multiplication result obtained by the mixed radix Booth coded approximate multiplier for processing the multiplier X and the multiplicand Y
is Z A , the approximate coding result for the multiplicand Y is Y A , and the exact multiplication result of the multiplier X and the multiplicand Y is Z;

所述被乘数Y的最大绝对误差EDmax{YA}＝max{ED}＝2m-4，所述被乘数Y的平均误差距离

其中PY为被乘数Y出现的概率；According to the approximate coding result Y A of the multiplicand Y, the absolute error ED{Y A } of the multiplicand Y after the
approximate coding by the mixed radix Booth coding approximate multiplier can be calculated as |E Y |= |Y A -Y|. The average error of the multiplicand Y is

The maximum absolute error of the multiplicand Y is EDmax{Y A }=max{ED}=2 m-4 , and the average error distance of the multiplicand Y is

Where P Y is the probability of the multiplicand Y occurring;
Further, the average error of the approximate multiplication result Z A of the mixed radix Booth coded approximate multiplier can be obtained:

The maximum absolute error of the approximate multiplication result Z A of the mixed radix Booth coded approximate multiplier is:
EDmax{Z A }＝max{|X|}·EDmax{Y A }＝2 n+m-5
The average error distance of the approximate multiplication result Z A of the mixed radix Booth coded approximate multiplier is:

https://patents.google.com/patent/CN111488133B/en?q=(high+radix)&oq=high+radix

18/32

2/14/26, 1:54 PM

CN111488133B - High-radix approximate Booth coding method and mixed-radix Booth coding approximate multiplier - Goo…

Where P X is the probability of the multiplier X occurring.
The beneficial effects of the present invention are as follows: the present invention proposes a novel high-radix approximate Booth encoding method, which, although
partially losing the calculation accuracy, greatly reduces the hardware complexity of the encoding circuit and the partial product compression circuit; based on the highradix approximate Booth encoding method proposed by the present invention, a mixed-radix Booth encoding approximate multiplier is designed, and the low-radix precise
Booth encoding and the high-radix approximate Booth encoding are combined to generate accurate partial products and approximate partial products respectively, which
reduces the structural complexity of the multiplier while ensuring high calculation accuracy; a sign bit extension method is proposed, which can avoid the accumulation
of a large number of identical numbers and simplify the hardware design; the present invention is suitable for computing scenarios that have a certain tolerance for the
error of the calculation result but require the power consumption and hardware complexity of the multiplier to be low; in addition, the error model of the mixed-radix
Booth encoding approximate multiplier based on the present invention is derived, and compared with the accuracy index obtained directly and accurately through
simulation, the simulation time is saved and the calculation of large amounts of data is avoided.
BRIEF DESCRIPTION OF THE DRAWINGS
FIG1 is a schematic diagram of the structure of a mixed radix Booth coded approximate multiplier proposed in the present invention.
FIG. 2 is a table showing the correspondence between approximate Booth encoding using radix-64 and partial product generation in an embodiment.
FIG3 is a partial product array diagram of an exact radix-4 and approximate radix-256 mixed encoding with a bit width of 16×16 bits after sign bit extension.
FIG. 4 is a partial product array diagram of an exact radix-4 and approximate radix-256 mixed encoding with a bit width of 16×16 bits after the sign bit is optimized.
FIG5 is a partial product array diagram of an exact radix-4 and approximate radix-64 mixed encoding with a bit width of 16×16 bits after the sign bit is optimized.
FIG6 is a partial product array diagram of an exact radix-4 and approximate radix-1024 mixed encoding with a bit width of 16×16 bits after the sign bit is optimized.
FIG. 7 is a table showing the relationship between the approximate interval and the generated approximate value.
DETAILED DESCRIPTION
The present invention will be further described below in conjunction with the accompanying drawings and specific embodiments.
The present invention proposes a novel high-radix approximate Booth coding method, which can significantly reduce the hardware complexity of the coding circuit and
the partial product compression circuit. For an n-bit multiplier and an n-bit multiplicand, the high-radix approximate Booth coding method proposed by the present
invention encodes the m bits with lower weight in the multiplicand, and obtains the partial product in combination with the n-bit multiplier. In multiplication and addition
operations, the closer the data is to the most significant bit, the greater the impact on the final result. Therefore, although the number of bits required for the three highradix Booth codings of base-64, base-256, and base-1024 in the low-significant bit part is 6, 8, and 10 bits respectively, the present invention only takes the upper 4 bits
for encoding. For these three high-radix Booth coding algorithms, the logic of the coding circuit is consistent with the hardware implementation, which is equivalent to a
traditional 4-16 decoder. By using approximate high-radix Booth coding, the data to be encoded is divided into 16 intervals. For the three high-radix Booth coding
algorithms of base-64, base-256, and base-1024, each interval contains 4, 16, and 64 input situations respectively; that is, for the high-radix Booth coding algorithm of
base- 2m , each interval contains 2m -4 input situations. The encoding method using the approximate high-radix Booth coding algorithm of base-64 as an example is
shown in Figure 2. For other approximate high-radix Booth coding, the grouping is still only determined by the upper 4 bits of the multiplicand.
After the coding grouping is obtained, it is hoped that the error of the partial product generated is as small as possible, and the circuit for generating the partial product is
as simple as possible. Therefore, the high-radix approximate Booth coding method proposed in the present invention adopts an incomplete coding method. The general
coding method only generates one partial product. The high-radix approximate Booth coding proposed in the present invention adopts an incomplete coding method to
generate two partial products A and B, and then the two partial products with the same weight are added in the compression tree to obtain the coding result. Although
one partial product is added in this way, the number of partial products generated is still less than that of the base-4 Booth coding. At the same time, the two
incompletely coded partial products generated by the method of the present invention are both generated by shifting the multiplicand, so complex calculation logic is not
introduced in the generation process. As shown in FIG2 , the intermediate result of coding is obtained according to the group signal, and then the partial products A and B
are obtained according to the intermediate result of coding combined with the multiplier. In the first group of coding groups, the intermediate result of coding is 0 and 2,
the multiplier × 0 obtains the partial product A, and the multiplier × 2 obtains the partial product B; in the second group of coding groups, the intermediate result of coding
is 4 and 2, the multiplier × 4 obtains the partial product A, and the multiplier × 2 obtains the partial product B. By analogy, it can be seen that the radix-64 Booth code
produces two incompletely coded partial products A and partial products B, and the addition of the two obtains an approximation of the high radix Booth code, and
maintains a low error.
By approximating Booth coding with a base-2 m high radix, a general logical expression can be obtained. The logical expression of the coding circuit that generates the
coding group is:

https://patents.google.com/patent/CN111488133B/en?q=(high+radix)&oq=high+radix

19/32

2/14/26, 1:54 PM

CN111488133B - High-radix approximate Booth coding method and mixed-radix Booth coding approximate multiplier - Goo…

Among them, sel 0 to sel 15 are grouping signals of the first interval to the sixteenth interval respectively, which are used to generate 16 groups, and the grouping signals
will be given to two partial product encoding circuits.
The encoding circuit logic expression of the partial product is:

https://patents.google.com/patent/CN111488133B/en?q=(high+radix)&oq=high+radix

20/32

2/14/26, 1:54 PM

CN111488133B - High-radix approximate Booth coding method and mixed-radix Booth coding approximate multiplier - Goo…

Wherein, c k is the sign correction bit, which is used to distinguish the positive and negative of the multiplicand after encoding. M2 m-4 A, M2 m-3 A, M2 m-2 A, M2 m- 5 B,
M2 m -4 B, M2 m-3 B, M2 m-2 B are the intermediate results of the encoding. The relevant codes M2 m- 4 A, M2 m-3 A, M2 m-2 A used to generate the partial product A in the
intermediate results respectively mean that the multiplier is multiplied by 2 m-4 , 2 m-3 , and 2 m-2 . For example, for the radix-64 Booth code, m=6 means that the multiplier
is multiplied by 4, 8, and 16. The same applies to other radixes and incomplete partial products B.
After obtaining the above sign correction bit c k and the encoded intermediate result, the incomplete partial products A and B are obtained according to the following
formula:

Ai is the i-th bit in the partial product A, Bi is the i-th bit in the partial product B, xi -m+5 is the i-m+5th bit in the multiplier, xi -m+4 is the i-m+4th bit in the multiplier, xi -m+3 is
the i-m+3th bit in the multiplier, xi -m+2 is the i-m+2th bit in the multiplier, i∈[0,n-1], the partial product A and the partial product B have a total of n bits, which are recorded
as the 0th to the n-1th bit respectively.
The present invention also proposes an approximate multiplier based on the above high-radix approximate Booth coding method. The approximate multiplier adopts
mixed-radix Booth coding. The main idea is to combine low-radix precise Booth coding and high-radix approximate Booth coding. Low-radix precise Booth coding is used
in the part with higher weight to generate accurate partial products, and high-radix approximate Booth coding is used in the part with lower weight to generate
approximate partial products. As shown in FIG1 , the approximate multiplier proposed by the present invention comprises an accurate Booth encoding module, a highradix approximate Booth encoding module, a sign bit expansion module, a tree compression module, and a carry adder module; wherein the input end of the accurate
Booth encoding module is connected to external input data, and the output end thereof is connected to the first input end of the partial product sign bit expansion
module and the first input end of the tree compression module; the input end of the high-radix approximate Booth encoding module is connected to external input data,
and the output end thereof is connected to the second input end of the partial product sign bit expansion module and the second input end of the tree compression
module; the output end of the partial product sign bit expansion module is connected to the third input end of the tree compression module; the output end of the tree
compression module is connected to the input end of the carry adder module; the output end of the carry adder module outputs the final result of the multiplier;
The precise Booth coding module can select a base-4 precise Booth coding module, which is used to receive external data of a fixed word length, including a multiplier
and a multiplicand, and the external data is binary data with any bit. The n-m bits with higher weights in the n-bit multiplier and the multiplicand are operated by precise
base-4 Booth coding to obtain the values of all bits of the precise partial product; the high-radix approximate Booth coding module is used to receive external data of a
fixed word length, including a multiplier and a multiplicand, and the external data is binary data with any bit. The n bits with lower weights in the n-bit multiplier and the
multiplicand are encoded by the high-radix approximate Booth coding method proposed in the present invention to obtain the values of all bits of the approximate partial
product. Since the multiplier and the multiplicand can be exchanged in the multiplication operation, the present application takes the encoding of the multiplicand as an
example for explanation, and those skilled in the art should be aware that the encoding method and multiplier proposed in the present invention are also applicable to
encoding the multiplier.
The partial product sign bit extension module generates a sign extension bit through the sign bit extension algorithm proposed in the present invention, adds the
determined sign extension bit to the partial product array, and uses it as the input of the tree compression module. The approximate tree compression module receives
data from the base-4 precise Booth encoding module, the high-radix approximate Booth encoding module, and the partial product sign bit extension module, and
compresses the partial product using arithmetic units such as a 4-2 compressor, a full adder, and a half adder. The carry adder module receives the data output by the
tree compression module, and adds it to obtain the final calculation result of the approximate multiplier proposed in the present invention.
In the mixed radix Booth coding approximate multiplier proposed by the present invention, the high radix approximate Booth coding module includes a grouping unit, a
coding unit and a partial product generating unit, wherein the grouping unit is used to group the input to generate grouping signals sel 0 to sel 15 , and divide the input
data into 16 intervals. For the high radix Booth coding algorithm of radix- 2m , each interval contains 2m -4 input situations, and the grouping is determined by the upper 4
bits of the multiplicand input to the high radix approximate Booth coding module; the coding unit is used to encode the grouping result to obtain the encoded

https://patents.google.com/patent/CN111488133B/en?q=(high+radix)&oq=high+radix

21/32

2/14/26, 1:54 PM

CN111488133B - High-radix approximate Booth coding method and mixed-radix Booth coding approximate multiplier - Goo…

intermediate result, and give the sign correction bit c k to indicate positive or negative; the partial product generating unit adopts an incomplete coding method to
generate partial products A and partial products B according to the sign correction bit c k generated by the coding module and the encoded intermediate result. The two
incompletely coded partial products are both generated by shifting the multiplicand, so no complex calculation logic is introduced in the generation process.
In the process of partial accumulation and addition, the sign bit extension problem needs to be considered. For n×n multipliers, all partial product sign bits must be
extended to the most significant bit with a weight of 2 2n-1 . For a multiplier with a bit width of 16×16 bits using mixed radix Booth coding of exact radix-4 and
approximate radix-256, the partial product array after the sign bit extension is shown in Figure 3. Among them, the black triangle represents the partial product generated
by the approximate radix-256 Booth coding, the black circle represents the partial product generated by the exact radix-4 Booth coding, the hollow triangle and the hollow
circle represent the sign correction bit of the corresponding partial product, and the black square represents the sign bit extension. Therefore, the black squares in the
same row of all partial product arrays of the multiplier in Figure 3 have the same value and have data correlation. If they are compressed directly, hardware resources will
be wasted. Therefore, the present invention proposes an optimization method for sign bit extension, which avoids data redundancy and simplifies hardware design.
In conjunction with FIG3 , a multiplier with a bit width of 16×16 bits using the mixed radix Booth coding of the exact radix-4 and the approximate radix-256 proposed in
the present invention is used as an example for explanation. Sk represents the most significant bit of the partial product of the kth row, that is, the sign bit to be extended;
Sign represents the sum of all extended sign bits, that is, the sum of the values represented by all the black squares in FIG3 . In addition, by combining the basic
mathematical and logical principles shown in formula (11) and formula (12), the derivation of formula (13) can be obtained.

https://patents.google.com/patent/CN111488133B/en?q=(high+radix)&oq=high+radix

22/32

2/14/26, 1:54 PM

CN111488133B - High-radix approximate Booth coding method and mixed-radix Booth coding approximate multiplier - Goo…

The first term in the last equation in formula (13) has a weight of 2 32 , while the actual most significant bit weight of the multiplier is 2 31 , so the term with a weight of 2
32

can be simplified to obtain formula (14).

By converting the subtraction constant term in formula (14) according to the principle of binary complement, the result shown in formula (15) can be obtained.

https://patents.google.com/patent/CN111488133B/en?q=(high+radix)&oq=high+radix

23/32

2/14/26, 1:54 PM

CN111488133B - High-radix approximate Booth coding method and mixed-radix Booth coding approximate multiplier - Goo…

表明了所有部分积的最高有效位都需要取反并与所在权重位一一对应，即部分积阵列中221权重位的第0行部分积最高有效位S0取反，部分积阵列中221权重位的第1行部分
积最高有效位S1取反，部分积阵列中224权重位的第2行部分积最高有效位S2取反，部分积阵列中226权重位的第3行部分积最高有效位S3取反，部分积阵列中228权重位的第
4行部分积最高有效位S4取反，部分积阵列中230权重位的第5行部分积最高有效位S5取反，本实施例中部分积阵列共6行，分别记为第0行至第5行，操作对应图4中灰色三

角形和灰色圆形。公式(15)最后一个等式的第二项(1010101011)补·222表明了在部分积阵列中需要在对应权重上引入多个“1”进行累加，而其中“0”的位置可以直接忽略，具
体结合图4可知，分别在部分积阵列中222权重位的第1行部分积最高有效位前补1，在部分积阵列中223权重位的第1行部分积最高有效位前补1，在部分积阵列中225权重位
的第2行部分积最高有效位前补1，在部分积阵列中227权重位的第3行部分积最高有效位前补1，部分积阵列中229权重位的第4行部分积最高有效位前补1，部分积阵列中231
权重位的第5行部分积最高有效位前补1。基于以上分析，根据本发明提出的符号位扩展的优化方法可以将图3的部分积阵列优化为图4所示的部分积阵列。The last
equation of formula (15) shows the location of the optimized partial product sign bit in the compression tree.

It shows that the most significant bits of all partial products need to be inverted and correspond one-to-one with the weight bits, that is, the most significant bit S0 of the
partial product in the 0th row of 2 21 weight bits in the partial product array is inverted, the most significant bit S1 of the partial product in the 1st row of 2 21 weight bits in
the partial product array is inverted, the most significant bit S2 of the partial product in the 2nd row of 2 24 weight bits in the partial product array is inverted, the most
significant bit S3 of the partial product in the 3rd row of 2 26 weight bits in the partial product array is inverted, the most significant bit S4 of the partial product in the 4th
row of 2 28 weight bits in the partial product array is inverted, and the most significant bit S5 of the partial product in the 5th row of 2 30 weight bits in the partial product
array is inverted. In this embodiment, the partial product array has a total of 6 rows, which are recorded as row 0 to row 5 respectively, and the operations correspond to
the gray triangles and gray circles in Figure 4. The second term (1010101011) of the last equation of formula (15) complements · 2 22, indicating that multiple "1"s need to
be introduced in the corresponding weights for accumulation in the partial product array, and the positions of "0" can be directly ignored. Specifically, in conjunction with
FIG4, it can be seen that the most significant bit of the partial product in the first row of the 2 22 weight positions in the partial product array is complemented by 1, the
most significant bit of the partial product in the first row of the 2 23 weight positions in the partial product array is complemented by 1, the most significant bit of the
partial product in the second row of the 2 25 weight positions in the partial product array is complemented by 1, the most significant bit of the partial product in the third
row of the 2 27 weight positions in the partial product array is complemented by 1, the most significant bit of the partial product in the fourth row of the 2 29 weight
positions in the partial product array is complemented by 1, and the most significant bit of the partial product in the fifth row of the 2 31 weight positions in the partial
product array is complemented by 1. Based on the above analysis, the optimization method for sign bit extension proposed in the present invention can optimize the
partial product array of FIG3 into the partial product array shown in FIG4.
Similarly, for an n×n multiplier using the mixed radix Booth coding of exact radix-4 and approximate radix- 2m proposed in the present invention, a simplified derivation
process of the general conclusion is shown in formula (16).

https://patents.google.com/patent/CN111488133B/en?q=(high+radix)&oq=high+radix

24/32

2/14/26, 1:54 PM

CN111488133B - High-radix approximate Booth coding method and mixed-radix Booth coding approximate multiplier - Goo…

https://patents.google.com/patent/CN111488133B/en?q=(high+radix)&oq=high+radix

25/32

2/14/26, 1:54 PM

CN111488133B - High-radix approximate Booth coding method and mixed-radix Booth coding approximate multiplier - Goo…

Among them, the third and fourth steps cancel out the terms that have no effect on the result; the fourth and fifth steps merge the constant terms; in the complement
representation method of the final derivation result, the total number of "1" is (n-m)/2+2. The corresponding method of the sign bit optimization result in the partial
product compressed tree array is similar to Figure 3. Here, only the mixed coding of radix-64 and radix-1024 approximation is used for the multiplier with a bit width of
16×16 bits, as shown in Figures 5 and 6, and other cases are not listed.
For the above-mentioned mixed radix Booth coded approximate multiplier, the present invention also proposes a method for deriving an error model. Assume that the bit
width of the two's complement multiplier X and the multiplicand Y is n, the exact result of the multiplier is Z, the approximate result is Z A , and the approximate coding
result of the multiplicand Y is Y A . Based on the approximate method of the present invention, the error generated by the multiplier is as shown in formula (17). It can be
known that the error of the multiplication result is essentially caused by the approximate coding error E Y of the multiplicand Y, and the multiplier X plays a linear scaling
effect on the error.
Error{Z A }＝X·Y A -X·Y＝X·(Y A -Y)＝X·E Y (17)
By expanding the approximate coding error E Y of the multiplicand Y, it can be found that since the high-significant bits are all encoded with accurate radix-4 Booth
coding, no error is introduced. Therefore, three approximate high-radix Booth codings, radix-64, radix-256, and radix-1024, are analyzed.
The specific rules of the base-64 approximate high-radix Booth coding can be seen in Figure 2. In order to have a more complete analysis, the average error Emean, error
distance (also called absolute error) ED, maximum absolute error EDmax, and average error distance MED are selected for discussion. For a set of base-64 approximate
high-radix Booth coding, there are 6 inputs, and there are 2 6 or 64 input situations. Assuming that the probability of each input being "1" or "0" is consistent, which is 1/2,
the probability of each situation occurring is 1/64. Then the mean error formula is shown in formula (18), where P Y is the probability of the corresponding multiplicand Y
occurring.

The derivation formulas of the error distance ED, the maximum absolute error EDmax, and the average error distance MED are shown in formula (19), where max{～}
represents the maximum value in the set {～}.

https://patents.google.com/patent/CN111488133B/en?q=(high+radix)&oq=high+radix

26/32

2/14/26, 1:54 PM

CN111488133B - High-radix approximate Booth coding method and mixed-radix Booth coding approximate multiplier - Goo…

The above information can be obtained according to the calculation formula and Figure 2. For three approximate high-radix Booth codes of base-64, base-256, and base1024, a unified expression can be obtained. If the high-radix approximate Booth code of base-2 m is used, the relevant error parameters can be derived as follows:

A unified error parameter expression can be obtained because the values in the 16 approximate intervals divided by the high cardinality approximate Booth coding of
base- 2m have the characteristics shown in Figure 7, and each interval contains 2m-4 values.
From Figure 7, we can find that the specific numerical distribution is related to the parameter m of the high cardinality approximate Booth coding of base-2 m . Moreover,
the 8 groups of 0, 1, 2, 4, 11, 13, 14, and 15 have the same output mode; the 4 groups of 3, 5, 6, and 8 have the same output mode; and the 4 groups of 7, 9, 10, and 12
have the same output mode. These characteristics provide a practical basis for the derivation of error analysis.
Combining the essential meaning of formula (17) with the specific derivation of formula (20), we can further obtain the derivation method of multiplier related
parameters. Assuming that the input of the multiplier is completely independent and random, the bit width of the multiplier X and the multiplicand Y is n, the precise
result of the multiplier is Z, and the approximate result is Z A , then the derivation of the average error Emean of the multiplier is shown in formula (21).

https://patents.google.com/patent/CN111488133B/en?q=(high+radix)&oq=high+radix

27/32

2/14/26, 1:54 PM

CN111488133B - High-radix approximate Booth coding method and mixed-radix Booth coding approximate multiplier - Goo…

It can be seen that the mean error introduced by this approximation method is very small and has nothing to do with the bit width n of the multiplier and the selected
radix parameter m. In the error analysis of the multiplier, the normalized mean error is more general. The normalized mean error is a statistical value obtained by dividing
the mean error by the maximum output. Even for a multiplier with a smaller bit width of 8×8, the normalized mean error of the high radix approximate mixed radix Booth
coding proposed by the present invention is only -2 -18 .
The maximum absolute error EDmax of the multiplier is derived as shown in formula (22).

https://patents.google.com/patent/CN111488133B/en?q=(high+radix)&oq=high+radix

28/32

2/14/26, 1:54 PM

CN111488133B - High-radix approximate Booth coding method and mixed-radix Booth coding approximate multiplier - Goo…

The maximum absolute error is normalized, and the normalized maximum absolute error is a statistical value obtained by dividing the maximum absolute error by the
maximum output. The normalized maximum absolute error of the high-radix approximate mixed-radix Booth coding proposed by the present invention is 2 mn-5 , which
means that the smaller the value of m and the larger the value of n, the smaller the normalized maximum absolute error, which shows the controllability of the error.
The mean error distance MED of the multiplier is derived as shown in formula (23):

The average error distance is normalized, and the normalized average error distance is a statistical value obtained by dividing the average error distance by the maximum
output. The normalized average error distance of the high-radix approximate mixed-radix Booth coding proposed by the present invention is 1.5×2 mn-8 , which means
that the smaller the value of m and the larger the value of n, the smaller the normalized average error distance, which also shows the controllability of the error.
Compared with the traditional accurate radix-4 Booth coded multiplier, the multiplier using accurate radix-4 and approximate radix-1024 mixed Booth coding proposed in
the present invention has a delay reduced by 8.3%, a power consumption reduced by 39.1%, and has a mean error of almost 0 and a small normalized error. At the same
time, the present invention proposes a derivation method for an error model of an approximate mixed radix Booth coded multiplier, which saves simulation time and
avoids large data volume calculations compared to obtaining accuracy indicators through simulation.
In summary, the present invention proposes a high-radix approximate Booth coding method and an approximate multiplier based on high-radix approximate mixed-radix
Booth coding. Since the error of high-radix approximate Booth coding is not easy to control, the multiplier of the present invention combines low-radix precise Booth
coding and high-radix approximate Booth coding in a mixed-radix manner, which are respectively used to generate the precise part and the approximate part of the
multiplier partial product, thereby achieving the purpose of reducing circuit complexity while ensuring the high accuracy of the calculation result; at the same time, since
there is a large amount of repeated data in the sign extension bit of the mixed-radix Booth coding, the present invention also redesigns the sign extension bit to avoid
data redundancy. In addition, in view of the problem that the precision simulation of high-bitwidth multipliers requires a long time and a large amount of data, the present
invention establishes an error model of an approximate multiplier, and calculates the corresponding error index data by deducing the error model of the multiplier.
Those skilled in the art will appreciate that the embodiments described herein are intended to help readers understand the principles of the present invention, and should
be understood that the protection scope of the present invention is not limited to such specific statements and embodiments. Those skilled in the art can make various

https://patents.google.com/patent/CN111488133B/en?q=(high+radix)&oq=high+radix

29/32

2/14/26, 1:54 PM

CN111488133B - High-radix approximate Booth coding method and mixed-radix Booth coding approximate multiplier - Goo…

other specific variations and combinations that do not deviate from the essence of the present invention based on the technical revelations disclosed by the present
invention, and these variations and combinations are still within the protection scope of the present invention.

Patent Citations (8)
Publication number

Priority date

Publication date

Assignee

Title

CN1201182A *

1997-04-30

1998-12-09

朗迅科技公司

Method for providing pure carry save input for multiplier

CN101122850A *

2007-09-21

2008-02-13

清华大学

Large Number Multiplier Based on Quadratic Booth Coding

CN102257473A *

2008-10-30

2011-11-23

音频专用集成电路公司

A high radix digital multiplier

CN102591615A *

2012-01-16

2012-07-18

中国人民解放军国防科学技
术大学

Structured mixed bit-width multiplying method and structured mixed bit-width
multiplying device

CN110362292A *

2019-07-22

2019-10-22

电子科技大学

A kind of approximate multiplying method based on approximate 4-2
compressor and approximate multiplier

CN110673823A *

2019-09-30

2020-01-10

上海寒武纪信息科技有限公
司

Multiplier, data processing method and chip

CN110955403A *

2019-11-29

2020-04-03

电子科技大学

Approximate Radix-8 Booth Coder and Approximate Binary Multiplier for Hybrid
Booth Codes

2017-06-20

2021-05-18

Intel Corporation

High radix subset code multiplier architecture

Family To Family Citations
US11010134B2 *

* Cited by examiner, † Cited by third party

Non-Patent Citations (3)
Title
Design and Analysis of Area and Power Efficient Approximate Booth Multipliers;Suganthi Venkatachalam等;《IEEE》;1697-1703 *
Design of an Energy-Efficient Approximate Compressor for Error-Resilient Multiplications;Xilin Yi等;《IEEE》;1-5 *

基于近似计算的二进制乘法器设计及其在可容错系统中的应用;衣溪琳;《中国优秀硕士论文电子期刊网 信息科技辑》;I137-44 *
* Cited by examiner, † Cited by third party

Cited By (12)
Publication number

Priority date

Publication date

Assignee

Title

CN111966323B *

2020-08-18

2022-09-13

合肥工业大学

Approximate multiplier based on unbiased compressor and calculation method

US12169702B2 *

2020-09-08

2024-12-17

Macronix International
Co., Ltd.

In-memory computing method and in-memory computing apparatus

CN112612446B *

2020-12-15

2024-02-23

东南大学

16 x 8 approximate multiplier on-chip dynamic computing system based on
Booth coding

CN112732224B *

2021-01-12

2024-01-05

东南大学

Reconfigurable approximate tensor multiplication and addition unit and method
for convolutional neural network

CN113778377B *

2021-08-19

2024-03-29

南京航空航天大学

Squarer structure based on base 8 Booth folding codes

CN114489566B *

2022-02-16

2025-06-24

南京航空航天大学

A method for identifying characteristics of an approximate squarer and an
approximate circuit

CN115826913B *

2022-11-04

2025-09-12

电子科技大学

Approximate Binary Multiplier Based on Static Segment Compensation Method

CN115982528B *

2022-11-25

2025-12-12

上海交通大学

A Method and System for Approximate Precoding Convolution Operation Based
on Booth Algorithm

CN116069292B *

2023-02-08

2025-09-05

南京航空航天大学

A multiplier for retraining approximate neural networks and its retraining
algorithm

CN116048455B *

2023-03-07

2023-06-02

南京航空航天大学

Insertion type approximate multiplication accumulator

CN116991359B *

2023-09-26

2023-12-22

上海为旌科技有限公司

Booth multiplier, hybrid Booth multiplier and operation method

Family To Family Citations

https://patents.google.com/patent/CN111488133B/en?q=(high+radix)&oq=high+radix

30/32

2/14/26, 1:54 PM

CN111488133B - High-radix approximate Booth coding method and mixed-radix Booth coding approximate multiplier - Goo…

CN119645344B *

2024-11-21

2025-10-31

南京大学

An implementation method for an approximate multiplier for neural networks and
the approximate multiplier itself.

* Cited by examiner, † Cited by third party, ‡ Family to family citation

Similar Documents
Publication

Publication Date

Title

CN111488133B

2023-03-28

High-radix approximate Booth coding method and mixed-radix Booth coding approximate multiplier

CN105183424B

2017-09-01

A Fixed Bit Width Multiplier with High Accuracy and Low Energy Consumption

CN110362292B

2022-12-20

Approximate multiplication method and approximate multiplier based on approximate 4-2 compressor

TWI783295B

2022-11-11

Multiplier and multiplication method

Liu et al.

2024

Spark: Scalable and precision-aware acceleration of neural networks via efficient encoding

CN111832719A

2020-10-27

A Fixed-Point Quantized Convolutional Neural Network Accelerator Computing Circuit

Wang et al.

2020

DSP-efficient hardware acceleration of convolutional neural network inference on FPGAs

JP7292297B2

2023-06-16

probabilistic rounding logic

CN114647399A

2022-06-21

Low-energy-consumption high-precision approximate parallel fixed-width multiplication accumulation device

CN106775577A

2017-05-31

A kind of high-performance non-precision redundant manipulators multiplier and its method for designing

Chen et al.

2024

High reliable and accurate stochastic computing-based artificial neural network architecture design

CN113778377B

2024-03-29

Squarer structure based on base 8 Booth folding codes

Haritha et al.

2017

Design of an enhanced array based approximate arithmetic computing model for multipliers and squarers

Amirafshar et al.

2022

An approximate carry disregard multiplier with improved mean relative error distance and probability of correctness

CN110825346B

2023-09-12

An unsigned approximate multiplier with low logic complexity

CN119512501A

2025-02-25

A multiplier based on radix-4 Booth coding and improved Wallace compression tree

Niknia et al.

2023

Nanoscale accelerators for artificial neural networks

CN111506293B

2022-10-21

A High-Base Divider Circuit Based on SRT Algorithm

Song et al.

2024

Design of multiplier circuit based on signed-digit hybrid stochastic computing

Ipek

2019

Memristive accelerators for dense and sparse linear algebra: from machine learning to high-performance scientific computing

Selvakumari et al.

2016

Transistor level implementation of a 8 bit multiplier using vedic mathematics in 180nm technology

Vuthaj et al.

2023

From Floats To Posits: A Conversion Framework

Chen et al.

2021

High-accuracy and Low-latency Hybrid Stochastic Computing for Artificial Neural Network

Liu et al.

2025

ESACT: An End-to-End Sparse Accelerator for Compute-Intensive Transformers via Local Similarity

Huang et al.

2023

Decomposable architecture and fault mitigation methodology for deep learning accelerators

Priority And Related Applications
Priority Applications (1)
Application

Priority date

Filing date

Title

CN202010292881.4A

2020-04-15

2020-04-15

High-radix approximate Booth coding method and mixed-radix Booth coding approximate multiplier

Applications Claiming Priority (1)
Application

Filing date

Title

CN202010292881.4A

2020-04-15

High-radix approximate Booth coding method and mixed-radix Booth coding approximate multiplier

Legal Events

https://patents.google.com/patent/CN111488133B/en?q=(high+radix)&oq=high+radix

31/32

2/14/26, 1:54 PM

CN111488133B - High-radix approximate Booth coding method and mixed-radix Booth coding approximate multiplier - Goo…

Date

Code

Title

2020-08-04

PB01

Publication

2020-08-04

PB01

Publication

2020-08-28

SE01

Entry into force of request for substantive examination

2020-08-28

SE01

Entry into force of request for substantive examination

2023-03-28

GR01

Patent grant

2023-03-28

GR01

Patent grant

About

Description

Send Feedback

Public Datasets

Terms

https://patents.google.com/patent/CN111488133B/en?q=(high+radix)&oq=high+radix

Privacy Policy

Help

32/32

2/14/26, 2:01 PM

EP2461254B1 - Table-driven routing in a dragonfly processor interconnect network - Google Patents

Patents
Back to results

16 of 125,048

high radix

(high radix);

Table-driven routing in a dragonfly processor interconnect network
Images (9)

EP2461254B1
European Patent Office

Download PDF

Find Prior Art

Similar

Other languages: German, French
Inventor: Mike Parker, Steve Scott, Albert Cheng, Robert
Alverson

Classifications

Current Assignee : Intel Corp

G06F15/17362 Indirect interconnection networks hierarchical topologies
G06F15/17312 Routing techniques specific to parallel machines, e.g. wormhole, store and
forward, shortest path problem congestion

Worldwide applications
2011 JP EP EP US 2014 US 2015 JP 2016 US

H04L45/10 Routing in connection-oriented networks, e.g. X.25 or ATM

Application EP11187953.2A events

H04L45/125 Shortest path evaluation based on throughput or bandwidth

2011-11-04

Application filed by Intel Corp

H04L45/586 Association of routers of virtual routers

2011-11-04

Priority to EP16180770.6A

H04L45/745 Address table lookup; Address filtering

2012-06-06

Publication of EP2461254A1

2020-09-16

Application granted

2020-09-16

Publication of EP2461254B1

Status

Active

2031-11-04

Anticipated expiration

Hide more classifications

Landscapes

Engineering & Computer Science
Computer Networks & Wireless Communication

Info: Patent citations (44), Cited by (81), Legal events, Similar
documents, Priority and Related Applications

Show more

External links: Espacenet, EPO GPI, EP Register, Global Dossier,
Discuss

Hide Dependent

Claims (3)

1.

A multiprocessor computer system including a dragonfly processor interconnect network, comprising:
at least one router (104, 105, 106) operable to route data by selecting from among a plurality of network paths from a target node to a destination node in the network
based on one or more global and local routing tables and using one or more of network congestion information from neighboring routers and failed network link
information from neighboring routers in selecting a route, wherein the network is a hierarchical network of two layers, a first layer that connects all router chips of the
computer systems within a given local group (101) and a second layer that connects all local groups (101, 102, 103), wherein global tables are used to determine how to
route to a remote group (102) when a source group is not a target group, and wherein the routing tables comprise:
a global non-minimal table set to route non-minimal traffic from a source local group to an intermediate group within the second layer that is not a destination group;
a global minimal table to determine a direct path from a source local group to a target local group in the second layer,
a local non-minimal table set to pick a router chip in a local group that is used as a root for non-minimal routing within the local group, and
a local minimal table to be used for minimal routing within a target group,
wherein the global non-minimal table set comprises one or more tables including a first table to select which rank to traverse to leave a source group, a second table
to select an inter chassis path to traverse, and a third table to select an optical port to leave the a current router chip on, and
wherein the tables are arranged hierarchically in fixed priority order,
wherein the failed network link information is based on a link alive signal being broadcasted from each tile in the router to all other tiles in the router,
wherein the link alive signal for each tile indicates whether a corresponding tile has an established serial link with the router it is connected to,
wherein ports for which the link is not alive are considered invalid.

https://patents.google.com/patent/EP2461254B1/en?q=(high+radix)&oq=high+radix&page=1

1/20

2/14/26, 2:01 PM
2.

EP2461254B1 - Table-driven routing in a dragonfly processor interconnect network - Google Patents

A method of operating a multiprocessor computer system, comprising:
routing data by selecting from among a plurality of network paths from a target node to a destination node in a network based on one or more global and local routing
tables and using one or more of network congestion information from neighboring routers and failed network link information from neighboring routers in selecting a
route, wherein the network is a hierarchical network of two layers, a first layer that connects all router chips of the computer systems within a given local group and a
second layer that connects all local groups (101, 102, 103), wherein global tables are used to determine how to route to a remote group (102) when a source group is not
a target group, and wherein the routing tables comprise:
a global non-minimal table set to route non-minimal traffic from a source local group to an intermediate group within the second layer that is not a destination group;
a global minimal table to determine a direct path from a source local group to a target local group in the second layer,
a local non-minimal table set to pick a router chip in a local group that is used as a root for non-minimal routing within the local group, and
a local minimal table to be used for minimal routing within a target group,
wherein the global non-minimal table set comprises one or more tables including a first table to select which rank to traverse to leave a source group, a second table
to select an inter chassis path to traverse, and a third table to select an optical port to leave the a current router chip on, and
wherein the tables are arranged hierarchically in fixed priority order,
wherein the failed network link information is based on a link alive signal being broadcasted from each tile in the router to all other tiles in the router,
wherein the link alive signal for each tile indicates whether a corresponding tile has an established serial link with the router it is connected to,
wherein ports for which the link is not alive are considered invalid.
3.

A carrier medium carrying computer readable code for controlling a multiprocessor computer system to carry out the method of claim 2.

Description

Field of the Invention
[0001] The invention relates generally to computer interconnect networks, and more specifically in one embodiment to table-driven routing in a dragonfly topology
processor interconnect network.
Limited Copyright Waiver
[0002] A portion of the disclosure of this patent document contains material to which the claim of copyright protection is made. The copyright owner has no objection
to the facsimile reproduction by any person of the patent document or the patent disclosure, as it appears in the U.S. Patent and Trademark Office file or records,
but reserves all other rights whatsoever.
Background
[0003] Computer systems have long relied on network connections to transfer data, whether from one computer system to another computer system, one computer
component to another computer component, or from one processor to another processor in the same computer. Most computer networks link multiple
computerized elements to one another, and include various functions such as verification that a message sent over the network arrived at the intended recipient,
confirmation of the integrity of the message, and a method of routing a message to the intended recipient on the network.
[0004] Processor interconnect networks are used in multiprocessor computer systems to transfer data from one processor to another, or from one group of processors
to another group. The number of interconnection links can be very large with computer systems having hundreds or thousands of processors, and system
performance can vary significantly based on the efficiency of the processor interconnect network. The number of connections, number of intermediate nodes
between a sending and receiving processing node, and the speed or type of connection all play a factor in the interconnect network performance.
[0005] Similarly, the network topology, or pattern of connections used to tie processing nodes together affects performance, and remains an area of active research. It
is impractical to directly link each node to each other node in systems having many tens of processors, and all but impossible as the number of processors
reaches the thousands.
[0006] Further, the cost of communications interfaces, cables, and other factors can add significantly to the cost of poorly designed or inefficient processor
interconnect networks, especially where long connections or high-speed fiber optic links are required. A processor interconnect network designer is thereby
challenged to provide fast and efficient communication between the various processing nodes, while controlling the number of overall links, and the cost and
complexity of the processor interconnect network.
[0007] The topology of a network, or the method used to determine how to link a processing node to other nodes in a multiprocessor computer system, is therefore an
area of interest.
[0008] Prior art document US2010049942A1 discloses the DragonFly interconnect and an a adaptive routing algorithm that chooses between minimal (MIN) and
randomized non minimal (VAL) routes on a packet by packet basis to load-balance the network. The choice is made using hop count and queue length.
[0009] Prior art document US2008285562A1 discloses a router for high radix networks. Mechanisms for routing are source routing, table driven routing and algorithmic
routing. Routing table size can be reduced by splitting into a local routing table and a global routing table. Deterministic and adaptive routing depends on a flag
in the packet. Each routing table comprises two entries, one entry for normal use an another in case of fault of a link.
[0010] Prior art document VARADARAJAN S ET AL: "Reinforcing reachable routes",COMPUTER NETWORKS discloses link-state algorithms where each router
broadcasts its local connectivity to every other router in the network. Every router independently assimilates the topology information to build a complete map of
the network, which is then used to construct routing tables.
Summary
[0011] The invention comprises in one example a multiprocessor computer system having a dragonfly processor interconnect network that comprises a plurality of
processor nodes and a plurality of routers. The routers are operable route data by selecting from among a plurality of network paths from a target node to a
destination node in the dragonfly network based on one or more routing table, such as local and global routing tables, and minimal and non-minimal routing
tables.
[0012] The present invention encompasses a carrier medium carrying machine readable instructions for controlling a multiprocessor computer system to perform a
method according to the invention. The carrier medium can comprise any storage medium such as a floppy disk, CD ROM, DVD ROM, hard disk, magnetic tape,
or programmable memory device, or a transient medium such as an electrical, optical, microwave, RF, electromagnetic, magnetic or acoustical signal. An
example of such a signal is an encoded signal carrying a machine readable code over a communications network, e.g. a TCP/IP signal carrying computer code
over an IP network such as the Internet, an intranet, or a local area network.
Brief Description of the Figures
[0013]

Figure 1 is a block diagram of a dragonfly network topology, consistent with an example embodiment of the invention.

https://patents.google.com/patent/EP2461254B1/en?q=(high+radix)&oq=high+radix&page=1

2/20

2/14/26, 2:01 PM

EP2461254B1 - Table-driven routing in a dragonfly processor interconnect network - Google Patents
Figure 2 is a graph illustrating scalability of a dragonfly network in nodes for various router radices, consistent with an example embodiment of the
invention.
Figure 3 is a block diagram illustrating a dragonfly network topology, consistent with an example embodiment of the invention.
Figure 4 is block diagram of dragonfly network topology groups, consistent with some example embodiments of the invention.
Figure 5 is a block diagram of a dragonfly network illustrating minimal and non-minimal routing using virtual channels, consistent with an example
embodiment of the invention.
Figure 6 is a graph illustrating latency v. offered load for a variety of routing algorithms using various traffic patterns, consistent with an example
embodiment of the invention.
Figure 7 is a node group diagram of a dragonfly topology network illustrating adaptive routing via global channels using backpressure from intermediate
nodes, consistent with an example embodiment of the invention.
Figure 8 is a node diagram illustrating credit round trip latency tracking, consistent with an example embodiment of the invention.
Figure 9 shows a router configuration, consistent with an example embodiment of the invention.
Figure 10 shows a group of nodes in a dragonfly processor interconnect network, consistent with an example embodiment of the invention.
Figure 11 shows connections between several node groups in a dragonfly processor interconnect network, consistent with an example embodiment of
the invention.
Figure 12 shows a router table configuration for a dragonfly processor interconnect network router, consistent with an example embodiment of the
invention.

Detailed Description
[0014] In the following detailed description of example embodiments of the invention, reference is made to specific examples by way of drawings and illustrations.
These examples are described in sufficient detail to enable those skilled in the art to practice the invention, and serve to illustrate how the invention may be
applied to various purposes or embodiments. Other embodiments of the invention exist and are within the scope of the invention, and logical, mechanical,
electrical, and other changes may be made without departing from the subject or scope of the present invention. Features or limitations of various embodiments
of the invention described herein, however essential to the example embodiments in which they are incorporated, do not limit the invention as a whole, and any
reference to the invention, its elements, operation, and application do not limit the invention as a whole but serve only to define these example embodiments.
The following detailed description does not, therefore, limit the scope of the invention, which is defined only by the appended claims.
[0015] Interconnection networks are widely used to connect processors and memories in multiprocessors, as switching fabrics for high-end routers and switches, and
for connecting I/O devices. As processor and memory performance continues to increase in a multiprocessor computer system, the performance of the
interconnection network plays a central role in determining the overall performance of the system. The latency and bandwidth of the network largely establish
the remote memory access latency and bandwidth.
[0016] A good interconnection network typically designed around the capabilities and constraints of available technology. Increasing router pin bandwidth, for example,
has motivated the use of high-radix routers in which increased bandwidth is used to increase the number of ports per router, rather than maintaining a small
number of ports and increasing the bandwidth per port. The Cray Black Widow system, one of the first systems to employ a high-radix network, uses a variant of
the folded-Clos topology and radix-64 routers-a significant departure from previous low-radix 3-D torus networks. Recently, the advent of economical optical
signaling enables topologies with long channels. However, these long optical channels remain significantly more expensive than short electrical channels. A
Dragonfly topology was therefore introduced, exploiting emerging optical signaling technology by grouping routers to further increase the effective radix of the
network.
[0017] The topology of an interconnection network largely determines both the performance and the cost of the network. Network cost is dominated by the cost of
channels, and in particular the cost of the long, global, inter-cabinet channels. Thus, reducing the number of global channels can significantly reduce the cost of
the network. To reduce global channels without reducing performance, the number of global channels traversed by the average packet must be reduced. The
dragonfly topology reduces the number of global channels traversed per packet using minimal routing to one.
Dragonfly Topology Example
[0018] To achieve this global diameter of one, very high-radix routers, with a radix of approximately 2√N (where N is the size of the network) are used. While radix 64
routers have been introduced, and a radix of 128 is feasible, much higher radices in the hundreds or thousands are needed to build machines that scale to 8K 1M nodes if each packet is limited to only one global hop using traditional very high radix router technology. To achieve the benefits of a very high radix with
routers without requiring hundreds or thousands of ports per node, the Dragonfly network topology proposes using a group of routers connected into a
subnetwork as one very high radix virtual router. This very high effective radix in turn allows us to build a network in which all minimal routes traverse at most one
global channel. It also increases the physical length of the global channels, exploiting the capabilities of emerging optical signaling technology.
[0019] Achieving good performance on a wide range of traffic patterns on a dragonfly topology involves selecting a routing algorithm that can effectively balance load
across the global channels. Global adaptive routing (UGAL) can perform such load balancing if the load of the global channels is available at the source router,
where the routing decision is made. With the Dragonfly topology, however, the source router is most often not connected to the global channel in question.
Hence, the adaptive routing decision is made based on remote or indirect information.
[0020] The indirect nature of this decision leads to degradation in both latency and throughput when conventional UGAL (which uses local queue occupancy to make
routing decisions) is used. We propose two modifications to the UGAL routing algorithm for the Dragonfly network topology that overcome this limitation with
performance results approaching an ideal implementation using global information. Adding selective virtual-channel discrimination to UGAL (UGAL-VC H)
eliminates bandwidth degradation due to local channel sharing between minimal and non-minimal paths. Using credit-round trip latency to both sense global
channel congestion and to propagate this congestion information upstream (UGAL-CR) eliminates latency degradation by providing much stiffer backpressure
than is possible using only queue occupancy for congestion sensing.
[0021] High-radix networks reduce the diameter of the network but require longer cables compared to low-radix networks. Advances in signaling technology and the
recent development of active optical cables facilitate implementation of high-radix topologies with longer cables.
[0022] An interconnection network is embedded in a packaging hierarchy. At the lowest level, the routers are connected via circuit boards, which are then connected via
a backplane or midplane. One or more backplanes are packaged in a cabinet, with multiple cabinets connected by electrical or optical cables to form a complete
system. The global (inter-cabinet) cables and their associated transceivers often dominate the cost of a network. To minimize the network cost, the topology
should be matched to the characteristics of the available interconnect technologies, such as cost and performance.
[0023] The maximum bandwidth of an electrical cable drops with increasing cable length because signal attenuation due to skin effect and dielectric absorption
increases linearly with distance. For typical high-performance signaling rates (10-20Gb/s) and technology parameters, electrical signaling paths are limited to
about 1m in circuit boards and 10m in cables. At longer distances, either the signaling rate must be reduced or repeaters inserted to overcome attenuation.
[0024] Historically, the high cost of optical signaling limited its use to very long distances or applications that demanded performance regardless of cost. Although
optical cables have a higher fixed cost, their ability to transmit data over long distances at several times the data rate of copper cables results in a lower cost per
unit distance than electrical cables. Based on the data available using current technologies, the break-even point is at 10m. For distances shorter than 10m,
electrical signaling is less expensive. Beyond 10m, optical signaling is more economical. The Dragonfly topology exploits this relationship between cost and
distance. By reducing the number of global cables, it minimizes the effect of the higher fixed overhead of optical signaling, and by making the global cables
longer, it maximizes the advantage of the lower per-unit cost of optical fibers.

https://patents.google.com/patent/EP2461254B1/en?q=(high+radix)&oq=high+radix&page=1

3/20

2/14/26, 2:01 PM

EP2461254B1 - Table-driven routing in a dragonfly processor interconnect network - Google Patents

[0025] The dollar cost of a dragonfly also compares favorably to a flattened butterfly for networks larger than 1k nodes, showing approximately a 10% savings for up to
4k nodes, and approximately a 20% cost savings relative to flattened butterfly topologies for more than 4k nodes as the dragonfly has fewer long, global cables.
Folded Clos and 3-d torus networks suffer in comparison, because of the larger number of cables needed to support high network diameters. For a network of
only 1k nodes, the dragonfly is 62% the cost of a 3-d torus network and 50% that of a folded Clos network. This reduction in network cost is directly correlated to
a reduction in network power consumed, which is a significant advantage for large networks as well as for installations that are desirably environmentally
friendly.
[0026] The example embodiments of a dragonfly network presented here show how use of a group of routers as a virtual router can increase the effective radix of a
network, and hence reduce network diameter, cost, and latency. Because the dragonfly topology reduces the number global cables in a network, while at the
same time increasing their length, the dragonfly topology is particularly well suited for implementations using emerging active optical cables-which have a high
fixed cost but a low cost per unit length compared to electrical cables. Using active optical cables for the global channels, a dragonfly network reduces cost by
20% compared to a flattened butterfly and by 52% compared to a folded Clos network of the same bandwidth.
[0027] To show an example Dragonfly network topology, the following symbols are used in the description of the dragonfly topology and in example routing algorithms
presented later:

N Number of network terminals
p Number of terminals connected to each router
a Number of routers in each group
k Radix of the routers
k_ Effective radix of the group (or the virtual router)
h Number of channels within each router used to connect to other groups
g Number of groups in the system
q Queue depth of an output port
qvc Queue depth of an individual output VC
H Hop count
Outi Router output port i
[0028] The Dragonfly topology is a hierarchical network with three levels, as shown in Figure 1: routers (104, 105, and 106), groups (101, 102, and 103), and system. At
the router level, each router has connections to p nodes, a - 1 local channels-to other routers in the same group-and h global channels-to routers in other groups.
Therefore the radix (or degree) of each router is defined as k = p + a + h - 1. A group consists of a routers connected via an intra-group interconnection network
formed from local channels, as shown at 101 in Figure 1. Each group has ap connections to terminals and ah connections to global channels, and all of the
routers in a group collectively act as a virtual router with radix k' = a(p + h). This very high radix, k' >> k enables the system level network to be realized with very
low global diameter (the maximum number of expensive global channels on the minimum path between any two nodes). Up to g = ah + 1 groups (N = ap(ah + 1)
terminals) can be connected with a global diameter of one. In contrast, a system-level network built directly with radix k routers would require a larger global
diameter.
[0029] In a maximum-size (N = ap(ah + 1)) dragonfly, there is exactly one connection between each pair of groups. In smaller dragonflies, there are more global
connections out of each group than there are other groups. These extra global connections are distributed over the groups with each pair of groups connected
by at least_ah+1 g_channels.
[0030] The dragonfly parameters a, p, and h can have any values. However, to balance channel load, the network in this example has a = 2p = 2h. Because each packet
traverses two local channels along its route (one at each end of the global channel) for one global channel and one terminal channel, this ratio maintains
balance. Because global channels are expensive, deviations from this 2:1 ratio are done in some embodiments in a manner that overprovisions local and
terminal channels, so that the expensive global channels remain fully utilized. That is, the network is balanced in such examples so that a ≥ 2h, 2p ≥ 2h.
[0031] The scalability of a balanced dragonfly is shown in Figure 2. By increasing the effective radix, the dragonfly topology is highly scalable - with radix-64 routers, the
topology scales to over 256k nodes with a network diameter of only three hops. Arbitrary networks can be used for the intra-group and inter-group networks in
Figure 1. In the example presented here, we use a 1-D flattened butterfly or a completely-connected topology for both networks. A simple example of the
dragonfly is shown in Figure 3 with p = h = 2 (two processing nodes per router and two channels within each router coupled to other groups), a = 4 (four routers
in each group) that scales to N = 72 (72 nodes in the network) with k = 7 (radix 7) routers. By using virtual routers, the effective radix is increased from k = 7 to k'
= 16, as group G0 of Figure 3 has eight global connections and eight node connections.
[0032] The global radix, k', can be increased further by using a higher-dimensional topology for the intra-group network. Such a network may also exploit intra-group
packaging locality. For example, a 2-D flattened butterfly is shown in Figure 4 at 401, which has the same k' as the group shown in Figure 5 but exploits
packaging locality by providing more bandwidth to local routers. A 3-dimension flattened butterfly is used in Figure 4 at 402 to increase the effective radix from k'
= 16 to K' = 32 - allowing the topology to scale up to N = 1056 using the same k = 7 router as in Figure 1.
[0033] To increase the terminal bandwidth of a high-radix network such as a dragonfly, channel slicing can be employed. Rather than make the channels wider, which
would decrease the router radix, multiple network can be connected in parallel to add capacity. Similarly, the dragonfly topology in some embodiments can also
utilize parallel networks to add capacity to the network. In addition, the dragonfly networks described so far assumed uniform bandwidth to all nodes in the
network. However, if such uniform bandwidth is not needed, bandwidth tapering can be implemented by removing inter-group channels among some of the
groups.
Dragonfly Routing Examples
[0034] A variety of minimal and non-minimal routing algorithms can be implemented using the dragonfly topology. Some embodiments of global adaptive routing using
local information lead to limited throughput and very high latency at intermediate loads. To overcome these problems, we introduce new mechanisms to global
adaptive routing, which provide performance that approaches an ideal implementation of global adaptive routing.
[0035] Minimal routing in a dragonfly from source node s attached to router Rs in group Gs to destination node d attached to router Rd in group Gd traverses a single
global channel and is accomplished in three steps:
Step 1 : If Gs_= Gd and Rs does not have a connection to Gd, route within Gs from Rs to Ra, a router that has a global channel to Gd.
Step 2 : If Gs_= Gd, traverse the global channel from Ra to reach router Rb in Gd.
Step 3 : If Rb_= Rd, route within Gd from Rb to Rd.
[0036] This minimal routing works well for load-balanced traffic, but results in poor performance on adversarial traffic patterns. To load-balance adversarial traffic
patterns, Valiant's algorithm can be applied at the system level - routing each packet first to a randomly-selected intermediate group Gi and then to its final
destination d. Applying Valiant's algorithm to groups suffices to balance load on both the global and local channels. This randomized non-minimal routing
traverses at most two global channels and requires five steps:
Step 1 : If Gs_= Gi and Rs does not have a connection to Gi, route within Gs from Rs to Ra, a router that has a global channel to Gi.
Step 2 : If Gs_= Gi traverse the global channel from Ra to reach router Rx in Gi.
Step 3 : If Gi_= Gd and Rx does not have a connection to Gd, route within Gi from Rx to Ry, a router that has a global channel to Gd.
Step 4 : If Gi_= Gd, traverse the global channel from Ry to router Rb in Gd.
Step 5 : If Rb_= Rd, route within Gd from Rb to Rd.

https://patents.google.com/patent/EP2461254B1/en?q=(high+radix)&oq=high+radix&page=1

4/20

2/14/26, 2:01 PM

EP2461254B1 - Table-driven routing in a dragonfly processor interconnect network - Google Patents

[0037] To prevent routing deadlock, two virtual channels (VCs) are employed for minimal routing and three VCs are required for non-minimal routing, as shown in Figure
5. These virtual router assignments eliminate all channel dependencies due to routing. For some applications, additional virtual channels may be required to
avoid protocol deadlock - e.g., for shared memory systems, separate sets of virtual channels may be required for request and reply messages.
[0038] A variety of routing algorithms for the dragonfly topology have been evaluated, including:

Minimal (MIN) : The minimal path is taken as described previously.
Valiant (VAL) [32] : Randomized non-minimal routing as described previously.
Universal Globally-Adaptive Load-balanced [29] (UGALG,UGAL-L) UGAL chooses between MIN and VAL on a packet-by-packet basis to load-balance the
network. The choice is made by using queue length and hop count to estimate network delay and choosing the path with minimum delay. We implement
two versions of UGAL.

UGAL-L - uses local queue information at the current router node.
UGAL-G - uses queue information for all the global channels in Gs - assuming knowledge of queue lengths on other routers. While difficult to implement,
this represents an ideal implementation of UGAL since the load-balancing is required of the global channels, not the local channels.
[0039] The different routing algorithms are evaluated using both benign and adversarial synthetic traffic patterns, as shown in Figure 6. Latency v. offered load is shown
for the four routing algorithms, using both uniform random traffic at 601 and adversarial traffic at 602. The use of a synthetic traffic pattern allows us to stress
the topology and routing algorithm to fully evaluate the network. For benign traffic such as uniform random (UR), MIN is sufficient to provide low latency and high
throughput, as shown at 601 of Figure 6. VAL achieves approximately half of the network capacity because its load-balancing doubles the load on the global
channels. Both UGAL-G and UGAL-L approach the throughput of MIN, but with slightly higher latency near saturation. The higher latency is caused by the use of
parallel or greedy allocation where the routing decision at each port is made in parallel. The use of sequential allocation will reduce the latency at the expense of
a more complex allocator.
[0040] Adaptive routing on the dragonfly is challenging because it is the global channels, the group outputs, that need to be balanced, not the router outputs. This leads
to an indirect routing problem. Each router picks a global channel to use using only local information that depends only indirectly on the state of the global
channels. Previous global adaptive routing methods used local queue information, source queues and output queues, to generate accurate estimates of network
congestion. In these cases, the local queues were an accurate proxy of global congestion, because they directly indicated congestion on the routes they initiated.
With the dragonfly topology, however, local queues only sense congestion on a global channel via backpressure over the local channels. If the local channels are
overprovisioned, significant numbers of packets must be enqueued on the overloaded minimal route before the source router will sense the congestion. This
results in a degradation in throughput and latency as shown earlier in Figure 6 at 602.
[0041] A throughput issue with UGAL-L arises due to a single local channel handling both minimal and non-minimal traffic. For example, in Figure 7, a packet in R1 has a
minimal path which uses gc7 and a nonminimal path which uses gc6. Both paths share the same local channel from R1 to R2. Because both paths share the
same local queue (and hence have the same queue occupancy) and the minimal path is shorter (one global hop vs two), the minimal channel will always be
selected, even when it is saturated. This leads to the minimal global channel being overloaded and the non-minimal global channels that share the same router
as the minimal channel being under utilized. With UGAL-G, the minimal channel is preferred and the load is uniformly balanced across all other global channels.
With UGAL-L, on the other hand, the non-minimal channels on the router that contains the minimal global channel are under utilized - resulting in a degradation of
network throughput.
[0042] To overcome this limitation, we modify the UGAL algorithm to separate the queue occupancy into minimal and nonminimal components by using individual VCs
(UGAL-LVC).

if (qm vcHm ≤ qnm vcHnm )
route minimally;
else
route nonminimally;
where the subscript m and nm denote the minimal and nonminimal paths. If the VC assignment of Figure 5 is used, qm vc = q(V C1) and qnm vc = q(V C0).
43] When compared, UGAL-LVC matches the throughput of UGAL-G on a WC traffic pattern but for UR traffic, the throughput is limited, with approximately 30% reduction in
throughput. For the WC traffic, where most of the traffic needs to be sent non-minimally, UGALLVC performs well since the minimal queue is heavily loaded. However, for
load-balanced traffic when most traffic should be sent minimally, individual VCs do not provide an accurate representation of the channel congestion - resulting in
throughput degradation.
44] To overcome this limitation, we further modify the UGAL algorithm to separate the queue occupancy into minimal and non-minimal components only when the minimal
and nonminimal paths start with the same output port. Our hybrid modified UGAL routing algorithm (UGAL-LVC H) is:

if (qmHm ≤ qnmHnm && Outm_= Outnm)||(qm vcHm ≤ qnm vcHnm && Outm = Outnm)
route minimally;
else
route nonminimally;
45] Compared to UGAL-LVC, UGAL-LVC H provides the same throughput on WC traffic pattern but matches the throughput of UGAL-G on UR traffic but resulting in nearly 2 ×
higher latency at an offered load of 0.8, near saturation. ForWC traffic, UGAL-LVC H also results in higher intermediate latency compared to UGAL-G.
46] The high intermediate latency of UGAL-L is due to minimally-routed packets having to fill the channel buffers between the source and the point of congestion before
congestion is sensed. Our research shows that non-minimally routed packets have a latency curve comparable to UGAL-G while minimally-routed packets see
significantly higher latency. As input buffers are increased, the latency of minimally-routed packets increases and is proportional to the depth of the buffers. A histogram
of latency distribution shows two clear distributions - one large distribution with low latency for the non-minimal packets and another distribution with a limited number
of packets but with much higher latency for the minimal packets.
47] To understand this problem with UGAL-L, in the example dragonfly group shown in Figure 7, assume a packet in R1 is making its global adaptive routing decision of
routing either minimally through gc0 or non-minimally through gc7. The routing decision needs to load balance global channel utilization and ideally, the channel
utilization can be obtained from the queues associated with the global channels, q0 and q3. However, q0 and q3 queue informations are only available at R0 and R2 and
not readily available at R1 - thus, the routing decision can only be made indirectly through the local queue information available at R1.
48] In this example, q1 reflects the state of q0 and q2 reflects the state of q3. When either q0 or q3 is full, the flow control provides backpressure to q1 and q2 as shown with
the arrows in Figure 7. As a result, in steady-state measurement, these local queue information can be used to accurately measure the throughput. Since the throughput
is defined as the offered load when the latency goes to infinity (or the queue occupancy goes to infinity), this local queue information is sufficient. However, q0 needs to
be completely full in order for q1 to reflect the congestion of gc0 and allow R1 to route packets non-minimally. Thus, using local information requires sacrificing some
packets to properly determine the congestion - resulting in packets being sent minimally having much higher latency. As the load increases, although minimally routed
packets continue to increase in latency, more packets are sent non-minimally and results in a decrease in average latency until saturation.
49] In order for local queues to provide a good estimate of global congestion, the global queues need to be completely full and provide a stiff backpressure towards the local
queues. The stiffness of the backpressure is inversely proportional to the depth of the buffer - with deeper buffers, it takes longer for the backpressure to propagate while

https://patents.google.com/patent/EP2461254B1/en?q=(high+radix)&oq=high+radix&page=1

5/20

2/14/26, 2:01 PM

EP2461254B1 - Table-driven routing in a dragonfly processor interconnect network - Google Patents

with shallower buffers, a much stiffer backpressure is provided. As the buffer size decreases, the latency at intermediate load is decreased because of the stiffer
backpressure. However, using smaller buffers comes at the cost of reduced network throughput.
0] To overcome the high intermediate latency, we propose using credit round-trip latency to sense congestion faster and reduce latency. In credit-based flow control,
illustrated in Figure 8, credit counts are maintained for buffers downstream. As packets are sent downstream, the appropriate credit count is decremented and once the
packet leaves downstream router, credits are sent back upstream and the credit count is incremented. The latency for the credits to return is referred to as credit roundtrip latency (tcrt) and if there is congestion downstream, the packet cannot be immediately processed and results in an increase in tcrt.
1] Referring to Figure 8, conventional credit flow control is illustrated at 8A. As packets are sent downstream (1), the output credit count is decremented (2) and credits are
sent back upstream (3). This scheme is modified as shown at 8B to use credit round trip latency to estimate congestion in the network. In addition to the output credit
count being decremented (2), the time stamp is pushed into the credit time queue, denoted CTQ. Before sending the credit back upstream (4), the credit is delayed (3),
and when downstream credits are received (5), the credit count is updated as well as the credit round trip latency tcrt.
2] The value of tcrt can be used to estimate the congestion of global channels. By using this information to delay upstream credits, we stiffen the backpressure and more
rapidly propagate congestion information up stream. For each output O, tcrt(O) is measured and the quantity td(O) = tcrt(O) - tcrt0 is stored in a register. Then, when a flit
is sent to output O, instead of immediately sending a credit back upstream, the credit is delayed by td(O) - min [td(o)]. The credits sent across the global channels are not
delayed. This ensures that there is no cyclic loop in this mechanism and allows the global channels to be fully utilized.
3] The delay of returning credits provides the appearance of shallower buffers to create a stiff backpressure. However, to ensure that the entire buffer gets utilized and there
is no reduced throughput at high load, the credits needs to delayed by the variance of td across all outputs. We estimate the variance by finding min [td(o)] value and
using the difference. By delaying credits, the upstream routers observes congestion at a faster rate (compared to waiting for the queues to fill up) and leads to better
global adaptive routing decisions.
4] The UGAL-L routing algorithm evaluation using credit latency (UGAL-LCR) is investigated for both WC and UR traffic using buffers of depth 16 and 256. UGAL-LCR leads to
significant reduction in latency compared to UGALL and approaches the latency of UGAL-G. For WC traffic, UGAL-LCR reduces latency by up to 35% with 16 buffers and
up to over 20 × reduction in intermediate latency with 256 buffers compared to UGAL-L. Unlike UGAL-L, the intermediate latency with UGAL-LCR is independent of buffer
size. For UR traffic, UGAL-LCR provides up to 50% latency reduction near saturation compared to UGAL-LVC H. However, both UGAL-LCR and UGALLVC H fall short of the
throughput of UGAL-G with UR traffic because their imprecise local information results in some packets being routed non-minimally.
5] The implementation of this scheme results in minimal complexity overhead as the following three features are needed at each router:
tracking credits individually to measure tcrt
registers to store td values
a delay mechanism in returning credits
6] The amount of storage required for td is minimal as only O(k) registers are required. The credits are often returned by piggybacking on data flits and delaying credits to
wait for the transmission of the next data flit upstream is required. The proposed mechanism only requires adding additional delay.
7] As for tracking individual credits, credits are conventionally tracked as a pool of credits in credit flow control - i.e., a single credit counter is maintained for each output VC
and increments when a credit is received. The implementation of UGAL-LCR requires tracking each credit individually. This can be done by pushing a timestamp on the
tail of a queue each time a flit is sent, as shown in Figure 17(b) with the use of a credit timestamp queue (CTQ), and popping the timestamp off the head of the queue
when the corresponding credit arrives. Because flits and credits are 1:1 and maintain ordering, the simple queue suffices to measure round-trip credit latency. The depth
of the queue needs to be proportional to the depth of the data buffers but the queue size can be reduced to utilize imprecise information to measure congestion - e.g., by
having a queue which is only 1/4 of the data buffer size, only one of four credits are tracked to measure the congestion.
8] The cost of a dragonfly topology also compares favorably to a flattened butterfly, as well as to other topologies. The flattened butterfly topology reduces network cost of
a butterfly by removing intermediate routers and channels. As a result, the flattened butterfly reduces cost by approximately 50% compared to a folded-Clos on balanced
traffic. The dragonfly topology extends the flattened butterfly by increasing the effective radix of the routers to further reduce the cost and increase the scalability of the
network.
9] A comparison of dragonfly and flattened butterfly networks of 64k nodes shows that a flattened butterfly uses 50% of the router ports for global channels, while a
dragonfly uses 25% of the ports for global connections. The flattened butterfly requires two additional dimensions, while the dragonfly is a single dimension. In addition,
the dragonfly provides better scalability because the group size can be increased to scale the network whereas scaling the flattened butterfly requires adding additional
dimensions. With the hop count nearly identical, the dragonfly trades off longer global cables for smaller number of global cables required to provide a more cost-efficient
topology better matched to emerging signaling technologies.
0] Various embodiments of dragonfly networks described here also comprise two new variants of global adaptive routing that overcome the challenge of indirect adaptive
routing presented by the dragonfly. A dragonfly router will typically make a routing decision based on the state of a global channel attached to a different router in the
same group. Conventional global adaptive routing algorithms that use local queue occupancies to infer the state of this remote channel give degraded throughput and
latency. We introduce the selective use of virtual channel discrimination to overcome the bandwidth degradation. We also introduce the use of credit round-trip latency to
both sense and signal channel congestion. The combination of these two techniques gives a global adaptive routing algorithm that attempts to approach the
performance of an ideal algorithm with perfect knowledge of remote channel state.
Progressive Adaptive Routing in a Dragonfly Network
1] An improved routing method for Dragonfly processor interconnect networks is proposed here, providing deadlock-safe adaptive routing that is operable to choose among
multiple legal routes based on congestion or down links. This adaptive routing method provides improved routing performance and tolerance for downed or busy links
than prior methods, and explicitly communicates congestion across channels as opposed to withholding credits, which may negatively impact bandwidth.
2] In some embodiments, a network route is selected from among multiple minimal routes, such as routing in different dimensions first, and optionally further selected from
one or more non-minimal routes, such as using randomly chosen hops to avoid congestion or downed links.
3] Routing choices are presented via tables in one example, and may be biased toward certain routes or toward minimal or non-minimal routes depending on the network
configuration and state. For example, route choice may be biased toward minimal routing by default for highest efficiency, with a bias switch toward non-minimal routing
to protect a certain network link from arbitrarily or unnecessarily receiving additional traffic.
4] Congestion information is utilized in some embodiments by deriving an anticipated next link congestion from elements such as counting the number of messages in an
output queue and establishing a receiving buffer congestion estimate based on factors such as credits or messages in-flight. A node can query a potential receiving node
for the average "next link" output congestion, enabling the node to make a routing decision based on avoiding congested or down links.
5] Figure 9 shows a Dragonfly network router, consistent with an example embodiment of the invention. The router block shown here comprises 48 tiles, with each tile
corresponding to an input/output pair. The tiles are organized in an 8x6 matrix, such that incoming packet data at a particular tile is routed across the row to one of the 8
columns, then up or down the 8 columns to one of the 6 rows, arriving at the appropriate tile for output. The channels in further embodiments feature multiple virtual
channels, virtual channel switching in-flight, error correction such as SECDED, and input buffering including dynamic allocation to virtual channels as needed to improve
network performance.
6] Referring again to the example of Figure 9, forty of the tiles connect to external network links, while eight of the tiles connect to processor cores local to the processor
node. Each tile comprises an input queue, a subswitch, and a column buffer. The input queue receives packets from a serializer/deserializer interface to the network, and
determines how to route the packet. The packet is sent across the row bus to the subswitch in the appropriate column. The subswitch receives the packets, switches
them to the appropriate virtual channel, and sends the packet out one of the six column buses to the column buffer in the appropriate row. The column buffer collects the
packet data from the six tiles within the column and sends the packet data across the network channel.

https://patents.google.com/patent/EP2461254B1/en?q=(high+radix)&oq=high+radix&page=1

6/20

2/14/26, 2:01 PM

EP2461254B1 - Table-driven routing in a dragonfly processor interconnect network - Google Patents

7] The dragonfly network topology in this example is a hierarchical network of two layers of a flattened butterfly topology. The first layer is a two-dimensional flattened
butterfly that connects all of the router chips within a local group, such as a computer cabinet or chassis. Each group is treated as a very high-radix router, and a single
dimension flattened butterfly (all-to-all) connects all of the groups to form the second layer of the dragonfly topology example presented here.
8] The first dimension within the group, referred to for convenience as the "green" dimension, connects the 16 routers within a chassis. The second dimension within a
group is similarly called the "black" dimension, and connects the six chassis within a two cabinet group. This is reflected in the network configuration shown in the
network "group" of Figure 10, which illustrates six chassis (represented as the six rows), made up of 16 routers per chassis (represented as the 16 columns).
9] Groups such as are illustrated in Figure 10 are further coupled to one another using links in the "blue" dimension, as shown in Figure 11. These "blue" links between
groups connect each group to each other group, to a maximum of 240 blue links per group in this example, or 241 groups per system. Each link can comprise multiple
ports, such as four ports per link or optical cable, resulting in four ports connecting each pair of groups over a single cable. In systems having fewer groups, unused ports
from the 240 blue ports per group can be used to provide additional bandwidth between configured groups, such as two links per group pair in a network having 120
groups providing eight ports connecting each pair of groups.
0] In the network, packets route from a source node to a target node, traversing at least one but possibly all three dimensions shown in Figures 9-11. A routing path
traversing all three dimensions will likely first be routed in the green dimension and then the black dimension to reach the appropriate node in a group to link to the target
group, then the blue dimension to reach the intended target group. The packet is then routed in the green and black dimensions within the group to reach the intended
target node in the target group, resulting in five routings within three dimensions to reach the target.
1] The network supports both adaptive and deterministic routing in one embodiment. Deterministic routing sends a given packet over a predetermined route over the
network irrespective of network congestion. When multiple deterministic paths are available, deterministic traffic can be hashed based on a destination node, address, or
other such characteristics to distribute traffic between the multiple paths. Packets traveling between the same source and target will in some embodiments arrive at the
target in order, as all packets between the source and target take the same deterministic path.
2] Adaptive routing permits packets to take different routes based on congestion levels within the network. In some embodiments, packets may arrive out of order when
using adaptive routing, and may take non-minimal paths when congestion dictates avoiding a minimal path.
3] Minimal routing in a dragonfly occurs when a packet traverses at most one link in a given dimension. Minimal routing within a group, such as shown in Figure 10, will
therefore take at the most one hop in the "green" dimension and one hop in the "black" dimension. A minimal path between nodes in different groups will take at most one
hop in the green dimension and one hop in the black dimension in each group, and will take one additional hop to travel from the source group to the target group.
4] As either the black or green dimensions may be traversed first, there are multiple minimal paths, both in the source and destination groups. If multiple links between
groups exist, one path may not require a hop in the black or green dimension in either the source or destination groups, reducing the total number of hops needed to
complete a minimal path to less than five.
5] Non-minimal routing can take multiple hops in either the black or green dimension in the source or target groups, resulting in more than five hops. Additional hops may be
desirable in circumstances where congestion is present in the minimal path or paths available to the router, improving the speed of message delivery to the target while
avoiding further congesting an already congested network link. Further embodiments attempt to spread traffic over available links, such as by randomizing or hashing
path selection to avoid creating additional congested network regions as a result of repeatedly routing the same path around a previously congested link.
6] In one such embodiment, an intermediate node is chosen in the group such as that of Figure 10, such that the message is first minimally routed to the intermediate node,
and then routed from the intermediate node to the final node in the group. This results in up to two hops in each of the green and black dimensions, or double the number
of hops in minimal routing within a group. Routing may be nonminimal within the source group, nonminimal within the target group, or nonminimal in both the source and
target groups.
7] Nonminimal routing can also occur between groups, such as where a message is routed minimally within the source and target groups but is routed through an
intermediate group between the source and target groups to avoid congestion in the link between the source and target groups. Routing within the source, intermediate,
and target groups may further be minimal or nonminimal, depending on congestion within each of the groups.
8] The type of routing used for a given packet or message is determined in one embodiment by a routing control field in the packet header. For example, the routing control
symbol may indicate that deterministic non-minimal hashed routing is to be used when preserving packet order is desired. Packets are distributed across available paths
using the target node as a hash. Traffic is routed nonminimally, but distributing the packets among various intermediate nodes in the group results in reduced hot spots
or congestion.
9] Deterministic minimal hashed routing provides hashing of packets over minimal paths, which reduces the number of hops in a given group by permitting routing over
alternate minimal paths, such as black dimension before green dimension or green dimension before black dimension. This can result in severe network congestion in
certain situations, and so may not be desirable unless global traffic is particularly uniformly distributed.
0] Deterministic minimal non-hashed routing uses a single deterministic minimal path for all traffic, which provides packet ordering but does not provide good bandwidth or
load distribution among available paths. Such routing may be used for infrequent or small messages, such as control messages or latency-critical messages.
1] Adaptive routing can be sued as a default routing type when ordering is not required. Packets will attempt to route minimally, but may take non-minimal paths in groups
or between groups to avoid network congestion. Adaptive routing is provided in some embodiments using routing tables that provide two or more minimal and two or
more non-minimal ports for consideration in making a routing choice. A congestion value is computed for each node or tile in a router is calculated and distributed to
other tiles in the router, such as the router tiles shown in Figure 9. The adaptive routing algorithm considers in this example the two minimal and two nonminimal paths
available, and selects from them based on the congestion values and optionally on various configured biases.
2] Port congestion values are derived in a further embodiment from factors such as downstream port congestion, estimated far-end link congestion, and near-end link
congestion. In a specific example, two bits of downstream port congestion information are propagated across the external channel corresponding to each tile in a router
chip, and updated periodically. These bits will be generated at the transmitting router chip by combining a view of congestion of downstream ports on the chip. The
downstream ports that are combined into this 2-bit congestion value are selected via an MMR-configurable mask at each tile. The congestion values of these
downstream ports are summed and compared to three programmable thresholds. If the sum is greater than the highest threshold, the congestion is 2'b11. If the sum is
less than the highest threshold, but greater than the middle threshold, the congestion is 2'b10. If the sum is less than the middle threshold and greater than the lowest
threshold, the congestion is 2'b01. Otherwise, if the sum is less than the lowest threshold, the congestion is 2'b00.
3] On the receiving side of the channel, this 2-bit value is mapped to a 4-bit value by indexing into a 4-entry by 4-bit wide downstream congestion remapping table. The
estimated far-end link congestion is computed by tracking the number of flits sent longer than the channel round trip latency in the past that have not yet been
acknowledged, and adjusting by the relative rates of flit transmission and acknowledgement receipt. The mechanism used to do this is a 5-bit wide 32-entry deep delay
chain. For an MMR-configurable number of cycles (1 to 31), the router counts the number of flits transmitted into the tail position of this delay chain. After this delay, all of
the values are shifted. The total expected outstanding flits on the channel (transmitted and ones for this an ack is expected) is the sum of the values in this chain. This
value is compared to the outstanding credit count. The total number of outstanding credits minus the expected flits on the channel represents an estimate for the
number of flits stored in the remote Input Queue.
4] The estimated far-end congestion is calculated as a 10-bit number. This number is converted to a 4-bit index according to a mapping table, and this 4-bit number is then
remapped to another programmable 4-bit value by indexing into a 16-entry far-end congestion remapping table.
5] The near-end link congestion is computed by summing the flits queued in the column buffer waiting to be transmitted across the link. This sum is also a 10-bit value and
is converted to a 4-bit value according to a mapping table. This 4-bit number is then remapped to another programmable 4-bit value by indexing into a 16-entry
near-end congestion remapping table.
6] The remapped 4-bit downstream port congestion value, the remapped 4-bit far-end link congestion value, and the remapped 4-bit near-end link congestion value are
combined to produce a single 4-bit congestion value per tile. This combination is done as a 3-input 4-bit unsigned saturating addition. This 4-bit congestion value is

https://patents.google.com/patent/EP2461254B1/en?q=(high+radix)&oq=high+radix&page=1

7/20

2/14/26, 2:01 PM

EP2461254B1 - Table-driven routing in a dragonfly processor interconnect network - Google Patents

propagated to all other tiles on the chip to aid those tiles in making informed adaptive choices.
7] A "link alive" signal is broadcast from each ntile on the chip to all other tiles on the chip. This link alive signal for each ntile indicates whether the corresponding tile has
an established serial link with the router it is connected to. Ports for which the link is not alive will be considered invalid from a port selection perspective. This allows the
router to adaptively avoid recently failed links which software has not yet been able to remove from the routing tables.
8] The link alive signals are propagated around the router via a 2-wire serial chain that connects all of the network tiles. Each tile places its link status information on the
serial chain at the appropriate bit timing. If all of the ports presented to the congestion logic are invalid, the packet will be discarded. In this case, it will be up to end-point
hardware to timeout on the missing packet and up to higher-level software to retransmit or handle the error as appropriate.
9] At each Input Queue, the broadcast congestion values are used in making the adaptive choice between the two minimal and two non-minimal port candidates. Before
using these congestion values, bias values are applied to the selected two minimal and non-minimal port congestion values. First, the values are logically extended to a
6-bit value by prepending two zeros to the most significant part of the value. The adaptive routing control type (adaptive0, adaptive1, adaptive2, or adaptive3) is used to
select a set of biases from a four entry bias table. Each entry has a pair of 2-bit shift value that determines how far left to shift the minimal ports and non-minimal ports
congestion values respectively. The 6-bit expanded congestion value can be shifted by zero, one, or two bits. The encoding of this field is 2'b00 = shift left by zero bits
(multiply by one), 2'b01 = shift left by one bit (multiply by two), 2'b10 = shift left by two bits (multiply by four), 2'b11 = reserved.
0] Each bias MMR also contains a pair of 6-bit values that is added to the 6-bit expanded minimal and non-minimal congestion values. The addition is performed as a
saturating add, resulting in a 6-bit number. The port corresponding to the lowest congestion is picked. If there is a tie between a minimal and a non-minimal port, the
router favors the minimal port. If there is a tie between the two ports presented as non-minimal or between the two ports presented as minimal, the choice is arbitrary
and may be made in any suitable way.
Table-Driven Routing Mechanism in a Dragonfly Network
1] The routing example presented here uses a variety of tables to determine paths available in routing a packet or message, and provide routing flexibility in the dragonfly
network configuration. Different tables exist to provide routing within a group and between groups, and for minimal and non-minimal routing paths.
2] The routing structures in the example router architecture presented here are divided into four distinct table sets: a global non-minimal (GN) table set, a global minimal
(GM) table, a local non-minimal (LN) table set, and a local minimal (LM) table. The logical flow of this specific example is shown in Figure 12.
3] The global tables are used to determine how to route to a remote group, when the current group is not the target group. They are used to route toward a particular optical
port on which to exit the local group. Local tables are used to route to a particular router chip within the current group. They are used for "up" or "down" routing within the
group for local routing or for "up" routing in the intermediate group. Minimal tables specify minimal local or global routes. They are used when routing down or, in the case
of adaptive routing, when attempting to take a minimal path on the way up. Non-minimal tables specify non-minimal paths, and are only used when routing "up". They
also provide a "root-detect" mechanism for determining when to stop routing up.
4] The global non-minimal table set is used to route non-minimal traffic to an intermediate group. It contains a list of ports that lead to "safe" intermediate groups, where a
"safe" intermediate group is one that is connected to all other groups. (In a healthy network, all groups are safe. In a partially healthy network, the tables should be
programmed to avoid sending traffic to an intermediate group that may not connect to
the target group.) This table set consists of three tables. The first table selects which rank in the green dimension to traverse to leave the current (source) group. The
second table selects the black dimension to traverse. The third table selects the optical port to leave the current router chip on.
5] The tables are arranged hierarchically in a fixed priority order. The green dimension table has the highest priority, and the blue dimension table has the lowest. Each table
lists a set of port numbers to leave the Aries on, or a special value that indicates that the current table is deferring its priority and the next table in the priority hierarchy
should be consulted. A special value on the lowest priority (blue) table, if referenced, will result in an error condition. Each table consists of 128 entries, each of which is a
6-bit port number or the special value of 6'b11xxxx. Each table is organized as 16 by 8 entries, with an accompany 7-bit ECC per each block of 8 entries.
6] This table should only contain routes to other router chips or optical port numbers that ultimately lead to an intermediate group that can safely route to all other groups in
the system. The table also provides the mechanism that distributes non-minimal traffic roughly evenly over the groups in the system. There are 128 entries in each table
so that even with an effective radix-18 dimension, each port is listed 7 or 8 times, leading to at most a 14.3% imbalance between two ports in the dimension. This
imbalance can be minimized by having the imbalanced ports differ on the multiple copies of the table throughout the group.
7] For global deterministic routing, this table set is indexed into by a hash value including the target, the tgtID, (possibly the local port number), and the optional hash field
from the packet header (which comes from the packet address). Each table will get a different index. For global adaptive routing, one of the blocks of 8 entries is
selected from the table at random. A second entry is selected at random from that 8-entry block. The two ports are compared with each other and with two entries from
the global minimal table to determine which path to route the packet.
8] The green tables in the ptiles will generally have each of the 15 green ports listed 8 times and will have 8 special values. Further, at the ptiles, the black tables will have
each of the 15 black ports listed approximately 7 times, with approximately 21 entries containing special values. The blue tables will have each of the optical ports listed
about 13 times each.
9] The green ntile ports will generally have all of the entries in the green table as the special value. The black and blue tables will be configured in the same proportions as in
the ptile case. The black ntile ports will generally have all of the entries in the green and black tables as the special value. The blue tables will be configured in the same
proportions as the ptiles.
0] The global minimal table is used to determine a direct path from the current group to the target group. It consists of 256 entries, each of which is 81-bits wide. Each entry
is divided in to two parts, a full port set and a restricted port set. The full port set consist of 8 6-bit port entries and a 3-bit modulo specifier. The modulo field indicates
the total number of valid ports in the associated entry. The modulo specifier is encoded as the modulo minus one. That is, a value of 7 in the modulo field will result in a
modulo of 8 operation. The restricted port set consists of 4 6-bit ports and a 2-bit modulo specifier. Each 81-bit entry will also have an 8-bit ECC.
1] This table is organized by target group numbers. Each target group corresponds to a "block" of 1, 2, 4, 8, 16, 32, 64, or 128 entries in the table, according to the size of the
system. A system with 241 groups would have 1 entry per block in the table. (15 of the entries would be unused.) A system with 65-128 groups would use 2 entries per
block. A system with 33-64 groups would use a block of four entries, and so forth. The group number along with zero to seven additional random (adaptive routing) or
hash (deterministic routing) bits are used to index into the table. Each entry contains a list of ports leading to Aries reachable from the current point in routing that
connect minimally to the associated target group, or leading directly to the target group over a blue link.
2] The full port set is used when just beginning to route minimally within a group (either at a ptile or an optical ntile) toward another group, or at any tile when routing nonminimally within the intermediate group and the root is detected in the local non-minimal table (see below). This side of the table lists all possible paths to all possible
optical ports that are connected minimally to the group specified by the index. The restricted port set is used for routing within the group other than in the root detect and
injection cases mention for the full port set table. This half of the table only represents paths in the network that are legal from the current point in the group network,
assuming we are routing minimally.
3] The key purpose of the restricted port list is to prevent packets from flowing back in the direction from whence they came. At a green port, the restricted table entries
should normally only list black and blue ports. At a black port, the restricted table entries should normally only list blue ports.
4] When all of the ports listed in the restricted set are invalid, this indicates to the adaptive routing logic that a packet has diverged from a legal minimal path. In this case
the adaptive routing logic will pick one of the non-minimal choices. (This should never occur for deterministically or minimally routed traffic as the tables should be
written in a consistent manner such that a packet never arrives at a point where it cannot route to the destination. If this does occur, the router will flag an error and
discard the packet.
5] When there are no legal restricted routed in a tile, the mod value can be set to any value. The route table should contain the special value of 6'b11xxxx in all of the entries
associated with the group number. When there is only one legal route, the port list should contain the legal route listed at least twice and the mod value set to two or
higher to match.

https://patents.google.com/patent/EP2461254B1/en?q=(high+radix)&oq=high+radix&page=1

8/20

2/14/26, 2:01 PM

EP2461254B1 - Table-driven routing in a dragonfly processor interconnect network - Google Patents

6] For deterministic routing one of the valid entries in either the full or restricted set is selected by computing a modulo of a hash by the number of valid entries in the
associated index. Like in the cases above, adaptive routing will choose 2 entries from the table but computing the mod on a random number and a second modulo of N-1
to add to the first number plus one to get the offset of a second random but unique entry in the table.
7] The local non-minimal table set is used to pick a router chip in the local group that is used as the root for non-minimal routing within the group. This table is used for nonminimal routing when the source and target group are the same. It is also used for non-minimal routing in the intermediate group. This table set is structured like the
global non-minimal table, except that there is no blue table.
8] The local non-minimal table is indexed randomly for adaptive routing or by a hash for non-minimal deterministic routing. Similar to the global non-minimal table, for
adaptive routing two entries are produced by this table and compared. To reduce the number of total RAM macros in the design, these tables will be physically combined
with the global non-minimal tables in RAM.
9] This table lists Aries that are reachable from this tile that are safe to use for local non-minimal routing. In a healthy network, the ptiles and blue (optical) tiles should list
all Aries in the group roughly evenly. Approximately 15/16 of the entries in the green table should list green ports, and ∼1/6 should contain the special value indicating
that the green dimension has already been satisfied and that the black table should be used. Similarly, ∼5/6 of the entries in the black table should list black ports, and

∼1/6 should contain the special value indicating that the black dimensions has been satisfied. A special value in both the green and black tables indicates that the root
has been reached ("root detect") and that the packet should be downrouted from this point.
0] The green tiles should fill the green table with special values (indicating that the green dimension has been satisfied), and should list the 6 aries reachable (including self,
using the special value) in the black table evenly. The black tiles should fill both the green and black tables with the special root detect value. The ptiles and optical tiles
need the full table set. Ntiles could technically do without the green table, however, the router table example presented here implements them for flexibility.
1] The local minimal table is used for minimal routing ("downrouting") within the target group, and also when adaptively "uprouting" in the target group. This table has 128
entries. Each entry is 52 bits wide, consisting of 8 6-bit port numbers, a "diverged" bit, and a mod value indicating how many entries are valid in this line of the table. The
diverged bit indicates that the path within the target group has diverged from a minimal path, and thus this path cannot be used as a minimal path when adaptively
uprouting, and can only be used for downrouting. It is similar to the case in the global minimal table where all the ports in the restricted set are invalid.
2] This table is organized by the "target" Aries number within the group. Each local Aries number corresponds to a block of 1, 2, 4, 8, or 16 entries in the table, according to
the size of the group. A group of 65-128 Aries would used a block size of 1 entry per local Aries number. A group size of 33-64 Aries would use a block size of 2, and so
forth. The local Aries number along with zero to four additional random (adaptive routing) or hash (deterministic routing) bits are used to index into the table. Each entry
contains a list of ports leading to the associated local Aries.
3] For deterministic routing one of the valid entries in the table is selected by computing a modulo of a hash by the number of valid entries in the associated index. Like in
the cases above, adaptive routing will choose 2 entries from the table but computing the mod on a random number and a second modulo of N-1 to add to the first
number plus one to get the offset of a second random but unique entry in the table.
4] The global non-minimal tables are only used in the source group for traffic headed to another group. The global non-minimal and local non-minimal tables are never used
concurrently. Therefore, to reduce the total number of RAMs needed, the global non-minimal green table is stored in the same RAM as the local non-minimal green table.
The global non-minimal black table is stored in the same RAM as the local non-minimal black table. The global table is stored in the lower index value portion of each of
those two RAMs.
Conclusion
5] The above examples illustrate how routing in a Dragonfly network can be improved by using adaptive routing that is able to select a network path based on factors such
as network congestion or traffic type, and routing tables for various routings including minimal and non-minimal, and local and global routing.
6] Adaptive routing provides deadlock-safe routing that chooses among multiple legal routes based on congestion or down links, providing improved routing performance
and tolerance by explicitly communicating congestion across channels. Routing is performed across multiple minimal routes, such as routing in different dimensions
first, and optionally further selected from one or more non-minimal routes, such as using randomly chosen hops to avoid congestion or downed links.
7] Congestion information is based on anticipated next link congestion from elements such as counting the number of messages in an output queue and establishing a
receiving buffer congestion estimate through factors such as credits or messages in-flight. A node can query a potential receiving node for the average "next link" output
congestion, enabling the node to make a routing decision based on avoiding congested or down links. Other features, such as using a deterministic hash or a random
number to spread traffic in choosing a routing path are also provided, and are useful in spreading traffic to prevent congestion.
8] Routing choices are presented via tables in one example, and may be biased toward certain routes or toward minimal or non-minimal routes depending on the network
configuration and state. For example, route choice may be biased toward minimal routing by default for highest efficiency, with a bias switch toward non-minimal routing
to protect a certain network link from arbitrarily or unnecessarily receiving additional traffic. In a further example, routing tables include tables having local and global
routing tables, and minimal and non-minimal paths.
9] Although specific embodiments have been illustrated and described herein, it will be appreciated by those of ordinary skill in the art that any arrangement which is
calculated to achieve the same purpose may be substituted for the specific embodiments shown. This application is intended to cover any adaptations or variations of
the example embodiments of the invention described herein. It is intended that this invention be limited only by the claims, and the full scope of equivalents thereof.

Patent Citations (44)
Publication number

Priority date

Publication date

Assignee

Title

US5970232A *

1997-11-17

1999-10-19

Cray Research, Inc.

Router table lookup mechanism

US4970658A

1989-02-16

1990-11-13

Tesseract Corporation

Knowledge engineering tool

US5079738A

1989-09-29

1992-01-07

Rockwell International
Corporation

Processor interconnect network for printing press system forming a star
network

US5249283A

1990-12-24

1993-09-28

Ncr Corporation

Cache coherency method and apparatus for a multiple path interconnection
network

DE69314353T2

1992-06-15

1998-03-05

British Telecommunications
P.L.C., London

SERVICE FACILITIES

US5425029A

1993-09-20

1995-06-13

Motorola, Inc.

Fast packet adaptation method for ensuring packet portability across
diversified switching type networks

US5864738A

1996-03-13

1999-01-26

Cray Research, Inc.

Massively parallel processing system using two data paths: one connecting
router circuit to the interconnect network and the other connecting router
circuit to I/O controller

Family To Family Citations

https://patents.google.com/patent/EP2461254B1/en?q=(high+radix)&oq=high+radix&page=1

9/20

2/14/26, 2:01 PM

EP2461254B1 - Table-driven routing in a dragonfly processor interconnect network - Google Patents

JP2998688B2

1997-04-09

2000-01-11

日本電気株式会社

Disaster recovery system

US6212636B1

1997-05-01

2001-04-03

Itt Manufacturing Enterprises

Method for establishing trust in a computer network via association

JP3553398B2

1999-01-08

2004-08-11

日本電信電話株式会社

Routing apparatus and routing method

US6611872B1

1999-01-11

2003-08-26

Fastforward Networks, Inc.

Performing multicast communication in computer networks by using
overlay routing

US6766424B1

1999-02-09

2004-07-20

Hewlett-Packard
Development Company, L.P.

Computer architecture with dynamic sub-page placement

US6643764B1 *

2000-07-20

2003-11-04

Silicon Graphics, Inc.

Multiprocessor system utilizing multiple links to improve point to point
bandwidth

US6477618B2 *

2000-12-28

2002-11-05

Emc Corporation

Data storage system cluster architecture

US7035202B2 *

2001-03-16

2006-04-25

Juniper Networks, Inc.

Network routing using link failure information

US7139926B1

2002-08-30

2006-11-21

Lucent Technologies Inc.

Stateful failover protection among routers that provide load sharing using
network address translation (LSNAT)

US8018860B1 *

2003-03-12

2011-09-13

Sprint Communications
Company L.P.

Network maintenance simulator with path re-route prediction

WO2005036839A2 *

2003-10-03

2005-04-21

Avici Systems, Inc.

Rapid alternate paths for network destinations

US7852836B2 *

2003-11-19

2010-12-14

Cray Inc.

Reduced arbitration routing system and method

US20050177344A1 *

2004-02-09

2005-08-11

Newisys, Inc. A Delaware
Corporation

Histogram performance counters for use in transaction latency analysis

US20050289101A1

2004-06-25

2005-12-29

Doddaballapur Jayasimha

Methods and systems for dynamic partition management of sharedinterconnect partitions

US20070198675A1 *

2004-10-25

2007-08-23

International Business
Machines Corporation

Method, system and program product for deploying and allocating an
autonomic sensor network ecosystem

JP2006185348A *

2004-12-28

2006-07-13

Fujitsu Ltd

Multiprocessor system and lock flag operation method

US8260922B1 *

2005-09-16

2012-09-04

Cisco Technology, Inc.

Technique for using OER with an ECT solution for multi-homed sites

US7675857B1

2006-05-03

2010-03-09

Google Inc.

Method and apparatus to avoid network congestion

WO2008011712A1 *

2006-07-28

2008-01-31

Michael Tin Yau Chan

Wide-area wireless network topology

US7830905B2 *

2007-04-20

2010-11-09

Cray Inc.

Speculative forwarding in a high-radix router

US8285789B2 *

2007-10-05

2012-10-09

Intel Corporation

Flattened butterfly processor interconnect network

US20100049942A1 *

2008-08-20

2010-02-25

John Kim

Dragonfly processor interconnect network

US9100269B2 *

2008-10-28

2015-08-04

Rpx Clearinghouse Llc

Provisioned provider link state bridging (PLSB) with routed back-up

US8301654B2 *

2009-02-24

2012-10-30

Hitachi, Ltd.

Geographical distributed storage system based on hierarchical peer to peer
architecture

US8391303B2 *

2009-04-16

2013-03-05

Futurewei Technologies, Inc.

Border gateway protocol (BGP) grouped route withdrawals

US8576715B2

2009-10-26

2013-11-05

Mellanox Technologies Ltd.

High-performance adaptive routing

US8639885B2

2009-12-21

2014-01-28

Oracle America, Inc.

Reducing implementation costs of communicating cache invalidation
information in a multicore processor

US20110191088A1 *

2010-02-01

2011-08-04

Yar-Sun Hsu

Object-oriented network-on-chip modeling

US8489718B1

2010-05-19

2013-07-16

Amazon Technologies, Inc.

Torroidal backbone connections for network deployment

US20120059938A1 *

2010-06-28

2012-03-08

Cray Inc.

Dimension-ordered application placement in a multiprocessor computer

US8495194B1 *

2010-06-29

2013-07-23

Amazon Technologies, Inc.

Connecting network deployment units

US8427980B2

2010-07-21

2013-04-23

Hewlett-Packard
Development Company, L. P.

Methods and apparatus to determine and implement multidimensional
network topologies

US20120020349A1

2010-07-21

2012-01-26

GraphStream Incorporated

Architecture for a robust computing system

US8837517B2 *

2010-09-22

2014-09-16

Amazon Technologies, Inc.

Transpose boxes for network interconnection

US8621111B2

2010-09-22

2013-12-31

Amazon Technologies, Inc.

Transpose box based network scaling

JP5860670B2

2010-11-05

2016-02-16

インテル コーポレイション

Table-driven routing in a Dragonfly processor interconnect network

https://patents.google.com/patent/EP2461254B1/en?q=(high+radix)&oq=high+radix&page=1

10/20

2/14/26, 2:01 PM

EP2461254B1 - Table-driven routing in a dragonfly processor interconnect network - Google Patents
2016-04-27

インテル コーポレイション

Innovative Adaptive Routing in Dragonfly Processor Interconnect Network

Priority date

Publication date

Assignee

Title

US20100049942A1

2008-08-20

2010-02-25

John Kim

Dragonfly processor interconnect network

JP5860670B2

2010-11-05

2016-02-16

インテル コーポレイション

Table-driven routing in a Dragonfly processor interconnect network

JP5913912B2

2010-11-05

2016-04-27

インテル コーポレイション

Innovative Adaptive Routing in Dragonfly Processor Interconnect Network

US8730965B2 *

2011-01-05

2014-05-20

Google Inc.

Systems and methods for dynamic routing in a multiprocessor network
using local congestion sensing

US8776207B2 *

2011-02-16

2014-07-08

Fortinet, Inc.

Load balancing in a network with session information

US8488601B1

2011-07-12

2013-07-16

Qlogic, Corporation

Method and system for link aggregation

US8467395B1 *

2011-07-12

2013-06-18

Qlogic, Corporation

Method and system for link aggregation

US9094309B2 *

2012-03-13

2015-07-28

International Business
Machines Corporation

Detecting transparent network communication interception appliances

US9274299B2

2012-08-29

2016-03-01

International Business
Machines Corporation

Modular optical backplane and enclosure

US9577918B2 *

2012-11-19

2017-02-21

Cray Inc.

Increasingly minimal bias routing

CN104919442B *

2012-12-13

2017-10-10

相干逻辑公司

Multicomputer system with improved auxiliary interference networks

US9774498B2 *

2012-12-21

2017-09-26

Netspeed Systems

Hierarchical asymmetric mesh with virtual routers

US9634940B2

2013-01-31

2017-04-25

Mellanox Technologies, Ltd.

Adaptive routing using inter-switch notifications

CN103973564B *

2013-01-31

2017-12-15

清华大学

The adaptive routing method of interconnected network system

GB2511089A

2013-02-22

2014-08-27

Ibm

All-to-all message exchange in parallel computing systems

US9471726B2

2013-07-25

2016-10-18

Netspeed Systems

System level simulation in network on chip architecture

US9548960B2

2013-10-06

2017-01-17

Mellanox Technologies Ltd.

Simplified packet routing

US9197536B2

2013-11-22

2015-11-24

Dell Products L.P.

Use of alternate paths in forwarding of network packets

US9699079B2

2013-12-30

2017-07-04

Netspeed Systems

Streaming bridge design with host interfaces and network on chip (NoC)
layers

US10320746B2 *

2014-05-12

2019-06-11

Michael C. Wood

Computer security system and method based on user-intended final
destination

US9729473B2

2014-06-23

2017-08-08

Mellanox Technologies, Ltd.

Network high availability using temporary re-routing

US9806994B2

2014-06-24

2017-10-31

Mellanox Technologies, Ltd.

Routing via multiple paths with efficient traffic distribution

CN104079490B *

2014-06-27

2017-09-22

清华大学

Multi-level dragonfly interference networks and adaptive routing method

WO2015196461A1 *

2014-06-27

2015-12-30

Tsinghua University

Deadlock-free adaptive routing of interconnect network

US9519605B2

2014-07-08

2016-12-13

International Business
Machines Corporation

Interconnection network topology for large scale high performance
computing (HPC) systems

US9699067B2

2014-07-22

2017-07-04

Mellanox Technologies, Ltd.

Dragonfly plus: communication over bipartite node groups connected by a
mesh network

US9571341B1

2014-10-01

2017-02-14

Netspeed Systems

Clock gating for system-on-chip elements

US9660942B2

2015-02-03

2017-05-23

Netspeed Systems

Automatic buffer sizing for optimal network-on-chip design

US10348563B2

2015-02-18

2019-07-09

Netspeed Systems, Inc.

System-on-chip (SoC) optimization through transformation and generation
of a network-on-chip (NoC) topology

US9894005B2

2015-03-31

2018-02-13

Mellanox Technologies, Ltd.

Adaptive routing controlled by source node

US9864728B2

2015-05-29

2018-01-09

Netspeed Systems, Inc.

Automatic generation of physically aware aggregation/distribution networks

JP5913912B2 *

2010-11-05

* Cited by examiner, † Cited by third party

Cited By (81)
Publication number
Family To Family Citations

https://patents.google.com/patent/EP2461254B1/en?q=(high+radix)&oq=high+radix&page=1

11/20

2/14/26, 2:01 PM

EP2461254B1 - Table-driven routing in a dragonfly processor interconnect network - Google Patents

US9825809B2

2015-05-29

2017-11-21

Netspeed Systems

Dynamically configuring store-and-forward channels and cut-through
channels in a network-on-chip

US10218580B2

2015-06-18

2019-02-26

Netspeed Systems

Generating physically aware network-on-chip design from a physical systemon-chip specification

US9973435B2

2015-12-16

2018-05-15

Mellanox Technologies Tlv
Ltd.

Loopback-free adaptive routing

US10819621B2

2016-02-23

2020-10-27

Mellanox Technologies Tlv
Ltd.

Unicast forwarding of adaptive-routing notifications

US10178029B2

2016-05-11

2019-01-08

Mellanox Technologies Tlv
Ltd.

Forwarding of adaptive routing notifications

US10389636B2 *

2016-07-01

2019-08-20

Intel Corporation

Technologies for adaptive routing using network traffic characterization

US10630590B2 *

2016-07-14

2020-04-21

Mellanox Technologies Tlv
Ltd.

Credit loop deadlock detection and recovery in arbitrary topology networks

US10452124B2

2016-09-12

2019-10-22

Netspeed Systems, Inc.

Systems and methods for facilitating low power on a network-on-chip

US10281659B2 *

2016-11-03

2019-05-07

Alcatel Lucent

Fiber-management solution for an optical-network node

US20180159786A1

2016-12-02

2018-06-07

Netspeed Systems, Inc.

Interface virtualization and fast path for network on chip

US10200294B2

2016-12-22

2019-02-05

Mellanox Technologies Tlv
Ltd.

Adaptive routing based on flow-control credits

US10313269B2

2016-12-26

2019-06-04

Netspeed Systems, Inc.

System and method for network on chip construction through machine
learning

US10063496B2

2017-01-10

2018-08-28

Netspeed Systems Inc.

Buffer sizing of a NoC through machine learning

US10084725B2

2017-01-11

2018-09-25

Netspeed Systems, Inc.

Extracting features from a NoC for machine learning construction

US10469337B2

2017-02-01

2019-11-05

Netspeed Systems, Inc.

Cost management against requirements for the generation of a NoC

US10298485B2

2017-02-06

2019-05-21

Netspeed Systems, Inc.

Systems and methods for NoC construction

US10476780B2

2017-09-29

2019-11-12

Hewlett Packard Enterprise
Development Lp

Routing packets based on congestion of minimal and non-minimal routes

US11321136B2 *

2017-12-28

2022-05-03

Intel Corporation

Techniques for collective operations in distributed systems

US10644995B2

2018-02-14

2020-05-05

Mellanox Technologies Tlv
Ltd.

Adaptive routing in a box

US10983910B2

2018-02-22

2021-04-20

Netspeed Systems, Inc.

Bandwidth weighting mechanism based network-on-chip (NoC)
configuration

US10547514B2

2018-02-22

2020-01-28

Netspeed Systems, Inc.

Automatic crossbar generation and router connections for network-on-chip
(NOC) topology generation

US11144457B2

2018-02-22

2021-10-12

Netspeed Systems, Inc.

Enhanced page locality in network-on-chip (NoC) architectures

US10896476B2

2018-02-22

2021-01-19

Netspeed Systems, Inc.

Repository of integration description of hardware intellectual property for
NoC construction and SoC integration

US11023377B2

2018-02-23

2021-06-01

Netspeed Systems, Inc.

Application mapping on hardened network-on-chip (NoC) of fieldprogrammable gate array (FPGA)

US11176302B2

2018-02-23

2021-11-16

Netspeed Systems, Inc.

System on chip (SoC) builder

CN110324249B *

2018-03-28

2023-05-26

清华大学

A dragonfly network architecture and its multicast routing method

US10887217B2

2018-06-29

2021-01-05

Hewlett Packard Enterprise
Development Lp

Routing packets based on congestion metric thresholds and weights

US10944843B2

2018-11-05

2021-03-09

International Business
Machines Corporation

Topology aware computing device to reduce network latency

US11005724B1

2019-01-06

2021-05-11

Mellanox Technologies, Ltd.

Network topology having minimal number of long connections among
groups of network elements

US11792114B2

2019-05-23

2023-10-17

Hewlett Packard Enterprise
Development Lp

System and method for facilitating efficient management of non-idempotent
operations in a network interface controller (NIC)

US11316713B2 *

2019-11-25

2022-04-26

International Business
Machines Corporation

Virtual drawers in a server

US11561840B2

2020-01-30

2023-01-24

Alibaba Group Holding
Limited

Efficient inter-chip interconnect topology for distributed parallel deep
learning

https://patents.google.com/patent/EP2461254B1/en?q=(high+radix)&oq=high+radix&page=1

12/20

2/14/26, 2:01 PM

EP2461254B1 - Table-driven routing in a dragonfly processor interconnect network - Google Patents

GB2592211A

2020-02-19

2021-08-25

Nchain Holdings Ltd

Adapting connections of a layered network

GB2592225A

2020-02-19

2021-08-25

Nchain Holdings Ltd

Attestation service for use with a blockchain network

GB2594684A *

2020-02-19

2021-11-10

Nchain Holdings Ltd

Layered network

US11575594B2

2020-09-10

2023-02-07

Mellanox Technologies, Ltd.

Deadlock-free rerouting for resolving local link failures using detour paths

US11411911B2

2020-10-26

2022-08-09

Mellanox Technologies, Ltd.

Routing across multiple subnetworks using address mapping

US11870682B2

2021-06-22

2024-01-09

Mellanox Technologies, Ltd.

Deadlock-free local rerouting for handling multiple local link failures in
hierarchical network topologies

US11765103B2

2021-12-01

2023-09-19

Mellanox Technologies, Ltd.

Large-scale network with high port utilization

WO2023161831A1 *

2022-02-22

2023-08-31

Marvell Israel (M.I.S.L) Ltd.

Notification-based load balancing in a network

WO2023163858A1 *

2022-02-28

2023-08-31

Arris Enterprises Llc

Tunable latency with minimum jitter

CN115987702B *

2022-05-10

2025-08-26

清华大学

Broadcasting method, device, electronic device and storage medium

US12155563B2

2022-09-05

2024-11-26

Mellanox Technologies, Ltd.

Flexible per-flow multipath managed by sender-side network adapter

US12328251B2

2022-09-08

2025-06-10

Mellano Technologies, Ltd.

Marking of RDMA-over-converged-ethernet (RoCE) traffic eligible for
adaptive routing

US11765041B1 *

2022-09-15

2023-09-19

Huawei Technologies Co.,
Ltd.

Methods and systems for implementing a high radix network topology

KR20240080980A *

2022-11-30

2024-06-07

삼성전자주식회사

Device for networt systems

US12418475B2

2022-12-08

2025-09-16

Google Llc

Fault-tolerant routing algorithm for toroidal network topologies

CN117081984B *

2023-09-27

2024-03-26

新华三技术有限公司

Route adjustment method, device and electronic equipment

US20250240237A1 *

2024-01-24

2025-07-24

Cornelis Networks, Inc.

Topology for deadlock prevention in a dragonfly using two virtual lanes

US20250240238A1 *

2024-01-24

2025-07-24

Cornelis Networks, Inc.

Deadlock prevention in a dragonfly using two virtual lanes

* Cited by examiner, † Cited by third party, ‡ Family to family citation

Similar Documents
Publication

Publication Date

Title

EP2461254B1

2020-09-16

Table-driven routing in a dragonfly processor interconnect network

EP2451127B1

2015-03-11

Progressive adaptive routing in a dragonfly processor interconnect network

US10153985B2

2018-12-11

Dragonfly processor interconnect network

US20240039836A1

2024-02-01

Algorithms for use of load information from neighboring nodes in adaptive routing

US7623455B2

2009-11-24

Method and apparatus for dynamic load balancing over a network link bundle

JP6093867B2

2017-03-08

Non-uniform channel capacity in the interconnect

US8531968B2

2013-09-10

Low cost implementation for a device utilizing look ahead congestion management

US8085659B2

2011-12-27

Method and switch for routing data packets in interconnection networks

CN102263697A

2011-11-30

Method and device for sharing aggregated link traffic

EP2297903A1

2011-03-23

Method of data delivery across a network

Lei et al.

2015

Multipath routing in SDN-based data center networks

Gómez et al.

2003

VOQ/sub SW: a methodology to reduce HOL blocking in InfiniBand networks

CN117135107A

2023-11-28

Network communication topology system, routing method, device and medium

Soni et al.

0

A way of managing data center networks

Balfour et al.

2005

EE382C Project

Abts et al.

2011

Routing

Anglano et al.

2003

Network interface multicast protocols for wormhole-based networks of workstations

https://patents.google.com/patent/EP2461254B1/en?q=(high+radix)&oq=high+radix&page=1

13/20

2/14/26, 2:01 PM

EP2461254B1 - Table-driven routing in a dragonfly processor interconnect network - Google Patents

Priority And Related Applications
Child Applications (2)
Application

Priority date

Filing date

Relation

Title

EP16180770.6A

2010-11-05

2011-11-04

Division

Table-driven routing in a dragonfly processor interconnect network

EP16180770.6A

2010-11-05

2011-11-04

Division-Into

Table-driven routing in a dragonfly processor interconnect network

Priority Applications (1)
Application

Priority date

Filing date

Title

EP16180770.6A

2010-11-05

2011-11-04

Table-driven routing in a dragonfly processor interconnect network

Applications Claiming Priority (1)
Application

Filing date

US41064110P

2010-11-05

Title

Legal Events
Date

Code

Title

Description

2012-06-05

PUAI

Public reference made under article 153(3) epc to a published international application that has entered the european phase

Free format text: ORIGINAL
CODE: 0009012

2012-06-06

AK

Designated contracting states

Kind code of ref document
A1
Designated state(s): AL AT
BE BG CH CY CZ DE DK EE
ES FI FR GB GR HR HU IE IS
IT LI LT LU LV MC MK MT N
NO PL PT RO RS SE SI SK S
TR

2012-06-06

AX

Request for extension of the european patent

Extension state: BA ME

2012-10-31

RAP1

Party data changed (applicant data changed or rights of an application transferred)

Owner name: INTEL
CORPORATION

2013-01-16

17P

Request for examination filed

Effective date: 20121206

2017-07-14

STAA

Information on the status of an ep patent application or granted ep patent

Free format text: STATUS:
EXAMINATION IS IN
PROGRESS

2017-08-16

17Q

First examination report despatched

Effective date: 20170712

2020-05-25

REG

Reference to a national code

Ref country code: DE
Ref legal event code: R079
Ref document number:
602011068607
Country of ref document: D
Free format text: PREVIOU
MAIN CLASS:
G06F0015173000
Ipc: H04L0012000000

2020-07-01

RIC1

Information provided on ipc code assigned before grant

Ipc: H04L 12/00
20060101AFI20200525BH

2020-07-05

GRAP

Despatch of communication of intention to grant a patent

Free format text: ORIGINAL
CODE: EPIDOSNIGR1

https://patents.google.com/patent/EP2461254B1/en?q=(high+radix)&oq=high+radix&page=1

14/20

2/14/26, 2:01 PM

EP2461254B1 - Table-driven routing in a dragonfly processor interconnect network - Google Patents

2020-07-05

STAA

Information on the status of an ep patent application or granted ep patent

Free format text: STATUS:
GRANT OF PATENT IS
INTENDED

2020-08-05

INTG

Intention to grant announced

Effective date: 20200706

2020-08-08

GRAS

Grant fee paid

Free format text: ORIGINAL
CODE: EPIDOSNIGR3

2020-08-14

GRAA

(expected) grant

Free format text: ORIGINAL
CODE: 0009210

2020-08-14

STAA

Information on the status of an ep patent application or granted ep patent

Free format text: STATUS:
THE PATENT HAS BEEN
GRANTED

2020-09-16

AK

Designated contracting states

Kind code of ref document
B1
Designated state(s): AL AT
BE BG CH CY CZ DE DK EE
ES FI FR GB GR HR HU IE IS
IT LI LT LU LV MC MK MT N
NO PL PT RO RS SE SI SK S
TR

2020-09-16

REG

Reference to a national code

Ref country code: GB
Ref legal event code: FG4D

2020-09-30

REG

Reference to a national code

Ref country code: CH
Ref legal event code: EP

2020-10-01

REG

Reference to a national code

Ref country code: DE
Ref legal event code: R096
Ref document number:
602011068607
Country of ref document: D

2020-10-14

REG

Reference to a national code

Ref country code: IE
Ref legal event code: FG4D

2020-10-15

REG

Reference to a national code

Ref country code: AT
Ref legal event code: REF
Ref document number:
1315134
Country of ref document: A
Kind code of ref document
Effective date: 20201015

2021-01-29

PG25

Lapsed in a contracting state [announced via postgrant information from national office to epo]

Ref country code: SE
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20200916
Ref country code: GR
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20201217
Ref country code: HR

https://patents.google.com/patent/EP2461254B1/en?q=(high+radix)&oq=high+radix&page=1

15/20

2/14/26, 2:01 PM

EP2461254B1 - Table-driven routing in a dragonfly processor interconnect network - Google Patents
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20200916
Ref country code: NO
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20201216
Ref country code: FI
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20200916
Ref country code: BG
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20201216

2021-02-15

REG

Reference to a national code

Ref country code: AT
Ref legal event code: MK05
Ref document number:
1315134
Country of ref document: A
Kind code of ref document
Effective date: 20200916

2021-02-24

REG

Reference to a national code

Ref country code: NL
Ref legal event code: MP
Effective date: 20200916

2021-02-26

PG25

Lapsed in a contracting state [announced via postgrant information from national office to epo]

Ref country code: LV
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20200916
Ref country code: RS
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20200916

2021-04-12

REG

Reference to a national code

Ref country code: LT
Ref legal event code: MG4

2021-04-30

PG25

Lapsed in a contracting state [announced via postgrant information from national office to epo]

Ref country code: EE
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO

https://patents.google.com/patent/EP2461254B1/en?q=(high+radix)&oq=high+radix&page=1

16/20

2/14/26, 2:01 PM

EP2461254B1 - Table-driven routing in a dragonfly processor interconnect network - Google Patents
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20200916
Ref country code: CZ
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20200916
Ref country code: LT
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20200916
Ref country code: PT
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20210118
Ref country code: NL
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20200916
Ref country code: SM
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20200916
Ref country code: RO
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20200916

2021-05-31

PG25

Lapsed in a contracting state [announced via postgrant information from national office to epo]

Ref country code: IS
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20210116
Ref country code: ES
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20200916
Ref country code: AL
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO

https://patents.google.com/patent/EP2461254B1/en?q=(high+radix)&oq=high+radix&page=1

17/20

2/14/26, 2:01 PM

EP2461254B1 - Table-driven routing in a dragonfly processor interconnect network - Google Patents
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20200916
Ref country code: AT
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20200916
Ref country code: PL
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20200916

2021-06-17

REG

Reference to a national code

Ref country code: DE
Ref legal event code: R097
Ref document number:
602011068607
Country of ref document: D

2021-06-30

PG25

Lapsed in a contracting state [announced via postgrant information from national office to epo]

Ref country code: MC
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20200916
Ref country code: SK
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20200916

2021-06-30

REG

Reference to a national code

Ref country code: CH
Ref legal event code: PL

2021-07-23

PLBE

No opposition filed within time limit

Free format text: ORIGINAL
CODE: 0009261

2021-07-23

STAA

Information on the status of an ep patent application or granted ep patent

Free format text: STATUS:
NO OPPOSITION FILED
WITHIN TIME LIMIT

2021-07-30

PG25

Lapsed in a contracting state [announced via postgrant information from national office to epo]

Ref country code: LU
Free format text: LAPSE
BECAUSE OF NON-PAYMEN
OF DUE FEES
Effective date: 20201104

2021-08-11

REG

Reference to a national code

Ref country code: BE
Ref legal event code: MM
Effective date: 20201130

2021-08-25

26N

No opposition filed

Effective date: 20210617

2021-08-25

GBPC

Gb: european patent ceased through non-payment of renewal fee

Effective date: 20201216

https://patents.google.com/patent/EP2461254B1/en?q=(high+radix)&oq=high+radix&page=1

18/20

2/14/26, 2:01 PM
2021-08-31

EP2461254B1 - Table-driven routing in a dragonfly processor interconnect network - Google Patents
PG25

Lapsed in a contracting state [announced via postgrant information from national office to epo]

Ref country code: SI
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20200916
Ref country code: LI
Free format text: LAPSE
BECAUSE OF NON-PAYMEN
OF DUE FEES
Effective date: 20201130
Ref country code: DK
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20200916
Ref country code: CH
Free format text: LAPSE
BECAUSE OF NON-PAYMEN
OF DUE FEES
Effective date: 20201130

2021-10-29

PG25

Lapsed in a contracting state [announced via postgrant information from national office to epo]

Ref country code: FR
Free format text: LAPSE
BECAUSE OF NON-PAYMEN
OF DUE FEES
Effective date: 20201116
Ref country code: IE
Free format text: LAPSE
BECAUSE OF NON-PAYMEN
OF DUE FEES
Effective date: 20201104
Ref country code: IT
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20200916

2021-11-30

PG25

Lapsed in a contracting state [announced via postgrant information from national office to epo]

Ref country code: GB
Free format text: LAPSE
BECAUSE OF NON-PAYMEN
OF DUE FEES
Effective date: 20201216

2022-06-01

PG25

Lapsed in a contracting state [announced via postgrant information from national office to epo]

Ref country code: IS
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20210116
Ref country code: TR
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20200916
Ref country code: MT

https://patents.google.com/patent/EP2461254B1/en?q=(high+radix)&oq=high+radix&page=1

19/20

2/14/26, 2:01 PM

EP2461254B1 - Table-driven routing in a dragonfly processor interconnect network - Google Patents
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20200916
Ref country code: CY
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20200916

2022-06-30

PG25

Lapsed in a contracting state [announced via postgrant information from national office to epo]

Ref country code: MK
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20200916

2022-07-29

PG25

Lapsed in a contracting state [announced via postgrant information from national office to epo]

Ref country code: BE
Free format text: LAPSE
BECAUSE OF NON-PAYMEN
OF DUE FEES
Effective date: 20201130

2023-06-21

P01

Opt-out of the competence of the unified patent court (upc) registered

Effective date: 20230518

2026-01-08

PGFP

Annual fee paid to national office [announced via postgrant information from national office to epo]

Ref country code: DE
Payment date: 20250923
Year of fee payment: 15

Data provided by IFI CLAIMS Patent Services

About

Send Feedback

Public Datasets

Terms

Privacy Policy

https://patents.google.com/patent/EP2461254B1/en?q=(high+radix)&oq=high+radix&page=1

Help

20/20

2/14/26, 2:05 PM

EP2708000B1 - Flexible radix switching network - Google Patents

Patents
Back to results

2 of 125,048

radix-3

(radix-3);

Flexible radix switching network
Images (26)

EP2708000B1
European Patent Office

Download PDF

Find Prior Art

Similar

Other languages: German, French
Inventor: Ratko V. Tomic, Christopher John Williams, Leigh
Richard TURNER, Reed Graham LEWIS

Classifications

Current Assignee : J SCOTT BENSON LIVING TRUST

H04L12/462 LAN interconnection over a bridge based backbone
H04L41/0806 Configuration setting for initial configuration or provisioning, e.g. plug-and-play

Worldwide applications
2012 CA US WO EP

H04L45/06 Deflection routing, e.g. hot-potato routing
H04L49/118 Address processing within a device, e.g. using internal ID or tags for routing
within a switch

Application EP12723560.4A events
2012-05-08

Application filed by J Scott Benson Living Trust

H04L49/1553 Interconnection of ATM switching modules, e.g. ATM switching fabrics

2014-03-19

Publication of EP2708000A1

H04L49/356 Switches specially adapted for specific applications for storage area networks

2020-03-25

Application granted

2020-03-25

Publication of EP2708000B1

Status

Active

2032-05-08

Anticipated expiration

Hide more classifications

Landscapes

Engineering & Computer Science
Info: Patent citations (11), Non-patent citations (2) , Cited by

Computer Networks & Wireless Communication

(73), Legal events, Similar documents, Priority and Related
Applications
External links: Espacenet, EPO GPI, EP Register, Global Dossier,

Show more

Discuss

Hide Dependent

Claims (12)

1.

A method of constructing a wired network for the transfer of data from a source device (X1,....X8) to a destination device (Y1,....Y8) wherein:
the network is symmetric in structure;
the topology of the symmetric network structure is substantially a Cayley graph;
and the network has the following defined parameters:
the number of end devices of the network, wherein an end device is a device which is a source (m) device and a destination (m) device,
the number of switches (N) of the network,
the number of ports (d) per switch, and
the network oversubscription ratio (∼200:1; ∼40:1; ∼5:1);
the method comprising:
performing a function outputting a generator matrix, the function taking as input at least the defined parameters of the number of ports per switch and the
network oversubscription ratio;
determining a wiring pattern for interconnecting each of the switches as a function of the generator matrix; and
interconnecting the switches with interconnecting wires according to the wiring pattern to form the wired network for transferring data from a source device to
a destination device.
2.

The method according to claim 1 wherein the network structure substantially corresponds to a hypercube having a dimension d.

https://patents.google.com/patent/EP2708000B1/en?q=(radix-3)&oq=radix-3

1/51

2/14/26, 2:05 PM
3.

EP2708000B1 - Flexible radix switching network - Google Patents
The method according to claim 2 wherein the generator matrix is determined as a function of the number of interconnections between switches of the network
and the dimension, d, of the hypercube.

4.

The method according to claim 1 wherein the generator matrix is an error correcting code generating matrix and the wiring pattern is determined by rotating the
error correcting code generating matrix.

5.

The method according to claim 4, wherein the error correcting code generating matrix is rotated counterclockwise.

6.

The method according to claim 1 wherein the network oversubscription ratio is determined as a function of a number of ports (d) defined for connection to source
(m) computers and destination (m) computers and a bisection of the network.

7.

The method according to claim 6 wherein the bisection is determined as a function of a Walsh function.

8.

The method according to claim 7 wherein the bisection is determined by constructing primary equipartitions defined by patterns of 1's and 0s in a Walsh function.

9.

The method according to claim 7, wherein the bisection is determined by constructing primary equipartitions defined by the sign pattern in an algebraic Walsh
function.

10.

The method according to claim 6 wherein the bisection is determined by mapping network interconnection parameters to digital (t, m, s) nets parameters using
error correction code (ECC) distance metrics for point distribution.

11.

The method according to claim 10 wherein the ECC distance metrics are constructed using a Lee distance.

12.

The method according to claim 10 wherein the ECC distance metrics are constructed using a Hamming distance.

Description

FIELD OF THE INVENTION
[0001] The disclosure relates generally to the interconnection of nodes in a network. More specifically, the disclosure relates to interconnected nodes of a
communication network that provide some combination of computation and/or data storage and provide an efficient interchange of data packets between the
interconnected nodes. The network can be defined by a base network structure that can be optimized by selectively defining and connecting long hops between
sections of the network, for example, to reduce the network diameter.
[0002] The disclosure provides a method for creating a cost- effective way of connecting together a very large number of producers and consumers of data streams.
BACKGROUND OF THE INVENTION
[0003] One real world analogy is the methods for constructing roadway networks that allow drivers to get from a starting location to a destination while satisfying real
world constraints such as 1) a ceiling on the amount of tax people are willing to pay to fund roadway construction; 2) a desire to maximize speed of travel
subject to safety constraints; and 3) a desire to avoid traffic jams at peak travel times of day.
[0004] In this analogy, the cars are similar to the data sent over a computer network and the starting locations and destinations represent the host computers
connected to the network. The constraints translate directly into cost, speed and congestion constraints in computer networks.
[0005] The basic quality of solving these types of problems is that they get much harder to solve efficiently as the number of starting locations and destinations (e.g.,
host computers) increases. As a starting point, consider how three destinations can be connected together. Figs. 1A and 1B show that there are really only two
alternatives. The number of ways to connect the destinations together grows along with the number of destinations, for example, with four destinations, some of
the possible methods of connection are shown in Figs. 1C, 1D, 1E and 1F.
[0006] As can be seen from the figures, both the number of connections between nodes and the number of different ways of making those connections grows faster
than the number of connections. For example, a set of 6 nodes can have more than twice as many alternative ways to connect the nodes as a set of 3 nodes.
Also, the possible number of connections between the nodes can vary from, on the low side, the number of nodes (N) minus 1 for destinations connected, for
example, along a single line as shown in Fig. 1C, to N(N-1)/2 connections as shown in Fig. 1F, where every single node has a direct connection to every other
node.
[0007] Another measure of the performance of a network is the diameter of the network, which refers to how many connections need to be traveled in order to get from
any one destination to another. In the network shown in Fig. 1C, its economy in the number of connections (3) is offset by the consequence that the only path,
from one end of the network to the other, requires travel across three connections, thus slowing the journey. On the other hand as shown in Fig. 1F, the large
number of connections results in every destination only being one connection away from any other, permitting more rapid travel.
[0008] The two networks shown in Figs. 1C and 1F can also have very different behavior at peak traffic times. Assuming that each connection can support the same
rate of traffic flow, the two end point nodes of the network shown in Fig. 1C will be affected if there is a lot of traffic traveling between the two nodes in the
middle of the line. Conversely, in network shown in Fig. 1F, since there is an individual connection between every possible combination of nodes, traffic flowing
between two nodes is not affected at all by traffic flowing between a different pair of nodes.
[0009] Another difficulty arises in the construction of computer networks: It is difficult to have a large number of connections converging on a single point, such as
shown in Fig. IF. In a computer data center, the devices that allow multiple connections to converge are called switches. These switches that allow multiple
connections to converge typically have physical limitations on the number of connections or ports, for example, around 50 ports for inexpensive switches, and
can approach 500 ports for more modern, expensive switches. This means that for a fully- meshed network like that shown in Fig. IF where delays and
congestion are minimized, no more than, for example, 499 destination hosts could be connected together. John D. Dixon "Groups with a Cayley graph isomorphic
to a hypercube", Bulletin of the Australian Mathematical Society, vol. 55, no. 03, 1 June 1997, page 385 describes a process for enumerating the Cayley graphs
isomorphic to a binary d-cube for small values of d. US Patent No. 5, 844,887 describes a distributed, scalable and modular asynchronous transfer mode
switching fabric. Lakshmivarahan S et al "Ring, torus and hypercube architectures/algorithms for parallel computing", Parallel Computing, Elsevier Publishers,
Amsterdam, NL, vol. 25, no. 13-14, 1 December 1999, pages 1877-1906 provides a survey of both architectural and algorithmic aspects of solving problems
using parallel processors with ring, torus and hypercube interconnection.
SUMMARY OF THE INVENTION
[0010] The sample network layouts shown in Figs. 1A-1F, 2A-2C, and in fact all other network layouts conceived to date, suffer from a fundamental tradeoff between the
cost and difficulty of building the network, and the ability of the network to support high traffic rates. The present invention allows for the design of networks that
can include a very large number of connections and a high level of complexity of the switches that manage those connections, while providing very high
immunity from the congestion that limits the ability of all nodes to communicate with each other at maximum speed, no matter how other nodes are using the
network.
[0011] The emergence of "cloud computing", supported by huge data centers where hundreds of thousands of computers all connected to one network provide
economies of scale and thereby reduced costs, has stressed the ability of current network designs to provide a reliable and cost effective way of allowing data

https://patents.google.com/patent/EP2708000B1/en?q=(radix-3)&oq=radix-3

2/51

2/14/26, 2:05 PM

EP2708000B1 - Flexible radix switching network - Google Patents

to be exchanged between the computers.
[0012] A number of approaches have been tried by both academia and industry, but to date, all the approaches fall short of theoretical limits by a factor of 2 to 5 times.
Some embodiments of the invention include a method for constructing networks that can be within 5-10% of the theoretical maximum for data throughput
across networks with multiple simultaneously communicating hosts, a highly prevalent use case in modern data centers.
[0013] In accordance with some embodiments of the invention, methods for constructing highly ordered networks of hosts and switches are disclosed that make
maximum use of available switch hardware and interconnection wiring. The basic approach can include the following: selecting a symmetrical network base
design, such as, a hypercube, a star, or another member of the Cayley graph family;
developing an appropriate topological routing method that simplifies data packet forwarding; and adding short cuts or long hops to the base symmetrical
network to reduce the network diameter.
[0014] The regularity of symmetrical networks makes them well suit for topological addressing schemes.
[0015] It is one of the objects of the present invention to provide an improved network design that can be expanded greatly without performance penalty.
[0016] It is another object of the present invention to provide an improved network design allows the network to be more easily operated and managed. In some
embodiments, the entire network can be operated and managed as a single switch.
[0017] It is another object of the present invention to provide an improved network design that provided improved network performance. In some embodiments, the
network can have 2 to 5 times greater bisection bandwidth than with conventional network architectures that use the same number of component switches and
ports.
[0018] The invention also includes flexible methods for constructing physical embodiments of the networks using commercially available switches and method for
efficiently, accurately and economically interconnecting (wiring) the switched together to form a high performance network having improved packet handing. The
invention is defined as a method according to claim 1. Various embodiments are defined in the dependent claims.
Description of Drawings
[0019]

Figures 1A - 1F show sample network layouts.
Figures 2A - 2C show symmetrical network structures according to some embodiments of the invention.
Figures 3A and 3B show an example of topological routing.
Figure 4A shows an order 3 hypercube and Figure 4B shows an order 3 hypercube with shortcuts added.
Figure 5 illustrates a typical large data center layer 2 network architecture.
Figure 6 illustrates hypercube notation and construction.
Figure 7 illustrates partitioning between topology and external ports.
Figure 8 illustrates packet non-blocking with 4 switches and 8 paths.
Figure 9 illustrates a network bisection according to some embodiments of the invention.
Figure 10 illustrates an 8 node network with long hops added.
Figures 11 - 15 are charts comparing long hop networks with alternative network configurations..
Figure 16 illustrates data center available bandwidth and cost for 4x external/topology port ratio.
Figure 17 illustrates data center available bandwidth and cost for 1x external/topology port ratio.
Figure 18 illustrates the reduction in average and maximum hops.
Figure 19 illustrates optimized wiring pattern using port dimension mapping according to an embodiment of the invention.
Figure 20 illustrates the integrated super switch architecture across an entire data center according to an embodiment of the invention.
Figure 21 illustrates a network architecture showing a flexible radix switch fabric according to an embodiment of the invention.
Figure 22 illustrates the flow of a data packet from an ingress switch through a network according to an embodiment of the present invention.
Figure 23 illustrates various network logical topographies according to an embodiment of the present invention.
Figure 24 illustrates a network architecture according to one embodiment of the invention.
Figure 25 illustrates a system including a Data Factory according to some embodiments of the invention.
Figure 26 illustrates a system interconnecting a control plane executive (CPX) according to some embodiments of the invention.
DETAILED DESCRIPTION OF EMBODIMENTS OF THE INVENTION

[0020] The present invention is directed methods and systems for designing large networks and the resulting large networks. In accordance with some embodiments
of the invention, a way of connecting large numbers of nodes, consisting of some combination of computation and data storage, and providing improved
behaviors and features. These behaviors and features can include: a) practically unlimited number of nodes, b) throughput which scales nearly linearly with the
number of nodes, without bottlenecks or throughput restriction, c) simple incremental expansion where increasing the number of nodes requires only a
proportional increase in the number of switching components, while maintaining the throughput per node, d) maximized parallel multipath use of available node
interconnection paths to increase node-to-node bandwidth, e) Long hop topology enhancements which can simultaneously minimize latency (average and
maximum path lengths) and maximize throughput at any given number of nodes, f) a unified and scalable control plane, g) a unified management plane, h)
simple connectivity - nodes connected to an interconnection fabric do not need to have any knowledge of topology or connection patterns, i) streamlined
interconnection path design - dense interconnections can be between physically near nodes, combined with a reduced number of interconnections between
physically distant nodes, resulting in simple interconnection or wiring.
[0021] In one embodiment of the invention, the nodes can represent servers or hosts and network switches in a networked data center, and the interconnections
represent the physical network cables connecting the servers to network switches, and the network switches to each other.
[0022] In another embodiment of the invention, the nodes can represent geographically separated clusters of processing or data storage centers and the network
switches that connect them over a wide area network. The interconnections in this case can be the long distance data transfer links between the geographically
separated data centers.
[0023] Those skilled in the art will realize that the described invention can be applied to many other systems where computation or data storage nodes require high
bandwidth interconnection, such as central processing units in a massively parallel supercomputer or other multiple CPU or multi-core CPU processing arrays.
[0024] In accordance with some embodiments of the invention, component switches can be used as building blocks, wherein the component switches are not
managed by data center administrators as individual switches. Instead, switches can be managed indirectly via the higher level parameters characterizing
collective behavior of the network, such as latency (maximum and average shortest path lengths), bisection (bottleneck capacity), all-to-all capacity, aggregate
oversubscription, ratio of external and topological ports, reliable transport behavior, etc. Internal management software can be used to translate selected values
for these collective parameters into the internal configuration options for the individual switches and if necessary into rewiring instructions for data center
technicians. This approach makes management and monitoring scalable.
[0025] Hypercubes and their variants have attracted great deal of attention within parallel and supercomputer fields, and recently for data center architectures as well
due to their highly efficient communications, high fault tolerance and reliable diagnostics, lack of bottlenecks, simple routing & processing logistics, and simple,
regular construction. In accordance with some embodiments of the invention, a method of designing an improved network includes modifying a basic hypercube
network structure in order to optimize latency and bandwidth across the entire network. Similar techniques can be used to optimize latency and bandwidth
across other Cayley graph symmetrical networks such as star, pancake and truncated hypercube networks.
[0026] A symmetrical network is one that, from the perspective of a source or a destination looks the same no matter where you are in the network and which allows
some powerful methods to be applied for developing both routing methods for moving traffic through the network and for adding short cuts to improve

https://patents.google.com/patent/EP2708000B1/en?q=(radix-3)&oq=radix-3

3/51

2/14/26, 2:05 PM

EP2708000B1 - Flexible radix switching network - Google Patents

throughput and reduce congestion. One commonly known symmetrical network structure is based on the structure of a hypercube. The hypercube structured
network can include a set of destinations organized as the corners of a cube, such as shown in Fig. 2A. The structure shown in Fig. 2A is known as an order 3
hypercube, based on each destination having three connections to neighboring destinations. To generate a higher order hypercube, copy the original hypercube
and connect all the destinations in the first hypercube with the corresponding destination in the copy as shown in Fig. 2B.
[0027] Hypercubes are just one form of symmetrical network. Another form of symmetrical network is the star graph shown in Fig. 2C. There are many other types of
symmetrical networks, known formally as Cayley graphs that can be used as a basis on which to apply the methods of the invention.
[0028] In accordance with some embodiments of the present invention, topological routing can be used route messages through the symmetrical network. Topological
routing can include a method for delivering messages from a source node to a destination node through a series of intermediate locations or nodes where the
destination address on the message describes how to direct the message through the network. A simple analogy is the choice of method for labeling streets
and numbering houses in a city. In some planned areas such as Manhattan, addresses not only describe a destination location, "425 17th Street", but also
describe how to get there from a starting point. If it is known that house numbers are allocated 100 per block, and the starting location is 315 19th Street, it can
be determined that the route includes going across one block and down two streets to get to the destination. Similarly, for the organization shown in Fig. 3A,
traveling from N 200 W. 2nd Street to N 100 E 1st Street can include going east 3 blocks and south one block.
[0029] In contrast, a typical unplanned town like Concord, MA, shown in Fig. 3B, has road that are not laid out in any regular pattern and the names for streets have no
pattern either. This "plan" requires a "map" to determine how to get from one place to another.
[0030] Topological addressing is important in large networks because it means that a large map does not have to be both generated and then consulted at each step
along the way of sending a message to a destination. Generating a map is time consuming and consumes a lot of computing resources, and storing a map at
every step along the way between destinations consumes a lot of memory storage resources and requires considerable computation to look up the correct
direction on the map each time a message needs to be sent on its way towards its destination. The small maps required by topological addressing are not just a
matter of theoretical concern. Present day data centers have to take drastic, performance impacting measures to keep their networks divided into small enough
segments that the switches that control the forwarding of data packets do not get overwhelmed with building a map for the large number of destinations for
which traffic flows through each switch.
[0031] The regularity of symmetrical networks makes them excellent candidates for having topological addressing schemes applied to them, just as a regular, basically
symmetrical, arrangement of streets allows addresses to provide implied directions for getting to them.
[0032] In accordance with some embodiments of the invention, the performance of these symmetrical networks can be greatly improved by the select placement of
"short cuts" or long hops according to the invention. The long hops can simultaneously reduce the distance between destinations and improve the available
bandwidth for simultaneous communication. For example, Fig. 4A show a basic order 3 hypercube, where the maximum distance of three links between
destination nodes occurs at the opposite corners. In accordance with some embodiments of the invention, adding shortcuts across all three corners as shown
in Fig. 4B reduces the distance between the destinations that used to have the worst case distance of three to a distance of one link.
[0033] In accordance with some embodiments of the invention, this method can be applied to hypercubes of higher order with many more destinations. In accordance
with some of the embodiments of the invention, a method for identifying select long hops in higher order hypercube networks and symmetric networks can
include determining a generator matrix using linear error correcting codes to identify potential long hops within the network.
[0034] Figure 5 shows a diagram of a typical commercial data center. Figure 5 also shows the typical port oversubscription ratios, and hence bottlenecks, at each level
(core, aggregation, and edge) of the network, that result from the traditional approaches to building data centers. In addition, none of these approaches work
well as the number of devices connected to the network increase exponentially, as has happened as a result of adoption of highly centralized data centers with
large numbers of host computers or servers at a single location.
[0035] All real world network implementations are limited by the physical constraints of constructing switches and wiring them together. With the limitations of
conventional wiring techniques, one of the parameters that can be adjusted to improve network performance is to increase the number of ports per network
switch, which allows that group of ports to exchange data with very high throughput within the single physical device. Problems then arise maintaining that high
throughput when groups of switches have to be assembled in order to connect a large number of servers together. Switch manufacturers have been able to
increase the number of ports per switch into the several hundreds (e.g., 500), and some new architectures claim the ability to create switch arrays that have
several thousand ports. However, that is two to three orders of magnitude less than the number of servers in large data centers. The number of switch ports is
referred to as the "radix" of the switch.
[0036] In accordance with some embodiments of the invention, one difference between networks according to the invention and the prior art, networks according to the
invention can be expanded (increasing the number of host computer ports) practically, without limit or performance penalty. The expansion can be flexible, using
commodity switches having a variable radix. Although there are presently switches which can be upgraded from an initial configuration with a smaller radix to a
configuration with a higher radix, the latter maximum radix is fixed in advance to at most a few hundred ports. Further, the 'radix multiplier' switching fabric for
the maximum configuration is hardwired in the switch design. For example, a typical commercial switch such as the Arista 7500 can be expanded to 384 ports
by adding up to 8 line cards, each providing 48 ports; but the switching fabric gluing the 8 separate 48 port switches into one 384 port switch is rigidly fixed by
the design and it is even included in the basic unit. In contrast, the networks constructed according some embodiments of the invention have no upper limit on
the maximum number of ports it can provide. And this holds for an initial network design as well as any subsequent expansion of the same network. In
accordance with some embodiments of the invention, for any given type of switch having radix R, the upper limit for simple expansion without performance
penalty is 2 R -1 component switches. Since typical R is at least 48, even this conditional limit of 247≈ 1.4·1014 on the radix expansion is already far larger than
the number of ports in the entire internet, let alone in any existing or contemplated data center.
[0037] Another difference between networks according to some embodiments of the invention and prior art data centers is that data center layer 2 networks are
typically operated and managed as networks of individual switches where each switch requires individual installation, configuration, monitoring and
management. In accordance with some embodiments of the invention, the data center network can be operated and managed as a single switch. This allows the
invention to optimize all aspects of performance and costs (of switching fabric, cabling, operation and management) to a far greater degree than existing
solutions.
[0038] In addition, networks according to some embodiments of the invention can provide improved performance over any existing data center Layer 2 networks, on the
order of 2 to 5 times greater bisection bandwidth than conventional network architectures that use the same number of component switches and ports.
[0039] The invention also describes novel and flexible methods for realizing physical embodiments of the network systems described, both in the area of wiring
switches together efficiently, accurately and economically, as well as ways to use existing functionality in commercial switches to improve packet handing.
[0040] Hypercubes can be characterized by their number of dimensions, d. To construct a ( d +1)-cube, take two d -cubes and connect all 2 d corresponding nodes
between them, as shown in Figure 6 for transitions d : 0 → 1 → 2 → 3 (red lines indicate added links joining two d -cubes).
[0041] For purpose of illustrating one embodiment of the invention, a d-cube can be a d-dimensional binary cube (or Hamming cube, hypercube graph) with network
switches as its nodes, using d ports per switch for the d connections per node. By convention, coordinate values for nodes can be 0 or 1, e.g. a 2-cube has nodes
at (x, y) = (0,0), (0,1), (1,0), (1,1), or written concisely as binary 2-bit strings: 00, 01, 10 and 11.
[0042] Each switch can have some number of ports dedicated to interconnecting switches, and hosts can be connected to some or all of the remaining ports not used
to interconnect switches. Since the maximum number of switches N in a d -cube is N=2 d , the dimensions d of interest for typical commercial scalable data
center applications can include, for example, d = 10..16, i.e. d-cubes with 1K-64K switches, which corresponds to a range of 20K-1280K physical host
(computers or servers), assuming a typical subscription of 20 hosts per switch.

https://patents.google.com/patent/EP2708000B1/en?q=(radix-3)&oq=radix-3

4/51

2/14/26, 2:05 PM

EP2708000B1 - Flexible radix switching network - Google Patents

[0043] In accordance with some embodiments of the invention, a concise binary d -bit notation for nodes (and node labels) of a d -cube can be used. The hops, defined
as the difference vectors between directly connected nodes, can be d -bit strings with a single bit=1 and ( d -1) bits=0. The jumps (difference vectors) between
any two nodes S 1 and S 2 can be: J 12 = S1 ^ S2 (^ is a bitwise XOR) and the minimum number of hops (distance or the shortest path) L between them is the
Hamming weight (count of 1's) of the jump J 12 i.e. L ≡ L(J12 ) = |J 12|. There are exactly L! distinct shortest paths of equal length L between any two nodes S 1
and S 2 at distance L. The diameter D (maximum shortest path over all node pairs) of a d -cube is D = log(N) = d hops, which is also realized for each node. For
any node S, its bitwise complement (∼S) is at the maximum distance D from S. The average number of hops between two nodes is d /2 and bisection (minimum
number of links to cut in order to split a d -cube into 2 equal halves) is N/2.
[0044] In accordance with some embodiments of the invention, the d -cube coordinates of the switches ( d -bit strings with d ∼ 10..16) can be used as their physical
MAC addresses M, and the optimal routing becomes very simple. Routing can be done entirely locally, within each switch using only O (log( N )) resources
(where O can be _ and N is the maximum number of switches). When a frame with M dst arrives at a switch M, the switch M computes J = M^M dst and if J=0,
then the switch M is the destination. Otherwise it selects the next hop h corresponding to any bit = 1 in J which will bring the frame one hop closer to the M dst
since the next node after the hop at M nxt = M ^ h will have one less bit = 1, hence one less hop, in its jump vector to M dst which is J nxt= M nxt^ M dst.
[0045] In accordance with some embodiments of the invention, the total number of switches N s in the network is not an exact power of 2, so in this case, the d- cubes
can be truncated so that for any accessible M the relation M<Ns holds, where bit string M is interpreted as an integer (instead of M< 2 d which is used for a
complete d-cube). Hence, instead of the O(N) size forwarding table and an O(N) routing tree, the switches only need one number Ns and their own MAC address

M to forward frames along the shortest paths.
[0046] In accordance with some embodiments of the invention, one useful parameter of the hypercubic network topology is the port distribution or ratio of the internal
topology ports (T-ports) used to interconnect switches and external ports(E-ports) that the network uses to connect the network to hosts (servers and routers).
Networks built according to some embodiments of the invention can use a fixed ratio: λ≡ E/T (E=#E-ports, T=#T-ports) for all IPA switches. In accordance with
one embodiment, the ratio is λ=1 (ports are split evenly between E and T), is shown in Figure 7 for d=2.
[0047] For a hypercube of dimension d there are m≡d ·2 d total T-ports and E-ports, d ports per switch for either type. Since the ports are duplex, each E-port can be
simultaneously a source and a sink (destination) of data packets. Hence, there are m sources X 1, X 2,...Xm and m destinations Y 1, Y 2,... Ym. The non-blocking
(NB) property of a network or a switch can usually be defined via performance on the 'permutation task'- each source Xi (i=1..m) is sending data to a distinct
destination Yj (where j=πm[i] and π m a permutation of m elements), and if these m transmissions can occur without collisions/blocking, for all m! permutations
of Ys, the network is NB. The evaluation of the NB property of a network can depend on the specific meaning of "sending data" as defined by the queuing model.
Based on kinds of "sending data", there can be two forms of NB, Circuit NB (NB-C) and Packet NB (NB-P). For NB-C, each source X can send a continuous
stream at its full port capacity to its destination Y. For NB-P, each source can send one frame to its destination Y. In both cases, for NB to hold, for any π(Y) there
exists a set of m paths (sequences of hops), each path connecting its XY pair. The difference in these paths for the two forms of NB is that for NB-C each XY
path has to have all its hops reserved exclusively for its XY pair at all times, while for NB-P, the XY path needs to reserve a hop only for the packet forwarding
step in which the XY frame is using it. Hence NB-C is a stronger requirement, i.e. if a network is NB-C then it is also NB-P.
[0048] In accordance with some embodiments of the invention, a hypercube network with a λ=1 has Packet Non-Blocking Property. This is self-evident for d=1, where
there are only 2 switches, two ports per switch, one T-port and one E-port. In this case m=2, hence there are only 2!=2 sets of XY pairing instances to consider:
I1= [X 1 → Y 1 , X 2 → Y2 ] and I2 = [X 1 → Y 2, X2 → Y 1]. The set of m =2 paths for I1 are: {(X 10 Y 1 ), (X 2 1 Y 2 )}, each taking 0 hops to reach its destination (i.e.
there were no hops between switches, since the entire switching function in each path was done internally within the switch). The paths are shown as (X S1 S2 ...
S k Y), where Si sequence specifies switches visited by the frame in each hop from X. This path requires k-1hops between the switches (X and Y are not
switches but ports on S1 and Sk respectively). For the pairing I 2 , the two paths are {(X 10 1 Y 2 ), ( X 2 1 0 Y 1 )}, each 1 hop long. Since there were no collisions
in either instance I 1 or I 2, the d =1 network is NB-P. For the next size hypercube, d =2, m =8 and there are 8! (40320) XY pairings, so we will look at just one
instance (selected to maximize the demands over the same links) and show the selection of the m =8 collision free paths, before proving the general case.
[0049] Fig. 8 shows the 8 paths with their properties discernable by splitting the diagram into (a) and (b) parts, but the two are actually running on the same switches
and lines simultaneously. The short arrows with numbers show direction of the frame hop and the switching step/phase at which it takes place. It is evident that
at no stage of the switching, which lasts 3 hops, is any link required to carry 2 or more frames in the same direction (these are duplex lines, hence 2 frames can
share a link in opposite direction) hence NB-P holds for this instance. Not all paths are the shortest ones possible (e.g. X 1 → Y 3 which took 3 hops, although the
shortest path is 1 hop, the same one as the path X 2 → Y 4).
[0050] To prove that in the general case all m= d·N frames sent by X 1 , X 2 ,...Xm can be delivered to proper destinations, in a finite time and without collisions or
dropped frames, the following routing algorithm can be used. In the initial state when m frames are injected by the sources into the network, each switch
receives d frames from its d E-ports. If there were just one frame per switch instead of d, the regular hypercube routing could solve the problem, since there are
no conflicts between multiple frames targeting the same port of the same switch. Since each switch also has exactly d T-ports, if each switch sends d frames,
one frame to each port in any order, in the next stage each switch again has exactly d frames (received via its d T-ports), without collisions or frame drops so far.
While such 'routing' can go on forever without collisions/frame drops, it does not guarantee delivery. In order to assure a finite time delivery, each switch must
pick out of the maximum d frames it can have in each stage, the frame closest to its destination (the one with the lowest Hamming weight of its jump
vectorDst^Current) and send it to the correct port. The remaining d -1 frames (at most; there may be fewer) are sent on the remaining d -1 ports applying the
same rule (the closest one gets highest priority, etc). Hence after this step is done on each of the N switches, there are at least N frames (the N "winners" on N
switches) which are now closer by 1 hop to their destinations i.e. which are now at most d -1 hops away from their destination (since the maximum hop distance
on a hypercube is d). After k such steps, there will be at least N frames which are - at most d-k hops away from their destinations. Since the maximum distance
on a hypercube is d hops, in at most d steps from start at least N frames are delivered to their destinations and there are no collisions/drops. Since the total
number of frames to deliver is d·N, the above sequence of steps need not be repeated more than d times, therefore all frames are delivered in at most d 2 steps
after the start. QED.
[0051] In accordance with some embodiments of the invention, load balancing can be performed locally at each switch. For each arriving frame, the switch can select
the next hop along a different d-cube dimension than the last one sent, if one is available. Since for any two points with distance (shortest path) L there are L!
alternative paths of equal length L , there are plenty of alternatives to avoid congestion, especially if aided by a central control and management system with a
global picture of traffic flows.
[0052] Much of this look ahead at the packet traffic flow and density at adjacent nodes required to decide which among the equally good alternatives to pick can be
done completely locally between switches with a suitable lightweight one-hop (or few hops) self-terminating (time to live set to 1 or 2) broadcast through all
ports, notifying neighbors about its load. The information packet broadcast in such manner by a switch M can also combine its knowledge about other
neighbors (with their weight/significance scaled down geometrically, e.g. by a factor 1/ d for each neighbor). The division of labor between this local behavior of
switches and a central control and management system can be that switching for short distance and near time regions can be controlled by switches and that
switching for long distance and long time behavior can be controlled by the central control and management system.

https://patents.google.com/patent/EP2708000B1/en?q=(radix-3)&oq=radix-3

5/51

2/14/26, 2:05 PM

EP2708000B1 - Flexible radix switching network - Google Patents

[0053] In accordance with some embodiments of the invention, symmetrical networks with long hop shortcuts are used to achieve high performance in the network,
however additional forwarding management can be used to optimize the network and achieve higher levels of performance. As the size of the network (number
of hosts) becomes large, it is useful to optimize the forwarding processes to improve network performance.
[0054] One reason for current data center scaling problems is the non-scalable nature forwarding tables used in current switches. These tables grow as O ( N 2 ) where
N is the number of edge devices (hosts) connected to the network. For large networks, this quickly leads to forwarding tables that can not be economically
supported with current hardware, leading to various measures to control forwarding table size by segmenting networks, which leads to further consequences
and sub-optimal network behavior.
[0055] In accordance with some embodiments of to the invention, each switch can maintain a single size forwarding table (of size O(N)) and network connection matrix
(of size O(N·R), where R is the switch radix and N the number of switches). The scalable layer 2 topology and forwarding tables maintained by the switches can
be based on hierarchical labeling and corresponding hierarchical forwarding behavior of the switches, which require only m·N 1/m table entries for the m -level
hierarchy (where m is a small integer parameter, typically m = 2 or 3).
[0056] In accordance with one embodiment of the invention, the network can be divided into a hierarchy of clusters, which for performance reasons align with the
actual network connectivity. The 1st level clusters contain R nodes (switches) each, while each higher level cluster contains R sub-clusters of the previous lower
level. Hence, each node belongs to exactly one 1st level cluster, which belongs to exactly one 2nd level cluster, etc. The number of levels m needed for a network
with N nodes and a given R, is then determined from the relations R m-1 < N ≤ Rm, i.e. m = ┌log( N )/log( R )┐. The forwarding identifier (FID or Forwarding ID) of a
node consists of m separate fields (digits of the node ordinal 0..N-1 expressed in radix R ), FID = F 1 .F 2...Fm where F 1 specifies the node index (number 0.. R -1)
within its 1st level cluster, F 2 the index of the node's 1st level cluster within its second level cluster, etc.
[0057] For example, in an N =100 node network and selecting R =10, each node is labeled via two decimal digits, e.g. a node 3.5 is a node with index 3 in a cluster with
index 5. In this embodiment, if node 3.5 needs to forward to some node 2.8, all that 3.5 needs to know is how to forward to a single node in cluster 8, as long as
each node within the cluster 8 knows how to forward within its own cluster. For multi-path topologies, nodes have more than single destination forwarding
address. Or, for a general node - each node needs to know how to forward to 9 nodes in its own cluster and to a single node in each of other 9 clusters, hence it
needs tables with only 2∗9=18 elements (instead of 99 elements that conventional forwarding uses).
[0058] In accordance with some embodiments of the invention, the forwarding tables for each node can consist of m arrays Ti, i=1..m, each of size R elements (the
elements are forwarding ports). For example, for R =16 and a network with N=64∗ 1024 switches (corresponding to a network with 20∗ N =1280∗1024 hosts),
the forwarding tables in each switch consist of 4 arrays, each of size 16 elements, totaling 64 elements.
[0059] For any node F with FID( F ) = F .F ...F the array T [R] contains ports which F needs to use to forward to each of the R nodes in its own 1st level cluster. This
1
2
m
1
forwarding is not assumed to be a single hop, so the control algorithm can seek to minimize the number of hops when constructing these tables. A convenient
topology, such as hypercube type, makes this task trivial since each such forwarding step is a single hop to the right cluster. In accordance with some
embodiments of the invention, in the hypercube network, the control algorithm can harmonize node and cluster indexing with port numbers so that no
forwarding tables are needed at all. The array T 2 contains ports F needed for forwarding to a single node in each of the R 2nd level clusters belonging to the
same third level cluster as node F;T 3 contains ports F needed for forwarding to a single node in each of the R 3rd level clusters belonging to the same 4th level
cluster as F,... and finally T m contains ports F needs to use to forward to a single node in each of the Rm th level cluster belonging to the same ( m +1)th cluster
(which is a single cluster containing the whole network).
[0060] In accordance with some embodiments of the invention, forwarding can be accomplished as follows. A node F with FID(F) = F 1 .F 2...Fm receiving a frame with
final destination FID(Z) = Z 1 .Z 2..Zm determines the index i = 1..m of the highest 'digit' Zi that differs from its own corresponding 'digit' Fi and forward the frame
to the port Ti [Zi ]. The receiving node G then has (from the construction of tables Ti ) for its i -th digit the value Gi = Zi. Hence, repeating the procedure, node G
determines the index j<i of the highest digit Zj differing from corresponding Gj and forwards to port Tj[Zj ]... etc., until j =1, at which point the node is performing
the final forwarding within its own cluster.
[0061] In accordance with some embodiments of the invention, the implementation of this technique can involve the creation of hierarchical addresses. Since the
forwarding to clusters at levels > 1 involves approximation (a potential loss of information, and potentially sub-optimal forwarding), for the method to forward
efficiently it can be beneficial to a) reduce the number of levels m to the minimum needed to fit the forwarding tables into the CAMs (content addressable
memories) and b) reduce the forwarding approximation error for m >1 selecting the formal clustering used in the construction of the network hierarchy to match
as closely as possible the actual topological clustering of the network.
[0062] Forwarding efficiency can be improved by reducing the number of levels m to the minimum needed to fit the forwarding tables into the CAMs. In situations where
one can modify only the switch firmware but not the forwarding hardware to implement hierarchical forwarding logic, the conventional CAM tables can be used.
The difference from the conventional use is that instead of learning the MAC addresses, which introduce additional approximation and forwarding inaccuracy,
the firmware can program the static forwarding tables directly with the hierarchical tables.
[0063] Since m levels reduce the size of the tables from N to m·N 1/m entries (e.g. m =2 reduces the tables from N entries to 2·√N entries), a 2-3 level hierarchy may be
sufficient to fit the resulting tables in the C =16K entries CAM memory (e.g. m =2, C =16K allows 2·8K entries, or N =64·206 nodes). Generally, m is the lowest
value satisfying inequality: m·N 1/m ≤ C.
[0064] In order to reduce the forwarding approximation error for m >1, the formal clustering used in the construction of the hierarchical should match as closely as
possible the actual topological clustering of the network. For enhanced hypercube topologies used by the invention, optimum clustering is possible since
hypercubes are a clustered topology with m =log(N). In practice, where minimum m is preferred, the hypercubes of dimension d are intrinsically clustered into
lower level hypercubes corresponding to partition of d into m parts. E.g. partition d = a+b corresponds to 2 a clusters (hypercube of dim= a ) of size 2 b each
(hypercubes of dim= b ). The following clustering algorithm performs well in practice and can be used for general topologies:
[0065] A node which is the farthest node from the existent complete clusters is picked as the seed for the next cluster (the first pick, when there are no other clusters, is
arbitrary). The new cluster is grown by adding to it one of the unassigned nearest neighbors x based on the scoring function: V(x) = #i - #e, where #i is the
number of intra-cluster links and #e is the number of extra-cluster links in the cluster resulting from adding node x to it. The neighbor x with max value of V(x)
score is then assigned to the cluster. The cluster growth stops when there are no more nodes or when the cluster target size is reached (whichever comes first).
When no more unassigned nodes are available the clustering layer is complete. The next layer clusters are constructed by using the previous lower layer clusters
as the input to this same algorithm.
[0066] In accordance with some embodiments of the invention, networks can be considered to include n "switches" (or nodes) of radix (number of ports per switch) Ri
for the i -th switch, where i = 1..n. The network thus has the total of P T = ∑ i R i ports. Some number of ports PI is used for internal connections between
switches ("topological ports") leaving P = P T - P I ports free ("external ports"), available for use by servers, routers, storage,... etc. The number of cables CI used
by the internal connections is CI = PI /2. For regular networks (graphs), those in which all nodes have the same number of topological links per node m (i.e. m is
a node degree), it follows that PI = n·m.

https://patents.google.com/patent/EP2708000B1/en?q=(radix-3)&oq=radix-3

6/51

2/14/26, 2:05 PM

EP2708000B1 - Flexible radix switching network - Google Patents

[0067] The network capacity or throughput is commonly characterized via the bisection (bandwidth) which is defined in the following manner: network is partitioned
into two equal subsets (equipartition) S1 + S2 so that each subset contains n/2 nodes (within ±1 for odd n ). The total number of links connecting S1 and S2 is
called a cut for partition S1+S2. Bisection B is defined as the smallest cut (min-cut) for all possible equipartitions S1+S2 of the network. Fig. 9 illustrates this
definition on an 8 node network with B=2.
[0068] Bisection is thus an absolute measure of the network bottleneck throughput. A related commonly used relative throughput measure is the network
oversubscription φ defined by considering the P/2 free ports in each min-cut half, S1 and S2, with each port sending and receiving at its maximum capacity
to/from the ports in the opposite half. The maximum traffic that can be sent in each direction this way without overloading the network is B link (port) capacities
since that's how many links the bisection has between the halves. Any additional demand that free ports are capable of generating is thus considered to be an
"oversubscription" of the network. Hence, the oversubscription φ is defined as the ratio:
ϕ ≡

𝑃/2
𝐵

[0069] The performance comparisons between network topologies, such as [1]-[5], [9]-[10], typically use non-oversubscribed networks (φ=1) and compare the costs in
terms of number of switches n of common radix R and number of internal cables CI used in order to obtain a given target number of free ports P. Via eq. (3.1),
that is equivalent to comparing the costs n and CI needed to obtain a common target bisection B.
[0070] Therefore, the fundamental underlying problem is how to maximize B given the number of switches n each using some number of topological ports per switch m
(node degree). This in turn breaks down into two sub-problems:
(i) Compute bisection B for given network
(ii) Modify/select links which maximize B computed via (i)
[0071] For general networks (graphs), both sub-problems are computationally intractable, i.e. NP-complete problems. For example, the 'easier' of the two tasks is (i),
since (ii) requires multiple evaluations of (i) as the algorithm (ii) iterates/searches for the optimum B. Task (i) involves finding the graph equipartition H0+H1
which has the minimum number of links between the two halves, in general case would have to examine every possible equipartition H0+H1 and in each case
count the links between the two, then pick the one with the lowest count. Since there are C(n, n/2) ≈ 2

𝑛

2

/ √πn / 2

ways to split the set of n nodes into two equal halves, the exact brute force solution has exponential complexity. The problem with approximate bisection
algorithms is the poor solution quality as network size increases - the polynomial complexity algorithms bisection applied to general graphs cannot guarantee to
find an approximate cut even to within merely a constant factor from the actual minimum cut as n increases. And without an accurate enough measure of
network throughput, the subtask (ii) cannot even begin to optimize the links.
[0072] An additional problem with (ii) becomes apparent, that even for small networks such as those with few dozen nodes, for which one can compute exact B via
brute force and also compute the optimum solution by examining all combinations of the links. Namely, a greedy approach for solving (ii), successively
computes B for all possible addition of the next link, then picks the link which produces the largest increment of B among all possible additions. That procedure
continues until the target number of links per node is reached. The numerical experiments on small networks show that in order to get the optimum network in
step m → m +1 links per node, one often needs to replace one or more existent links as well, the links which were required for optimum at previous smaller
values of m .
[0073] In addition to bandwidth optimization for a given number of switches and cables, the latency, average or maximum (diameter), is another property that is often a
target of optimization. Unlike the B optimization, where an optimum solution dramatically reduces network costs, yielding ∼2-5 fewer switches and cables
compared to conventional and approximate solutions, the improvements in latency are less sensitive to the distinction between the optimal and approximate
solutions, with typical advantage factors of only 1.2-1.5. Accordingly, greater optimization can be achieved in LH networks by optimizing the bisection than by
optimizing the network to improve latency.
[0074] The present invention is directed to Long Hop networks and methods of creating Long Hop networks. The description provides illustrative examples of methods
for constructing a Long Hop network in accordance with the invention. In accordance with one embodiment, one function of a Long Hop network is to create a
network interconnecting a number of computer hosts to transfer data between computer hosts connected to the network. In accordance some embodiments,
the data can be transferred simultaneously and with specified constraints on the rate of data transmission and the components (e.g., switches and switch
interconnect wiring) used to build the network.
[0075] In accordance with the invention, a Long Hop network includes any symmetrical network whose topography can be represented by a Cayley graph, and the
corresponding Cayley graphs have generators corresponding to the columns of Error Correcting Code (ECC) generator matrices G (or their isometric equivalents,
also instead of G one can use equivalent components of the parity check matrix H). In addition, the Long Hop networks in accordance with some embodiments
of the invention can have performance (bisection in units of n/2) within 90% of the lower bounds of the related ECC, as described by the Gilbert-Varshamov
bound theorem. In accordance with some embodiments of the invention, Long Hop networks will include networks having 128 or more switches (e.g., dimension
7 hypercube or greater) and/or direct networks. In accordance with some embodiments of the invention, Long Hop networks can include networks having the
number of interconnections m not equal to d, d+1,..d+d-1 and m not equal to n-1, n-2. In accordance with some embodiments of the invention, the wiring pattern
for connecting the switches of the network can be determined from a generator matrix that is produced from the error correcting code that corresponds to the
hypercube dimension and the number of required interconnections determined as function of the oversubscription ratio.
[0076] In other embodiments of the invention, similar methods can be used to create networks for interconnecting central processing units (CPUs) as is typically used
in supercomputers, as well as to interconnect data transfer channels within integrated circuits or within larger hardware systems such as backplanes and buses.
[0077] In accordance with some embodiments of the invention, the Long Hop network can include a plurality of network switches and a number of network cables
connecting ports on the network switches to ports on other network switches or to host computers.
[0078] Each cable connects either a host computer to a network switch or a network switch to another network switch. In accordance with some embodiments of the
invention, the data flow through a cable can be bidirectional, allowing data to be sent simultaneously in both directions. In accordance with some embodiments
of the invention, the rate of data transfer can be limited by the switch or host to which the cable is connected. In accordance with other embodiments of the
invention, the data flow through the cable can be uni-directional. In accordance with other embodiments of the invention, the rate of data transfer can be limited
only the physical capabilities of the physical cable media (e.g., the construction of the cable). In accordance with some embodiments, the cable can be any
medium capable of transferring data, including metal wires, fiber optic cable, and wired and wireless electromagnetic radiation (e.g., radio frequency signals and
light signals). In accordance with some embodiments, different types of cable can be used in the same Long Hop network.
[0079] In accordance with some embodiments of the invention, each switch has a number of ports and each port can be connected via a cable to another switch or to a
host. In accordance with some embodiments of the invention, at least some ports can be capable of sending and receiving data, and at least some ports can
have a maximum data rate (bits per second) that it can send or receive. Some switches can have ports that all have the same maximum data rate, and other
switches can have groups of ports with different data rates or different maximum data transfer rates for sending or receiving. In accordance with some
embodiments, all switches can have the same number of ports, and all ports can have the same send and receive maximum data transfer rate. In accordance

https://patents.google.com/patent/EP2708000B1/en?q=(radix-3)&oq=radix-3

7/51

2/14/26, 2:05 PM

EP2708000B1 - Flexible radix switching network - Google Patents

with other embodiments of the invention, at least some of the switches in a Long Hop network can have different numbers of ports, and at least some of the
ports can have different maximum data transfer rates.
[0080] The purpose of a switch is to receive data on one of its ports and to send that data on out another port based on the content of the packet header fields.
Switches can receive data and send data on all their ports simultaneously. A switch can be thought of as similar to a rail yard where incoming train cars on
multiple tracks can be sent onward on different tracks by using a series of devices that control which track among several options a car continues onto.
[0081] In accordance with some embodiments of the invention, the Long Hop network is constructed of switches and cables. Data transferred between a host
computer or a switch and another switch over a cable. The data received from a sending host computer enters a switch, which can then forward the data either
directly to a receiving host computer or to another switch which in turn decides whether to continue forwarding the data to another switch or directly to a host
computer connected to the switch. In accordance with some embodiments of the invention, all switches in the network can be both connected to other switches
and to hosts. In accordance with other embodiments of the invention, there can be interior switches that only send and receive to other switches and not to
hosts as well.
[0082] In accordance with some embodiments, the Long Hop network can include a plurality of host computers. A host computer can be any device that sends and/or
receives data to or from a Switch over a Cable. In accordance with some embodiments of the invention, host computers can be considered the source and/or
destination of the data transferred through the network, but not considered to be a direct part of the Long Hop network being constructed. In accordance with
some embodiments of the invention, host computers cannot send or receive data faster than the maximum data transfer rate of the Switch Port to which they
are connected.
[0083] In accordance with some embodiments of the invention, at least some of following factors can influence the construction of the network. The factors can
include 1) the number of Hosts that must be connected; 2) the number of switches available, 3) the number of ports on each switch; 4) the maximum data
transfer rate for switch ports; and 5) the sum total rate of simultaneous data transmission by all hosts. Other factors, such as the desired level of fault tolerance
and redundancy can also be factor in the construction of a Long Hop network.
[0084] In accordance with some embodiments of the invention, the desired characteristics of the Long Hop network can limit combinations of the above factors used in
the construction of a Long Hop network that can actually be built. For example, it is not possible to connect more hosts to a network than the total number of
switches multiplied by the number of ports per switch minus the number of ports used to interconnect switches. As one ordinary skill would appreciate, a
number of different approaches can be used to design a network depending on the desired outcome. For example, for a specified number of hosts, switches
with a given maximum data transfer rate, and ports per switch, how many switches are needed and how should they be connected in order to allow all hosts to
send and receive simultaneously at 50% of their maximum data transfer rate, Alternatively, for a specified number of hosts, number of switches with a given
number of ports and maximum data transfer rate, how much data can be simultaneously transferred across the network and what switch connection pattern(s)
supports that performance.
[0085] For purposes of illustration, the following description explains how to construct a Long Hop network according to some embodiments of the invention. In this
embodiment, the Long Hop network includes 16 switches and uses up to 7 ports per switch for network interconnections (between switches). As one of ordinary
skill will appreciate any number of switches can be selected and the number ports for network interconnection can be selected in accordance with the desired
parameters and performance of the Long Hop network.
[0086] In accordance with some embodiments of the invention, the method includes determining how to wire the switches (or change the wiring of an existing network
of switches) and the relationship between the number of attached servers per switch and the oversubscription ratio.
[0087] In accordance with some embodiments of the invention, the ports on each switch can be allocated to one of two purposes, external connections (e.g., for
connecting the network to external devices including host computers, servers and external routers or switches that serve as sources and destinations within the
network), and topological or internal connections. An external network connection is a connection between a switch and a source or destination device that
enables data to enter the network from a source or exit the network to a destination. A topological or internal network connection is a connection between
networks switches that form the network (e.g., that enables data to be transferred across network).
[0088] In accordance with some embodiments of the invention, the oversubscription ratio can be determined as the ratio between the total number of host connections
(or more generally, external ports) and the bisection (given as number of links crossing the min-cut partition). In accordance with some embodiments of the
invention, an oversubscription ratio of 1 indicates that in all cases, all hosts can simultaneously send at the maximum data transfer rate of the switch port. In
accordance with some embodiments of the invention, an oversubscription ratio of 2 indicates that the network can only support a sum total of all host traffic
equal to half of the maximum data transfer rate of all host switch ports. In accordance with some embodiments of the invention, an oversubscription ratio of 0.5
indicates that the network has twice the capacity required to support maximum host traffic, which provides a level of failure resilience such that if one or more
switches or connections between switches fails, the network will still be able to support the full traffic volume generated by hosts.
[0089] In accordance with some embodiments of the invention, the base network can be an n-dimensional hypercube. In accordance with other embodiments of the
invention, the base network can be another symmetrical network such as a star, a pancake and other Cayley graphs based network structure. In accordance with
some embodiments of the invention, an n-dimensional hypercube can be selected as a function of the desired number of switches and interconnect ports.
[0090] In accordance with some embodiments of the invention, a generator matrix is produce for the linear error correcting code that matches the underlying hypercube
dimension and the number of required interconnections between switches as determined by the network oversubscription ratio. In accordance with some
embodiments of the invention, the generator matrix can be produced by retrieving it from one of the publicly available lists, such as the one maintained by the
MinT project (http://mint.sbg.ac.at/index.php). In accordance with other embodiments of the invention, the generator matrix can be produced using a computer
algebra system such as the Magma package (available fromhttp://magma.maths.usyd.edu.au/magma/). For example, in Magma package a command entered
into Magma claculator (http://magma.maths.usyd.edu.au/calc/):
C:=BKLC(GF(2),7,4); C;
produces as output the generator matrix for the binary linear code [7,4,3]:
[7, 4, 3] Linear Code over GF(2)
Generator matrix:
1

0

0

0

0

1

1

0

1

0

0

1

0

1

0

0

1

0

1

1

0

0

0

0

1

1

1

1

[0091] In accordance with some embodiments of the invention, a linear error correcting code generator matrix can be converted into a wiring pattern matrix by rotating
the matrix counterclockwise 90 degrees, for example, as shown in Table 4.9.

https://patents.google.com/patent/EP2708000B1/en?q=(radix-3)&oq=radix-3

8/51

2/14/26, 2:05 PM

EP2708000B1 - Flexible radix switching network - Google Patents

[0092] In the illustrative example shown in Table 4.9, each switch has 7 ports connected to other switches and 16 total switches corresponding to an LH augmented
dimension 4 hypercube. Generators h1 through h7 correspond to the original columns from rotated [G4,7] matrix that can be used to determine how the switches
are connected to each other by cables.
[0093] In accordance with some embodiments of the invention, the 16 switches can be labeled with binary addresses, 0000, 0001, through 1111. The switches can be
connected to each other using the 7 ports assigned for this purpose, labeled h1 through h7, by performing the following procedure for each of the sixteen
switches. For example, connect a cable between each source switch network port (1 - 7) and the same port number on the destination switch whose number is
determined by performing an exclusive or logical operation between the source switch number and the value of the Cayley graph generator h1 to h7 (column 2 in
the table below) for the network port number.
[0094] For example, to determine how to connect the 7 wires going from switch number 3 (binary 0011), take the graph generator (number in 2nd column) and exclusive
or (XOR) it with 0011 (the source switch number), which results in "Destination switch number" in the following connection map (the XOR of columns 2 and 3
yields column 4):
Port on switch 3

Generators h1-h7

Source switch number

Destination switch number

1

0001 (1)

0011 (3)

0010 (2)

2

0010 (2)

0011 (3)

0001 (1)

3

0100 (4)

0011 (3)

0111 (7)

4

1000 (8)

0011 (3)

1011 (11)

5

0111 (7)

0011 (3)

0100 (4)

6

1110 (14)

0011 (3)

1101 (13)

7

1011 (11)

0011 (3)

1000 (8)

[0095] This wiring procedure describes how to place the connections to send from a source switch to a destination switch, so for each connection from a source
switch to a destination switch, there is also a connection from a destination switch to a source switch. As a practical matter, in this embodiment, a single bidirectional cable is used for each pair of connections.
Construction of Long Hop Networks
[0096] The LH networks are direct networks constructed using general Cayley graphs Cay(Gn, Sm) for the topology of the switching network. The preferred embodiment
for LH networks belongs to the most general hypercubic-like networks, with uniform number of external (E) and topological ( m ) ports per switch (where
E+m=R='switch radix'), which retain the vertex and edge symmetries of the regular d -cube

. The resulting LH network with n=2d switches in that case is a Cayley graph of type Cay𝑍 𝑑2 𝑆m

with n -1 > m > d +1 (these restiction on m exclude well known networks such as d-cube

which has m = d, folded d -cube

with m = d +1, as well as fully meshed network m=n and m=n-1). It will become evident that the construction method shown on 𝑍

𝑑
2

example applies directly to the general group 𝑍𝑞

𝑑

with q > 2. For q > 2, the resulting Cay𝑍𝑑𝑞 𝑆m

is the most general LH type construction of a d -dimensional hyper-torus-like or flattened butterfly-like networks of extent q (which is equivalent to a hyper-meshlike network with cyclic boundary conditions). The preferred embodiment will use q = 2, since 𝑍

𝑑
2

is the most optimal choice from practical perspective due to the shortest latency (average and max), highest symmetry, simplest forwarding and routing,
simplest job partitioning (e.g. for multi-processor clusters), easiest and most economical wiring in the 𝑍𝑞

𝑑

class.
[0097] Following the overall task breakdown in section 3, the LH construction proceeds in two main phases:
(i) Constructing a method for efficient computation of the exact bisection B
(ii) Computing the optimal set of m links (hops) S m per node maximizing this B
[0098] For the sake of clarity, the main phases are split further into smaller subtasks, each described in the sections that follow.
Generators and Adjacency Matrix
[0099] Network built on Cay𝑍𝑑𝑞 𝑆m

graph has n = q d vertices (syn. nodes), and for q = 2 which is the preferred embodiment, n = 2 d nodes. These n nodes make the n element vertex set V={v 0,v 2
.... v n-1}. We are using 0-based subscripts since we need to do modular arithmetic with them.
Node labels and group operation table
[0100] The nodes v i are labeled using d -tuples in alphabet of size q : v i ≡ i ∈ {0,1,... n -1} expressed as d -digit integers in base q . The group operation, denoted as ⊕, is

not the same as integer addition mod n but rather it is the component-wise addition modulo q done on d components separately. For q = 2, this is equivalent to a
bitwise XOR operation between the d -tuples, as illustrated in Table 2.1 (Appendix A) which shows the full 𝑍 𝑑2

group operation table for d = 4.

https://patents.google.com/patent/EP2708000B1/en?q=(radix-3)&oq=radix-3

9/51

2/14/26, 2:05 PM

EP2708000B1 - Flexible radix switching network - Google Patents

[0101] Table 4.1 illustrates analogous 𝑍3

𝑑

group operation table for d =2 and q =3, hence there are n =32=9 group elements and the operations table has n × n = 9×9 = 81 entries. The 2-digit entries have
digits which are from alphabet {0,1,2}. The n rows and n columns are labeled using 2-digit node labels. Table entry at row r and column c contains result of r⊕c

(component-wise addition mod q =3). For example, the 3rd row labeled 02, and the 6-th column labeled 12, yield table entry 02⊕12 = (0+1)%3, (2+2)%3 =1,1 = 11.

[0102] It can be noted in Table 4.1 for 𝑍3

𝑑

and in Table 2.1 (Appendix A) for 𝑍

𝑑
2

that each row r and column c contains all n group elements, but in a unique order. The 0-th row or 0-th column contain the unmodified r and c values since the
'identity element' is I 0=0. Both tables are symmetrical since the operation r ⊕ c = c ⊕ r is symmetrical (which is a characteristic of the abelian group 𝑍𝑑𝑞

used in the example).
Construction of adjacency matrix [A]
[0103] Generator set Sm contains m "hops" h 1 , h 2,... h m (they are also elements of the group Gn in Cay(Gn, Sm)), which can be viewed as the labels of the m nodes to
which the "root" node, v 0=0 is connected. Hence, the row r=0 of the adjacency matrix [A] has m ones, at columns A(0,h) for m hops h ∈ Sm and 0 elsewhere.
Similarly, the column c =0 has m ones at rows A(h,0) for m hops h ∈ Sm and 0 elsewhere. In a general case, some row r = y has m ones at columns A(y,y⊕h) for

h ∈ Sm and 0 elsewhere. Similarly a column c = x has m ones at rows A(x ⊕ h,x) for h ∈ Sm and 0 elsewhere. Denoting contributions of a single generator h ∈
Sm to the adjacency matrix [A] as a matrix T( h ), these conclusions can be written more compactly via Iverson brackets and bitwise OR operator '|' as:
𝑇𝑎𝑖, 𝑗 ≡ 𝑖 ⊕ 𝑎 = 𝑗|𝑗 ⊕ 𝑎 = 𝑖 𝑎 ∈ 𝐺𝑛

𝑚

𝐴 = ∑

𝑇ℎ = ∑

ℎ ∈ 𝑆

𝑇ℎ

𝑠 = 1

𝑚

𝑠

[0104] Note that eq. (4.1) defines T(a) for any element a (or vertex) of the group Gn . Since the right hand side expression in eq. (4.1) is symmetric in i and j it follows
that T(a) is a symmetric matrix, hence it has real, complete eigenbasis:
𝑇𝑎𝑖, 𝑗 = 𝑇𝑎𝑗, 𝑖

𝑑

[0105] For the group 𝐺n = 𝑍 ,
2

the group operator ⊕ becomes regular XOR '^' operation, simplifying eq. (4.1) to:
∧

𝑇𝑎𝑖, 𝑗 = 𝑖 𝑗 = 𝑎,

𝑎 ∈ 𝑍

𝑑
2

[0106] Table 4.2 illustrates the T(a) matrices for q =2, d =3, n =8 and all group elements a = 0..7. For given a =0..7, value 1 is placed on row r and column c iff r ^ c = a,
and 0 otherwise (0s are shown as '-').

https://patents.google.com/patent/EP2708000B1/en?q=(radix-3)&oq=radix-3

10/51

2/14/26, 2:05 PM

EP2708000B1 - Flexible radix switching network - Google Patents

[0107] Table 4.3 (a) shows the 8×8 adjacency matrix [A] obtained for the generator set S4 ≡ {1, 2, 4, 7}hex ≡ {001, 010, 100, 111}bin by adding the 4 generators from Table
4.2: [A] = T(1)+T(2)+T(4)+T(7), via eq. (4.2). For pattern clarity, values 0 are shown as '-'. Table 4.3 (b) shows the indices of the 4 generators (1, 2, 3, 4) which
contributed 1 to a given element of [A] in Table 4.3 (a).

[0108] Fig. 10 shows the resulting 8-node network (folded 3-cube,

). Actions (bitwise XOR) of the 4 generators T(a)∈(001, 010, 100, 111}bin on the node 000 are indicated by the arrows pointing to the target vertex. All other links
are shown without arrows. The total number of links is C= n·m /2=8·4/2=16, which can be observed directly in the figure.
Eigenvectors of T(a) and [A]
[0109] To solve the eigen-problem of [A], couple additional properties of T( a ) are derived from eq. (4.4) (using x^x=0 and x^y=^x):
𝑛−1

𝑛−1
∧

∧

𝑇𝑎𝑇𝑏𝑖, 𝑗 = ∑ 𝑇𝑎𝑖, 𝑘 𝑇𝑏𝑘, 𝑗 = ∑ 𝑘 = 𝑖 𝑎𝑘 = 𝑗 𝑏 =
𝑘 = 0

𝑘 = 0

∧

∧

∧

∧

∧

= 𝑖 𝑎 = 𝑗 𝑏 = 𝑖 𝑗 = 𝑎 𝑏 = 𝑇𝑎 𝑏𝑖, 𝑗 ⇒
∧

𝑇𝑎𝑇𝑏 = 𝑇𝑎 𝑏

∧

∧

𝑇𝑎𝑇𝑏 = 𝑇𝑎 𝑏 = 𝑇𝑏 𝑎 = 𝑇𝑏𝑇𝑎

[0110] Eq. (4.5) shows that T(a) matrices are a representation of the group G n and eq. (4.6) that they commute with each other. Since via eq. (4.2), [A] is the sum of
T(a) matrices, then [A] commutes with all T(a) matrices as well. Therefore, since they are all also symmetric matrices, the entire set {[A], T(a) ∀a} has a common
eigenbasis (via result (M 4 ) in section 2.F). The next sequence of equations shows that Walsh functions viewed as n -dimensional vectors |U k〉 are the
eigenvectors for T(a) matrices. Using eq. (4.4) for the matrix elements of the T( a ), the action of T( a ) on Walsh ket vector |U k〉 yields for the i -th component of
the resulting vector:
𝑛−1

𝑇𝑎|𝑈𝑘 〉

𝑛−1
∧

∧

= ∑ 𝑇𝑎𝑖, 𝑗 𝑈𝑘 𝑗 = ∑ 𝑗 = 𝑖 𝑎𝑈𝑘 𝑗 = 𝑈𝑘 𝑖 𝑎

𝑖

𝑗 = 0

𝑗 = 0

[0111] The result U k( i^a) is transformed via eq. (2.5) for the general function values of U k (x):
𝑑−1

∑

∧

𝑈𝑘 𝑖 𝑎 = −1

𝜇 = 0

𝑑−1

∑

= −1

𝜇 = 0

𝑑−1

∧

𝑘𝜇 𝑖 𝑎𝜇

∑

= −1

𝜇 = 0

𝑑−1

𝑘𝜇 𝑎𝜇 + ∑

𝜇 = 0

𝑘𝜇 𝑖𝜇

=

𝑑−1

𝑘𝜇 𝑎𝜇

∑

⋅ −1

𝜇 = 0

𝑘𝜇 𝑖𝜇

= 𝑈 𝑎𝑈 𝑖 = 𝑈 𝑎|𝑈
𝑘

𝑘

𝑘

𝑘

〉

𝑖

[0112] Collecting all n components of the left side of eq. (4.7) and right side of eq. (4.8) yields in vector form:
𝑇𝑎|𝑈𝑘 〉 = 𝑈𝑘 𝑎|𝑈𝑘 〉

https://patents.google.com/patent/EP2708000B1/en?q=(radix-3)&oq=radix-3

11/51

2/14/26, 2:05 PM

EP2708000B1 - Flexible radix switching network - Google Patents

[0113] Hence, the orthogonal basis set {|U k〉, k =0.. n -1} is the common eigenbasis for all T(a) matrices and for the adjacency matrix [A]. The n eigenvalues for T(a) are
Walsh function values U k(a), k =0..n -1. The eigenvalues for [A] are obtained by applying eq.(4.9) to the expansion of [A] via T(h), eq. (4.2):
𝑚

𝑚

𝐴|𝑈𝑘 〉 = ∑ 𝑇ℎ𝑠 |𝑈𝑘 〉 = ∑ 𝑈𝑘 ℎ𝑠 ⋅ |𝑈𝑘 〉 ≡ 𝜆𝑘 |𝑈𝑘 〉
𝑠 = 1

𝑠 = 1

𝑚

where:

𝜆

𝑘

≡ ∑ 𝑈𝑘 ℎ𝑠
𝑠 = 1

[0114] Since U 0 (x)=1 is constant (for x =0.. n -1), the eigenvalue λ 0 of [A] for the eigenvector |U 0〉 is:
𝜆

= 𝑚 ≥ 𝜆
0

𝑘

[0115] From eq. (4.11) it also follows that λ 0 ≥ λ k for k =1.. n -1 since the sum in eq. (4.11) may contain one or more negative addends U k( h s )=-1 for k >0, while for
the k=0 case all addends are equal to +1.
Computing Bisection
Cuts from adjacency matrix and partition vector
[0116] The bisection B is computed by finding the minimum cut C(X) in the set E={X} of all possible equipartitions X=S1+S2 of the set of n vertices. An equipartition X
can be represented by an n -dimensional vector

containing n /2 values +1 selecting nodes of group S1, and n /2 values -1 selecting the nodes of group S2. Since the cut value of a given equipartition X does not
depend on particular +1/-1 labeling convention (e.g. changing sign of all elements x i defines the same graph partition), all vectors |X〉 will have by convention the
1st component set to 1 and only the remaining n -1 components need to be varied (permuted) to obtain all possible distinct equipartitions from E. Hence, the
𝑛−1

equipartitions set E consists of all vectors X = (x 0, x1,... x µ-1), where x 0=1, x i∈{+1,-1 } and ∑

𝑖 = 0

𝑥𝑖 = 0 .

[0117] The cut value C(X) for a given partition X = (x 0, x 1,... x n-1) is obtained as the count of links which cross between nodes in S1 and S2. Such links can be easily
identified via E and adjacency matrix [A], since [A]i,j is 1 iff nodes i and j are connected and 0 if they are not connected. The group membership of some node i is
stored in the component x i of the partition X. Therefore, the links (i,j) that are counted have [A]i,j =1, i.e. nodes i and j must be connected, and they must be in
opposite partitions i.e. xi ≠ x j . Recalling that x i and x j have values +1 or -1, the " x i ≠ x j" is equivalent to "(x i·x j)=-1". To express that condition as a contribution
+1 when x i ≠ x j and a contribution 0 when x i = x j , expression (1- x i·x j )/2 is constructed which yields precisely the desired contributions +1 and 0 for any x i, x j
= ±1. Hence, the values added to the link count can be written as C i,j ≡(1- x i x j )·[A]i,j /2 since Ci,j =1 iff nodes i and j are connected ([A]i,j =1) and they are in
different groups (x i·x i =-1). Otherwise Ci,j is 0, thus adding no contribution to the C(X).
[0118] A counting detail that needs a bit of care arises when adding Ci,j terms for all i,j =0.. n -1. Namely, if the contribution of e.g. C 3,5 for nodes 3 and 5 is 1, because
[A] 3,5=1 (3,5 linked), x 3=-1 and x 5=+1, then the contribution of the same link will contribute also via C 5,3 term since [A] 5,3=1, x 5=+1, x 3=-1. Hence the sum of
Ci,j for all i,j =0.. n -1 counts the contribution for each link twice. Therefore, to compute the cut value C(X) for some partition X, the sum of Ci,j terms must be
𝑑−1

divided by 2. Noting also that for any vector 𝑋 ∈ 𝐸 ⇒ 〈 𝑋|𝑋 〉 = ∑

𝜇 = 0

𝑥𝑖 𝑥𝑖 = 𝑛

𝑛−1

and ∑

𝑖, 𝑗 = 0

𝐴

=
𝑖, 𝑗

𝑛−1

∑

𝑚 = 𝑛 ⋅ 𝑚,

𝑗 = 0

yields for the cut C(X):
𝑛−1

𝐶𝑋 =

1
2

∑
𝑖, 𝑗 = 0

1
2

𝑛−1

1 − 𝑥𝑖 𝑥𝑗 𝐴𝑖, 𝑗 =

nm
4

−

1
4

∑ 𝑥𝑖 𝑥𝑗 𝐴𝑖, 𝑗
𝑖, 𝑗 = 0

=

𝑛
4

𝑚−

〈 𝑋|𝐴|𝑋 〉
〈 𝑋 | 𝑋 〉

[0119] To illustrate operation of the formula (4.14), the Table 4.5 shows adjacency matrix [A] for Cay42 𝑆 ,
5

which reproduces

(folded 4-cube), with d =4, n =2d=24=16 nodes, m =5 links per node, produced by the generator set S5 ={1, 2, 4, 8, F}hex={0001, 0010, 0100, 1000, 1111}bin. The
row and column headers show the sign pattern of the example partition X=(1,1,1,1, -1,-1,-1,-1, 1,1,1,1, -1,-1,-1,-1) and the shaded areas indicate the blocks of [A] in
which eq. (4.14) counts ones - elements of [A] where row r and column c have opposite signs of the X components x r and x c . The cut is computed as C(X)=½

https://patents.google.com/patent/EP2708000B1/en?q=(radix-3)&oq=radix-3

12/51

2/14/26, 2:05 PM

EP2708000B1 - Flexible radix switching network - Google Patents

(sum of ones in shaded blocks) = 1/2∗(4∗8) = 16 which is the correct B for

Note that the zeros (they don't contribute to C(X)) in the matrix [A] are shown as '-' symbol.

Finding the minimum cut (bisection)
[0120] Bisection B is computed as the minimum cut C(X) for all X∈E, which via eq. (4.14) yields:
𝐵 = min

𝑛

𝑋 ∈ 𝐸4

〈 𝑋|𝐴|𝑋 〉

𝑚−

nm
=

𝑛
−

4

〈 𝑋 | 𝑋 〉

〈 𝑋|𝐴|𝑋 〉
max
4𝑋 ∈ 𝐸 〈 𝑋 | 𝑋 〉

nm
≡

4

−

𝑛
4

𝑀E

where:
𝑀𝐸 ≡ max

𝑋 ∈ 𝐸

〈 𝑋|𝐴|𝑋 〉
〈 𝑋 | 𝑋 〉

[0121] Despite the apparent similarity between the max{} term M E in eq. (4.16) to the max{} term M V in eq. (2.46), the Rayleigh-Ritz eqs. (2.45)-(2.46) do not directly
apply to min{} and max{} expressions in eq. (4.15). Namely, the latter extrema are constrained to the set E of equipartitions, which is a proper subset of the full
vector space

to which the Rayleigh-Ritz applies. The ME ≡ max{} in eq. (4.16) can be smaller than the M V ≡ max{} computed by eq. (2.46) since the result M

V can be a vector from

which doesn't belong to E (the set containing only the equipartition vectors X) i.e. if M V is solved only by some vectors Y which do not

consist of exactly n /2 elements +1 and n /2 elements -1.
[0122] As an illustration of the problem, M E is analogous to the "tallest programmer in the world" while M V is analogous to the "tallest person in the world." Since the
set of "all persons in the world" (analogous to

) includes as a proper subset the set of "all programmers in the world" (analogous to E) the tallest programmer

may be shorter than the tallest person (e.g. the latter might be a non-programmer). Hence in general case the relation between the two extrema is M E ≤ M V .
The equality holds only if at least one solution from M V belongs also to M E , or in the analogy, if at least one person among the "tallest person in the world" is
also a programmer. Otherwise, strict inequality holds M E < M V .
[0123]

In order to evaluate M E ≡ max{} in eq. (4.16), the n -dimensional vector space

(the space to which vectors |X〉 belong) is decomposed into a direct sum of

two mutually orthogonal subspaces:

[0124]

Subspace

is one dimensional space spanned by a single 'vector of all ones' 〈1| defined as:
〈 1| ≡ 1, 1, 1, … , 1

while

is the ( n -1) dimensional orthogonal complement of

within

i.e.

is spanned by some basis of n -1 vectors which are orthogonal to 〈1|. Using

the eq. (2.6) for Walsh function U 0 (x), it follows:
〈 1| ≡ 1, 1, 1, … , 1 = 〈 𝑈 |
0

[0125]

Hence,

is spanned by the remaining orthogonal set of n -1 Walsh functions |U k〉, k =1..n-1. For convenience the latter subset of Walsh functions is labeled as

set Φ below:
Φ ≡ |𝑈𝑘 〉 :

𝑘 = 1. .𝑛 − 1

[0126] Since all vectors X∈E contain n /2 components equal +1 and n/2 components equal -1, then via (4.18):
𝑛−1

〈 1 | 𝑋 〉 = ∑
𝑖 = 0

1 ⋅ 𝑥𝑖 = 0,

∀𝑋 ∈ 𝐸

i.e. 〈1| is orthogonal to all equipartion vectors X from E, hence the entire set E is a proper subset of

(which is the set of all vectors ∈

orthogonal to 〈1|).

Using M E in eq. (4.16) and eq. (2.46) results in:

https://patents.google.com/patent/EP2708000B1/en?q=(radix-3)&oq=radix-3

13/51

2/14/26, 2:05 PM

EP2708000B1 - Flexible radix switching network - Google Patents

[0127] The M V in eq. (4.22) is solved by an eigenvector |Y〉 of [A] for which [A]|Y〉=λmax|Y〉 since:
〈 𝑌|𝐴|𝑌 〉
〈 𝑌 | 𝑌 〉

[0128]

𝜆

〈 𝑌|𝜆max |𝑌 〉

=

=

〈 𝑌 | 𝑌 〉

max

〈 𝑌 | 𝑌 〉

〈 𝑌 | 𝑌 〉

= 𝜆max

Recalling, via eq. (4.10), that the eigenbasis of the adjacency matrix [A] in eq. (4.22) is the set of Walsh functions |U k〉, and that

in which the M V=max{} is

searched for, is spanned by the n -1 Walsh functions |U k〉 ∈ Φ, it follows that the eigenvector |Y〉 of [A] in eq. (4.23) can be selected to be one of these n -1 Walsh
functions from Φ (since they form a complete eigenbasis of [A] in

) i.e.:

|𝑌 〉 ∈ 𝛷 ≡ |𝑈𝑘 〉 :

𝑘 = 1. .𝑛 − 1

[0129] The equality in (4.22) holds iff at least one solution

is also a vector from the set E. In terms of the earlier analogy, this can be stated as: in the statement "the tallest student" ≤ "the tallest person", the equality holds
iff at least one among the "tallest person" happens to be a "programmer."
[0130] Since |Y〉 is one of the Walsh functions from Φ and since all |U k〉 ∈ Φ have, via eqs. (2.5) and (2.7), exactly n /2 components equal +1 and n /2 components
equal -1, |Y〉 belongs to the set E. Hence the exact solution for M E in eq. (4.22) is the Walsh functions |U k〉 ∈ Φ with the largest eigenvalue λ k. Returning to the
original bisection eq. (4.15), where M E is the second term, it follows that B is solved exactly by this same solution |Y〉=|U k〉 ∈ Φ. Combining thus eq. (4.15) with
equality case for M E in eq. (4.22) yields:
nm
𝐵 =

4

𝑛

−

4

𝑀E =

𝑛
4

𝑚−𝜆

max

=

𝑛
4

𝑚 − max 𝜆
𝑘 ∈ 1𝑛

𝑘

[0131] Therefore, the computation of B is reduced to evaluating n -1 eigenvalues λ k of [A] for k=1.. n -1 and finding a t ≡ ( k with the largest λ k) i.e. a t such that λ t ≥ λ k
for k =1.. n -1. The corresponding Walsh function Ut provides the equipartition which achieves this bisection B (the exact minimum cut). The evaluation of λ k in
eq. (4.25) can be written in terms of the m generators h s ∈ Sm via eq. (4.11) as:
𝐵 =

𝑛
4

𝑚

𝑚 − max ∑ 𝑈𝑘 ℎ𝑠
𝑘 ∈ 1𝑛

𝑠 = 1

[0132] Although the function values Uk(x) above can be computed via eq. (2.5) as 𝑈𝑘 𝑥 = −1

ℙ𝑘&𝑥

,

due to parallelism of binary operation on a regular CPU, it is computationally more efficient to use binary form of Walsh functions, W k(x). The binary ↔ algebraic
translations in eqs. (2.8) can be rewritten in vector form for U k and W k , with aid of definition of |1〉 from eq. (4.18), as:
|𝑊𝑘 〉 ≡

1
2

|1 〉 − |𝑈𝑘 〉

|𝑈𝑘 〉 = |1 〉 − 2 ⋅ |𝑊𝑘 〉

[0133] Hence, the B formula (4.26) can be written in terms of W k via eq. (4.28) and Wk formula eq. (2.10) as:
𝑚

𝐵 =

𝑛
4

𝑚

𝑚 − max ∑ 1 − 2 ⋅ 𝑊 ℎ
𝑘 ∈ 1𝑛

𝑘

𝑠

=

𝑛
4

𝑚 − max 𝑚 − 2 ∑ 𝑊 ℎ

𝑠 = 1

𝑘 ∈ 1𝑛

𝑘

𝑠

𝑠 = 1

⇒

𝑛
⇒

𝐵 =

𝑚

𝑚

𝑠 = 1

𝑠 = 1

𝑛
min ∑ 𝑊𝑘 ℎ𝑠 =
min ∑ ℙ𝑘&ℎ𝑠
4 𝑘 ∈ 1𝑛
2 𝑘 ∈ 1𝑛

[0134] The final expression in (4.29) is particularly convenient since for each k =1.. n -1 it merely adds parities of the bitwise AND terms: ( k&hs ) for all m Cayley graph
generators h s ∈ S m. The parity function

in eq. (4.29) can be computed efficiently via a short C function ([14] p. 42) as follows:

//-- Parity for 32-bit integers
inline int Parity(unsigned int x)
(4.30)
{
x^=x>>16, x^=x>>8, x^=x>>4, x^=x>>2;
return (x^(x>>1))&1;
}
5] Using a

https://patents.google.com/patent/EP2708000B1/en?q=(radix-3)&oq=radix-3

14/51

2/14/26, 2:05 PM

EP2708000B1 - Flexible radix switching network - Google Patents

implementation Parity(x), the entire computation of B via eq. (4.29) can be done by a small C function Bisection(n,hops[],m) as shown in code (4.31).
int Bisectionint n, int ∗ ha, int m
{

6] The inner loop in (4.31) executes m times and the outer loop ( n -1) times, yielding total of ∼ m·n steps. Hence, for n -1 values of k, the total computational complexity of
B is ∼ O( m·n 2).
"Symmetry Optimization" of B computation
7] A significant further speedup can be obtained by taking full advantage of the symmetries of Walsh functions W k particularly evident in the recursive definition of
Hadamard matrix H n in eq. (2.1). The corresponding recursion for the binary Walsh matrix [W n] can be written as:
𝑊

=
2

0

0

0

1

𝑊
𝑊

=

𝑛

𝑊𝑛

2⁢
𝑛

𝑊

𝑛

‾

𝑊𝑛

where [ W n] denotes bitwise complement of matrix [W n]. For example, in the upper half of [W 2n] the left and right sub-matrices [W n] are the same, suggesting that after
computing in eq. (4.29) the partial sums of W k( h s) over h s< n and k < n (upper left quadrant of W 2n) the remaining n partial sums for k ≥ n (top right quadrant of W 2n)
can be copied from the computed left half. Similarly, in the lower half of [W 2n ] the left and right quadrants of sub-matrices are a complement of each other, which
replaces the above copying method with subtractions from some constant and copying (the constant is the number of hops h s ≥ n, i.e. the h s in the lower half of W 2n
matrix). The net result of these two computational short-circuits is a reduction of original computation in half. Since computation inside the halves W n are of the same
type as the as those just described for W 2n , applying the same symmetry method recursively log( n ) times to the halved matrices being generated in each stage, the net
complexity of the computation of B is reduced from the earlier O( m·n 2) to O( m·n· log( n )) i.e. the gain is a speedup factor of n /log( n ) over the original method of eq.
(4.29).
"Fast Walsh Transform Optimization" of B computation
8] Analogue of the above 'halving' optimization of B computation can be formulated for the algebraic form of Walsh functions U k by defining a function f ( x ) for x =0,1,... n
-1 as:
𝑓𝑥 ≡ 𝑥 ∈ 𝑆𝑚 = {

1

if 𝑥 ∈ 𝑆

0

if 𝑥 ∉ 𝑆𝑚

𝑚

where and 0 ≤ x < n and S m={ h 1, h 2,... h m } is the set of m graph generators. Hence, f ( x ) is 1 when x is equal to one of the generators h s ∈ S m and 0 elsewhere. This
function can be viewed as a vector | f 〉, with components f i= f ( i ). Recalling the computation of adjacency matrix [A] via eq. (4.2), vector | f 〉 can also be recognized as
the 0-th column of [A] i.e. f i = [A]0,i. With this notation, the eq. (4.26) for B becomes:
𝐵 =

𝑛
4

𝑚

𝑚 − max ∑ 𝑈𝑘 ℎ𝑠 =
𝑘 ∈ 1𝑛

𝑠 = 1

𝑛
4

𝑚 − max 〈 𝑈𝑘 | 𝑓 〉 ≡
𝑘 ∈ 1𝑛

𝑛
4

𝑚 − max 𝐹𝑘
𝑘 ∈ 1𝑛

𝐹𝑘 ≡ 〈 𝑈 𝑘 | 𝑓 〉

9] Therefore, the B computation consists of finding the largest element in the set { F k} of n -1 elements. Using the orthogonality and completeness of the n vectors |U k〉, 〈U j
|U k 〉 = n · δj,k from eq. (2.3), important property of the set (Fk } follows:
𝑛−1

∑
𝑘 = 0

1
𝑛

𝑛−1

𝐹𝑘 |𝑈𝑘 〉 = ∑
𝑘 = 0

1
𝑛

|𝑈𝑘 〉 〈 𝑈𝑘 | 𝑓 〉 =

1
𝑛

𝑛−1

∑ |𝑈𝑘 〉 〈 𝑈𝑘 ||𝑓 〉 = 𝐼𝑛 |𝑓 〉 = |𝑓 〉
𝑘 = 0

40] The eqs. (4.35),(4.36) can be recognized as the Walsh transform ([14] chap. 23) of function f( x ), with n coefficients F k/ n as the transform coefficients. Hence,
evaluation of all n coefficients F k, which in direct (4.35) computation requires O( n 2 ) steps, can be done via Fast Walsh Transform (FWT) in O( n·log( n )). Note that FWT
will produce n coefficients F k, including F 0, even though we don't need F0 i.e. according to eq. (4.34), we still need to look for the max {} in the set {F 1 , F 2 ,... F n-1}. Since

https://patents.google.com/patent/EP2708000B1/en?q=(radix-3)&oq=radix-3

15/51

2/14/26, 2:05 PM

EP2708000B1 - Flexible radix switching network - Google Patents

each step involves adding of m points, the net complexity of the B computation via (4.34) using FWT is O( m·n· log( n )), which is the same as the "symmetry optimization"
result in the previous section.
41] Although both methods above achieve a speedup by a factor n /log( n ) over the direct use of eqs. (4.26) and (4.29), the far greater saving has already occurred in the
original eq. (4.26). Namely, the eq. (4.26) computes B by computing only the n -1 cuts for equipartitions U k ∈ Ω, instead of computing the cuts for all equipartitions in the
set E of all possible equipartitions. The size of the full set E of "all possible equipartitions" is (factor ½ is due to convention that all partitions in E have +1 as the 1st
component):
𝐸 =

𝑛−1

𝑛

1

2 𝑛/2

≈

2
2

√𝜋 ⋅ 𝑛 / 2

42] To appreciate the savings by eq. (4.26) alone, consider a very small network of merely n =32 nodes. To obtain the exact B for this network the LH method needs to
compute n -1= 31 cuts, while the exact enumeration would need to compute |E|=0.5·C(32,16) = 300,540,195 cuts i.e. 9,694,845 times greater number of cuts. Further, this
ratio via eq. (2.37) grows exponentially in the size of the network n , nearly doubling for each new node added.
Optimizing Bisection
43] With the couple O( m·n· log( n )) complexity methods for computation of bisection B for a given set of generators S m described in the previous sections, the next task
identified is the optimization of the generator set Sm = { h 1, h 2, ...h m} i.e. the finding of the S m with the largest B. The individual hops h s are constrained to n -1 values:
1,2,... n -1 (0 is eliminated since no node is connected to itself), i.e. S m is an m element subset of the integer sequence 1.. n -1. For convenience, this set of all m- subsets
of integer sequence 1..n-1 is labeled as follows:
Ω𝑛𝑚 ≡ Ω 𝑆
𝑛

𝑚

:𝑆

𝑚

= ℎ ,ℎ , … ,ℎ
1

Ω ≡ Ω𝑛𝑚 =

2

𝑛−1

𝑚

and 0 < ℎ

= 0𝑛

𝑚

𝑠

< 𝑛

𝑚

44] With this notation and using the binary formula for B, eq. (4.29), the B optimization task is:
𝑚

𝑏 ≡

𝐵
max min ∑ 𝑊𝑘 ℎ𝑠
𝑛 / 2 𝑆𝑚 ∈ Ω𝑛 𝑘 ∈ 1𝑛
𝑠 = 1

45] For convenience, eq. (4.42) also defines a quantity b which is the bisection in units n /2. The worst case computational complexity the B optimization is thus O(( m·n· log(

n ))m), which is polynomial in n, hence, at least in principle, it is a computationally tractable problem as n increases. (The actual exponent m would be ( m - log( n ) - 1), not
m, since the Cayley graphs are highly symmetrical and one would not have to search over the symmetrically equivalent subsets S m . Note that m is typically a hardware
characteristics of the network components, such as switches, which usually don't get replaced often as network size n increases.
46] Since for large enough n , even a low power polynomial can render 'an in principle tractable' problem practically intractable, approximate methods for the max{} part of the
computation (4.42) would be used in practice. Particularly attractive for this purpose would be genetic algorithms and simulated annealing techniques used in [12] (albeit
for the task of computing B, which the methods of this invention solve efficiently and exactly). Some of the earlier implementations of this inventions have used fast
greedy algorithms, which work fairly well. The 'preferred embodiment' for the invention which is described next does not perform any such direct optimization of eq.
(4.42), but uses a more effective method instead.
Bisection B optimization via EC Codes
47] In order to describe this method, the inner-most term within the nested max{min{}} expression in the eq. (4.42) is identified and examined in more detail. For convenience,
this term, which has a meaning of a cut for a partition defined via the pattern of ones in the Walsh function Wk(x), is labeled as:
𝑚

𝑚

𝐶𝑘 ≡ ∑ 𝑊𝑘 ℎ𝑠 = ∑ ℙ𝑘&ℎ𝑠
𝑠 = 1

𝑠 = 1

48] Eq. (4.43) also expresses Wk(x) in terms of parity function

via eq. (2.10). The function

for some d -bit integer x =( x d-1... x 1 x 0)binary is defined as:
𝑑−1

ℙ𝑥 ≡

∑ 𝑥𝜇 mod 2 = 𝑥0

∧

𝑥1

∧

⋯

∧

𝑥𝑑 − 1

𝜇 = 0

49] The last expression in eq. (4.44) shows that ℙ𝑥 ≡ ℙ𝑥d − 1 … 𝑥1 𝑥0

is a "linear combination" in terms of the selected field GF(2)d, of the field elements provided in the argument. The eq. (4.43) contains a modified argument of type ℙ𝑘&ℎ,

for h ∈ S m, which can be reinterpreted as: the 'ones' from the integer k are selecting a subset of bits from the d -bit integer h, then

performs the linear combination of the selected subset of bits of h. For example, if k =11dec=1011bin than the action of 𝑊1011 ℎ ≡ ℙ1011&ℎ

https://patents.google.com/patent/EP2708000B1/en?q=(radix-3)&oq=radix-3

16/51

2/14/26, 2:05 PM

EP2708000B1 - Flexible radix switching network - Google Patents

is to compute linear combination of the bits bit-0,1 and 3 of h (bit numbering is zero based, from low/right to high/left significance). Since eq. (4.43) performs the above
"linear combination via ones in k " action of Wk on a series of d -bit integers h s, s =1..m , the Wk "action" on such series of integers is interpreted as the parallel linear
combination on the bit-columns of the list of h s as shown in the Table 4.6, for k =1011 and W1011 acting on a set of generators S 5={ 0001, 0010, 0100, 1101}. The 3 bitcolumns V 3, V 1 and V 0 selected by ones in k are combined via XOR into the resulting bit-column V: |V 3〉⊕|V 1〉⊕|V 0〉=|V).

0] Therefore, the action of a Wk on the generator set Sm ={ h 1, h 2, ...h m} can be seen as a "linear combination" of the length- m columns of digits (columns selected by
ones in k from Wk ) formed by the m generators h s. If instead of the 𝑍

𝑑
2

used in the example of Table 4.6, there was a more general Cayley graph group, such as 𝑍𝑞 ,
𝑑

Figure imgb0101
instead of the bit-columns there would have been length- m columns made of digits in alphabet of size q (i.e. integers 0.. q -1) and the XOR would have been replaced
with the appropriate GF( q ) field arithmetic e.g. addition modulo q on m- tuples for 𝑍𝑞

𝑑

as illustrated in an earlier example in Table 4.1. The construction of column vectors |V µ〉 of Table 4.6 can be expressed more precisely via an m × d matrix [R m,d] defined
as:
ℎ

〈ℎ |

𝑚

𝑅𝑚, 𝑑 ≡ ∑ |𝑒𝑠 〉 〈 ℎ𝑠 | =
𝑠 = 1

1

〈 ℎ

𝑚

|

ℎ

1, 𝑑 − 1

=

⋯

1, 𝑑 − 2

⋯
ℎ

𝑚, 𝑑 − 1

⋯
ℎ

⋯
⋯

𝑚, 𝑑 − 2

⋯

ℎ

1,0

⋯
ℎ

≡

𝑚, 0

≡ |𝑉𝑑 − 1 〉 , |𝑉𝑑 − 2 〉 , … |𝑉 〉
0

where:
|𝑉𝜇 〉

𝑠

≡ ℎ𝑠, 𝜇 = 〈 ℎ𝑠 |

𝜇

for 𝜇 = 0 . . 𝑑 − 1,

s = 1. .𝑚

1] Hence the m rows of matrix [R m,d] are m generators 〈 h s| ∈ S m and its d columns are d column vectors |V µ〉. The above 'linear combination of columns via ones in k '
becomes in this notation:
𝑑−1

|𝑉𝑘 〉 ≡

𝑑−1

∑ 𝑘𝑢 |𝑉𝜇 〉

where 𝑘 ≡ ∑

𝜇 = 0

𝜇

𝑘 2

𝜇 = 0

where the linear combination of k µ|V µ〉 is performed in GF( q ) i.e. mod q on each component of m -tuples k µ|V µ〉. The sum computing the cut Ck in eq. (4.43) is then
simply adding (without mod q ) all components of the vector |V(k)〉 from eq. (4.47). Recalling the definition of Hamming weight as the number of non-zero digits, this cut
Ck is recognizable as the Hamming weight of the vector |V(k)〉:
𝐶𝑘 = 〈 𝑉𝑘 〉

2] The next step is to propagate the new "linear combination" interpretation of Wk action back one more level, to the original optimization problem in eq. (4.42), in which the
cut Ck was only the innermost term. The min{} block of eq. (4.42), seeks a minimum value of Ck for all k =1..n -1. The set of n vectors |V(k)〉 obtained via eq. (4.47) when k
runs through all possible integers 0.. n -1 is a d- dimensional vector space, a linear span (subspace of m -tuples vector space

), which is denoted as

:

3] Therefore, the min{} level optimization in eq. (4.42) computing bisection b, seeks a non-zero vector |V(k)〉 from the linear span

with the smallest Hamming weight 〈V(k)〉:

https://patents.google.com/patent/EP2708000B1/en?q=(radix-3)&oq=radix-3

17/51

2/14/26, 2:05 PM

EP2708000B1 - Flexible radix switching network - Google Patents

4] While Hamming weight can be used in some embodiments of the invenition, any other weight, such as Lee weight, which would correspond to other Cayley graph groups
Gn and generator sets Sm , can also be used.
5] But b in eq. (4.50) is precisely the definition eq. (2.25) of the minimum weight w min in the codeword space (linear span)

of non-zero codewords Y. Note: In order to avoid the mix up in the notation between the two fields, the overlapping symbols [ n, k ] which have a different meaning in ECC,
will in this section have an underscore prefix, i.e. the linear code [ n, k ] is relabeled as [_n, _k ].
6] The mapping between the ECC quantities and LH quantities is then: w min ⇔ b, _k ⇔ d, _n ⇔ m, _k vectors 〈 g i| spanning linear space

of _n -tuples and constructing code generator matrix [G] (eq. (2.20)) ⇔ d columns |Vµ 〉 for µ=0.. d -1 spanning linear space

of m -tuples (digit-columns in the generator list). Since, via eq. (2.26) the minimum weight of the code w min is same as the minimum distance Δ between the codewords
Y, it follows that the bisection b is also the same quantity as the ECC Δ (even numerically). Table 4.7 lists some of the elements of this mapping. Table 4.7
Linear EC codes

_k

_n

Δ

_k rows of _ n -tuples 〈 g i |

LH Networks

d

m

b

d columns of m -tuples |V µ〉

q for GF( q )
q for 𝑍𝑞

𝑑

7] The optimization of linear code [_ n , _ k , Δ] that maximizes Δ is thus the same optimization as the outermost level of the LH optimization, max {} level in eq. (4.42) that
seeks the Cayley graph generator set Sm with the largest bisection b - other than difference in labeling conventions, both optimizations seek the d- dimensional subspace

of some vectors space

which maximizes the minimum non-zero weight w min⇔b of the subspace

. The two problems are mathematically one and the same.

8] Therefore, the vast numbers of good/optimal linear ECC codes computed over the last six decades (such as EC code tables [17] and [22]) are immediately available as
good/optimal solutions for the b optimization problem of the LH networks, such as eq. (4.42) for Cayley graph group 𝐺n = 𝑍𝑞 .
𝑑

Similarly any techniques, algorithms and computer programs (e.g. MAGMA ECC module
http://magma.maths.usyd.edu.au/magma/handbook/linear_codes_over_finite_fields) used for constructing and combining of good/optimum linear EC codes, such as
quadratic residue codes, Goppa, Justesen, BCH, cyclic codes, Reed-Muller codes,... [15],[16], via translation Table 4.7, automatically become techniques and algorithms
for constructing good/optimum LH networks.
9] As an illustration of the above translation procedure, a simple parity check EC code [4,3,1]2 with generator matrix [G 3,4] is shown in Table 4.8. The codeword has 1 parity
bit followed by 3 message bits and is capable of detecting all single bit errors. The translation to the optimum network shown on the right, is obtained by rotating 90°
counter-clockwise

the 3×4 generator matrix [G3,4]. The obtained block of 4 rows with 3 bits per row is interpreted as 4 generators h s, each 3 bits wide, for the Cay32 𝐶

4

graph. The resulting network thus has d =3, n =23=8 nodes and m =4 links/node. The actual network is a folded 3-cube shown within an earlier example in Table 4.4. Its
bisection is: b=2 and B=b·n /2=8 links.

0] A slightly larger and denser network using EC code [7,4,3]2 from Table 2.4 (Appendix A), is converted into an optimum solution, a graph Cay𝑍 42 𝐶 ,
7

with d =4, n =16 nodes and m =7 link/node as shown in Table 4.9.

1] The 4 row, 7 column generator matrix [G 4,7] of the linear EC code [7,4,3]2 on the left side was rotated 90° counter-clockwise and the resulting 7 rows of 4 digits are binary
values for the 7 generators h s (also shown in hex) of the 16 node Cayley graph. The resulting n =16 node network has relative bisection (in n /2 units) b=Δ=3 and
absolute bisection (in # of links) of: B = b·n /2 = 3·16/2 = 24 links. Since the network is a non-planar 4-dimensional cube with total n·m /2=16·7/2=56 links it is not drawn.
2] The above examples are captured by the following simple, direct translation recipe:
EC code_𝑛, _𝑘, Δ

𝑑

q

→ LH Cay𝑍 𝑆
𝑞

m

(i) Take EC code generator matrix [G _k,_n] and rotate it 90° (in either direction - direction of rotation merely selects order of generators in the list, which is an
arbitrary convention.)
(ii) The result is m =_n row by d =_k column matrix [R m,d] of GF( q )-digits 0.. q -1

https://patents.google.com/patent/EP2708000B1/en?q=(radix-3)&oq=radix-3

18/51

2/14/26, 2:05 PM

EP2708000B1 - Flexible radix switching network - Google Patents

(iii) Read m rows of d -tuples in base q from [R m,d] as m generators ℎs ∈ Sm ⊂ 𝑍𝑞

𝑑

(iv) Compute Cayley graph LH = Cay𝑍𝑞 𝑆m
𝑑

from the obtained generators Sm ={ h 1, h 2,.. h m}
(v) LH: n = q d nodes, m links/node, bisection: relative b=Δ, absolute B=Δ· n /2 links.
3] The methods of determining the bisection B can be implemented using a computer program or set of computer programs organized to perform the various steps
described herein. The computer can include one or more processors and associate member, including volatile and non-volatile memory to store the programs and data.
For example, a conventional IBM compatible computer running the Windows or Linix operating system or an Apple computer system can be used and the programs can
be written, for example, the in C programming language.
Implementation Notes
N-1. Equivalent LH networks
4] Order of elements in a generator set Sm = { h 1, h 2,... h m } is clearly a matter of convention and network performance characteristics don't depend on particular ordering.
Similarly, the subspace

of the column vectors can be generated using any linearly independent set of d vectors from

instead of the original subset {Vµ}. All these transformation of a given network yield equivalent networks, differing only in labeling convention but all with the same
distribution of cuts (including min-cut and max-cut) and the same network paths distribution (e.g. same average and max paths). This equivalence is used to compute
specific generators optimized for some other objective, beyond the cuts and paths. Some of these other objectives are listed in the notes below.
N-2. Minimum change network expansion
5] During expansion of the network, it is useful that the next larger network is produced with the minimum change from the previous configuration e.g. requiring the fewest
cables to be reconnected to other switches or ports. The equivalence transforms of N-1 are used to "morph" the two configuration, initial and final toward each other,
using the number of different links in Sm as the cost function being minimized. Techniques and algorithms of "Compressive Sensing" [CS] (see [20]) are particularly
useful as the source for the efficient "morphing" algorithms.
N-3. Diagonalization
6] It is often useful, especially in physical wiring, discovery and routing, to have a 𝑍𝑞

𝑑

based network in which (usually first) d hops from Sm are powers of q. This property of generator set Sm corresponds to systematic generator matrix [G _k,_n ] for linear
codes and can be recognized by the presence of identity matrix Id within [G _k,_n] (possibly with permuted columns). The two previous examples, Tables 4.8 and 4.9 were
of this type (the digits of Id sub-matrix were in bold).
7] A simple, efficient method for computing a "systematic generator" from non-systematic one is to select for each column c = 0.. d -1 a row r ( c )=1..m that contains a digit
1 in column c . If row r (c) doesn't contain any other ones, then we have one column with desired property (the h r(c) is a power of 2). If there are any other columns, such
as c' which contain ones in row r ( c ), the column Vc is XOR-ed into these columns Vc' , clearing the excessive ones in r ( c ). Finally, when there is a single 1 in row r ( c )
and column c , the hop h r(c) is swapped with hop h c+1 so that the resulting matrix contains generator h c+1=2c. The process is repeated for the remaining columns c < d.
8] The number of XOR operations between columns needed to reduce some row r ( c ) to a single 1 in column c , is 〈 h r(c)〉-1. Therefore, to reduce number of required XOR-s
(columns are m bits long which can be much larger than the machine word), for each new c to diagonalize, algorithm picks the row which has the smallest weight, min{〈 h
r(c)〉}.

N-4. Digital or (t,m,s) nets (or designs, orthogonal arrays)
9] This research field closely related to design of optimal linear codes [_ n ,_ k ,Δ]q (cf. [21],[22]). The basic problem in the field of 'digital nets' is to find distribution of points
on s-dimensional hypercubic (fish-) net with "binary intervals" layout of 'net eyes' (or generally analogous b-ary intervals via powers of any base b, not only for b=2) which
places the same number of points into each net eye. There is a mapping between (t,_m,s) b digital nets and [_ n ,_ k ]q codes via identities: _n =s, _k =s-_m, q =b. A large
database of optimal (t,_m,s) nets, which includes linear code translations is available via a web site [22]. Therefore, the solutions, algorithms and computer programs for
constructing good/optimal (t,_m,s) nets are immediately portable to construction of good/optimal LH networks via this mapping followed by the [_n ,_k ]q → LH mapping
in Table 4.7.
N-5. Non-binary codes
0] The linear codes with q >2 generate hyper-torus/-mesh type of networks of extent q when the Δ metrics of the code is Lee distance. When Hamming distance is used for

q >2 codes, the networks are of generalized hypercube/flattened butterfly type [1]. For q =2, which is the binary code, the two types of distance metrics are one and the
same.
N-6. Non-binary Walsh functions
1] Walsh functions readily generalize to other groups, besides cyclic group 𝑍 𝑑2

used here (cf. [23]). A simple generalization to base q >2 for groups 𝑍𝑞 ,
𝑑

for any integer q is based on defining function values via q -th primitive root of unity ω:
𝑑−1

∑

𝑈𝑞, 𝑘 𝑥 = 𝜔

𝜇 = 0

𝑘𝜇 𝑥𝜇

for 𝑥, 𝑘 < 𝑛 ≡ 𝑞

𝑑

where:
𝜔 ≡ 𝑒

2⁢
πi / 𝑞

2] For q =2, eq. (4.51) yields ω=(-1), which reduces Uq,k(x) from eq. (4.50) to the regular Walsh functions Uk(x), eq. (2.5). The q discrete values of Uq,k(x) can be also
mapped into integers in [0, q ) interval to obtain integer-valued Walsh functions Wq,k(x) (analogue of binary form Wk(x)), which is useful for efficient computer

https://patents.google.com/patent/EP2708000B1/en?q=(radix-3)&oq=radix-3

19/51

2/14/26, 2:05 PM

EP2708000B1 - Flexible radix switching network - Google Patents

implementation, via analogous mapping to the binary case e.g. via mapping a = w b for integer b =0..n -1, where b :integer, a :algebraic value, as in eq. (2.8) where this
same mapping (expressed differently) was used for q =2.
3] The non-binary Walsh functions U q,k can also be used to define graph partition into f parts where f is any divisor of q (including q ). For even q, this allows for efficient
computation of bisection. The method is a direct generalization of the binary case: the q distinct function values of U q,k (x) define partitions arrays X k[x]≡ U q,k( x )
containing n = q d elements indexed by x =0.. n -1. Each of q values of U q,k (x) indicates a node x belongs to one of the q parts. The partitions Xk for k =1... n -1 are
examined and cuts computed using the adjacency matrix [A] for Cay𝑍𝑑𝑞 𝑆m

graph, as in eq. (4.14) for q =2. The generators T(a) and adjacency matrix [A] are computed via general eqs. (4.1),(4.2), where ⊕ operator is GF( q ) addition (mod q ).
4] The algorithmic speed optimizations via "symmetry optimization" and "Fast Walsh Transform optimization" apply here as well (see [14] pp. 465-468 on fast transforms for
multi-valued Walsh functions).
N-7. Secondary Optimizations
5] Once the optimum solution for (4.42) is obtained (via ECC, Digital nets, or via direct optimization), secondary optimizations, such as seeking the minimum diameter (max
distance) or minimum average distance or largest max-cut, can be performed on the solution via local, greedy algorithms. Such algorithms were used in construction of
our data solutions data base, where each set of parameters ( d , m, q ) has alternate solutions optimized for some other criteria (usually diameter, then average distance).
6] The basic algorithm attempts replacement of typically 1 or 2 generators h s ∈ S m, and for each new configuration it evaluates (incrementally) the target utility function,
such as diameter, average distance or max-cut (or some hierarchy of these, used for tie-breaking rules). The number of simultaneous replacements depends on n, m and
available computing resources. Namely, there are ∼ n r possible simultaneous deletions and insertions (assuming the "best deletion" is followed by "best" insertion). The
utility function also uses indirect measures (analogous to sub-goals) as a tie-breaking selection criterion e.g. when minimizing diameter, it was found that an effective
indirect measure is the number of nodes #F in the farthest (from node 0) group of nodes. The indirect objective in this case would be to minimize the #F of such nodes,
whenever the examined change (swap of 1 or two generators) leaves the diameter unchanged.
7] In addition to incremental updates to the networks after each evaluated generators replacement, these algorithms rely on vertex symmetry of Cayley graphs to further
reduce computations. E.g. all distance tables are only maintained and updated for n -1 distances from node 0 ("root"), since the table is the same for all nodes (with mere
permutation of indices, obtainable via T(a) representation of Gn if needed).
8] Depending on network application, the bisection b can be maintained fixed for all replacements (e.g. if bisection is the highest valued objective), or one can allow b to
drop by some value, if the secondary gains are sufficiently valuable.
9] After generating and evaluating all replacements to a given depth (e.g. replacement of 1 or 2 generators), the "best" one is picked (according to the utility/cost function)
and replacement is performed. Then the outer iteration loop would continue, examining another set of replacements seeking the best one, etc. until no more
improvements to the utility/cost function can be obtained in the last iteration. Specialized solution
0] This section describes several optimum LH solutions with particularly useful parameters or simple construction patterns.
S-1. High Density LH Networks for modular switches (LH-HD)
1] This is a special case of LH networks with high topological link density, suitable for combining smaller number of high radix switches into a single large radix modular
switch. This is a specialized domain of network parameters where the 2-layer Fat Tree (FT-2) networks are currently used since they achieve the yield of E=R/3 external
ports/switch, which is the maximum mathematically possible for the worst case traffic patterns. The 'high density' LH networks (LH-HD) match the FT-2 in this optimum
E=R/3 external ports/switch yield for the worst case traffic patterns, while achieving substantially lower average latency and the cost in Gb/s of throughput on random or
'benign' (non-worst case) traffic.
2] In our preferred embodiment using Cay𝑑2 𝑆m

graph, the network size is n =2d switches and the number of links per node m is one of the numbers: n /2, n /2+ n /4, n /2+ n /4+ n /8,... , n /2+ n /4+ n /8+...+1, then the
optimum m generators for LH-HD are constructed as follows:
(i) h 1= n -1, h 2= n -2, h 3=n -3,... hm =n-m
(ii) Optionally diagonalize and sort Sm via procedure (N-3) (Of course, there are a large number of equivalent configurations obtained via equivalence transforms
N-1.)
3] The resulting bisection is: b=└( m +1)/2┘ or B=b·n /2, diameter is 2 and average hops is 2- m / n. The largest LH-HD m = n /2+ n /4+ n /8+...+1 = n -1 has b= n /2 and
corresponds to a fully meshed network.
4] Table 4.10 shows an example of LH-HD generators for n =26=64 nodes and m = n /2=32 hops/node, with the hops shown in hex and binary (binary 0s are shown as '-'
character). Table 4.10(a) shows the non-diagonalized hops after the step (i), and Table 4.10(b) shows the equivalent network with m =32 hops after diagonalization in
step (ii) and sorting. Other possible LH-HD m values for the same n =64 node network are m =32+16=48, m =48+8=56, m =56+4=60, m =60+2=62 and m =61+1=63 hops.
5] Additional modified LH-HD networks are obtained from any of the above LH-HD networks via removal of any one or two generators, which yields networks LH-HD1 with m
1 = m-1 and LH-HD2 with m 2= m -2 generators. Their respective bisections are b 1=b-1 and b 2=b-2. These two modified networks may be useful when an additional one

or two server ports are needed on each switch compared to the unmodified LH-HD network.
6] These three types of high density LH networks are useful for building modular switches, networks on a chip in multi-core or multi-processor systems, flash
memory/storage network designs, or generally any of the applications requiring very high bisection from a small number of high radix components and where FT-2 (two
level Fat Tree) is presently used. In all such cases, LH-HD will achieve the same bisections at a lower latency and lower cost for Gb/s of throughput. Table 4.10
1.

3F 111111

1.

1 .....1

2.

3E 11111.

2.

2 ....1.

3.

3D 1111.1

3.

4 ...1..

4.

3C 1111..

4.

8 ..1...

5.

38 111.11

5.

10 .1....

6.

3A 111.1.

6.

20 1.....

7.

39 111..1

7.

7 ...111

8.

38 111...

8.

B ..1.11
D ..11.1

9.

37 11.111

9.

10.

36 11.11.

10.

E ..111.

11.

35 11.1.1

11.

13 .1..11

12.

34 11.1..

12.

15 .1.1.1

https://patents.google.com/patent/EP2708000B1/en?q=(radix-3)&oq=radix-3

20/51

2/14/26, 2:05 PM

EP2708000B1 - Flexible radix switching network - Google Patents
13.

33 11..11

13.

14.

32 11..1.

14.

19 .11..1

15.

31 11...1

15.

1A .11.1.

16.

30 11....

16.

1C .111..

17.

2F 1.1111

17.

1F .11111

18.

2E 1.111.

18.

23 1...11

19.

2D 1.11.1

19.

25 1..1.1

20.

2C 1.11..

20.

26 1..11.

21.

2B 1.1.11

21.

29 1.1..1

22.

2A 1.1.1.

22.

2A 1.1.1.

23.

29 1.1..1

23.

2C 1.11..

24.

28 1.1...

24.

2F 1.1111

25.

27 1..111

25.

31 11...1

26.

26 1..11.

26.

32 11..1.

27.

25 1..1.1

27.

34 11.1..

28.

24 1..1..

28.

37 11.111

29.

23 1...11

29.

38 111...

30.

22 1...1.

30.

3B 111.11

31.

21 1....1

31.

3D 1111.1

32.

2θ 1.....

32.

3E 11111.

(a)

16 .1.11.

(b)

S-2. Low Density LH networks with b=3
7] This subset of LH networks is characterized by comparatively low link density and low bisection b=3 i.e. B=3 n /2 links. They are constructed as a direct augmentation of
regular hypercubic networks which have bisection b=1. The method is illustrated in Table 4.11 using augmentation of the 4-cube.

8] The d =4 hops h 1, h 2 , h 3 and h 4 for the regular 4-cube are enclosed in a 4×4 box on the top. The augmentation consists of 3 additional hops h 5 , h 6 and h 7 added in
the form of 4 columns C 1, C 2, C 3 and C 4, where each column C µ (µ=1.. d ) has length of L=3 bits. The resulting network has n =16 nodes with 7 links per node and it is
identical to an earlier example in Table 4.9 with b=3 obtained there via translation from a [7,4,3]2 EC code into the LH network. General direct construction of the b=3 LH
network from a d -cube is done by appending d columns C µ (µ=1..d ) of length L bits, such that each bit column has at least 2 ones and L is the smallest integer satisfying
inequality:
𝐿

2

−𝐿−1 ≥ 𝑑

9] The condition in eq. (4.60) expresses the requirement that d columns C must have at least 2 ones. Namely, there are total of 2 L distinct bit patterns of length L. Among
µ
all 2 L possible L-bit patterns, 1 pattern has 0 ones (00..0) and L patterns have a single one. By removing these two types, with 0 or single one, there are 2 L -(L+1)
remaining L-bit patterns with two or more ones, which is the left hand side of eq. (4.60). Any subset of d distinct patterns out of these 2 L -(L+1) remaining patterns can
be chosen for the above augmentation. The Table 4.12 shows values L (number of added hops to a d -cube) satisfying eq. (4.60) for dimensions d of practical interest.
Table 4.12

d min

d max

L

3

43

5

11 4

12

26 5

27

57 6

S-3. Augmentation of LH networks with b=odd integer
0] This is a very simple, yet optimal, augmentation of an LH network which has m links per node and bisection b=odd integer into LH network with bisection b 1=b+1 and m
1= m +1 links per node. The method is illustrated in Table 4.14 using the augmented 4-cube ( d =4, n =16 nodes) with m =7 links per node and bisection b=3, which was

https://patents.google.com/patent/EP2708000B1/en?q=(radix-3)&oq=radix-3

21/51

2/14/26, 2:05 PM

EP2708000B1 - Flexible radix switching network - Google Patents

used in earlier examples in Tables 4.9 and 4.12.

1] A single augmenting link h = h ∧ h ∧...∧ h (bitwise XOR of the list) is added to the network which increases bisection from b=3 to b=4 i.e. it increases the absolute
8
1
2
7
bisection B by n /2= 16/2=8 links. The general method for Cay𝑑2 𝑆m

with b='odd integer' consists of adding the link h m+1= h 1 ∧ h 2 ∧...∧ h m (the bitwise XOR of the previous m hops) to the generator set Sm . The resulting LH network
𝑑

Cay𝑍 2 𝑆m + 1

has bisection b1=b+1.
2] The only case which requires additional computation, beyond merely XOR-ing the hop list, is the case in which the resulting hop h m+1 happens to come out as 0 (which is
an invalid hop value, a self-link of node 0 to itself). In such case, it is always possible to perform a single hop substitution in the original list Sm which will produce the
new list with the same b value but a non-zero value for the list XOR result h m+1.
LH construction for a target network
3] In practice one would often need to construct a network satisfying requirements expressed in terms of some target number of external ports P having oversubscription
φ, obtained using switches of radix R. The resulting construction would compute the number n of radix-R switches needed, as well as the list for detailed wiring between
switches. For concreteness, each radix-R switch will be assumed to have R ports labeled as port #1, #2,... #R. Each switch will be connected to m other switches using
ports #1, #2,... # m (these are topological ports or links) and leave E ≡ R-m ports: #m +1, #m +2,... #R as "external ports" per switch available to the network users for
servers, routers, storage,... etc. Hence, the requirement of having total of P external ports is expressed in terms of E and number of switches n as:
𝐸 = 𝑃/𝑛

4] The oversubscription eq. (3.1) is then expressed via definition of bisection b in eq. (4.42) as:
ϕ ≡

𝑃/2
𝐵

=

𝐸⋅n / 2
𝐵

=

𝐸
𝐵

=

𝑛/2

𝐸
𝑏

=

𝑅−𝑚
𝑏

5] The illustrative construction below will use non-oversubscribed networks, φ=1, simplifying eq. (4.71):
𝐸 = 𝑏 = 𝑅−𝑚

i.e. for non-oversubscribed networks, the number of external ports/switch E must be equal to the relative bisection b (this the bisection in units n/2), or equivalently, the
number of links/switch: m = R - b.
6] In order to find appropriate n =2d and m parameters, LH solutions database, obtained by translating optimum EC code tables [17] and [22] via recipe (4.45), groups
solutions by network dimension d into record sets Dd , where d =3,4,... 24. These dimensions cover the range of network sizes n =2d that are of practical interest, from n =
23 = 8 to n = 224 ≅ 16 million switches. Each record set Dd contains solution records for m = d, d +1,... m max links/switch, where the present database has m max=256
links/switch. Each solution record contains, among others, the value m , bisection b and the hop list h 1, h 2,... h m.
7] For given P, R and φ, LH constructor scans record sets Dd , for d =3,4,... and in each set, inspects the records for m = d, d +1, ... computing for each ( d,m ) record values
E(d,m)=R-m ports/switch, total ports P(d,m) = n· E(d,m) = 2d·(R-m ) and oversubscription φ(d,m)=E(d,m)/b (value b is in each (d,m) record). The relative errors δP =
|P(d,m)-P|/P and δφ = |φ(d,m)- φ|/φ are computed and the best match (record ( d,m ) with the lowest combined error) is selected as the solution to use. If the
requirement is "at least P ports" then the constraint P(d,m)-P≥0 is imposed for the admissible comparisons. The requirements can also prioritize δP and δφ via weights
for each (e.g. 0.7·δP + 0.3·δφ for total error). After finding the best matching ( d,m ) record, the hop list h 1, h 2,... h m is retrieved from the record and the set of links L( v )
is computed for each node v , where v = 0, 1, ... n -1, as: L( v ) = { v ∧ h s for s =1..m }. Given n such sets of links, L(0), L(1),..., L( n -1), the complete wiring for the network is
specified. The examples below illustrate the described construction procedure.
Example 1. Small network with P=96 ports at φ=1, using switches with radix R=12
8] The LH database search finds the exact match (δP=0, δφ=0) for the record d =5, m =9, hence requiring n =2d=25=32 switches of radix R=12. The bisection b=3 and the
hop list (in hex base) for the record is: S9= {1, 2, 4, 8, 10, E, F, 14, 19}hex. The number of external ports per switch is E=b=3, combined with m =9 topological ports/switch,
results in radix R=3+9=12 total ports/switch as specified. The total number of external ports is P = E·n . = 3.32 = 96 as required. Diameter (max hops) for the network is
D=3 hops, and the average hops (latency) is Avg=1.6875 hops. Table 4.15 shows complete connection map for the network for 32 switches, stacked in a 32-row rack one
below the other, labeled in leftmost column "Sw" as 0, 1,... 1F (in hex). Switch 5 is outlined with connections shown for its ports #1 ,#2,... #9 to switches (in hex) 04, 07,
01, 0D, 15, 0B, 0A, 11 and 1C. These 9 numbers are computed by XOR-ing 5 with the 9 generators (row 0): 01, 02, 04, 08, 10, 0E, 0F, 14, 19. The free ports are #10, #11 and

https://patents.google.com/patent/EP2708000B1/en?q=(radix-3)&oq=radix-3

22/51

2/14/26, 2:05 PM

EP2708000B1 - Flexible radix switching network - Google Patents

#12.

9] To illustrate the interpretation of the links via numbers, the outlined switch "5:" indicates on its port #2 a connection to switch 7 (the encircled number 07 in the row 5:). In
the row 7:, labeled as switch "7:", there is an encircled number 05 at its port #2 (column #2), which refers back to this same connection between the switch 5 and the
switch 7 via port #2 on each switch. The same pattern can be observed between any pair of connected switches and ports.
Example 2. Small network with P=1536 (1.5K) ports at φ=1, using switches with radix R=24.
0] The LH solutions database search finds an exact match for d = 8, n = 256 switches of radix R=24 and m =18 topological ports/switch. Diameter (max hops) of the
network is D=3 hops, and average latency is Avg=2.2851562 hops. The bisection is b=6, providing thus E=6 free ports per switch at φ=1. The total number of ports
provided is E·n =6·256=1536 as required. The set of 18 generators is: S18 = {01, 02, 04, 08, 10, 20, 40, 80, 1A, 2D, 47, 78, 7E, 8E, 9D, B2, D1, FB}hex. Note that the first 8 links
are regular 8-cube links (power of 2), while the remaining 10 are LH augmentation links. These generators specify the target switches (as index 00..FFhex) connected to
switch 00 via ports #1, #2,... #18 (switches on both ends of a link use the same port number for mutual connections). To compute the 18 links (to 18 target switches) for
some other switch x ≠ 00, one would simply XOR number x with the 18 generators. Table 4.16 shows the connection table only for the first 16 switches of the resulting
network, illustrating this computation of the links. For example, switch 1 (row '1:') has on its port #4 target switch 09, which is computed as 1∧8=9, where 8 was the
generator in row '0:' for port #4. Checking then switch 9 (in row '9:'), on its port #4 is switch 01 (since 9∧8=1), i.e. switches 1 and 9 are connected via port #4 on each. The

https://patents.google.com/patent/EP2708000B1/en?q=(radix-3)&oq=radix-3

23/51

2/14/26, 2:05 PM

EP2708000B1 - Flexible radix switching network - Google Patents

table also shows that each switch has 6 ports #19, #20,... #24 free.

Example 3. Large network with P=655,360 (640K) ports at φ=1, using switches with radix R=48.
1] The database lookup finds the exact match using d =16, n =216 = 65,536 = 64K switches of radix R=48. Each switch uses m =38 ports for connections with other
switches leaving E=48-38=10 ports/switch free, yielding total of P = E·n = 10·64K=640K available ports as required. Bisection is b=10 resulting in φ=E/b=1. The list of m
=38 generators S38 = { h 1 , h 2,... h 38} is shown in Table 4.17 in hex and binary base. The 38 links for some switch x (where x : 0..FFFF) are computed as S38(x)≡ { x ∧ h 1 ,

x ∧ h 2,... x ∧ h 38}. Diameter (max hops) of the network is D=5 hops, and the average latency is Avg=4.061691 hops. Table 4.17
1.

1 ...............1

2.

2 ..............1.

3.

4 .............1..

4.

8 ............1...

5.

10 ...........1....

6.

20 ..........1.....

7.

40 .........1......

8.

80 ........1.......

9.

100 .......1........

10.

200 ......1.........

11.

400 .....1..........

12.

800 ....1...........

13.

1000 ...1............

14.

2000 ..1.............

15.

4000 .1..............

16.

8000 1...............

17.

6F2 .....11.1111..1.

18.

1BD0 ...11.1111.1....

19.

1F3D ...11111..1111.1

20.

3D72 ..1111.1.111..1.

21.

6B64 .11.1.11.11..1..

22.

775C .111.111.1.111..

23.

893A 1...1..1..111.1.

24.

8B81 1...1.111......1

25.

9914 1..11..1...1.1..

https://patents.google.com/patent/EP2708000B1/en?q=(radix-3)&oq=radix-3

24/51

2/14/26, 2:05 PM

EP2708000B1 - Flexible radix switching network - Google Patents
26.

A4C2 1.1..1..11....1.

27.

A750 1.1..111.1.1....

28.

B70E 1.11.111....111.

29.

BFF1 1.1111111111...1

30.

C57D 11...1.1.11111.1

31.

D0A6 11.1....1.1..11.

32.

D1CA 11.1...111..1.1.

33.

E6B5 111..11.1.11.1.1

34.

EAB9 111.1.1.1.111..1

35.

F2E8 1111..1.111.1...

36.

F313 1111..11...1..11

37.

F9BF 11111..11.111111

38.

FC31 111111....11...1

LH performance comparisons
2] The LH solutions database was used to compare LH networks against several leading alternatives from industry and research across broader spectrum of parameters.
The resulting spreadsheet charts are shown in Figures 11 - 15. The metrics used for evaluation were Ports/Switch yield (ratio P/n, higher is better) and the cables
consumption as Cables/Port (ratio: # of topological cables/P, lower is better). In order to maximize the fairness of the comparisons, the alternative networks were set up
to generate some number of ports P using switches of radix R, which are optimal parameters values for a given alternative network (each network type has its own
"natural" parameter values at which it produces the most efficient networks). Only then the LH network was constructed to match the given number of external ports P
using switches of radix R (as a rule, these are not the optimal or "natural" parameters for LH networks).
3] In Figs. 11 - 15, the Ports/Switch chart for each alternative network shows Ports/Switch yields for the LH network _ _ _ and the alternative network ..... , along with the
ratio LH/alternative _ with numbers on the right axis (e.g. a ratio 3 means that LH yields 3 times more Ports/Switch than the alternative). The second chart for each
alternative network shows the Cables/Port consumption for the LH and the alternative, along with the ratio: alternative/LH on the right axis (e.g. a ratio 3 means that LH
consumes 3 times fewer cables per port produced than the alternative). All networks are non-oversubscribed i.e. φ=1.
4] For example, the Ports/Switch chart in Fig. 11 shows yield for hypercube (HC), for network sizes from n =28 to 224 switches of radix R=64. The Ports/Switch for LH
network yielding the same total number of ports P is shown, along with the ratio LH/HC, which shows (on the right axis scale) that LH produces 2.6 to 5.8 times greater
Ports/Switch yield than hypercube, hence it uses 2.6-5.8 times fewer switches than HC to produce the same number of ports P as HC at the same throughput. The
second chart in Fig. 11 shows similarly the Cables/Port consumption for HC and LH, and the ratio HC/LH of the two (right axis scale), showing that LH consumes 3.5 to 7
times fewer cables to produce the same number of ports P as HC at the same throughput. The remaining charts in Figs. 12 - 14 show the same type of comparisons for
the other four alternatives.
Performance Measurement
5] It is desirable to maximize λ since λ quantifies the external port yield of each switch. Namely if each switch's port count (radix) is R , then R = E + T (where E is the number
of external ports and T number of topological ports) and the E-port yield per IPA port is: Yield ≡ E/R=λ(λ+1), i.e. increasing λ increases the Yield. But increasing λ for a
given N also lowers the bisection for that N, hence in practical applications, data center administrators need to select a balance of Yield vs. bisection and N suitable for
the usage patterns in the data center. The centralized control and management software provides modeling tools for such evaluations.
6] Denoting the number of external ports and topology ports per switch as E and T, the radix (number of ports) R of a switch is R = E + T. The topology ports in turn consist
of the d ports needed to connect a d -dimensional hypercube HC d and of h long hop ports used for trunking, so T=d+h. If the number of switches is N, the N =2 d or d
=log( N ), where log( x ) is the logarithm base 2 of x i.e. log( x ) = ln( x )/ln(2) ≈1.443·ln( x ). In order to relate formally the invention's long hops to terminology used with
conventional trunking (where each of the d HC d cables is replaced with q cables, a trunk quantum), define q ≡ T/ d, i.e. T = q·d. Hence q and h are related as: q = 1+ h/ d
and h = d·( q -1). Using the ratio: λ≡ E/T , E and T is expressed as T = R /(1+λ) and E =λ· R /(1+λ). Restating the bisection formula:
𝐵 ≡ 𝐵𝑁 = 𝑁 / 2 ⋅ 𝑞 ⋅ 𝐶 = 𝑁 / 2 ⋅ 1 + ℎ / 𝑑 ⋅ 𝐶

7] Where C is a single IPA switch port capacity (2·<Port Bit Rate> for duplex ports). Bisection B is the smallest total capacity of links connecting two halves of the network
(i.e. it's the minimum for all possible network cuts into halves). Consider two network halves with N /2 switches each and E external ports per switch, there are E·N /2
external ports in each half. If these two sets of eternal ports were to transmit to each other at full port capacity C, the total capacity needed to support it is E ·( N /2)· C.
Since bisection limits the worst case capacity between halves to B, the oversubscription φ is defined as the ratio between the capacity needed E·( N /2) ·C and the
capacity for the job available via B:
φ ≡ E ⋅ N / 2 ⋅ C / B = E / q = λ ⋅ d = λ ⋅ logN

8] Eq. (6) shows in what ratio λ=E/T ports must be divided in order to obtain oversubscription φ using N switches: λ=φ/log(N). The quantity most often of interest is the
total number of external ports provided by the network, P = N·E, which in terms of other quantities typically given as constraints (φ, N and radix R), and recalling that
E=λ·R/(1+λ), is then:
𝑃 =

𝜙⋅𝑅⋅𝑁
𝜙 + log𝑁

9] Although Eq. (7) doesn't yield a closed form expression for N, it does allow computation of the number of IPA switches N needed to get some target number of total
network ports P at IB-oversubscription φ, knowing the radix R of the switches being used. Qualitatively, the number of total network ports P increases slightly slower than
linearly in N (when φ is kept fixed) due to the denominator D≡(φ+log(N)) which also increases with N. Its effects diminish as N increases (or if φ is large or grows with N),
since doubling of N increments D by +1 (which is only by ∼5% for N=64K and φ=4). Within the log(log(P)) error margin, the N above grows as N ∼ P·log(P), which is an
unavoidable mathematical limit on performance of larger switches combined from N smaller switches at fixed φ.
0] Figure 16 (computed for the commercially available Pronto 3780 switch) shows the resulting network capacity based on a simple un-optimized configuration (for the
lowest commonly used fixed IB-oversubscription ϕ=4, other values of interest ϕ=1, 2, 5, 10, 15 and 20 are shown later). The slight log(N) nonlinearity when using fixed ϕ
can be seen in the price per port - while N increased by a factor 128K, the price per 10G port increased only 3.4 times (i.e. the cost per 10G port grew over 38,000 times
slower than the network size and capacity, which is why the slight non-linearity can be ignored in practice). If instead of using fixed ϕ the fixed λ(E/P ratio) is used, then
via Eq. (3): ϕ ≡ ϕ(N,λ) =λ·log(N), the port Eq. (6) becomes linear in N :
𝜆⋅𝑅⋅𝑁
𝑃 =

1+𝜆

https://patents.google.com/patent/EP2708000B1/en?q=(radix-3)&oq=radix-3

25/51

2/14/26, 2:05 PM

EP2708000B1 - Flexible radix switching network - Google Patents

i.e. we get a fixed cost and power per port as N grows. In this case the tradeoff is that it is ϕ which now grows as λ·log(N) as N grows. Recalling that typical aggregate
oversubscriptions on core switches and routers are -200+ in the current data centers, log( N ) is quite moderate in comparison. The network bandwidth properties for λ=1
are shown in Figure 17 where the cost per 10G port remains fixed at $500 (or $104 per 1G port) and power at 14.6W. Results for some values of λ≠ 1 are shown later.
Elimination of CAM Tables
1] By using mathematically convenient topologies such as an enhanced hypercube connection pattern or its hierarchical variants, the switch forwarding port can be
computed on the fly via simple hardware performing a few bitwise logical operations on the destination address field, without any expensive and slow forwarding Content
Addressable Memory (CAM) tables being required. Hence, for customized switches, price and power use advantages can be gained by removing CAM hardware entirely.
Exception and Fault Handling Using CAM
2] Although the most favorable embodiments of the invention can eliminate CAMs completely, a much smaller (by at least 3 orders of magnitude smaller) CAM hardware
can still be useful to maintain forwarding exceptions arising from faults or congestion. Since the enhanced hypercubic topology allows for forwarding via simple, small
logic circuits (in the ideal, exception free case), the only complication arises when some port P is faulty due to a fault at the port or failure/congestion at the nearest
neighbor switch connected to it. Since number of such exceptions is limited by the radix R of the switch, the necessary exception table needs a space for at most R small
entries (typical R =24..128, entry size 5-7 bits). A match of a computed output port with an entry in the reduced CAM overrides the routine forwarding decision based on
the Jump Vector computed by the logic circuit. Such a tiny table can be implemented in the substantially reduced residual CAMs, or even within the address decoding
logic used in forwarding port computation. This exception table can also be used to override the routine forwarding decisions for local and global traffic management
and load balancing.
Improved Trunking and Link Aggregation
3] In order to increase the pipe capacity along overloaded paths, while under the tree topology constraints, the conventional data center solution is trunking(or link
aggregation in the IEEE 802.1AX standard, or Cisco's commercial EtherChannel product), which amounts to cloning the link between two switches, resulting in multiple
parallel links between the two switches using additional pairs of ports. The invention shows a better version of trunking for increasing the bisection with a fixed number
of switches.
4] With the invention, this problem arises when number of switches in a network is fixed for some reason so bisection cannot be increased by increasing N . Generally, this
restriction arises when the building block switches are a smaller number of high radix switches (such as the Arista 7500) rather than the larger number of low radix
switches that allow the desirable high bisection bandwidth as provided by the invention. Data centers making use of the invention can use conventional trunking by
building hypercubes using multiple parallel cables per hypercube dimension. While that will increase the bisection as it does for regular tree based data center networks,
there are better approaches that can be used.
5] The procedure is basically the opposite of the approach used for traditional trunking. By adding a link from some switch A, instead of picking the target switch B from
those closest to A, B is picked such that it is the farthest switch from A. Since the invention's topologies maintain uniform bisection across the network, any target switch
will be equally good from the bisection perspective, which is not true for conventional trees or fat trees. By taking advantage of this uniformity, picking the farthest switch
B also maximally reduces the longest and the average hop counts across the network. For example, with a hypercube topology, the farthest switch from any switch A is
the switch B which is on the long diagonal from A. Adding that one link to A cuts its longest path by half, and reduce the average path by at least 1 hop. When the long
hops are added uniformly to all switches (hence N/2 wires are added per new long hop), the resulting topology is called enhanced hypercube. Figure 18 shows the
reductions in the maximum and average hops due to adding from 1 to 20 long hops. In Figure 18, LH shows hex bitmasks of the long hop, i.e. the index of the farthest
switch chosen.
6] The table was obtained by a simple 'brute force' counting and updates of distance tables as the new long hops were added. At each stage, the farthest node from the
origin is used as a new link (a variety of tiebreaking rules were explored to provide a pick when multiple 'farthest' nodes are equally far, which is the common occurrence).
After each link is added the distance table is updated. For Dim=4, N=16, adding long hops beyond 11 doesn't have an effect since the small network becomes fully
meshed (when total number of links is N -1), hence all distances become 1 hop.
Optimizing Wiring Using Port Dimension Mapping
7] In some embodiments of the invention, systems being implemented via a set of switches in a data center, (e.g. available as line cards in a rack), wiring such dense
networks can easily become very complex, error prone and inefficient. With ( d!) N topologically equally correct mappings between ports and dimensions for a ddimensional hypercube using N =2 d switches, d ports per switch, there are lots of ways to create an unmanageable, error prone, wasteful tangle. The invention optimizes
the mapping between the ports and HC/FB dimensions using the following rules:
(i) The same dimensions are mapped to the same ports on all switches
(ii) Consecutive dimensions (0,1,... d -1) are mapped onto consecutive ports ( a , a +1,... a+ d-1)
The resulting wiring pattern shown in Figure 19 has the following advantages over a general topologically correct mapping:
a) All cables belonging to the same dimension have the same length
b) All cables have the same port number on both ends (cables strictly vertical)
c) All cables in the same vertical column (dimension) have the same lengths
8] Provided the cables and corresponding port connectors in the same column are color coded using matching colors (properties (b) and (c) makes such coding possible),
and the cables are of the minimum length necessary in each vertical column, this port-dimension mapping makes the wiring of a rack of switches easy to learn, easy to
connect and virtually error proof (any errors can be spotted at a glance). The total length of cables is also the minimum possible (requiring no slack) and it has the fewest
number of distinct cable lengths allowed by the topology. In addition to economizing the quantity and complexity of the wiring, the shortening and uniformity of cables
reduces the power needed to drive the signals between the ports, a factor identified as having commercial relevance in industry research.

Details of Connecting 64=26 switches → 6-D hypercube
9] In Figure 19, the column headers show 6 color coded port numbers 0=red, 1=blue, 2=orange, 3=purple, 4=green and 5=cyan. The 64 switches are line cards mounted in a
rack one below the other and they are depicted as 64 separate rows 0, 1, 2,...63. The 6 ports/switch used for wiring these switches into a 6-D hypercube, line up into 6
columns (the wire colors match the port colors in each column).
0] The 6 numbers inside some row #k show the 6 switches connected to the 6 ports of the switch #k. E.g. row #7 shows that switch #7 is connected to switches # 6, 5, 3,
15, 23, 39 on its ports 0, 1, 2,... 5. Picking now say, port (column) #4 for switch (row) #7, it connects on port 4 to switch #23. Looking down to switch (row) #23, its port
(column) #4 it connects back to switch #7 i.e. switch 7 and switch 23 are connected to each other's port #4. This simple rule - two switches always connect on the same
port # with each other- holds generally for hypercubes. This leads to the proposed port and cable color coding scheme. E.g. green: 4 cables connect green ports #4 on
some pair of switches, red: 0 cables connect red ports #0 on some other pair of switches, blue:1 cables connect blue ports #1, etc.
1] The wiring pattern is as simple. All wires of the same color have the same length L=2port #, e.g. orange: 2 wire (connecting always ports #2, orange:2 ports) has length
22=4, green:4 24=16, red: 0 20=1, etc. Hence switch pairs connected on their port #2 with each other are 4 rows apart, e.g. switch (row) 0 connects on its port #2 to
switch 4 on its port #2 and they use orange:2 wire (the color of port #2). This connection is shown as the top orange: 2 arc connecting numbers 4 and 0. The next

https://patents.google.com/patent/EP2708000B1/en?q=(radix-3)&oq=radix-3

26/51

2/14/26, 2:05 PM

EP2708000B1 - Flexible radix switching network - Google Patents

orange:2 (port #2) wire start at the next unconnected row, which is row #1 (switch #1), and connects to row 1+4=5 (switch #5), and so on until the first row already
connected on port #2 is reached, which is row #4 (Step 1-4). At that point 8 top rows on port #2 are connected. Then proceed down to the next row with free port #2,
which is row 8. That port #2 is now connected with the port #2 down 4 rows, i.e. with row 8+4=12, which is shown with orange: 2 wire linking numbers 12 and 8. Now the
next two rows (orange: 2 arc connecting numbers 13 and 9), etc, until column (port) #2 is connected on all switches. Then follows purple: 3 port #3, using purple: 3 wires
23=8 slots long, and repeat the same procedure, except with longer wires... etc.

Containers for Prewired Internal Topology
2] While the above wiring of a 64-switch hypercube H6≡H(64) is not difficult since errors are unlikely because starting at the top row and going down, any new wire can go
into just one port of the matching color, the pattern above suggests a simple way to design easily connectable internally prewired containers, which eliminate much of
the tedium and expense of this kind of dense manual wiring.
3] Consider the above H(64) as being composed of two prewired H(32) boxes A and B (separated by the dotted horizontal line at 32/32). The first 5 dimensions, ports
0,1,...4, of each H(32) are already fully wired and the only missing connections are the 32 wires connecting ports #5 on the 32 switches from one to the other container, in
perfectly orderly manner (row 0 of container A to row 0 of container B, row 1 from A to row 1 from B,... etc). Hence, instead of wiring 32x6=192 wires for H(64), two
prewired containers and 32 wires now connect between them in a simple 1,2,3... order. The job is made even easier with a bundled, thick cable with these 32 lines and a
larger connector on each box, requiring thus only one cable to be connected.
4] Looking further at the wiring relation between port #4 and port #5, it is obvious that these thick cables (each carrying e.g. 64 or 128 Cat 5 cables) follow the exact pattern
as ports #1 and #2, except with cable bundles and big connectors instead of single Cat 5 cables and individual ports. Hence, if one had a row of internally prewired (e.g.
via ASIC) 128-switch containers (e.g. one rack 64RU tall, 2 line cards per slot), each container having 8 color coded big connectors lined up vertically on its back panel,
matching color thick cables may be used that repeat the above exact wiring pattern between these 28=256 containers (except it goes horizontally) to create a network
with 27+8 = 32K IPA switches (for only $393 million), providing 786,432 x 10Gports (1 port per 10G virtual server with 32 virtual machines (VMs), totaling 25,165,824
VMs; i.e. switching cost < $16/VM). For large setups a single frame may be used where any newly added container can just be snapped into the frame (without any
cables), that has built in frame-based connectors (with all the inter-container thick cabling prewired inside the frame base).
5] The ultimate streamlining of the wiring (and of a lot more) is achieved by using "merchant silicon", where all such dense wiring, along with the connectors and their
supporting hardware on the switch is replaced with ASICs tying together the bare switching fabric chips. This approach not only eliminates the wiring problem, but also
massively reduces the hardware costs and power consumption.
6] For ASIC wiring of the Figure 19 pattern, in order to reduce the number of circuit layers the connection order must be reversed, changing all wire intersects into nestings,
allowing for single layer wiring. The resulting hypercube is just another one among the alternate labelings.

Non-Power-of-2 Networks
7] The above manual wiring scheme can also be used to build a network that has a number of switches N which is not a power of 2 (thus it cannot form a conventional
hypercube). Consider the case of a network where that has 32 switches ( d =5, using ports #0..#4, rows 0..31) and now wish to add two more switches, (rows) #32 and
#33. This starts the 6th dimension (port #5, long cyan wires), but only having two of the 32 cyan lines connected on port #6 (the two are connecting port #6 in rows 0↔32
and 1↔33 for the 2 new switches #32 and #33). The first 5 ports #0-#4 of the two new switches have no switches to go to, since these haven't been filled in (these will
come later in the rows 34-63).
8] The problem with such partial wiring is that it severely restricts forwarding to and from the new switches (just 1 link instead of 6 links), along with reduced bandwidth and
fragility (due to single points of failure. This problem can be eliminated by using port (column) #4 of the new first new switch (row) #32. The port #32:4 normally
connects (via green wire going down to row #48) to switch: port #48:4, but the switch #48 isn't there yet. Switch #48 also connects on port #5 (via dotted cyan wire) back
to the existent switch #16:5. Thus, there are two broken links #32:4↔#48:4 and #48:5↔#16:5, with missing switch #48 in the middle. Therefore, the two ends of existing
switches can be connected directly to each other, i.e. #32:4↔#16:5 as shown by the top dotted green wire (which happens to be just the right length, too). Later, when
switch #48 is finally added, the shortcut (green dotted wire going up) moves down to #48:4 while the #16:5, which becomes free as well (after moving the green wire
down), now connects to #48:5 (dotted cyan wire). The same maneuver applies to switch #33 as shown with the 2nd green dotted wire. The analogous shortcuts follows
for lower ports of #32 and #33 e.g. the broken pairs #32:3↔#40:3 and #40:5↔#8:5 are short-circuited via #32:3↔#8:5 etc, resulting in full (with natural forwarding) 6-D
connectivity for the new switches and their neighbors. The general technique is to first construct correct links for the target topology (e.g. hypercube), which include the
non-existent nodes. Then one extends all shortest paths containing the non-existent nodes until they reach existent nodes on both ends. The existent nodes terminating
such "virtual" shortest paths (made of non-existent nodes on the inner links) are connected directly, using the available ports (reserved on existent nodes for connections
with as yet non-existent ones).
Programmable Connector Panel
9] Another approach according to embodiments of the invention for interconnecting switches can include building large, software controlled super-connectors ("CSwitches"), where making any desired connections between the physical connectors can be controlled by software.
0] Unlike a standard switch, which forwards packets dynamically based on the destination address in the packet frame header, a C-Switch forwards packets statically, where
the settings for the network of crossbar connections within the C-Switch can be provided by an external program at initialization time. Without any need for high speed
dynamic forwarding and buffering of data packets, the amount of hardware or power used by a C-Switch is several orders of magnitude smaller than a standard switch
with the same number of ports.
1] The individual connectors (or per-switch bundles of for example 48 individual circuit cables brought in via trunked thick cables, plugged into a large single connector),
plug into the C-Switch's panel (which can cover 3-5 sides of the C-Switch container), which can include a matrix containing hundreds or thousands of receptacles. Beyond
the simple external physical connection, everything else can be done via software controls. Any desired topology can be selected via an operator using software to select
from a library of topologies or topology modules or topology elements.
2] To facilitate physical placement and heat management, C-Switches can be modular, meaning that a single C-Switch module can combine several hundred to several
thousand connectors, and the modules can be connected via single or few cables (or fiber links), depending on the internal switching mechanism used by the C-Switch.
In such a modular implementation, the inter-module cabling can be done via the cabling built into the frame where the connections can be established indirectly, by
snapping a new module into the frame.
3] There is a great variety of possible ways to implement core functionality of a C-Switch, ranging from telephony style crossbar switches, to arrays of stripped down,
primitive hub or bridge elements, to nanotech optical switches and ASIC/FPGA techniques. Since the internal distances within a C-Switch are several orders of magnitude
smaller than standard Ethernet connections, it is useful (for the heat& power reduction) that the incoming signal power be downscaled by a similar factor before entering
the crossbar logic (the signals can be amplified back to the required levels on the output from the crossbar logic). In other embodiments, for example using MEMS based
devices, power reduction may not be necessary where optical signals are switched via piezo-electrically controlled nano-mirrors or other purely optical/photonic
techniques such as DLP normally used for projection screens, where such down/up-scaling is implicit in the transceivers.
4] The internal topology of the C-Switch can be multi-staged since the complexity of a single, flat crossbar grows as O( X 2) for X external ports. For example, a arrangable
non-blocking hypercubic topology requires a hypercube dimension of d, connecting N =2 d smaller crossbars, which is twice the number of external ports p per smaller
crossbar, i.e. d=2 p. Hence each small crossbar of radix 3 p has a circuit complexity (number of cross points) of O (9 p 2). The number of external ports X = N·p = 22p ·p
determines value p needed for a given X in implicit form where approximately p ≈ ½ log( X ) + O (log(log( X ))). Hence, the number of small crossbars is N =2 d ≈ X ·log( X
). With the small crossbar radix p = 72, the C-Switch hardware scales to X =224≈16 million ports.

https://patents.google.com/patent/EP2708000B1/en?q=(radix-3)&oq=radix-3

27/51

2/14/26, 2:05 PM

EP2708000B1 - Flexible radix switching network - Google Patents

5] This kind of software controlled multi-connector has a much wider applicability than data centers, or even than Ethernet LANs, since cabling and connectors are a major
problem in many other settings and at much smaller scales of connectivity.

Use of C-Switches for Layer 2 Network Optimization
6] The traffic patterns in a data center are generally not uniform all-to-all traffic. Instead, smaller clusters of servers and storage elements often work together on a common
task (e.g. servers and storage belonging to the same client in a server farm). The integrated control plane of the current invention allows traffic to be monitored, and to
identify these types of traffic clusters and reprogram the C-Switch so that the nodes within a cluster become topologically closer within the enhance hypercube of
Ethernet switches. By reducing the path lengths of the more frequent traffic patterns or flows by using a C-Switch, the load on the switching network is reduced since
fewer switching operations are needed on average from ingress to egress, hence increasing capacity. The C-Switch is used in this new division of labor between the
dynamic switching network of the Layer 2 switches and the crossbar network within the C-Switch, which offloads and increases the capacity of the more expensive
network (switches) by the less expensive network (crossbars). This is a similar kind of streamlining of the switching network by C-Switch that layer 2 switching networks
perform relative to the more expensive router/layer 3 networks. In both cases, a lower level, more primitive and less expensive form of switching takes over some of the
work of the more expensive form of switching.
Wiring Improvements
7] Although the d -cube wiring is highly regular and can be performed mechanically (a la weaving), the 'long hops' do complicate the simple pattern enough to make it error
prone for brute force manual wiring. Since this problem is shared by many other desirable topologies, a general solution is desirable to make networks built according to
the invention practical in the commercial world.

Computer assisted manual wiring
8] In this method, the switches are numerically labeled in a hierarchical manner tailored to the packaging and placement system used, allowing technicians to quickly locate
the physical switch. A wiring program displays the wiring instructions in terms of the visible numbers on the switches (containers, racks, boxes, rooms) and ports. The
program seeks to optimize localization/clustering of the wiring steps, so that all that is needed in one location is grouped together and need not be revisited.

C-Box - Prewired crossbar for fixed topologies
9] This is a more attainable lower tech variation of the C-Switch in the form of a connector box with pre-wired topologies, such as enhanced hypercubes, within certain
range of sizes. Front panels of the C-Box provide rows of connectors for each switch (with ∼10-20 connectors per switch) with numbered rows and columns for simple,
by the numbers, wiring for the entire rows of rack switches and hosts.
40] C-Box is as easy to hook up and functions exactly as the C-Switch (e.g. with a built in processor and a unified control plane per box), except that the topology is fixed. As
with the C-Switch, multiple C-Boxes can be connected via thick cables to form a larger network.

Automated wiring verification
41] This facility is useful for the manual wiring methods described above. Diagnostic software connected to the network can test the topology and connections, then
indicates which cables are not connected properly and what corrective actions need to be taken.
Data Center Application
42] Figure 20 shows an embodiment of the invention applied to a complete data center. The particular details of this diagram are illustrative only, and those skilled in the art
will be able to see that many other combinations of data center components with various attributes such as number or ports and port speed may also be used, and
connected in various topologies. The cables (vertical arrows) are coded by capacity and named according to their roles: S(erver)-Lines from server to TORs or
transceivers, U(plink)-Lines from edge to network ports, T(opology)-Lines: internal to the network (aggregate switching fabric via scalable topology & forwarding) and
W(AN)-Lines to routers/L3. The only long lines, thus posing cabling bulk problems, are U-Lines, but these already exist in a standard data center. The internal switching
fabric of the network consists of the fabric from variable number of common off-the-shelf (COTS) switches with firmware extensions, connected via the Topology Panel
(ITP). Depending on size and complexity of topology (which depends on the type of data center), the ITP block may merely symbolize a prescribed pattern of direct
connections between ports (by the number wiring), or it can be realized as a prewired connector panel or as programmable crossbar switch.
43] The network spanned by the T-Lines is the network backbone. The encircled "A" above the top-of-rack (TOR) switches represents fabric aggregation for parts of the TOR
fabric which reduces the TOR inefficiencies.
44] The control and management software, MMC (Management, Monitoring and Control module), CPX (Control Plane Executive) and IDF (Data Factory), can run on one or
more servers connected to the network switching fabric.

Virtual Machine Motion
45] In a data center using virtual machine instances, the MMC and CPX can cooperate to observe and analyze the traffic patterns between virtual machine instances. Upon
discovering a high volume of data communication between two virtual machine instances separated by a large number of physical network hops, the MMC and/or CPX
can issue instructions to the virtual machine supervisor that results in one or more virtual machine instances being moved to physical servers separated by a smaller
number of network hops or network hops that are less used by competing network communication. This function both optimizes the latency between the virtual
machines and releases usage of some network links for use by other communicating entities.

Layer 3+ Protocol Performance Improvement
46] The most commonly used layer 3 (or higher) reliable communication protocols, such as TCP and HTTP, which have large communication overheads and non-optimal
behaviors in data center environments, can be substantially optimized in managed data center networks with a unified control plane such as in the current invention.
47] The optimization consists of replacing the conventional multi-step sequence of protocol operations (such as three way handshake and later ACKs in TCP, or large
repetitive request/reply headers in http) which have source and destination addresses within the data center, with streamlined, reliable Layer 2 virtual circuits managed by
the central control plane where such circuits fit naturally into the flow-level traffic control. In addition to reducing communication overhead (number of frames sent, or
frame sizes via removal of repetitive, large headers) and short-circuiting the slow error detection and recovery (the problem known as "TCP incast performance
collapse"), this approach also allows for better, direct implementation of the QoS attributes of the connections (e.g. via reservation of the appropriate network capacity
for the circuit). The network-wide circuit allocation provides additional mechanism for global anticipatory traffic management and load balancing that operates
temporally ahead of the traffic in contrast to reactive load balancing. This approach of tightly integrating with the underlying network traffic management is a
considerable advance over current methods of improving layer 3+ protocol performance by locally "spoofing" remote responses without visibility into the network
behavior between the spoofing appliances at the network end points.
48] Further, by operating in the network stacks/hypervisor, the virtualized connections cooperate with the Layer 2 flow control, allowing for congestion/fault triggered
buffering to occur at the source of the data (the server memory), where the data is already buffered for transmission, instead of consuming additional and far more
expensive and more limited fast frame buffers in the switches. This offloading of the switch frame buffers further improves the effective network capacity, allowing
switches to handle much greater fluctuations of the remaining traffic without having to drop frames.
Flexible Radix Switch Control Plane
Control Plane Capabilities
49] The FRS Control Plane (FRS-CP) makes use of the advanced routing and traffic management capabilities of the Infinetics Super Switch (ISS) architecture. It can also be
used to control conventional switches, although some of the capabilities for Quality of Service control congestion control may be limited.
FRS-CP provides:

Performance
0]

Controls the flat fully meshed layer 2 substrate/fabric to maximize effective throughput to near physical limits
Self-configuring, self-balancing, self-healing dynamic networks

https://patents.google.com/patent/EP2708000B1/en?q=(radix-3)&oq=radix-3

28/51

2/14/26, 2:05 PM

EP2708000B1 - Flexible radix switching network - Google Patents

Device and service level bandwidth optimization and QoS guarantees

Management
1]

Unified logical management framework for all networked devices
Hierarchical group-based management to reduce large network complexity
Autonomic, self-healing traffic flow management

Security
2]

Single point of authentication for all points of attachment and services at origin
Group-based networked device isolation throughout physical and virtualized networks

Cost Savings
3]

Far less network infrastructure required; substantial savings on capital expenditures, power, and payroll
Subsumes the functionality of other monolithic appliances such as load balancers, NATs, firewalls
•
Control Plane Architecture

4] FRS-CP can include a central control system that connects directly to all the switches in the network, which may be replicated for redundancy and failover. Each switch
can run an identical set of services that discover network topology and forward data packets.
5] Switches can be divided into three types based upon their role in the network, as shown in Figure 24:
Ingress switches
Fabric switches
Egress switches
6] ARP and broadcast squelching. When a specific machine attempts to locate another machine on the network in a classic network, it sends out a broadcast ARP (sort of a
where are you type message), which will be transmitted across the entire network. This message needs to be sent to every machine across the network on every
segment which significantly lowers the throughput capacity of the network. We keep a master list(distributed to every switch) of every host on the network, so that any
host can find any other host immediately. Also any other broadcast type packets which would have been sent completely across the network are also blocked. (** See
CPX Controller / Data Factory)
Overview
Data Factory (IDF)
7] Fig. 25 shows a system according to one embodiment of the invention. The Data Factory component can be used to establish the behavior of the IPA Network. The
Control Plane Executive (CPX) uses the data stored in the data factory to configure the network and to set up services such as security and quality guarantees.
Management consoles access this component to modify system behavior and retrieve real time network status.
Control Plane Executive (CPX)
8] The Data Factory communicates with the Control Plane Executive (CPX) through a service interface using a communication mechanism such as Thrift or JSON as shown
in Fig. 26. Any form of encryption can be supported. In accordance with some embodiments of the invention, a public key encryption system can be used.
Universal Boundary Manager (UBM)
9] In accordance with some embodiments of the invention, the UBM can provide some or all of the following functions:
Abstracts the physical network to a unified and hierarchical logical group with rights-based inheritance for security and QoS parameters
Controls visibility of hosts and services
Provides a single "Firewall" around perimeter of entire layer 2 network managing routing decisions for quality assurance and security enforcement for network
access
Scriptable policy management based upon time-of-day, congestion and application type
Data stored in the Data Factory, and read by CPX for distribution to the switches.
0] A UBM entry can describe a name for an organization or a specific service. A UBM entry could be a company name like ReedCO which would contain all the machines
that the company ReedCO would use in the data center. A UBM entry can also be used to describe a service available in that data center. A UBM entry has the following
attributes:
Name of node
DNS Name of this node (for DNS lookup)
Port(s) - these are the port(s) that are allowed to the specified machines. If there are no ports, then this is a container Node which means it is used to store a list
of allowed machines.
QOS information
Parent Node. Each parent can have multiple child Nodes, but each child can only have one parent Node.
Allow Public Access
1] To allow external access, a flag can be provided in or associated with the Node definition that indicates that this Node can be accessible from anybody without
restrictions. So a typical company with a Database server, Backup Database server, WWW server, and Backup server could look like the following:
COMPCO (Lists all four computers, but no public access)
DB (lists just the Database server)
BACKUPDB (lists just the backup database server)
BACKUP (Lists just the backup server)
WWW (Lists just the WWW server, but allow public connections)
2] A machine table contains at least the following information:
MAC Address
IP Address (If the machine is defined as static)
Description of machine
3] The firewall rules that are necessary to allow dataflow across the network can be created from this table. Only flows that are allowed will be sent to the KLM.

https://patents.google.com/patent/EP2708000B1/en?q=(radix-3)&oq=radix-3

29/51

2/14/26, 2:05 PM

EP2708000B1 - Flexible radix switching network - Google Patents

UBM Service
4] The Universal Boundary Manager service can provide membership services, security services and QoS. There can be two or more types of UBM groups:
Transparent UBM Group
5] A transparent group can be used as an entry point into the IPA EcoSystem. It can be visible and allow standard IP traffic to flow over its interface - UBM Interfaces can be
determined by port number - e.g. Port 80. This type of group can be used to handle legacy IP applications such as Mail and associated Web Services. Since a Web
Service can be tied to an IP port, limited security (at the Port Level) and QoS attributes (such as Load Balancing) can be attributes of the UBM structure.
Transparent UBM Group Features
Visible
Limited Security (Port Level) / No Security
Allows legacy IP (v4 & v6) Traffic
Qos Lite
Transparent UBM Group Attributes
Traffic Sensitive Routing / Most Efficient Path to Destination
Layer
2 Ring Routing (Distributed Hash)
Layer 3 Fallback if Layer 2 Ring Routing Fails
Connectionless QoS Tunnel
Explicit Congestion Control Notification
Opaque UBM Group
6] An opaque group can have all the attributes of the Transparent group's attributes, but allows for the extension of pure IPA security, signaling (switch layer) and the ability
to provided guaranteed QoS.
Opaque UBM group Features
Hidden - group Members only know about group Members
Membership Driven
Secure (Utilizing Public Key Security or Lattice based cryptography)
Polymorphic Membership Model (The rise of Inheritance)
Pure IPA
Guaranteed QoS based upon proprietary meshed network
Opaque UBM group Attributes
All of the transparent groupbenefits
Infinetics Certificate Authority Security Sub-System
Guaranteed QoS Properties
Signaling
7] The major extensions to the Opaque group can include the security attributes along with the guaranteed QoS attributes. Multiple opaque or visible groups can be defined
from this core set of attributes.
Firewall
8] The firewall can be a network-wide mechanism to pre-authorize data flows from host to host. Since every host on the network must be previously configured by the
network administrator before it can be used, no host can successfully transmit or receive data unless it has been authorized in the network. Furthermore because of the
built in security model applied to all devices connected to the network, hosts can only communicate with other authorized hosts. There is no way a rogue host can
successfully communicate with any unauthorized host. The data defined in the UBM can control all access to hosts. The KLM loaded into each Hypervisor can provide
this functionality. Alternatively, this functionality can be provided on each switch for each attached physical host.
9] The ingress switch where a data packet from a host first arrives in the network can use the following rules to determine whether the data packet will be admitted to the
network as shown in Figure 22:

Forward Path Rules
Ingress Switch
0]

I. Is H2 using the correct Ethernet Address? (Drop point 1)
I. Use source IP address to fetch triplet, compare addresses
II.Can H2 send to H1 on the given destination port? (Drop point 2) I. (Use UBM group rules.)
III.Send packet to S1
IV.Create "reverse" rule for H1->H2 for given source Port
I. Time stamp and age out rule.
Egress Switch

1]

I. Can H2 send to H1 on the given destination port? (Drop point 3)
II.Create "reverse" rule for H1->H2 for given source Port
I. Time stamp and age out rule.
III.Send packet to H1

Reverse Path Rules
Ingress Switch
2]

I. Is H1 using the correct Ethernet Address? (drop point 4)
I. Use source IP # to fetch triplet, compare MAC #s
II.Can H1 send to H2 on the given destination port? (drop point 5)
I. UseUBM group information
III.Send encapsulated packet to S2
Egress Switch

https://patents.google.com/patent/EP2708000B1/en?q=(radix-3)&oq=radix-3

30/51

2/14/26, 2:05 PM
3]

EP2708000B1 - Flexible radix switching network - Google Patents

I. Can H2 send to H1 on the given destination port? (drop point 6)
I. Use reverse rule.
II. Send packet to H1

4] This is the opposite way to which traditional firewalls work, where data is allowed to enter the network from any source, the data then traverses the network and is
prevented from reaching a destination host once the data packet has nearly reached its intended destination. This significantly lowers "backbone" traffic on the network.

Central Services
Data Factory
5] This is the starting point for full control of the network. All static and dynamic data is stored here, and a user interface is used to view and modify this data.
CPX Controller
6] The CPX computer is the Control Plane Executive which controls all switches, and receives and sends data to the switches. This data is what is necessary to route data,
firewall info, etc. It also controls the ICP (Integrated Control Plane) module which determines topology, and controls the IFX (Firmware eXtensions) which are installed on
every switch and hypervisor.
7] CPX connects to the Data Factory to read all of the configuration data necessary to make the entire network work. It also writes both log data and current configuration
data to the Data Factory for presentation to users.
ICP (Integrated Control Plane)
8] This module controls each instance of IFX on each switch, and takes that neighbor data from each IFX instance and generates cluster data which is then sent back to
each IFX instance on each switch.
CPX Interaction with ICP
9] The types of data that will flow through CPX for the data plane are:
Triplets
Firewall Rules/QoS Data
Topology Information
Logging Data
0] Triplets (which contain the Host IP Address, Switch ID, and MAC address of the host) are generated by the Host detector that runs on each switch. The detected triplets
are sent through the Host Controller to the CPX controller. First the triplet's data is validated to make sure that this host MAC address (and IP address if defined), is a
valid one. Once validate, the triplet is enabled in the network. Optionally, before a host's triplet is added to the database, the host can be forced to validate themselves
using various standard methods such as 802.1x.
1] The triplets can be sent to the Data Factory for permanent storage, and are also sent to other switches that have previously requested that triplet. The sends will be timed
out, so that if a switch has not requested a specific triplet for a specific time, the CPX will not automatically send it if it changes again unless the ICP requests it.
2] When a switch needs to route data to a host that it does not have a triplet for, the host controller sends a request for the triplet associated with the specific IP address.
The CPX looks up that triplet and sends it to the IFX which in turn sends it to the KLM module so that the KLM can route data.
3] Firewall rules and Quality of Service (QOS) data travel along the same route as triplets. A switch always receives all the firewall rules involving hosts that are connected to
that switch so that quick decisions can be made by the KLM module. If a firewall rule changes, then it is sent to the IFX which sends it to the KLM module. In cases where
there are firewall rules with schedules or other "trigger points", the firewall rules are sent to the IFX and IFX sends them to the KLM module at the appropriate time.
4] Logging Data such as data sent/received, errors, etc is sent from the KLM (or some other module) to IFX, and then to CPX which sends it to the Data Factory.
ICP Interaction with IFX on Switches
5] CPX controls ICP which then controls each instance of IFX on each switch through ICP, telling it to send "discover" packets, and return back neighbor topology data to ICP.
All this data is stored in the Data Factory for permanent storage, and for presentation to users. This topology data is used by IFX to generate routes. When link states
change, the IFX module notifies ICP, and a new routing table will be generated by IFX. Initially IFX will reroute the data around the affected path.
CPX Interaction with Data Factory
6] CPX reads the following data from the Data Factory:
Host information to validate the host being allowed, including authorization keys, etc.
Firewall Rules and QoS for inter-host interaction
Triplets that have been previously deposited into the Factory
Routing and topology data
7] CPX writes the following data into the Data Factory:
Triplet information determined by host detectors
Topology and routing data determined by CPX and IFX
Log information about changes in network infrastructure, including routing, host, and other data
ICP Data Factory
8] The following information is needed by ICP.
9] This can happen at a very high rate upon startup, and can reoccur on a regular basis very slowly
Switch Information
Key Value will either be MAC or IP address
Data Returned will be information necessary for to calculate topology, and identify switches.
Topology information previously written by CPX before. This will be used as "hints" to restart routing in case of a failed switch for example
Routing information necessary to route data between switches. This will need to be updated on all affected switches whenever the ICPupdates the Datastore
Factory.
0] The following information will be written by ICP.
1] This can happen on a very regular basis (e.g., at least 1 per second and can occur more often), but the writes can be buffered and delayed for writing if need be. The data
will not be read on a regular basis, except for startup, but will need to be updated on all other switches. Of course the data will be read by the User for network status
monitoring.
Switch Status - Current Status of each switch, including port status
Topology information - links between switches including metadata about each link
Routing information. Calculated "best" routes between switches
ICP Data needed for Switches

https://patents.google.com/patent/EP2708000B1/en?q=(radix-3)&oq=radix-3

31/51

2/14/26, 2:05 PM

EP2708000B1 - Flexible radix switching network - Google Patents

2] The following information will be written by the switches
Triplets from switches for hosts. These will be written whenever a new host comes online, or a host goes away. They can happen anywhere from once every few seconds,
to much more often as hosts come online. There needs to be some sort of acknowledgement that the specific host being added already exists in the UBM so that we can
route to that host. If the host does not exist we need to flag that host's information so that the user can see that a undefined host has been activated on the network, and
allow the user to add it to the UBM.
3] The following information will be read by the switches.
4] All of these reads can occur as fast as possible. Any slowness in these reads may slow down the data path.
Triplets for hosts. This can happen quite often, and needs to be as fast as possible.
UBM data that allows all the data necessary to create the firewall/QOS rules, multi-server data, and everything else necessary to route to that host.
The data that will be delivered to the switches from the UBM is:
Firewall Rules with QOS information
Multi-server data. This is all the servers of an equivalent type.

Switch Services
5] The following services can run on all switches in the network.
IFX (Firmware extensions)
6] This module runs on each switch and is responsible for determining the topology of the neighbors. It sends data back to the ICP module about its local physical
connectivity, and also receives topology data from ICP. It supports multiple simultaneous network logical topologies, including n-cube, butterfly, torus, etc as shown in
Figure 23. It uses a raw Ethernet frame to probe the devices attached to this switch only. It also takes the topology data from ICP, and the cluster data from ICP and
calculates forwarding tables.
IFXS (Firmware extensions for Servers)
7] This module runs on each hypervisor and interact s with the Hypervisor/KLM module to control the KLM. Flow data related to how many bytes of data flowing from this
hypervisor to various destinations is accepted by this module and used to calculate forwarding tables.
Hypervisor Controller
8] This can include a Linux kernel loadable module (KLM) that implements the Data plane. It can be controlled by the Switch Controller.
9] The input to this module are:
Triplets from this and other switches
Firewall Rules and QoS Associated data
Routes from IFX
0] The KLM can route packets from hosts to either other hosts, or to outside the network if needed (and allowed by rules). All packets sent across the "backbone" can be
encrypted, if privacy is required.
1] The KLM switch module can have access to caches of the following data: triplets (they map IPv4 addresses into (Egress Switch ID, host Ethernet Address pairs); routes
(they define the outbound interfaces, and next hop Ethernet Address to use to reach a given Egress Switch); and firewall rules (they define which IPv4 flows are legal, and
how much bandwidth they may utilize).
2] The KLM can eavesdrop on all IP traffic that flows from VM instances (that are supported by the local hypervisor). It can, for example, use functionality (defined in the
Linux netfilter library) to STEAL, DROP, or ACCEPT individual IP datagrams that are transmitted by any VM.
3] When a datagram is transmitted by a VM, the KLM switch can intercepts (STEALs) it and determines if firewall rules classify the corresponding flow to be legal. If it's
illegal, the packet is dropped. If the flow is legal and it's destination is local to the hypervisor, it's made to obey QoS rules, and delivered. If the flow is legal and exogenous,
the local triplet cache is consulted with the destination IP address as an index. If a triplet exists, it determines the Egress Switch ID (which is just a six-byte Ethernet
address). If a route also exists to the Egress switch, then the packet will be forwarded with the destination switch Topological MAC address put into the Ethernet frame.
4] The KLM can use a dedicated Ethernet frame type to make it impossible for any backbone switch or rogue host to send a received frame up its protocol stack.
5] When a frame arrives at a hypervisor, it can be intercepted by its kernel's protocol handler (functionality inside the KLM) for Ethernet frame type defined. The protocol
handler can examine the IP datagram, extract the destination IP address, and then index it into it's triplet cache to extract the Ethernet address of the local VM. If no
triplet exists, the frame can dropped. The socket buffer's protocol type can switched from 0xbee5 to 0x0800, and the packet can be made to obey QoS rules before it is
queued for transmission to the local host.
6] The KLM can use IFXS, for example, as its method to talk with CPX to access the data factory.
Examples
7] Figure 24 shows a typical use case where switching systems according to various embodiments of the invention can be used within a data center.
8] Figure 15 shows one embodiment of the invention where the FRS is used alone to provide an ultra-high bisection bandwidth connection between multiple CPU cores and
a large array of flash memory modules. The prior art approach for having CPU cores transfer data to and from flash memory treats the flash memory modules as an
emulated disk drive where data is transferred serially from a single "location". The invention allows large numbers of CPUs or other consumers or generators of data
communicate in parallel to multiple different flash memory storage modules. In this embodiment of the invention, the ISS network can be designed using the physical
constraints of the various methods that semiconductor devices are packaged and interconnected. This embodiment results in a network that has a different connection
pattern than would be used in a data center, but still provides extremely high bisection bandwidth for the available physical connections within and between
semiconductor devices and modules.
9] Additional supporting information relating to the construction of Long Hop networks is provided in attached Appendix A, which is hereby incorporated by reference.
0] Those skilled in the art will realize that the methods of the invention may be used to develop networks than interconnect devices or nodes with arbitrary functionality and
with arbitrary types of information being exchanged between the nodes. For example, nodes may implement any combination of storage, processing or message
forwarding functions, and the nodes within a network may be of different types with different behaviors and types of information exchanged with other nodes in the
network or devices connected to the network.
1. Introduction
1] Rapid proliferation of large Data Center and storage networks in recent years has spurred great deal of interest from industry and academia in optimization of network
topologies [1]-[12]. The urgency of these efforts is further motivated by the inefficiencies and costs of the presently deployed large Data Center networks which are
largely based on non-scalable tree topology.
2] There are two main types of network topologies proposed as scalable alternatives to the non-scalable tree topology of the conventional Data Center:
Fat Tree (FT) (syn. folded Clos) based networks, a class of "indirect networks"
Hypercubic (HC) networks, a class of "direct networks" using Cartesian product construction recipe. This class includes plain hypercube variants (BCube,
MDCube), Folded Hypercube (FC), Flattened Butterfly (FB), HyperX (HX), hyper-mesh, hyper-torus, Dragonfly (DF),... etc.
3] While the HC networks are overall the more economical of the two types, providing the same capacity for random traffic as FT with fewer switches and fewer cables, the
FT is more economical on the worst case traffic, specifically on the task of routing the worst case 1-1 pairs permutation.

https://patents.google.com/patent/EP2708000B1/en?q=(radix-3)&oq=radix-3

32/51

2/14/26, 2:05 PM

EP2708000B1 - Flexible radix switching network - Google Patents

4] The Long Hop (LH) networks stand above this dichotomy by being simultaneously the most optimal for the common random traffic and for the worst case traffic. The LH
optimality is result of the new approach to network construction which is fundamentally different from the techniques used to construct all the leading alternatives.
Namely, while the alternative techniques build the network via simple mechanical, repetitive design patterns which are not directly related to the network performance
metrics such as throughput, the LH networks are constructed via an exact combinatorial optimization of the target metrics.
5] Although there have been some previous attempts to optimize the network throughput directly, such as the "entangled networks" described in [2] and [12], these
techniques sought to optimize general random networks. Since such optimization is computationally intractable for general graphs (it is an NP-complete problem), the
computations of both, the network performance and the search for its improvements, are by necessity very approximate (simulated annealing) and still, they become
prohibitively expensive as the network size n increases beyond few thousand nodes. For example, the largest computed size in [12] had n =2000 nodes. Further, since the
resulting approximate solutions have variable node degree and random connectivity, appearing to a network technician as massive, incoherent tangles of wires without
any pattern or logic, the "entangled networks" are in practice virtually impossible to wire and troubleshoot. Finally, the node degree irregularity and the complete lack of
symmetry of such networks compound their impracticality due to complicated, resource hungry routing algorithms and forwarding tables.
6] In contrast, the LH construction method optimizes the highly symmetrical and, from practical perspective, the most desirable subset of general networks, Cayley graphs
[11]. As result of its more focused and more careful identification of the target domain, the LH networks are optimal regarding throughput and latency within that domain,
practical to compute and discover, simple and economical to wire and troubleshoot and highly efficient in routing and forwarding resources ("self-routing" networks).
2. Mathematical Tools and Notation
7]

•A≡B

equality defining expression A via expression B (tautology)

•A⇔B

expression or statement "A is equivalent to B"

•A⇒B

"A implies B"

•∀a

iterator or a set defined by the statement "for all a "

• iff

"if and only if"

• |S|

sets: size of set S (number of elements in S), numbers: absolute value of S

•└a┘

floor( a ): the largest integer ≤ a

•

n -dimensional vector space (over some implicit field F q)
•

k -dimensional subspace of (linear span) over field F q
( k,n,q )
𝑛

scalar (dot) product of real vectors x and y: 〈 𝑥|𝑦 〉 ≡ ∑

𝑖 = 1

• 〈 x| y 〉

𝑥𝑖 𝑦

𝑖

norm (length) of vector x : ‖𝑥‖ ≡ √ 〈 𝑥|𝑥 〉
2

• ∥x ∥
• a..b

integer sequence a, a +1,..., b for some integers a ≤ b

• [a, b)

half-open interval: contains all x satisfying a ≤ x < b

• [a, b]

closed interval: contains all x satisfying a ≤ x ≤ b

• { a 1, a 2, a 3}

set of elements a 1, a 2 and a 3

• {x: E(x)}

set of elements x for which Boolean expression E(x) is true

• min E {set}

minimum element of a {set} under condition E; analogously for max { set }

•a%b

" a mod b" or " a modulo b" (remainder in integer division a / b)

• bitwise

operation on bit strings done separately in each bit position

• ã or a̅

NOT a (bitwise complement, toggles each bit of a)

•a&b

bitwise AND (bitwise a·b )

•a|b

bitwise OR (bitwise a + b - a·b )

•a^b

XOR, exclusive OR (bitwise: ( a + b ) mod 2, also a + b - 2·a·b)

•α⊕b

modular addition in ring (Z q)d: component-wise (a + b ) mod q

•a⊖b

synonym for a ⊕ (-b ); for q =2: a ⊖ b ⇔ a ⊕ b ⇔ a ^ b (bitwise XOR)

•
Vector space
• A∘B=B∘A

is direct sum of vector spaces

and

'

Objects (matrices, group elements, etc.) commute for operation '∘'

• [E]

Iverson bracket (E is a Boolean expression): E true (false) ⇒ [E]=1 (0)

• δij

Kronecker delta: δi,j ≡ [ i = j ] i.e. δi,j is 1 if i = j and 0 if i ≠ j
Dirac integer delta: δi ≡ δi,0 i.e. δi is 1 if i = 0 and 0 if i ≠ 0

• δi
•B=A ≡Ã

matrix B is a transpose of matrix A i.e. elements B i,j = A j,i

•A⊗B

Kronecker product of matrices A and B

• A ⊕n

Kronecker n -th power of matrix A: A ⊗n≡ A⊗A⊗···⊗A ( n times)

• A×B

Cartesian product of sets or groups A and B

T

•A

×n

Cartesian n -th power of a set or group A
Binomial coefficient 𝐶𝑛𝑘 ≡ 𝑛! / 𝑘!𝑛 − 𝑘! =

• C(n,k)

𝑛
𝑘

https://patents.google.com/patent/EP2708000B1/en?q=(radix-3)&oq=radix-3

33/51

2/14/26, 2:05 PM

EP2708000B1 - Flexible radix switching network - Google Patents

• O (N )

Big O notation characterizes the growth rate and complexity.
𝑑−1

8] Binary expansion of a d -bit integer 𝑋 ⇔ 𝑋 = ∑

𝜇

𝑥2

𝜇 = 0

where x µ is the " µ -th bit of X" (bits x µ have values 0 or 1). Bit-string form of the binary expansion of integer X is denoted as: X = x d-1... x 1 x 0 .
9] Parity of a d -bit integer X = x d-1... x 1 x 0 is: ℙ𝑋 ≡ 𝑥0 + 𝑥1 + … + 𝑥d − 1

mod 2 = x 0 ∧ x 1 ∧...∧ x d-1.
0] Hamming weight 〈X〉 or Δ(X) of n -tuple X≡ x 1 x 2... x n, where x i ∈ [0, q ), is the number of non-zero symbols in X. Hamming distance Δ(X,Y) between n -tuples X and Y is
the number of positions i where x i ≠ y i . For vectors X and Y this is equivalent to Δ(X,Y) = 〈X - Y〉 ≡ Δ(X - Y) i.e. to Hamming weight of (X-Y). For binary strings this yields
Δ(X,Y)= 〈X∧Y〉 i.e. the Hamming weight of X∧Y.
𝑛

1] Lee distance is Λ𝑋𝑌 ≡ ∑

𝑖 = 1

min𝑥𝑖 − 𝑦 , 𝑞 − 𝑥𝑖 − 𝑦 .
𝑖

𝑖

Lee weight is: Λ(X) ≡ Λ(X,0).
2] Binary intervals (or binary tiles) are intervals of size 2k (for k = 1,2,...) such that each "tile" of size 2k starts on an integer multiple of 2k e.g. [ m ·2k, ( m +1) ·2k) for any
integer m are "binary intervals" of size 2k.
3] Cyclic group Z n : set of integers {0,1,... n -1} with integer addition modulo n as the group operation. Note that Z 2 group operation is equivalent to a single bit XOR
operation (1∧0=0∧1=1, 0∧0=1∧1=0). The same symbol Z n is also used for commutative ring with integer additions and multiplication performed mod n .
4] Product group 𝑍𝑞 = 𝑍𝑞 × 𝑍𝑞 × ⋯ × 𝑍𝑞
𝑑

( d times): extension of Z q into a d -tuple. As with Z n, 𝑍𝑞

𝑑

also denotes a commutative ring in which the Z q operations (integer +,* mod q ) are done component-wise.
5] Finite Dyadic group D d of order n =2d is abelian group consisting of all d -bit integers 0.. n -1 using bitwise XOR (∧) as the group operation. Notes: (i) for n =2d and d >2 ⇒
Z n≠ D d; (ii) D d is an instance of 𝑍 .
𝑑
2

Table 2.1
Y^X

0

1

2

3

4

5

6

7

8

9

A

B

C

D

E

F

0:

0

1

2

3

4

5

6

7

8

9

A

B

C

D

E

F

:0

1:

1

0

3

2

5

4

7

6

9

8

B

A

D

C

F

E

:1

2:

2

3

0

1

6

7

4

5

A

B

8

9

E

F

C

D

:2

3:

3

2

1

0

7

6

5

4

B

A

9

8

F

E

D

C

:3

4:

4

5

6

7

0

1

2

3

C

D

E

F

8

9

A

B

:4

5:

5

4

7

6

1

0

3

2

D

C

F

E

9

8

B

A

:5

6:

6

7

4

5

2

3

0

1

E

F

C

D

A

B

8

9

:6

7:

7

6

5

4

3

2

1

0

F

E

D

C

B

A

9

8

:7

8:

8

9

A

B

C

D

E

F

0

1

2

3

4

5

6

7

:8

9

9

8

B

A

D

C

F

E

1

0

3

2

5

4

7

6

:9

A:

A

B

8

9

E

F

C

D

2

3

0

1

6

7

4

5

:A

B:

B

A

9

8

F

E

D

C

3

2

1

0

7

6

5

4

:B

C:

C

D

E

F

8

9

A

B

4

5

6

7

0

1

2

3

:C

D:

D

C

F

E

9

8

B

A

5

4

7

6

1

0

E:

E

F

C

D

A

B

8

9

6

7

4

5

2

3

0

1

F:

F

E

D

C

B

A

9

8

7

6

5

4

3

2

1

0

1

2

3

4

5

7

8

9

A

B

C

D

E

F

6

3

2

:D

:E
0

:F

6] Table 2.1 illustrates the group operation table for group D 4 with n = 24=16 elements 0, 1, 2,... F (all numbers are in base 16). Table entry in row Y and column X is the
result of bitwise X∧Y operation.
A. Matrices and Vectors in Dirac Notation
7] Dirac notation (also called "bra-ket" notation, [13]) is a mnemonic notation which encapsulates common matrix operations and properties in a streamlined, visually
intuitive form.
8] Matrix [A r,c] (also: [A] or just A) is a rectangular table with r rows and c columns of "matrix elements". An element on i -th row and j -th column of a matrix [A] is denoted
as [A]i,j. Identity matrix n × n is denoted as I n or I. Matrices with r = 1 or c = 1, row or column vectors, are denoted as follows:
𝑦

Row vector (bra): 〈 X | ≡ (x 1 x 2...xn ) Column vector (ket): |𝑌 〉 ≡

𝑦

1

2

⋮
𝑦

𝑛

https://patents.google.com/patent/EP2708000B1/en?q=(radix-3)&oq=radix-3

34/51

2/14/26, 2:05 PM

EP2708000B1 - Flexible radix switching network - Google Patents
𝑦

Inner (scalar) product: 〈 𝑋 | 𝑌 〉 ≡

𝑥1

𝑥2

⋯

𝑥𝑛

1

𝑦

⋅

𝑛
2

≡ ∑
𝑖 = 1

⋮
𝑦

𝑦

1

𝑦 𝑥1

𝑦 𝑥2

…

𝑦 𝑥𝑐

𝑦 𝑥1

𝑦 𝑥2

…

𝑦 𝑥𝑐

…

…

…

𝑦 𝑥2

…

1

Outer product: |𝑌 〉 〈 𝑋| ≡

𝑦
2

𝑥1

𝑥2

⋯

𝑥𝑐

≡

1

2

⋮
𝑦

𝑥𝑖 𝑦

𝑖

= "number"

𝑛

2

𝑦 𝑥1
𝑟

𝑟

1
2

…

= "matrix"

𝑦 𝑥𝑐
𝑟

𝑟

Translation bra ↔ ket × real matrix A: |u〉 = A|v) ⇔ 〈u| = 〈v|A T

i -th "canonical basis" bra vector: 〈ei | ≡ (0102 ... 0i-1 1 i 0 i+1 ... 0 n )
General "orthonormal basis" {B} ≡ {|bi 〉: i = 1..n}: 〈bi |bj 〉 ≡ δi,j
Orthogonal matrix U: UU T=I n, orthonormal bases {B},{C}: U = Σ i |bi 〉 〈ci |
Projector (matrix) onto the i -th canonical axis: P i ≡ |ei 〉 〈ei |
Projector (matrix) onto any normalized (〈u|u〉 = 1) vector the |u〉: P i ≡ |u〉 〈u|
Component (vector) of 〈 X | along axis 〈ei |: 〈 X |P i = 〈 X |ei 〉 · 〈ei | = 〈ei | xi
𝑛

"Resolution of identity" in any basis {B}: 𝐼𝑛 = ∑

|𝑏 〉 〈 𝑏 |
𝑖

𝑖 = 1

𝑖

9] The above examples illustrate a rationale for Dirac notation: product expressions of the form with two "pointy" ends such as <...> are always scalars (numbers), while
products of the form with two flat ends |...>...<...| are always matrices. Mixed ends products (those with one pointy and one flat end) such as <...| or |...> are always row or
column vectors. Due to associativity of matrix products, these "object type rules" are valid however many other matrix or vector factors may be inside and outside of the
selected sub-product of a given type. Also, the "resolution of identity" sums ∑|bi 〉〈bi | can be freely inserted between any two adjacent bars ('flat ends') within a large
product, further aiding in the breakup of longer chains of matrices into scalars. Such rules of thumb often suggest, purely visually, quick, mistake-proof simplifications
e.g. any scalars spotted as ...<...>... pattern can be immediately factored out.
B. Hadamard Matrices and Walsh Functions
0] Hadamard matrix H n (or H) is a square n × n matrix defined by equation 𝐻𝑛 𝐻𝑛 = 𝑛𝐼𝑛 .
T

Of interest here are the Sylvester type of H n matrices characterized by the size constraint n ≡ 2d. Under this constraint the H n matrices can be constructed recursively
(equivalent to Kronecker products of H 2) as follows [14]:
𝐻

=
2

1

1

1

−1

𝐻

=
2⁢
𝑛

𝐻𝑛

𝐻𝑛

𝐻𝑛

−𝐻𝑛

≡ 𝐻

https://patents.google.com/patent/EP2708000B1/en?q=(radix-3)&oq=radix-3

2

⊗ 𝐻𝑛 ≡ 𝐻

⊗𝑑 1
2

35/51

2/14/26, 2:05 PM

EP2708000B1 - Flexible radix switching network - Google Patents

1] The pattern of H 32 ( d =5) is shown in Table 2.2 with '-1' elements shown as '-' and coordinates in base 16.

2] From the construction eq. (2.1) of H (where n ≡ 2d) it follows that H is a symmetric matrix:
n
n
Symmetry:

𝐻𝑖, 𝑗 = 𝐻𝑗, 𝑖

3] Walsh function U k( x ) for k =0..n -1, x =0..n -1, is defined as the k -th row of H n. By virtue of H n symmetry, eq. (2.2), the k -th column of H n is also equal to U k( x ). The
row and column forms of U k( x ) can also be used as the n -dimensional bra/ket or row/column vectors 〈U k| and |U k〉. Some properties of U k( x ) are:
Orthogonality:
{
〈 𝑈𝑗 |𝑈𝑘 〉 = 𝑛 ⋅ 𝛿𝑗, 𝑘 =

𝑛

for 𝑗 = 𝑘

0

for 𝑗 ≠ 𝑘

Symmetry:
𝑈𝑘 𝑥 = 𝑈𝑥 𝑘

Function values:

𝑑−1

∑

𝑈0 𝑥 = −1

𝜇 = 0

𝑘 𝑥
𝜇

𝑑−1
𝜇

∑

= −1

𝜇 = 0

0⋅𝑥

𝜇

0

= −1

= 1,

∀

𝑛−1

∑ 𝑈𝑘 𝑥 = 0 for 𝑘 = 1 . . 𝑛 − 1
𝑥 = 0

𝑑−1

4] The exponent ∑

𝑘 𝑥

𝜇 = 0

𝜇

𝜇

in eq. (2.5) uses binary digits k µ and x µ of d -bit integers k and x . When this sum is even number U k( x ) is 1 and when the sum is odd number U k( x ) is -1. The second
equality in eq. (2.5) expresses the same results via parity function ℙ𝑘&𝑥,

where k&x is a bitwise AND of integers k and x . For example U 14(15)=(-1) from the table Fig. 1. Binary forms for k and x are: k =14=01110 and x =15=01111. The sum in
𝑑−1

the exponent is ∑

𝜇 = 0

𝑘 𝜇 𝑥𝜇

https://patents.google.com/patent/EP2708000B1/en?q=(radix-3)&oq=radix-3

36/51

2/14/26, 2:05 PM

EP2708000B1 - Flexible radix switching network - Google Patents
3

1

= 0·0+1·1+1·1+1·1+0·1 = 3 ⇒ U 14(15) = (-1) = (-1) = -1. The parity approach uses k & x = 01110 & 01111 = 01110 yielding exponent ℙ01110 = 0∧ 1∧ 1∧ 1∧ 0 = 1

and U 14(15)=(-1)1 = -1 i.e. the same result as the one obtained via the sum formula.
5] For efficiency, the LH network computations use mostly binary (also called boolean) form of U k and H n denoted respectively as W k and [W n]. When both forms are used
in the same context, the U k and H n forms are referred to as algebraic forms. Binary form is obtained from the algebraic form via mappings 1→0 and -1 → 1. Denoting
algebraic values as a and binary values as b, the translations between the two are:
Algebraic

a

Binary

b
𝑏 ≡

1−𝑎
2

-1

1
1

0

and 𝑎 = 1 − 2⁢
𝑏

6] The symmetry eq. (2.4) and function values eq. (2.5) become for the binary form W k( x ):
Symmetry:
𝑊𝑘 𝑥 = 𝑊𝑥 𝑘

Function values:

7] Binary Walsh functions W k( x ) are often treated as length n bit strings, which for k =1.. n -1 have exactly n /2 zeros and n /2 ones. In the bit string form one can perform
bitwise Boolean operations on W k as length n bit strings. Their XOR property will be useful for the LH computations:
𝑊

∧
𝑗

𝑊

𝑘

= 𝑊 ∧

𝑗 𝑘

i.e. the set {W k} ≡ {W k: k =0.. n -1} is closed with respect to bitwise XOR (denoted as ∧) operation and it forms a group of n -bit strings isomorphic to the dyadic group D d
of their indices k ( d -bit strings).
8] Table 2.3 below shows the binary form of Hadamard (also called Walsh) matrix [W 32] obtained via mapping eq. (2.8) from H 32 in Table 2.2 (binary 0's are shown as '-').

C. Error Correcting Codes
9] Error correcting coding (ECC) is a large variety of techniques for adding redundancy to messages in order to detect or correct errors in the decoding phase. Of interest for
the LH network construction are the linear EC codes, which are the most developed and in practice the most important type of ECC [15],[16].
40] Message X is a sequence of k symbols x 1, x 2,..., x k from alphabet A of size q ≥ 2 i.e. x i can be taken to be integers with values in interval [0, q ). EC code for X is a
codeword Y which is a sequence y 1, y 2,..., y n of n > k symbols from A*. The encoding procedure translates all messages from some set {X} of all possible messages
into codewords from some set {Y}. For block codes the sizes of the sets {X} and {Y} are q k i.e. messages are arbitrary k -symbol sequences. The excess symbols n-k > 0
in Y represent coding redundancy or "check bits" that support detection or correction of errors during decoding of Y into X.
41] For ECC algorithmic purposes, the set A is augmented with additional mathematical structure, beyond merely that of a bare set of q elements A. The common
augmentation is to consider symbols x i and y i to be elements of a Galois field GF( q ) where q ≡ p m for some prime p and some integer m ≥1 (this condition on q is a

https://patents.google.com/patent/EP2708000B1/en?q=(radix-3)&oq=radix-3

37/51

2/14/26, 2:05 PM

EP2708000B1 - Flexible radix switching network - Google Patents

necessary condition in order to augment a bare set A into a finite field F q). Codewords Y are then a subset of all n -tuples 𝐹𝑞

𝑛

over the field GF( q ). The GF( q ) field arithmetic (i.e. the + and scalar ·) for the n -tuples 𝐹𝑞

𝑛

is done component-wise i.e. 𝐹𝑞

𝑛

is n -dimensional vector space

over GF(q).
42] Linear EC codes are a special case of the above n -tuple 𝐹𝑞

𝑛

structure of codewords, in which the set {Y} of all codewords is a k -dimensional vector subspace (or span)

of

Hence, if two n -tuples Y 1 and Y 2 are codewords, then the n -tuple Y 3=Y 1+Y 2 is also a codeword. The number of distinct codewords Y in

is

This linear code is denoted in ECC convention as [ n,k ]q code, or just [ n,k ] code when q is understood from the context or otherwise unimportant in a context.
43] A particular [ n, k ] code can be defined by specifying k linearly independent n -dimensional row vectors 〈 g i | = (g i,1 g i,2...g i,n ) for i =1.. k , which are used to define the k ×

n "generator matrix" [G] of the [ n,k ] code as follows ([16] p. 84):
* More generally message X and codeword Y can use different alphabets, but this generality merely complicates exposition without adding anything useful for the LH
construction.
〈𝑔 |

𝑘

𝑔

1

𝐺 ≡ ∑ |𝑒𝑖 〉 〈 𝑔 | =

⋯

𝑖

𝑖 = 1

=

〈𝑔 |

𝑔

1,1

1,2

⋯

⋯

⋯

⋯

𝑔

𝑔

⋯

𝑘

𝑘, 1

𝑘, 2

𝑔

1, 𝑛

⋯
𝑔

𝑘, 𝑛

44] Encoding of a message X ≡ 〈X| ≡ ( x 1, x 2,..., x k) into the codeword Y ≡ 〈Y| ≡ ( y 1, y 2,..., y n) is:
𝑘

𝑘

𝑘

〈 𝑌| ≡ 〈 𝑋|𝐺 = ∑ 𝑥𝑗 〈 𝑒𝑗 | ∑ |𝑒𝑖 〉 〈 𝑔 | =
𝑖

𝑗 = 1

𝑖 = 1

𝑘

∑ 𝑥𝑗 𝛿𝑗, 𝑖 〈 𝑔 | = ∑ 𝑥𝑖 〈 𝑔 |
𝑖

𝑖, 𝑗 = 1

𝑖

𝑖 = 1

45] Individual component (symbol) y s (where s =1.. n ) of the codeword Y is then via eqs. (2.20)-(2.21):
𝑘

𝑦

𝑠

≡ 〈 𝑌|𝑒

𝑠

𝑘

〉 = ∑ 𝑥 〈 𝑔 |𝑒 〉 = ∑ 𝑥 𝑔
𝑖
𝑠
𝑖
𝑖

𝑖 = 1

𝑖, 𝑠

𝑖 = 1

46] The k × n matrix [G k,n] is called systematic generator iff the original message X = x 1, x 2,..., x k occurs as a substring of the output codeword Y. The systematic
generators [G] combine a k × k identity matrix I k as a sub-matrix of [G] i.e. [G] typically has a form [I k| A k,n-k] or [A k,n-k |I k], yielding unmodified substring X as a prefix or
a suffix of Y, which simplifies encoding and decoding operations. The remaining n-k symbols of Y are then called parity check symbols.
47] The choice of vectors 〈 g i | used to construct [G] depends on type of errors that the [ n , k ] code is supposed to detect or correct. For the most common assumption in
ECC theory, the independent random errors for symbols of codeword Y, the best choice of 〈 gi| are those that maximize the minimum Hamming distance Δ(Y 1,Y 2)
among all pairs (Y 1,Y 2) of codewords. Defining minimum codeword distance via:

the [ n , k ]q code is often denoted as [ n , k ,Δ]q or [ n , k ,Δ] code. The optimum choice for vectors 〈 g i | maximizes Δ for given n , k and q. The tables of optimum and near
optimum [ n , k ,Δ]q codes have been computed over decades for wide ranges of free parameters n, k and q (e.g. see web repository [17]).
48] Table 2.4 ([16] p. 34) illustrates optimum [7,4,3]2 code i.e. a systematic binary code with n = 7 bit codewords each containing 3 parity check bits, k =4 message bits
(appearing as suffix in the codeword Y ), with minimum distance Δ=3, thus capable of correcting all 1-bit errors and detecting all 2-bit errors.

49] Quantity closely related to Δ, and of importance for LH construction, is the minimum non-zero codeword weight w min defined via Hamming weight (Y) (the number of
non-zero symbols in Y ) as follows:

https://patents.google.com/patent/EP2708000B1/en?q=(radix-3)&oq=radix-3

38/51

2/14/26, 2:05 PM

EP2708000B1 - Flexible radix switching network - Google Patents

0] The property of w min (cf. Theorem 3.1, p. 83 in [16]) of interest is that for any linear code [ n , k ,Δ]q:
𝑤min = Δ

1] Hence, the construction of optimal [ n , k ,Δ)q codes (maximizing Δ) is a problem of finding k-dimensional subspace

of an n -dimensional space 𝐹𝑞

𝑛

which maximizes w min. Note also that since any set of k linearly independent vectors 〈 g i | (a basis) from

generates (spans) the same space

of q k vectors Y, w min and Δ are independent of the choice of the basis {〈 g i |: i = 1.. k}. Namely by virtue of uniqueness of expansion of all q k vectors Y ∈

in any basis and pigeonhole principle, the change of basis merely permutes the mapping X→Y, retaining exactly the same set of q k vectors of

. D. Graphs: Terms and Notation
• Γ(V,E)

Graph Γ with vertices V ={v 1,v 2,... v n} and edges E ={ε1, ε2,... εc}

• degree of v

Number of edges (links) connected to node v

• Γ1□Γ2

Cartesian product of graphs Γ1 and Γ2 (syn. "product graph")

• Γ□n

(Cartesian) n -th power of graph Γ

• εk=(v i,v j)

Edge εk connects vertices v i and v j

•vi∼vj

Vertices v i and v j are connected

•vi≁vj

Vertices v i and v j are not connected

• [A]

Adjacency matrix of a graph: [A]i,j≡A( i , j ) ≡ [v i ∼ v j]: 1 if v i ∼ v j, 0 if vi ≁ v j.
Number of ones on a row r (or column c) is the degree of node r (or c)

• A(i,j)=A(j,i)

Symmetry property of [A] (for undirected graphs)

•

Cycle graph: A ring with n vertices (syn. n -ring)

•

Path graph: n -ring with one link broken i.e. a line with n vertices (syn. n -path)
d-dimensional hypercube (syn. d -cube): ℙ

•

□ d

= ℙ

2

□ ℙ
2

2

□ … □ ℙ2

( d times)
•

Folded d -cube: d -cube with extra link on each long diagonal (see Table 4.4)

2] Cayley Graph Cay(G n , S m ), where: G n is a group with n elements {g 1≡I 0, g 2,... g n} and S m, called generator set, is a subset of G n with m elements: S m = { h 1, h 2,... h
m} such that (cf. [18] chap. 5):

(i) for any h ∈ S m ⇒ h -1 ∈ S m (i.e. S m contains inverse of any of its elements)
(ii) S m does not contain identity element (denoted as I 0) g 1 of G n*
3] Construction: Vertex set V of Cay(Gn, Sm ) is V≡{ g 1, g 2,... g n} and the edge set is E ≡{( g 1, g i ·h s), ∀ i , s }. In words, each vertex g i is connected to m vertices g i· h s for

s =1.. m. Generating elements h s are called here "hops" since for identity element g 1 ≡ I 0 ("root node") their group action is precisely the single hop transition from the
root node g 1 to its 1-hop neighbors h 1, h 2,... h m ∈ V (Gn ).
* The requirement for inverse h -1 to be in S m applies to undirected Cayley graphs, not to directed graphs. The exclusion of identity S m applies to graphs that have no
self-loops of a node to itself (i.e. a vertex v ∼ v ). These restrictions are not essential but mere conveniences of the 'preferred embodiment'.
4] The construction of ℚ

3

= Cay𝐷 𝑆
3

3

is illustrated in Fig. 10. Group is the 8 element Dyadic group D3 and the 3 generators h 1=001, h 2=010 and h 3=100 are shown with arrows indicating the group action
(XORs node labels with generators; all labels are in binary) on vertex v 1=000. The resulting graph is a 3-cube.
E. Properties of Matrices
5] This section lists several results about matrices (cf. [19]) needed in LH construction. All matrices below will be assumed to be real (rather than complex valued matrices).
6] M 1) Square n × n real matrix A is called normal matrix ([19] p. 100) iff it satisfies relation:
AA

T

= 𝐴

T

𝐴

7] This implies that any symmetrical (real) matrix S is normal matrix (since S=S T, hence SS T=S 2=ST S).
8] M 2) Any real, symmetrical n × n matrix [S] has n real eigenvalues λ i ( i =1.. n ) and the n corresponding orthonormal eigenvectors: |vi 〉 for i =1.. n (cf. [19] p.101):
𝑆|𝑣𝑖 〉 = 𝜆 |𝑣𝑖 〉
𝑖

for 𝑖 = 1 . . 𝑛

https://patents.google.com/patent/EP2708000B1/en?q=(radix-3)&oq=radix-3

39/51

2/14/26, 2:05 PM

EP2708000B1 - Flexible radix switching network - Google Patents
〈 𝑣𝑖 |𝑣𝑗 〉 = 𝛿𝑖, 𝑗

9]

M 3) Since set {|vi 〉} is a complete orthonormal set of vectors (a basis in

), any [S] from (M2) can be diagonalized via an orthogonal n × n matrix" [U] (orthogonal matrix

is defined via condition [U][U T]=I n) which can be constructed as follows (applying eqs. (2.41)-(2.42)):
𝑛

𝑈 ≡ ∑ |𝑒𝑖 〉 〈 𝑣𝑖 |
𝑖 = 1

𝑛

𝑈𝑆𝑈

T

𝑛

= ∑ |𝑒 〉 〈 𝑣 |𝑆 ∑ |𝑣 〉 〈 𝑒 | =
𝑖
𝑖
𝑗
𝑗
𝑖 = 1

𝑗 = 1

𝑛

∑ |𝑒𝑖 〉 〈 𝑒𝑗 |𝜆𝑗 𝛿𝑖, 𝑗
𝑖, 𝑗 = 1

𝑛

= ∑ 𝜆𝑖 |𝑒𝑖 〉 〈 𝑒𝑖 |
𝑖 = 1

0] The final sum in (2.44) is a diagonalized form of [S], with λ i's along main diagonal and 0's elsewhere.
1] M 4) A set of m symmetric, pairwise commuting matrices 𝐹m ≡ 𝑆r : 𝑆r 𝑆t = 𝑆t 𝑆r for 𝑡, 𝑟 = 1 . . 𝑛

is called commuting family (cf. [19] p. 51). For each commuting family

there is an orthonormal set of n vectors (eigenbasis in

) {|vi 〉} which are simultaneously eigenvectors of all 𝑆r ∈ 𝐹m

(cf. [19] p. 52).
2] M 5) Labeling the n eigenvalues of the symmetric matrix S from (M 1) as: λ min = λ 1 ≤ λ 2 ≤··· ≤ λ n = λ max, then the following equalities hold (Rayleigh-Ritz theorem, [19] p.
176):

References
3]

1. Taming the Flying Cable Monster: A Topology Design and Optimization Framework for Data-Center Networks J. Mudigonda, P. Yalagandula, J.C. Mogul (HP),
(slides) USENIX ATC-11, June 14. 2011, pp. 101-114
2. Network Topology Analysis D. S. Lee, J. L. Kalb (Sandia National Laboratories) Sandia Report SAND2008-0069, Jan 2008
3. Flattened butterfly: a cost-efficient topology for high-radix networks J. Kim, W. J. Dally, D. Abts (Stanford-Google), Proc. ISCA'07, May 2007, pp. 126-137
High-Radix Interconnection Networks J. Kim, PhD thesis, Stanford University, 2008.
4. High Performance Datacenter Networks: Architectures. Algorithms, and Opportunities D. Abts, J. Kim (Stanford-Google) Synthesis Lectures on .
5. Energy Proportional Datacenter Networks D. Abts , M. Marty, P. Wells, P. Klausler, H. Liu (Google), Proc. ISCA'10, June 2010, pp. 338-347
6. BCube: A High Performance. Server-centric Network Architecture for Modular Data Centers C. Guo, G. Lu, D. Li, H. Wu, X. Zhang, Y. Shi, C. Tian, Y. Zhang, and S.
Lu (Microsoft) Proc. SIGCOMM, 2009, pp. 63-74
7. MDCube: A High Performance Network Structure for Modular Data Center Interconnection H. Wu, G. Lu, D. Li, C. Guo, Y. Zhang (Microsoft) Proc. SIGCOMM,
2009, pp. 25-36
8. DCell: A Scalable and Fault-Tolerant Network Structure for Data Centers C. Guo, H. Wu, K. Tan, L. Shi, Y. Zhang, S. Lu (Microsoft) Proc. SIGCOMM, 2008, pp. 7586
9. HyperX: Topology. Routing, and Packaging of Efficient Large-Scale Networks J. H. Ahn, N. Binkert, M. M. L. Al Davis, R. S. Schreiber (HP) SC09 Nov 2009, pp. 1420
10. Technology-Driven, Highly-Scalable Dragonfly Topology J. Kim, W. J. Dally, S. Scott, D. Abts (Stanford-Google) Proc. ISCA'08, 2008, pp. 77-88
11. A Group-Theoretic Model for Symmetric Interconnection Networks S. B. Akers, B. Krishnamurthy IEEE Transactions on Computers, pp. 555-566, April, 1989
12. Optimal network topologies: Expanders. Cages. Ramanuian graphs. Entangled networks and all that L. Donetti, F. Neri, M. A. Munoz May 2006, arXiv:condmat/0605565v2 [cond-mat.other] url: http://arxiv.org/abs/cond-mat/0605565
13. Bra-Ket notation Wikipedia article (includes a link to the full text of Berkeley lecture notes), March 2012 url: http://en.wikipedia.org/wiki/Bra-ket_notation url:
http://bohr.physics.berkeley.edu/classes/221/1112/notes/hilbert.pdf
14. Matters Computational (Algorithms for Programmers) Jörg Arndt (c) 2011, Springer, ISBN 978-3-642-14763-0 Dec 2010 online edition, url:
http://www.jjj.de/fxt/fxtbook.pdf
15. The Theory of Error-Correcting Codes F. J. MacWilliams, N. J. A. Sloane (c) 1977 by North-Holland Publishing Co., ISBN: 0-444-85009-0
16. Error Correction Coding, Mathematical Methods and Algorithms T. K. Moon (c) 2005 by John Wiley & Sons, Inc., ISBN 0-471-64800-0
17. Code Tables A. E. Brouwer, M. Grassl Web repository 2012, url: http://www.codetables.de/
18. Representation Theory of Finite Groups B. Steinberg (c) 2011, Springer, ISBN 978-1-4614-0775-1
19. Matrix Analysis R. A. Horn, C. R. Johnson (c) 1985 Cambridge Univ. Press, 1990 edition, ISBN 0-521-30586-1
20. Compressive Sensing Resources C.S. web repository by Rice university http://dsp.rice.edu/cs
21. Ordered Orthogonal Arrays and Where to Find Them R. Schürer University of Salzburg PhD thesis 2006 http://mint.sbg.ac.at/rudi/projects/corrected_diss.pdf
22. MinT Database (Digital Nets. Orthogonal Arrays and Linear Codes) W. C. Schmid, R. Schürer url: http://mint.sbg.ac.at/index.php

https://patents.google.com/patent/EP2708000B1/en?q=(radix-3)&oq=radix-3

40/51

2/14/26, 2:05 PM

EP2708000B1 - Flexible radix switching network - Google Patents

23. Walsh Transforms, Balanced Sum Theorems and Partition Coefficients over Multary Alphabets M. T. Iglesias, A. Verschoren, B. Naudts, C. Vidal Proc. GECCO
'05 (Genetic and evolutionary computation), 2005

Patent Citations (11)
Publication number

Priority date

Publication date

Assignee

Title

US20090005981A1 *

2007-06-28

2009-01-01

Apple Inc.

Integration of Map Services and User Applications in a Mobile Device

WO1989012861A1 *

1988-06-20

1989-12-28

United States Department Of
Energy

Interconnection networks

US5684959A *

1995-04-19

1997-11-04

Hewlett-Packard Company

Method for determining topology of a network

CA2190425A1 *

1995-11-16

1997-05-17

Nicholas W. Dawes

Method of determining the topology of a network of objects

US6046988A *

1995-11-16

2000-04-04

Loran Network Systems Llc

Method of determining the topology of a network of objects

US5844887A *

1995-11-30

1998-12-01

Scorpio Communications
Ltd.

ATM switching fabric

US5793975A *

1996-03-01

1998-08-11

Bay Networks Group, Inc.

Ethernet topology change notification and nearest neighbor determination

US6697338B1 *

1999-10-28

2004-02-24

Lucent Technologies Inc.

Determination of physical topology of a communication network

JP4163023B2 *

2003-02-28

2008-10-08

三菱電機株式会社

Parity check matrix generation method and parity check matrix generation
apparatus

US7369513B1 *

2003-05-16

2008-05-06

Cisco Technology, Inc.

Method and apparatus for determining a network topology based on
Spanning-tree-Algorithm-designated ports

DE60332501D1 *

2003-05-28

2010-06-17

Mitsubishi Electric Corp

NEW TRANSFER CONTROL METHOD AND COMMUNICATION DEVICE

Family To Family Citations

* Cited by examiner, † Cited by third party

Non-Patent Citations (2)
Title
MOHAMMAD AL-FARES ET AL: "A scalable, commodity data center network architecture", COMPUTER COMMUNICATION REVIEW, ACM, NEW YORK, NY, US, vol. 38, no. 4, 17
August 2008 (2008-08-17), pages 63 - 74, XP058393102, ISSN: 0146-4833, DOI: 10.1145/1402946.1402967 *
RATKO V TOMIC: "Network Throughput Optimization via Error Correcting Codes", 17 January 2013 (2013-01-17), XP055478609, Retrieved from the Internet
<URL:https://arxiv.org/ftp/arxiv/papers/1301/1301.4177.pdf> [retrieved on 20180525] *
* Cited by examiner, † Cited by third party

Cited By (73)
Publication number

Priority date

Publication date

Assignee

Title

US8300798B1

2006-04-03

2012-10-30

Wai Wu

Intelligent communication routing system and method

US9008510B1

2011-05-12

2015-04-14

Google Inc.

Implementation of a large-scale multi-stage non-blocking optical circuit switch

JP5790312B2 *

2011-08-25

2015-10-07

富士通株式会社

COMMUNICATION METHOD, COMMUNICATION DEVICE, AND
COMMUNICATION PROGRAM

KR20140088069A *

2011-10-26

2014-07-09

인터내셔널 비지네스 머신
즈 코포레이션

Optimising data transmission in a hypercube network

US9515920B2 *

2012-04-20

2016-12-06

Futurewei Technologies,
Inc.

Name-based neighbor discovery and multi-hop service discovery in
information-centric networks

US9473422B1 *

2012-05-09

2016-10-18

Google Inc.

Multi-stage switching topology

US8983816B2 *

2012-06-18

2015-03-17

International Business
Machines Corporation

Efficient evaluation of network robustness with a graph

US9197509B1

2013-03-15

2015-11-24

Google Inc.

Logical topology in a dynamic data center network

US9817933B2 *

2013-03-15

2017-11-14

The Regents Of The
University Of California

Systems and methods for switching using hierarchical networks

Family To Family Citations

https://patents.google.com/patent/EP2708000B1/en?q=(radix-3)&oq=radix-3

41/51

2/14/26, 2:05 PM

EP2708000B1 - Flexible radix switching network - Google Patents

US9363204B2

2013-04-22

2016-06-07

Nant Holdings Ip, Llc

Harmonized control planes, systems and methods

US9246760B1

2013-05-29

2016-01-26

Google Inc.

System and method for reducing throughput loss responsive to network
expansion

CN103516613A *

2013-09-25

2014-01-15

汉柏科技有限公司

Quick message forwarding method

US9548960B2

2013-10-06

2017-01-17

Mellanox Technologies
Ltd.

Simplified packet routing

US9166692B1 *

2014-01-28

2015-10-20

Google Inc.

Network fabric reconfiguration

US10218538B1 *

2014-01-30

2019-02-26

Google Llc

Hybrid Clos-multidimensional topology for data center networks

US9678800B2

2014-01-30

2017-06-13

International Business
Machines Corporation

Optimum design method for configuration of servers in a data center
environment

NO2776466T3

2014-02-13

2018-01-20

CN105141434B

2014-05-26

2019-03-26

华为技术有限公司

Service chain fault detection method and device

US9729473B2

2014-06-23

2017-08-08

Mellanox Technologies,
Ltd.

Network high availability using temporary re-routing

US9703738B2 *

2014-06-24

2017-07-11

Palo Alto Research Center
Incorporated

Computing system framework with unified storage, processing, and network
switching fabrics incorporating network switches and method for making and
using the same

US9806994B2

2014-06-24

2017-10-31

Mellanox Technologies,
Ltd.

Routing via multiple paths with efficient traffic distribution

CN105337866B *

2014-06-30

2019-09-20

华为技术有限公司

A flow switching method and device

US9699067B2

2014-07-22

2017-07-04

Mellanox Technologies,
Ltd.

Dragonfly plus: communication over bipartite node groups connected by a
mesh network

CN104156282A *

2014-08-15

2014-11-19

上海斐讯数据通信技术有限
公司

System image file backup system and method

US9690734B2 *

2014-09-10

2017-06-27

Arjun Kapoor

Quasi-optimized interconnection network for, and method of, interconnecting
nodes in large-scale, parallel systems

CN105704180B *

2014-11-27

2019-02-26

英业达科技有限公司

Configuration method and system of data center network

GB2557433B *

2014-12-24

2019-05-01

Airties Kablosuz Iletism
Sanayi Ve Disticaret As

Mesh islands

EP3975429A1

2015-02-22

2022-03-30

Flex Logix Technologies,
Inc.

Mixed-radix and/or mixed-mode switch matrix architecture and integrated
circuit

US9894005B2

2015-03-31

2018-02-13

Mellanox Technologies,
Ltd.

Adaptive routing controlled by source node

US9973435B2

2015-12-16

2018-05-15

Mellanox Technologies Tlv
Ltd.

Loopback-free adaptive routing

US9699078B1 *

2015-12-29

2017-07-04

International Business
Machines Corporation

Multi-planed unified switching topologies

KR102389028B1 *

2016-01-04

2022-04-22

한국전자통신연구원

Apparatus and method for high speed data transfer between virtual desktop

US9893950B2 *

2016-01-27

2018-02-13

International Business
Machines Corporation

Switch-connected HyperX network

US10819621B2

2016-02-23

2020-10-27

Mellanox Technologies Tlv
Ltd.

Unicast forwarding of adaptive-routing notifications

US10225185B2

2016-04-18

2019-03-05

International Business
Machines Corporation

Configuration mechanisms in a switchless network

US10225153B2 *

2016-04-18

2019-03-05

International Business
Machines Corporation

Node discovery mechanisms in a switchless network

US10218601B2

2016-04-18

2019-02-26

International Business
Machines Corporation

Method, system, and computer program product for configuring an attribute for
propagating management datagrams in a switchless network

US10178029B2

2016-05-11

2019-01-08

Mellanox Technologies Tlv
Ltd.

Forwarding of adaptive routing notifications

JP6623939B2 *

2016-06-06

2019-12-25

富士通株式会社

Information processing apparatus, communication procedure determination
method, and communication program

US9780948B1 *

2016-06-15

2017-10-03

ISARA Corporation

Generating integers for cryptographic protocols

https://patents.google.com/patent/EP2708000B1/en?q=(radix-3)&oq=radix-3

42/51

2/14/26, 2:05 PM

EP2708000B1 - Flexible radix switching network - Google Patents

CN106126315A *

2016-06-17

2016-11-16

广东工业大学

A kind of virtual machine distribution method in the data center of
minimization communication delay

US10034407B2 *

2016-07-22

2018-07-24

Intel Corporation

Storage sled for a data center

US10225103B2

2016-08-29

2019-03-05

Vmware, Inc.

Method and system for selecting tunnels to send network traffic through

US10681131B2

2016-08-29

2020-06-09

Vmware, Inc.

Source network address translation detection and dynamic tunnel creation

CA3038147A1

2016-09-26

2018-03-29

Nant Holdings Ip, Llc

Virtual circuits in cloud networks

CN106533777B *

2016-11-29

2018-08-10

广东工业大学

Method and system are determined based on the intelligent transformer
substation information flow path of matrix ranks

US10263883B2 *

2016-12-14

2019-04-16

International Business
Machines Corporation

Data flow configuration in hybrid system of silicon and micro-electromechanical-switch (MEMS) elements

US10200294B2

2016-12-22

2019-02-05

Mellanox Technologies Tlv
Ltd.

Adaptive routing based on flow-control credits

US10614055B2 *

2016-12-29

2020-04-07

Emc Ip Holding Cimpany
Llc

Method and system for tree management of trees under multi-version
concurrency control

JP6834771B2 *

2017-05-19

2021-02-24

富士通株式会社

Communication device and communication method

US10862755B2 *

2017-06-30

2020-12-08

Oracle International
Corporation

High-performance data repartitioning for cloud-scale clusters

CN109327409B *

2017-07-31

2020-09-18

华为技术有限公司

Data center network DCN, method and switch for transmitting traffic in DCN

US10855656B2 *

2017-09-15

2020-12-01

Palo Alto Networks, Inc.

Fine-grained firewall policy enforcement using session app ID and endpoint
process ID correlation

US10931637B2 *

2017-09-15

2021-02-23

Palo Alto Networks, Inc.

Outbound/inbound lateral traffic punting based on process risk

FR3076142A1 *

2017-12-21

2019-06-28

Bull Sas

METHOD AND SERVER OF TOPOLOGICAL ADDRESS ALLOCATION TO
NETWORK SWITCHES, COMPUTER PROGRAM AND CLUSTER OF
CORRESPONDING SERVERS

US12063273B2

2018-02-05

2024-08-13

Microsoft Technology
Licensing, Llc.

Server system

US10809926B2

2018-02-05

2020-10-20

Microsoft Technology
Licensing, Llc

Server system

CN110139325B *

2018-02-09

2021-08-13

华为技术有限公司

A kind of network parameter tuning method and device

US10644995B2

2018-02-14

2020-05-05

Mellanox Technologies Tlv
Ltd.

Adaptive routing in a box

US11005724B1

2019-01-06

2021-05-11

Mellanox Technologies,
Ltd.

Network topology having minimal number of long connections among groups
of network elements

JP7437722B2

2019-01-31

2024-02-26

コネクトフリー株式会社

Data transmission method, communication processing method, device, and
communication processing program

CN110719170B *

2019-08-30

2021-04-16

南京航空航天大学

A Bit-Level Image Encryption Method Based on Compressed Sensing and
Optimized Coupled Image Lattice

US11184245B2

2020-03-06

2021-11-23

International Business
Machines Corporation

Configuring computing nodes in a three-dimensional mesh topology

US10812264B1 *

2020-04-30

2020-10-20

ISARA Corporation

Traversing a zigzag path tree topology in a supersingular isogeny-based
cryptosystem

US12380360B2 *

2020-05-26

2025-08-05

Nec Corporation

Interpretable imitation learning via prototypical option discovery for decision
making

US11948077B2 *

2020-07-02

2024-04-02

Dell Products L.P.

Network fabric analysis

US11575594B2

2020-09-10

2023-02-07

Mellanox Technologies,
Ltd.

Deadlock-free rerouting for resolving local link failures using detour paths

US11411911B2

2020-10-26

2022-08-09

Mellanox Technologies,
Ltd.

Routing across multiple subnetworks using address mapping

US11870682B2

2021-06-22

2024-01-09

Mellanox Technologies,
Ltd.

Deadlock-free local rerouting for handling multiple local link failures in
hierarchical network topologies

US11765103B2

2021-12-01

2023-09-19

Mellanox Technologies,
Ltd.

Large-scale network with high port utilization

US12155563B2

2022-09-05

2024-11-26

Mellanox Technologies,
Ltd.

Flexible per-flow multipath managed by sender-side network adapter

https://patents.google.com/patent/EP2708000B1/en?q=(radix-3)&oq=radix-3

43/51

2/14/26, 2:05 PM

EP2708000B1 - Flexible radix switching network - Google Patents

US12328251B2

2022-09-08

2025-06-10

Mellano Technologies, Ltd.

Marking of RDMA-over-converged-ethernet (RoCE) traffic eligible for adaptive
routing

US12452194B2 *

2023-07-03

2025-10-21

Amazon Technologies, Inc.

Network architecture with harmonic connections

* Cited by examiner, † Cited by third party, ‡ Family to family citation

Similar Documents
Publication

Publication Date

Title

EP2708000B1

2020-03-25

Flexible radix switching network

US7957385B2

2011-06-07

Method and apparatus for packet routing

US7957400B2

2011-06-07

Hierarchical network topology

Prisacari et al.

2013

Bandwidth-optimal all-to-all exchanges in fat tree networks

US20050044195A1

2005-02-24

Network topology having nodes interconnected by extended diagonal links

US20100250784A1

2010-09-30

Addressing Scheme and Message Routing for a Networked Device

EP2619682A1

2013-07-31

Transpose box based network scaling

Dominicini et al.

2020

Polka: Polynomial key-based architecture for source routing in network fabrics

Zhao et al.

2017

Scalable SDN architecture with distributed placement of controllers for WAN

US20110202682A1

2011-08-18

Network structure for data center unit interconnection

Das et al.

2022

Shufflecast: An optical, data-rate agnostic, and low-power multicast architecture for next-generation compute clusters

HK1196190B

2021-02-11

Flexible radix switching network

HK1196190A

2014-12-05

Flexible radix switching network

Li et al.

2015

Permutation generation for routing in BCube connected crossbars

Tomic

2013

Optimal networks from error correcting codes

Du et al.

2023

Hamiltonian properties of HCN and BCN networks

Castillo

2021

A comprehensive DCell network topology model for a data center

Wang et al.

2022

GravCPA: controller placement algorithm based on traffic gravitation in SDN

Li et al.

2015

ABCCC: An advanced cube based network for data centers

Tomic

2013

Network Throughput Optimization via Error Correcting Codes

Sue

2008

An enhanced universal N x N fully nonblocking quantum switch

Tasoulas et al.

2016

Fast hybrid network reconfiguration for large-scale lossless interconnection networks

Huang et al.

2012

SCautz: a high performance and fault-tolerant datacenter network for modular datacenters

Wang

2015

Bandwidth-efficiency-oriented topology optimization for integrated switching systems based on circulant graphs

Wu et al.

2024

CSVRF: A CAM‐based popularity‐aware egress group‐caching scheme for SVRF‐based packet forward engines

Priority And Related Applications
Applications Claiming Priority (3)
Application

Filing date

US201161483687P

2011-05-08

US201161483686P

2011-05-08

PCT/US2012/036960

2012-05-08

Title

Flexible radix switching network

Legal Events

https://patents.google.com/patent/EP2708000B1/en?q=(radix-3)&oq=radix-3

44/51

2/14/26, 2:05 PM

EP2708000B1 - Flexible radix switching network - Google Patents

Date

Code

Title

Description

2014-02-14

PUAI

Public reference made under article 153(3) epc to a published international application that has entered the european phase

Free format text: ORIGINAL
CODE: 0009012

2014-03-19

17P

Request for examination filed

Effective date: 20131209

2014-03-19

AK

Designated contracting states

Kind code of ref document
A1
Designated state(s): AL AT
BE BG CH CY CZ DE DK EE
ES FI FR GB GR HR HU IE IS
IT LI LT LU LV MC MK MT N
NO PL PT RO RS SE SI SK S
TR

2014-08-13

DAX

Request for extension of the european patent (deleted)

2014-12-05

REG

Reference to a national code

Ref country code: HK
Ref legal event code: DE
Ref document number:
1196190
Country of ref document: H

2015-06-10

RAP1

Party data changed (applicant data changed or rights of an application transferred)

Owner name: J. SCOTT
BENSON LIVING TRUST

2015-07-15

RAP1

Party data changed (applicant data changed or rights of an application transferred)

Owner name: THE J. SCOT
BENSON LIVING TRUST

2016-09-28

17Q

First examination report despatched

Effective date: 20160824

2017-01-06

STAA

Information on the status of an ep patent application or granted ep patent

Free format text: STATUS:
EXAMINATION IS IN
PROGRESS

2019-08-21

REG

Reference to a national code

Ref country code: DE
Ref legal event code: R079
Ref document number:
602012068714
Country of ref document: D
Free format text: PREVIOU
MAIN CLASS:
H04L0012560000
Ipc: H04L0012240000

2019-09-25

RIC1

Information provided on ipc code assigned before grant

Ipc: H04L 12/931
20130101ALI20190821BH
Ipc: H04L 12/24
20060101AFI20190821BH

2019-10-16

GRAP

Despatch of communication of intention to grant a patent

Free format text: ORIGINAL
CODE: EPIDOSNIGR1

2019-10-16

STAA

Information on the status of an ep patent application or granted ep patent

Free format text: STATUS:
GRANT OF PATENT IS
INTENDED

2019-11-13

INTG

Intention to grant announced

Effective date: 20191017

2020-02-17

GRAS

Grant fee paid

Free format text: ORIGINAL
CODE: EPIDOSNIGR3

2020-02-21

GRAA

(expected) grant

Free format text: ORIGINAL
CODE: 0009210

2020-02-21

STAA

Information on the status of an ep patent application or granted ep patent

Free format text: STATUS:
THE PATENT HAS BEEN

https://patents.google.com/patent/EP2708000B1/en?q=(radix-3)&oq=radix-3

45/51

2/14/26, 2:05 PM

EP2708000B1 - Flexible radix switching network - Google Patents
GRANTED

2020-03-25

AK

Designated contracting states

Kind code of ref document
B1
Designated state(s): AL AT
BE BG CH CY CZ DE DK EE
ES FI FR GB GR HR HU IE IS
IT LI LT LU LV MC MK MT N
NO PL PT RO RS SE SI SK S
TR

2020-03-25

REG

Reference to a national code

Ref country code: GB
Ref legal event code: FG4D

2020-04-15

REG

Reference to a national code

Ref country code: AT
Ref legal event code: REF
Ref document number:
1249838
Country of ref document: A
Kind code of ref document
Effective date: 20200415
Ref country code: IE
Ref legal event code: FG4D

2020-04-16

REG

Reference to a national code

Ref country code: DE
Ref legal event code: R096
Ref document number:
602012068714
Country of ref document: D

2020-07-31

PG25

Lapsed in a contracting state [announced via postgrant information from national office to epo]

Ref country code: NO
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20200625
Ref country code: RS
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20200325
Ref country code: FI
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20200325

2020-08-31

PG25

Lapsed in a contracting state [announced via postgrant information from national office to epo]

Ref country code: HR
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20200325
Ref country code: GR
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO

https://patents.google.com/patent/EP2708000B1/en?q=(radix-3)&oq=radix-3

46/51

2/14/26, 2:05 PM

EP2708000B1 - Flexible radix switching network - Google Patents
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20200626
Ref country code: LV
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20200325
Ref country code: SE
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20200325
Ref country code: BG
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20200625

2020-09-02

REG

Reference to a national code

Ref country code: NL
Ref legal event code: MP
Effective date: 20200325

2020-09-10

REG

Reference to a national code

Ref country code: LT
Ref legal event code: MG4

2020-09-30

PG25

Lapsed in a contracting state [announced via postgrant information from national office to epo]

Ref country code: NL
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20200325

2020-10-30

PG25

Lapsed in a contracting state [announced via postgrant information from national office to epo]

Ref country code: CZ
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20200325
Ref country code: IS
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20200725
Ref country code: RO
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20200325
Ref country code: SM
Free format text: LAPSE
BECAUSE OF FAILURE TO

https://patents.google.com/patent/EP2708000B1/en?q=(radix-3)&oq=radix-3

47/51

2/14/26, 2:05 PM

EP2708000B1 - Flexible radix switching network - Google Patents
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20200325
Ref country code: EE
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20200325
Ref country code: PT
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20200818
Ref country code: LT
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20200325
Ref country code: SK
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20200325

2020-11-15

REG

Reference to a national code

Ref country code: AT
Ref legal event code: MK05
Ref document number:
1249838
Country of ref document: A
Kind code of ref document
Effective date: 20200325

2021-01-12

REG

Reference to a national code

Ref country code: DE
Ref legal event code: R097
Ref document number:
602012068714
Country of ref document: D

2021-01-29

PG25

Lapsed in a contracting state [announced via postgrant information from national office to epo]

Ref country code: LI
Free format text: LAPSE
BECAUSE OF NON-PAYMEN
OF DUE FEES
Effective date: 20200531
Ref country code: AT
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20200325
Ref country code: DK
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO

https://patents.google.com/patent/EP2708000B1/en?q=(radix-3)&oq=radix-3

48/51

2/14/26, 2:05 PM

EP2708000B1 - Flexible radix switching network - Google Patents
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20200325
Ref country code: CH
Free format text: LAPSE
BECAUSE OF NON-PAYMEN
OF DUE FEES
Effective date: 20200531
Ref country code: ES
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20200325
Ref country code: IT
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20200325
Ref country code: MC
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20200325

2021-02-12

PLBE

No opposition filed within time limit

Free format text: ORIGINAL
CODE: 0009261

2021-02-12

STAA

Information on the status of an ep patent application or granted ep patent

Free format text: STATUS:
NO OPPOSITION FILED
WITHIN TIME LIMIT

2021-02-26

PG25

Lapsed in a contracting state [announced via postgrant information from national office to epo]

Ref country code: PL
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20200325

2021-03-17

26N

No opposition filed

Effective date: 20210112

2021-03-19

REG

Reference to a national code

Ref country code: BE
Ref legal event code: MM
Effective date: 20200531

2021-03-31

PG25

Lapsed in a contracting state [announced via postgrant information from national office to epo]

Ref country code: LU
Free format text: LAPSE
BECAUSE OF NON-PAYMEN
OF DUE FEES
Effective date: 20200508

2021-04-30

PG25

Lapsed in a contracting state [announced via postgrant information from national office to epo]

Ref country code: IE
Free format text: LAPSE
BECAUSE OF NON-PAYMEN
OF DUE FEES
Effective date: 20200508

2021-05-31

PG25

Lapsed in a contracting state [announced via postgrant information from national office to epo]

https://patents.google.com/patent/EP2708000B1/en?q=(radix-3)&oq=radix-3

Ref country code: BE

49/51

2/14/26, 2:05 PM

EP2708000B1 - Flexible radix switching network - Google Patents
Free format text: LAPSE
BECAUSE OF NON-PAYMEN
OF DUE FEES
Effective date: 20200531
Ref country code: SI
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20200325

2021-11-26

REG

Reference to a national code

Ref country code: DE
Ref legal event code: R079
Ref document number:
602012068714
Country of ref document: D
Free format text: PREVIOU
MAIN CLASS:
H04L0012240000
Ipc: H04L0041000000

2022-06-01

PG25

Lapsed in a contracting state [announced via postgrant information from national office to epo]

Ref country code: TR
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20200325
Ref country code: MT
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20200325
Ref country code: CY
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20200325

2022-06-30

PG25

Lapsed in a contracting state [announced via postgrant information from national office to epo]

Ref country code: MK
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20200325
Ref country code: AL
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20200325

2022-07-29

PGFP

Annual fee paid to national office [announced via postgrant information from national office to epo]

Ref country code: GB
Payment date: 20220520
Year of fee payment: 11
Ref country code: FR
Payment date: 20220523

https://patents.google.com/patent/EP2708000B1/en?q=(radix-3)&oq=radix-3

50/51

2/14/26, 2:05 PM

EP2708000B1 - Flexible radix switching network - Google Patents
Year of fee payment: 11
Ref country code: DE
Payment date: 20220519
Year of fee payment: 11

2023-12-01

REG

Reference to a national code

Ref country code: DE
Ref legal event code: R119
Ref document number:
602012068714
Country of ref document: D

2024-01-24

GBPC

Gb: european patent ceased through non-payment of renewal fee

Effective date: 20230508

2024-04-30

PG25

Lapsed in a contracting state [announced via postgrant information from national office to epo]

Ref country code: DE
Free format text: LAPSE
BECAUSE OF NON-PAYMEN
OF DUE FEES
Effective date: 20231201
Ref country code: GB
Free format text: LAPSE
BECAUSE OF NON-PAYMEN
OF DUE FEES
Effective date: 20230508

2024-05-31

PG25

Lapsed in a contracting state [announced via postgrant information from national office to epo]

Ref country code: FR
Free format text: LAPSE
BECAUSE OF NON-PAYMEN
OF DUE FEES
Effective date: 20230531

Concepts
machine-extracted

Download

Name

Image

matrix material

claims,description

87

0.000

method

claims,description

86

0.000

mapping

claims,description

18

0.000

transfer

claims,description

18

0.000

distribution

claims,description

6

0.000

correction

claims,description

3

0.000

Sections

Count

Filter table

Query match

Show all concepts from the description section

Data provided by IFI CLAIMS Patent Services

About

Send Feedback

Public Datasets

https://patents.google.com/patent/EP2708000B1/en?q=(radix-3)&oq=radix-3

Terms

Privacy Policy

Help

51/51

2/14/26, 2:00 PM

EP3726389B1 - Apparatuses, methods, and systems for memory interface circuit allocation in a configurable spatial accelera…

Patents
Back to results

14 of 125,048

high radix

(high radix);

Apparatuses, methods, and systems for memory interface circuit allocation in a configurable spatial accelerator
Images (89)

EP3726389B1
European Patent Office

Download PDF

Find Prior Art

Similar

Other languages: German, French
Inventor: Kermin Chofleming, Yu Bai, Simon C. Steely Jr.
Current Assignee : Intel Corp

Classifications
G06F13/102 Program control for peripheral devices where the programme performs an
interfacing function, e.g. device driver

Worldwide applications
2019 US 2020 EP CN

G06F12/0846 Cache with multiple tag or data arrays being simultaneously accessible
Application EP20157809.3A events
G06F13/1668 Details of memory controller
2020-02-18

Application filed by Intel Corp

2020-10-21

Publication of EP3726389A1

2021-11-17

Application granted

2021-11-17

Publication of EP3726389B1

Status

Active

2040-02-18

Anticipated expiration

G06F12/0806 Multiuser, multiprocessor or multiprocessing cache systems
G06F12/0853 Cache with multiport tag or data arrays
G06F12/0857 Overlapped cache accessing, e.g. pipeline by multiple requestors
G06F12/0866 Addressing of a memory level in which the access to the desired data or data
block requires associative addressing means, e.g. caches for peripheral storage systems, e.g. disk
cache
G06F13/1673 Details of memory controller using buffers
G06F13/1684 Details of memory controller using multiple buses
G06F13/20 Handling requests for interconnection or transfer for access to input/output bus
G06F15/8007 Architectures of general purpose stored program computers comprising an

Info: Patent citations (356), Cited by (27), Legal events, Similar
documents, Priority and Related Applications
External links: Espacenet, EPO GPI, EP Register, Global Dossier,
Discuss

array of processing units with common control, e.g. single instruction multiple data processors
single instruction multiple data [SIMD] multiprocessors
G06F16/9024 Graphs; Linked lists

G06F2212/1024 Latency reduction
G06F2212/1041 Resource optimization
G06F2212/601 Reconfiguration of cache memory
Y02D10/00 Energy efficient computing, e.g. low power processors, power management or
thermal management

Hide more classifications

Landscapes

Engineering & Computer Science
Theoretical Computer Science
Show more

Hide Dependent

Claims (15)

1.

An apparatus (1600) comprising:
a spatial array of processing elements (1608);

https://patents.google.com/patent/EP3726389B1/en?q=(high+radix)&oq=high+radix&page=1

1/76

2/14/26, 2:00 PM

EP3726389B1 - Apparatuses, methods, and systems for memory interface circuit allocation in a configurable spatial accelera…

a cache (1602);
a first memory interface circuit (1606) comprising a first port into the cache (1602), a first plurality of input queues (1622) to store data for memory requests from the
spatial array of processing elements (1608), and a first memory operation register;
a second memory interface circuit (1606) comprising a second port into the cache (1602), a second plurality of input queues (1622) to store data for memory
requests from the spatial array of processing elements (1608), and a second memory operation register; and the apparatus being characterized in that it comprises
an allocator circuit (1610) to:
set respective first values into the first memory operation register and the second memory operation register according to a first allocation mode to couple the
first port to a first input queue of the first plurality of input queues (1622) that stores data for memory requests from a first processing element of the spatial array
of processing elements (1608), couple the second port to a first input queue of the second plurality of input queues (1622) that stores data for memory requests
from a second processing element of the spatial array of processing elements (1608), and couple the first port to a second input queue of the first plurality of
input queues (1622) that stores data for memory requests from a third processing element of the spatial array of processing elements (1608), and
set respective second values into the first memory operation register and the second memory operation register according to a second allocation mode to couple
the first port to the first input queue of the first plurality of input queues (1622) that stores data for memory requests from the first processing element of the
spatial array of processing elements (1608), couple the second port to the first input queue of the second plurality of input queues (1622) that stores data for
memory requests from the second processing element of the spatial array of processing elements (1608), and couple the second port to a second input queue of
the second plurality of input queues (1622) that stores data for memory requests from the third processing element of the spatial array of processing elements
(1608).
2.

The apparatus (1600) of claim 1, wherein the respective first values set in the first memory operation register and the second memory operation register causes a
first completion buffer (1624) of the first memory interface circuit (1606) to receive a completion indication from the cache (1602) for memory requests from the
first processing element, a first completion buffer (1624) of the second memory interface circuit (1606) to receive a completion indication from the cache (1602)
for memory requests from the second processing element, and a second completion buffer (1624) of the first memory interface circuit (1606) to receive a
completion indication from the cache (1602) for memory requests from the third processing element.

3.

The apparatus (1600) of claim 2, wherein the first completion buffer (1624) of the first memory interface circuit (1606) is a first proper subset of slots of a unified
completion buffer of the first memory interface circuit (1606), the second completion buffer (1624) of the first memory interface circuit (1606) is a second proper
subset of slots of the unified completion buffer of the first memory interface circuit (1606), and the allocator circuit (1610) assigns a largest number of buffer
slots of the unified completion buffer to the one of the first processing element or the third processing element that issues a largest number of memory requests
for a dataflow graph (1614).

4.

The apparatus (1600) of claim 2, wherein the first completion buffer (1624) of the first memory interface circuit (1606) is a first proper subset of slots of a unified
completion buffer of the first memory interface circuit (1606), the second completion buffer (1624) of the first memory interface circuit (1606) is a second proper
subset of slots of the unified completion buffer of the first memory interface circuit (1606), and the allocator circuit (1610) assigns a largest number of buffer
slots of the unified completion buffer to the one of the first processing element or the third processing element that has a longest latency for memory requests for
a dataflow graph (1614).

5.

The apparatus (1600) of claim 1, wherein the second allocation mode allocates input queues based on issuance by the first processing element of a largest
number of memory requests for a dataflow graph (1614), the second processing element of a next largest number of memory requests for the dataflow graph
(1614), and the third processing element of a smaller number of memory requests for the dataflow graph (1614) than the next largest number of memory
requests.

6.

The apparatus (1600) of claim 1, wherein the allocator circuit (1610) allocates a next input queue (1622) of the first memory interface circuit (1606) or the second
memory interface circuit (1606) in program order to the one of the first memory interface circuit (1606) or the second memory interface circuit (1606) with a
fewest number of memory requests assigned to its input queues for a dataflow graph (1614).

7.

The apparatus (1600) of claim 1, wherein the allocator circuit (1610) switches from the first allocation mode to the second allocation mode in runtime for a
dataflow graph (1614).

8.

The apparatus (1600) of any one of claims 1-7, wherein the first memory interface circuit (1606), when in the first allocation mode, sends a first backpressure
value to stall the first processing element from issuing an additional memory request when the first input queue (1622) of the first memory interface circuit (1606)
is not available for data for the additional memory request, the second memory interface circuit (1606), when in the first allocation mode, sends a second
backpressure value to stall the second processing element from issuing an additional memory request when the first input queue (1622) of the second memory
interface circuit (1606) is not available for data for the additional memory request, and the first memory interface circuit (1606), when in the first allocation mode,
sends a third backpressure value to stall the third processing element from issuing an additional memory request when the second input queue (1622) of the first
memory interface circuit (1606) is not available for data for the additional memory request.

9.

A method (1500) comprising:
coupling a spatial array of processing elements to a first memory interface circuit comprising a first port into a cache, a first plurality of input queues to store data for
memory requests from the spatial array of processing elements, and a first memory operation register, and to a second memory interface circuit comprising a second
port into the cache, a second plurality of input queues to store data for memory requests from the spatial array of processing elements, and a second memory
operation register (1502);
setting respective first values into the first memory operation register and the second memory operation register according to a first allocation mode to couple the
first port to a first input queue of the first plurality of input queues that stores data for memory requests from a first processing element of the spatial array of
processing elements, couple the second port to a first input queue of the second plurality of input queues that stores data for memory requests from a second
processing element of the spatial array of processing elements, and couple the first port to a second input queue of the first plurality of input queues that stores data
for memory requests from a third processing element of the spatial array of processing elements (1504); and
setting respective second values into the first memory operation register and the second memory operation register according to a second allocation mode to couple
the first port to the first input queue of the first plurality of input queues that stores data for memory requests from the first processing element of the spatial array of
processing elements, couple the second port to the first input queue of the second plurality of input queues that stores data for memory requests from the second

https://patents.google.com/patent/EP3726389B1/en?q=(high+radix)&oq=high+radix&page=1

2/76

2/14/26, 2:00 PM

EP3726389B1 - Apparatuses, methods, and systems for memory interface circuit allocation in a configurable spatial accelera…

processing element of the spatial array of processing elements, and couple the second port to a second input queue of the second plurality of input queues that
stores data for memory requests from the third processing element of the spatial array of processing elements (1506).
10.

The method (1500) of claim 9, wherein setting the respective first values in the first memory operation register and the second memory operation register (1504)
causes a first completion buffer of the first memory interface circuit to receive a completion indication from the cache for memory requests from the first
processing element, a first completion buffer of the second memory interface circuit to receive a completion indication from the cache for memory requests from
the second processing element, and a second completion buffer of the first memory interface circuit to receive a completion indication from the cache for
memory requests from the third processing element.

11.

The method (1500) of claim 10, wherein the first completion buffer of the first memory interface circuit is a first proper subset of slots of a unified completion
buffer of the first memory interface circuit, the second completion buffer of the first memory interface circuit is a second proper subset of slots of the unified
completion buffer of the first memory interface circuit, and the setting of the respective first values or the respective second values comprises assigning a largest
number of buffer slots of the unified completion buffer to the one of the first processing element or the third processing element that issues a largest number of
memory requests for a dataflow graph.

12.

The method (1500) of claim 10, wherein the first completion buffer of the first memory interface circuit is a first proper subset of slots of a unified completion
buffer of the first memory interface circuit, the second completion buffer of the first memory interface circuit is a second proper subset of slots of the unified
completion buffer of the first memory interface circuit, and the setting of the respective first values or the respective second values comprises assigning a largest
number of buffer slots of the unified completion buffer to the one of the first processing element or the third processing element that has a longest latency for
memory requests for a dataflow graph.

13.

The method (1500) of claim 9, wherein the second allocation mode allocates input queues based on issuance by the first processing element of a largest number
of memory requests for a dataflow graph, the second processing element of a next largest number of memory requests for the dataflow graph, and the third
processing element of a smaller number of memory requests for the dataflow graph than the next largest number of memory requests.

14.

The method (1500) of claim 9, wherein the setting of the respective first values or the respective second values (1504, 1506) comprises allocating a next input
queue of the first memory interface circuit or the second memory interface circuit in program order to the one of the first memory interface circuit or the second
memory interface circuit with a fewest number of memory requests assigned to its input queues for a dataflow graph.

15.

The method (1500) of any one of claims 9-14, wherein the first memory interface circuit, when in the first allocation mode, sends a first backpressure value to stall
the first processing element from issuing an additional memory request when the first input queue of the first memory interface circuit is not available for data for
the additional memory request, the second memory interface circuit, when in the first allocation mode, sends a second backpressure value to stall the second
processing element from issuing an additional memory request when the first input queue of the second memory interface circuit is not available for data for the
additional memory request, and the first memory interface circuit, when in the first allocation mode, sends a third backpressure value to stall the third processing
element from issuing an additional memory request when the second input queue of the first memory interface circuit is not available for data for the additional
memory request.

Description

TECHNICAL FIELD
[0001] The disclosure relates generally to electronics, and, more specifically, an embodiment of the disclosure relates to allocation circuitry for memory interface
circuits of a configurable spatial accelerator.
BACKGROUND
[0002] A processor, or set of processors, executes instructions from an instruction set, e.g., the instruction set architecture (ISA). The instruction set is the part of the
computer architecture related to programming, and generally includes the native data types, instructions, register architecture, addressing modes, memory
architecture, interrupt and exception handling, and external input and output (I/O). It should be noted that the term instruction herein may refer to a macroinstruction, e.g., an instruction that is provided to the processor for execution, or to a micro-instruction, e.g., an instruction that results from a processor's
decoder decoding macro-instructions.
[0003] US 2019/042513 A1 relates to circuitry to control unstructured data flow in a configurable spatial accelerator. For example, a configurable spatial accelerator
includes a first processing element that includes a configuration register within the first processing element to store a configuration value that causes the first
processing element to perform an operation according to the configuration value, a plurality of input queues, an input controller to control enqueue and dequeue
of values into the plurality of input queues according to the configuration value, a plurality of output queues, and an output controller to control enqueue and
dequeue of values into the plurality of output queues according to the configuration value.
SUMMARY
[0004] The invention is set out in the appended independent claims.
[0005] Advantageous embodiments are defined by the appended dependent claims.
BRIEF DESCRIPTION OF THE DRAWINGS
[0006] The present disclosure is illustrated by way of example and not limitation in the figures of the accompanying drawings, in which like references indicate similar
elements and in which:
Figure 1 illustrates an accelerator tile according to embodiments of the disclosure.
Figure 2 illustrates a hardware processor coupled to a memory according to embodiments of the disclosure.
Figure 3A illustrates a program source according to embodiments of the disclosure.
Figure 3B illustrates a dataflow graph for the program source of Figure 3A according to embodiments of the disclosure.
Figure 3C illustrates an accelerator with a plurality of processing elements configured to execute the dataflow graph of Figure 3B according to
embodiments of the disclosure.
Figure 4 illustrates an example execution of a dataflow graph according to embodiments of the disclosure.
Figure 5 illustrates a program source according to embodiments of the disclosure.
Figure 6 illustrates an accelerator tile comprising an array of processing elements according to embodiments of the disclosure.
Figure 7A illustrates a configurable data path network according to embodiments of the disclosure.
Figure 7B illustrates a configurable flow control path network according to embodiments of the disclosure.
Figure 8 illustrates a circuit switched network according to embodiments of the disclosure.
Figure 9 illustrates a hardware processor tile comprising an accelerator according to embodiments of the disclosure.
Figure 10 illustrates a processing element according to embodiments of the disclosure.

https://patents.google.com/patent/EP3726389B1/en?q=(high+radix)&oq=high+radix&page=1

3/76

2/14/26, 2:00 PM

EP3726389B1 - Apparatuses, methods, and systems for memory interface circuit allocation in a configurable spatial accelera…
Figure 11 illustrates a request address file (RAF) circuit according to embodiments of the disclosure.
Figure 12 illustrates another request address file (RAF) circuit according to embodiments of the disclosure.
Figure 13 illustrates a plurality of request address file (RAF) circuits coupled between a plurality of accelerator tiles and a plurality of cache banks
according to embodiments of the disclosure.
Figure 14 illustrates a plurality of request address file (RAF) circuits coupled between a plurality of accelerator tiles and a plurality of cache banks
according to embodiments of the disclosure.
Figure 15 illustrates a flow diagram according to embodiments of the disclosure.
Figure 16 illustrates a high level view of a configurable spatial accelerator and its memory sub-systems according to embodiments of the disclosure.
Figure 17 illustrates a network between RAF circuits and cache banks that utilizes bid groups for RAF circuit allocation according to embodiments of
the disclosure.
Figure 18 illustrates another request address file (RAF) circuit according to embodiments of the disclosure.
Figure 19 illustrates a software flow for allocation according to embodiments of the disclosure.
Figure 20 illustrates a data flow graph of a pseudocode function call according to embodiments of the disclosure.
Figure 21 illustrates a spatial array of processing elements with a plurality of network dataflow endpoint circuits according to embodiments of the
disclosure.
Figure 22 illustrates a network dataflow endpoint circuit according to embodiments of the disclosure.
Figure 23 illustrates data formats for a send operation and a receive operation according to embodiments of the disclosure.
Figure 24 illustrates another data format for a send operation according to embodiments of the disclosure.
Figure 25 illustrates to configure a circuit element (e.g., network dataflow endpoint circuit) data formats to configure a circuit element (e.g., network
dataflow endpoint circuit) for a send (e.g., switch) operation and a receive (e.g., pick) operation according to embodiments of the disclosure.
Figure 26 illustrates a configuration data format to configure a circuit element (e.g., network dataflow endpoint circuit) for a send operation with its
input, output, and control data annotated on a circuit according to embodiments of the disclosure.
Figure 27 illustrates a configuration data format to configure a circuit element (e.g., network dataflow endpoint circuit) for a selected operation with its
input, output, and control data annotated on a circuit according to embodiments of the disclosure.
Figure 28 illustrates a configuration data format to configure a circuit element (e.g., network dataflow endpoint circuit) for a Switch operation with its
input, output, and control data annotated on a circuit according to embodiments of the disclosure.
Figure 29 illustrates a configuration data format to configure a circuit element (e.g., network dataflow endpoint circuit) for a SwitchAny operation with
its input, output, and control data annotated on a circuit according to embodiments of the disclosure.
Figure 30 illustrates a configuration data format to configure a circuit element (e.g., network dataflow endpoint circuit) for a Pick operation with its
input, output, and control data annotated on a circuit according to embodiments of the disclosure.
Figure 31 illustrates a configuration data format to configure a circuit element (e.g., network dataflow endpoint circuit) for a PickAny operation with its
input, output, and control data annotated on a circuit according to embodiments of the disclosure.
Figure 32 illustrates selection of an operation by a network dataflow endpoint circuit for performance according to embodiments of the disclosure.
Figure 33 illustrates a network dataflow endpoint circuit according to embodiments of the disclosure.
Figure 34 illustrates a network dataflow endpoint circuit receiving input zero (0) while performing a pick operation according to embodiments of the
disclosure.
Figure 35 illustrates a network dataflow endpoint circuit receiving input one (1) while performing a pick operation according to embodiments of the
disclosure.
Figure 36 illustrates a network dataflow endpoint circuit outputting the selected input while performing a pick operation according to embodiments of
the disclosure.
Figure 37 illustrates a flow diagram according to embodiments of the disclosure.
Figure 38 illustrates a floating point multiplier partitioned into three regions (the result region, three potential carry regions, and the gated region)
according to embodiments of the disclosure.
Figure 39 illustrates an in-flight configuration of an accelerator with a plurality of processing elements according to embodiments of the disclosure.
Figure 40 illustrates a snapshot of an in-flight, pipelined extraction according to embodiments of the disclosure.
Figure 41 illustrates a compilation toolchain for an accelerator according to embodiments of the disclosure.
Figure 42 illustrates a compiler for an accelerator according to embodiments of the disclosure.
Figure 43A illustrates sequential assembly code according to embodiments of the disclosure.
Figure 43B illustrates dataflow assembly code for the sequential assembly code of Figure 43A according to embodiments of the disclosure.
Figure 43C illustrates a dataflow graph for the dataflow assembly code of Figure 43B for an accelerator according to embodiments of the disclosure.
Figure 44A illustrates C source code according to embodiments of the disclosure.
Figure 44B illustrates dataflow assembly code for the C source code of Figure 44A according to embodiments of the disclosure.
Figure 44C illustrates a dataflow graph for the dataflow assembly code of Figure 44B for an accelerator according to embodiments of the disclosure.
Figure 45A illustrates C source code according to embodiments of the disclosure.
Figure 45B illustrates dataflow assembly code for the C source code of Figure 45A according to embodiments of the disclosure.
Figure 45C illustrates a dataflow graph for the dataflow assembly code of Figure 45B for an accelerator according to embodiments of the disclosure.
Figure 46A illustrates a flow diagram according to embodiments of the disclosure.
Figure 46B illustrates a flow diagram according to embodiments of the disclosure.
Figure 47 illustrates a throughput versus energy per operation graph according to embodiments of the disclosure.
Figure 48 illustrates an accelerator tile comprising an array of processing elements and a local configuration controller according to embodiments of
the disclosure.
Figures 49A-49C illustrate a local configuration controller configuring a data path network according to embodiments of the disclosure.
Figure 50 illustrates a configuration controller according to embodiments of the disclosure.
Figure 51 illustrates an accelerator tile comprising an array of processing elements, a configuration cache, and a local configuration controller
according to embodiments of the disclosure.
Figure 52 illustrates an accelerator tile comprising an array of processing elements and a configuration and exception handling controller with a
reconfiguration circuit according to embodiments of the disclosure.
Figure 53 illustrates a reconfiguration circuit according to embodiments of the disclosure.
Figure 54 illustrates an accelerator tile comprising an array of processing elements and a configuration and exception handling controller with a
reconfiguration circuit according to embodiments of the disclosure.
Figure 55 illustrates an accelerator tile comprising an array of processing elements and a mezzanine exception aggregator coupled to a tile-level
exception aggregator according to embodiments of the disclosure.
Figure 56 illustrates a processing element with an exception generator according to embodiments of the disclosure.

https://patents.google.com/patent/EP3726389B1/en?q=(high+radix)&oq=high+radix&page=1

4/76

2/14/26, 2:00 PM

EP3726389B1 - Apparatuses, methods, and systems for memory interface circuit allocation in a configurable spatial accelera…
Figure 57 illustrates an accelerator tile comprising an array of processing elements and a local extraction controller according to embodiments of the
disclosure.
Figures 58A-58C illustrate a local extraction controller configuring a data path network according to embodiments of the disclosure.
Figure 59 illustrates an extraction controller according to embodiments of the disclosure.
Figure 60 illustrates a flow diagram according to embodiments of the disclosure.
Figure 61 illustrates a flow diagram according to embodiments of the disclosure.
Figure 62A is a block diagram of a system that employs a memory ordering circuit interposed between a memory subsystem and acceleration hardware
according to embodiments of the disclosure.
Figure 62B is a block diagram of the system of Figure 62A, but which employs multiple memory ordering circuits according to embodiments of the
disclosure.
Figure 63 is a block diagram illustrating general functioning of memory operations into and out of acceleration hardware according to embodiments of
the disclosure.
Figure 64 is a block diagram illustrating a spatial dependency flow for a store operation according to embodiments of the disclosure.
Figure 65 is a detailed block diagram of the memory ordering circuit of Figure 62 according to embodiments of the disclosure.
Figure 66 is a flow diagram of a microarchitecture of the memory ordering circuit of Figure 62 according to embodiments of the disclosure.
Figure 67 is a block diagram of an executable determiner circuit according to embodiments of the disclosure.
Figure 68 is a block diagram of a priority encoder according to embodiments of the disclosure.
Figure 69 is a block diagram of an exemplary load operation, both logical and in binary according to embodiments of the disclosure.
Figure 70A is flow diagram illustrating logical execution of an example code according to embodiments of the disclosure.
Figure 70B is the flow diagram of Figure 70A, illustrating memory-level parallelism in an unfolded version of the example code according to
embodiments of the disclosure.
Figure 71A is a block diagram of exemplary memory arguments for a load operation and for a store operation according to embodiments of the
disclosure.
Figure 71B is a block diagram illustrating flow of load operations and the store operations, such as those of Figure 71A, through the microarchitecture
of the memory ordering circuit of Figure 66 according to embodiments of the disclosure.
Figures 72A, 72B, 72C , 72D, 72E, 72F , 72G, and 72H are block diagrams illustrating functional flow of load operations and store operations for an
exemplary program through queues of the microarchitecture of Figure 72B according to embodiments of the disclosure.
Figure 73 is a flow chart of a method for ordering memory operations between a acceleration hardware and an out-of-order memory subsystem
according to embodiments of the disclosure.
Figure 74A is a block diagram illustrating a generic vector friendly instruction format and class A instruction templates thereof according to
embodiments of the disclosure.
Figure 74B is a block diagram illustrating the generic vector friendly instruction format and class B instruction templates thereof according to
embodiments of the disclosure.
Figure 75A is a block diagram illustrating fields for the generic vector friendly instruction formats in Figures 74A and 74B according to embodiments of
the disclosure.
Figure 75B is a block diagram illustrating the fields of the specific vector friendly instruction format in Figure 75A that make up a full opcode field
according to one embodiment of the disclosure.
Figure 75C is a block diagram illustrating the fields of the specific vector friendly instruction format in Figure 75A that make up a register index field
according to one embodiment of the disclosure.
Figure 75D is a block diagram illustrating the fields of the specific vector friendly instruction format in Figure 75A that make up the augmentation
operation field 7450 according to one embodiment of the disclosure.
Figure 76 is a block diagram of a register architecture according to one embodiment of the disclosure
Figure 77A is a block diagram illustrating both an exemplary in-order pipeline and an exemplary register renaming, out-of-order issue/execution pipeline
according to embodiments of the disclosure.
Figure 77B is a block diagram illustrating both an exemplary embodiment of an in-order architecture core and an exemplary register renaming, out-oforder issue/execution architecture core to be included in a processor according to embodiments of the disclosure.
Figure 78A is a block diagram of a single processor core, along with its connection to the on-die interconnect network and with its local subset of the
Level 2 (L2) cache, according to embodiments of the disclosure.
Figure 78B is an expanded view of part of the processor core in Figure 78A according to embodiments of the disclosure.
Figure 79 is a block diagram of a processor that may have more than one core, may have an integrated memory controller, and may have integrated
graphics according to embodiments of the disclosure.
Figure 80 is a block diagram of a system in accordance with one embodiment of the present disclosure.
Figure 81 is a block diagram of a more specific exemplary system in accordance with an embodiment of the present disclosure.
Figure 82 , shown is a block diagram of a second more specific exemplary system in accordance with an embodiment of the present disclosure.
Figure 83 , shown is a block diagram of a system on a chip (SoC) in accordance with an embodiment of the present disclosure.
Figure 84 is a block diagram contrasting the use of a software instruction converter to convert binary instructions in a source instruction set to binary
instructions in a target instruction set according to embodiments of the disclosure.

DETAILED DESCRIPTION
[0007] In the following description, numerous specific details are set forth. However, it is understood that embodiments of the disclosure may be practiced without
these specific details. In other instances, well-known circuits, structures and techniques have not been shown in detail in order not to obscure the understanding
of this description.
[0008] References in the specification to "one embodiment," "an embodiment," "an example embodiment," etc., indicate that the embodiment described may include a
particular feature, structure, or characteristic, but every embodiment may not necessarily include the particular feature, structure, or characteristic. Moreover,
such phrases are not necessarily referring to the same embodiment. Further, when a particular feature, structure, or characteristic is described in connection
with an embodiment, it is submitted that it is within the knowledge of one skilled in the art to affect such feature, structure, or characteristic in connection with
other embodiments whether or not explicitly described.
[0009] A processor (e.g., having one or more cores) may execute instructions (e.g., a thread of instructions) to operate on data, for example, to perform arithmetic,
logic, or other functions. For example, software may request an operation and a hardware processor (e.g., a core or cores thereof) may perform the operation in
response to the request. One non-limiting example of an operation is a blend operation to input a plurality of vectors elements and output a vector with a blended
plurality of elements. In certain embodiments, multiple operations are accomplished with the execution of a single instruction.
[0010] Exascale performance, e.g., as defined by the Department of Energy, may require system-level floating point performance to exceed 10^18 floating point
operations per second (exaFLOPs) or more within a given (e.g., 20MW) power budget. Certain embodiments herein are directed to a spatial array of processing
elements (e.g., a configurable spatial accelerator (CSA)) that targets high performance computing (HPC), for example, of a processor. Certain embodiments

https://patents.google.com/patent/EP3726389B1/en?q=(high+radix)&oq=high+radix&page=1

5/76

2/14/26, 2:00 PM

EP3726389B1 - Apparatuses, methods, and systems for memory interface circuit allocation in a configurable spatial accelera…

herein of a spatial array of processing elements (e.g., a CSA) target the direct execution of a dataflow graph to yield a computationally dense yet energy-efficient
spatial microarchitecture which far exceeds conventional roadmap architectures. Certain embodiments herein overlay (e.g., high-radix) dataflow operations on a
communications network, e.g., in addition to the communications network's routing of data between the processing elements, memory, etc. and/or the
communications network performing other communications (e.g., not data processing) operations. Certain embodiments herein are directed to a
communications network (e.g., a packet switched network) of a (e.g., coupled to) spatial array of processing elements (e.g., a CSA) to perform certain dataflow
operations, e.g., in addition to the communications network routing data between the processing elements, memory, etc. or the communications network
performing other communications operations. Certain embodiments herein are directed to network dataflow endpoint circuits that (e.g., each) perform (e.g., a
portion or all) a dataflow operation or operations, for example, a pick or switch dataflow operation, e.g., of a dataflow graph. Certain embodiments herein include
augmented network endpoints (e.g., network dataflow endpoint circuits) to support the control for (e.g., a plurality of or a subset of) dataflow operation(s), e.g.,
utilizing the network endpoints to perform a (e.g., dataflow) operation instead of a processing element (e.g., core) or arithmetic-logic unit (e.g. to perform
arithmetic and logic operations) performing that (e.g., dataflow) operation. In one embodiment, a network dataflow endpoint circuit is separate from a spatial
array (e.g. an interconnect or fabric thereof) and/or processing elements.
[0011] Below also includes a description of the architectural philosophy of embodiments of a spatial array of processing elements (e.g., a CSA) and certain features
thereof. As with any revolutionary architecture, programmability may be a risk. To mitigate this issue, embodiments of the CSA architecture have been codesigned with a compilation tool chain, which is also discussed below.
INTRODUCTION
[0012] Exascale computing goals may require enormous system-level floating point performance (e.g., 1 ExaFLOPs) within an aggressive power budget (e.g., 20 MW).
However, simultaneously improving the performance and energy efficiency of program execution with classical von Neumann architectures has become difficult:
out-of-order scheduling, simultaneous multi-threading, complex register files, and other structures provide performance, but at high energy cost. Certain
embodiments herein achieve performance and energy requirements simultaneously. Exascale computing power-performance targets may demand both high
throughput and low energy consumption per operation. Certain embodiments herein provide this by providing for large numbers of low-complexity, energyefficient processing (e.g., computational) elements which largely eliminate the control overheads of previous processor designs. Guided by this observation,
certain embodiments herein include a spatial array of processing elements, for example, a configurable spatial accelerator (CSA), e.g., comprising an array of
processing elements (PEs) connected by a set of light-weight, back-pressured (e.g., communication) networks. One example of a CSA tile is depicted in Figure 1.
Certain embodiments of processing (e.g., compute) elements are dataflow operators, e.g., multiple of a dataflow operator that only processes input data when
both (i) the input data has arrived at the dataflow operator and (ii) there is space available for storing the output data, e.g., otherwise no processing is occurring.
Certain embodiments (e.g., of an accelerator or CSA) do not utilize a triggered instruction.
[0013] Figure 1 illustrates an accelerator tile 100 embodiment of a spatial array of processing elements according to embodiments of the disclosure. Accelerator tile
100 may be a portion of a larger tile. Accelerator tile 100 executes a dataflow graph or graphs. A dataflow graph may generally refer to an explicitly parallel
program description which arises in the compilation of sequential codes. Certain embodiments herein (e.g., CSAs) allow dataflow graphs to be directly
configured onto the CSA array, for example, rather than being transformed into sequential instruction streams. Certain embodiments herein allow a first (e.g.,
type of) dataflow operation to be performed by one or more processing elements (PEs) of the spatial array and, additionally or alternatively, a second (e.g.,
different, type of) dataflow operation to be performed by one or more of the network communication circuits (e.g., endpoints) of the spatial array.
[0014] The derivation of a dataflow graph from a sequential compilation flow allows embodiments of a CSA to support familiar programming models and to directly
(e.g., without using a table of work) execute existing high performance computing (HPC) code. CSA processing elements (PEs) may be energy efficient. In Figure
1, memory interface 102 may couple to a memory (e.g., memory 202 in Figure 2) to allow accelerator tile 100 to access (e.g., load and/store) data to the (e.g., off
die) memory. Depicted accelerator tile 100 is a heterogeneous array comprised of several kinds of PEs coupled together via an interconnect network 104.
Accelerator tile 100 may include one or more of integer arithmetic PEs, floating point arithmetic PEs, communication circuitry (e.g., network dataflow endpoint
circuits), and in-fabric storage, e.g., as part of spatial array of processing elements 101. Dataflow graphs (e.g., compiled dataflow graphs) may be overlaid on the
accelerator tile 100 for execution. In one embodiment, for a particular dataflow graph, each PE handles only one or two (e.g., dataflow) operations of the graph.
The array of PEs may be heterogeneous, e.g., such that no PE supports the full CSA dataflow architecture and/or one or more PEs are programmed (e.g.,
customized) to perform only a few, but highly efficient operations. Certain embodiments herein thus yield a processor or accelerator having an array of
processing elements that is computationally dense compared to roadmap architectures and yet achieves approximately an order-of-magnitude gain in energy
efficiency and performance relative to existing HPC offerings.
[0015] Certain embodiments herein provide for performance increases from parallel execution within a (e.g., dense) spatial array of processing elements (e.g., CSA)
where each PE and/or network dataflow endpoint circuit utilized may perform its operations simultaneously, e.g., if input data is available. Efficiency increases
may result from the efficiency of each PE and/or network dataflow endpoint circuit, e.g., where each PE's operation (e.g., behavior) is fixed once per configuration
(e.g., mapping) step and execution occurs on local data arrival at the PE, e.g., without considering other fabric activity, and/or where each network dataflow
endpoint circuit's operation (e.g., behavior) is variable (e.g., not fixed) when configured (e.g., mapped). In certain embodiments, a PE and/or network dataflow
endpoint circuit is (e.g., each a single) dataflow operator, for example, a dataflow operator that only operates on input data when both (i) the input data has
arrived at the dataflow operator and (ii) there is space available for storing the output data, e.g., otherwise no operation is occurring.
[0016] Certain embodiments herein include a spatial array of processing elements as an energy-efficient and high-performance way of accelerating user applications. In
one embodiment, applications are mapped in an extremely parallel manner. For example, inner loops may be unrolled multiple times to improve parallelism. This
approach may provide high performance, e.g., when the occupancy (e.g., use) of the unrolled code is high. However, if there are less used code paths in the loop
body unrolled (for example, an exceptional code path like floating point de-normalized mode) then (e.g., fabric area of) the spatial array of processing elements
may be wasted and throughput consequently lost.
[0017] One embodiment herein to reduce pressure on (e.g., fabric area of) the spatial array of processing elements (e.g., in the case of underutilized code segments) is
time multiplexing. In this mode, a single instance of the less used (e.g., colder) code may be shared among several loop bodies, for example, analogous to a
function call in a shared library. In one embodiment, spatial arrays (e.g., of processing elements) support the direct implementation of multiplexed codes.
However, e.g., when multiplexing or demultiplexing in a spatial array involves choosing among many and distant targets (e.g., sharers), a direct implementation
using dataflow operators (e.g., using the processing elements) may be inefficient in terms of latency, throughput, implementation area, and/or energy. Certain
embodiments herein describe hardware mechanisms (e.g., network circuitry) supporting (e.g., high-radix) multiplexing or demultiplexing. Certain embodiments
herein (e.g., of network dataflow endpoint circuits) permit the aggregation of many targets (e.g., sharers) with little hardware overhead or performance impact.
Certain embodiments herein allow for compiling of (e.g., legacy) sequential codes to parallel architectures in a spatial array.
[0018] In one embodiment, a plurality of network dataflow endpoint circuits combine as a single dataflow operator, for example, as discussed in reference to Figure 21
below. As non-limiting examples, certain (for example, high (e.g., 4-6) radix) dataflow operators are listed below.
[0019] An embodiment of a "Pick" dataflow operator is to select data (e.g., a token) from a plurality of input channels and provide that data as its (e.g., single) output
according to control data. Control data for a Pick may include an input selector value. In one embodiment, the selected input channel is to have its data (e.g.,
token) removed (e.g., discarded), for example, to complete the performance of that dataflow operation (or its portion of a dataflow operation). In one
embodiment, additionally, those non-selected input channels are also to have their data (e.g., token) removed (e.g., discarded), for example, to complete the
performance of that dataflow operation (or its portion of a dataflow operation).
[0020] An embodiment of a "PickSingleLeg" dataflow operator is to select data (e.g., a token) from a plurality of input channels and provide that data as its (e.g., single)
output according to control data, but in certain embodiments, the non-selected input channels are ignored, e.g., those non-selected input channels are not to

https://patents.google.com/patent/EP3726389B1/en?q=(high+radix)&oq=high+radix&page=1

6/76

2/14/26, 2:00 PM

EP3726389B1 - Apparatuses, methods, and systems for memory interface circuit allocation in a configurable spatial accelera…

have their data (e.g., token) removed (e.g., discarded), for example, to complete the performance of that dataflow operation (or its portion of a dataflow
operation). Control data for a PickSingleLeg may include an input selector value. In one embodiment, the selected input channel is also to have its data (e.g.,
token) removed (e.g., discarded), for example, to complete the performance of that dataflow operation (or its portion of a dataflow operation).
[0021] An embodiment of a "PickAny" dataflow operator is to select the first available (e.g., to the circuit performing the operation) data (e.g., a token) from a plurality of
input channels and provide that data as its (e.g., single) output. In one embodiment, PickSingleLeg is also to output the index (e.g., indicating which of the
plurality of input channels) had its data selected. In one embodiment, the selected input channel is to have its data (e.g., token) removed (e.g., discarded), for
example, to complete the performance of that dataflow operation (or its portion of a dataflow operation). In certain embodiments, the non-selected input
channels (e.g., with or without input data) are ignored, e.g., those non-selected input channels are not to have their data (e.g., token) removed (e.g., discarded),
for example, to complete the performance of that dataflow operation (or its portion of a dataflow operation). Control data for a PickAny may include a value
corresponding to the PickAny, e.g., without an input selector value.
[0022] An embodiment of a "Switch" dataflow operator is to steer (e.g., single) input data (e.g., a token) so as to provide that input data to one or a plurality of (e.g., less
than all) outputs according to control data. Control data for a Switch may include an output(s) selector value or values. In one embodiment, the input data (e.g.,
from an input channel) is to have its data (e.g., token) removed (e.g., discarded), for example, to complete the performance of that dataflow operation (or its
portion of a dataflow operation).
[0023] An embodiment of a "SwitchAny" dataflow operator is to steer (e.g., single) input data (e.g., a token) so as to provide that input data to one or a plurality of (e.g.,
less than all) outputs that may receive that data, e.g., according to control data. In one embodiment, SwitchAny may provide the input data to any coupled output
channel that has availability (e.g., available storage space) in its ingress buffer, e.g., network ingress buffer in Figure 22. Control data for a SwitchAny may
include a value corresponding to the SwitchAny, e.g., without an output(s) selector value or values. In one embodiment, the input data (e.g., from an input
channel) is to have its data (e.g., token) removed (e.g., discarded), for example, to complete the performance of that dataflow operation (or its portion of a
dataflow operation). In one embodiment, SwitchAny is also to output the index (e.g., indicating which of the plurality of output channels) that it provided (e.g.,
sent) the input data to. SwitchAny may be utilized to manage replicated sub-graphs in a spatial array, for example, an unrolled loop.
[0024] Certain embodiments herein thus provide paradigm-shifting levels of performance and tremendous improvements in energy efficiency across a broad class of
existing single-stream and parallel programs, e.g., all while preserving familiar HPC programming models. Certain embodiments herein may target HPC such
that floating point energy efficiency is extremely important. Certain embodiments herein not only deliver compelling improvements in performance and
reductions in energy, they also deliver these gains to existing HPC programs written in mainstream HPC languages and for mainstream HPC frameworks.
Certain embodiments of the architecture herein (e.g., with compilation in mind) provide several extensions in direct support of the control-dataflow internal
representations generated by modern compilers. Certain embodiments herein are direct to a CSA dataflow compiler, e.g., which can accept C, C++, and Fortran
programming languages, to target a CSA architecture.
[0025] Figure 2 illustrates a hardware processor 200 coupled to (e.g., connected to) a memory 202 according to embodiments of the disclosure. In one embodiment,
hardware processor 200 and memory 202 are a computing system 201. In certain embodiments, one or more of accelerators is a CSA according to this
disclosure. In certain embodiments, one or more of the cores in a processor are those cores disclosed herein. Hardware processor 200 (e.g., each core thereof)
may include a hardware decoder (e.g., decode unit) and a hardware execution unit. Hardware processor 200 may include registers. Note that the figures herein
may not depict all data communication couplings (e.g., connections). One of ordinary skill in the art will appreciate that this is to not obscure certain details in
the figures. Note that a double headed arrow in the figures may not require two-way communication, for example, it may indicate one-way communication (e.g.,
to or from that component or device). Any or all combinations of communications paths may be utilized in certain embodiments herein. Depicted hardware
processor 200 includes a plurality of cores (O to N, where N may be 1 or more) and hardware accelerators (O to M, where M may be 1 or more) according to
embodiments of the disclosure. Hardware processor 200 (e.g., accelerator(s) and/or core(s) thereof) may be coupled to memory 202 (e.g., data storage device).
Hardware decoder (e.g., of core) may receive an (e.g., single) instruction (e.g., macro-instruction) and decode the instruction, e.g., into micro-instructions and/or
micro-operations. Hardware execution unit (e.g., of core) may execute the decoded instruction (e.g., macro-instruction) to perform an operation or operations.
[0026] Section 1 below discloses embodiments of CSA architecture. In particular, novel embodiments of integrating memory within the dataflow execution model are
disclosed. Section 2 delves into the microarchitectural details of embodiments of a CSA. In one embodiment, the main goal of a CSA is to support compiler
produced programs. Section 3 below examines embodiments of a CSA compilation tool chain. The advantages of embodiments of a CSA are compared to other
architectures in the execution of compiled codes in Section 4. Finally the performance of embodiments of a CSA microarchitecture is discussed in Section 5,
further CSA details are discussed in Section 6, and a summary is provided in Section 7.
1. CSA ARCHITECTURE
[0027] The goal of certain embodiments of a CSA is to rapidly and efficiently execute programs, e.g., programs produced by compilers. Certain embodiments of the
CSA architecture provide programming abstractions that support the needs of compiler technologies and programming paradigms. Embodiments of the CSA
execute dataflow graphs, e.g., a program manifestation that closely resembles the compiler's own internal representation (IR) of compiled programs. In this
model, a program is represented as a dataflow graph comprised of nodes (e.g., vertices) drawn from a set of architecturally-defined dataflow operators (e.g., that
encompass both computation and control operations) and edges which represent the transfer of data between dataflow operators. Execution may proceed by
injecting dataflow tokens (e.g., that are or represent data values) into the dataflow graph. Tokens may flow between and be transformed at each node (e.g.,
vertex), for example, forming a complete computation. A sample dataflow graph and its derivation from high-level source code is shown in Figures 3A-3C, and
Figure 5 shows an example of the execution of a dataflow graph.
[0028] Embodiments of the CSA are configured for dataflow graph execution by providing exactly those dataflow-graph-execution supports required by compilers. In
one embodiment, the CSA is an accelerator (e.g., an accelerator in Figure 2) and it does not seek to provide some of the necessary but infrequently used
mechanisms available on general purpose processing cores (e.g., a core in Figure 2), such as system calls. Therefore, in this embodiment, the CSA can execute
many codes, but not all codes. In exchange, the CSA gains significant performance and energy advantages. To enable the acceleration of code written in
commonly used sequential languages, embodiments herein also introduce several novel architectural features to assist the compiler. One particular novelty is
CSA's treatment of memory, a subject which has been ignored or poorly addressed previously. Embodiments of the CSA are also unique in the use of dataflow
operators, e.g., as opposed to lookup tables (LUTs), as their fundamental architectural interface.
[0029] Turning to embodiments of the CSA, dataflow operators are discussed next.
1.1 Dataflow Operators
[0030] The key architectural interface of embodiments of the accelerator (e.g., CSA) is the dataflow operator, e.g., as a direct representation of a node in a dataflow
graph. From an operational perspective, dataflow operators behave in a streaming or data-driven fashion. Dataflow operators may execute as soon as their
incoming operands become available. CSA dataflow execution may depend (e.g., only) on highly localized status, for example, resulting in a highly scalable
architecture with a distributed, asynchronous execution model. Dataflow operators may include arithmetic dataflow operators, for example, one or more of
floating point addition and multiplication, integer addition, subtraction, and multiplication, various forms of comparison, logical operators, and shift. However,
embodiments of the CSA may also include a rich set of control operators which assist in the management of dataflow tokens in the program graph. Examples of
these include a "pick" operator, e.g., which multiplexes two or more logical input channels into a single output channel, and a "switch" operator, e.g., which
operates as a channel demultiplexor (e.g., outputting a single channel from two or more logical input channels). These operators may enable a compiler to
implement control paradigms such as conditional expressions. Certain embodiments of a CSA may include a limited dataflow operator set (e.g., to relatively
small number of operations) to yield dense and energy efficient PE microarchitectures. Certain embodiments may include dataflow operators for complex
operations that are common in HPC code. The CSA dataflow operator architecture is highly amenable to deployment-specific extensions. For example, more

https://patents.google.com/patent/EP3726389B1/en?q=(high+radix)&oq=high+radix&page=1

7/76

2/14/26, 2:00 PM

EP3726389B1 - Apparatuses, methods, and systems for memory interface circuit allocation in a configurable spatial accelera…

complex mathematical dataflow operators, e.g., trigonometry functions, may be included in certain embodiments to accelerate certain mathematics-intensive
HPC workloads. Similarly, a neural-network tuned extension may include dataflow operators for vectorized, low precision arithmetic.
[0031] Figure 3A illustrates a program source according to embodiments of the disclosure. Program source code includes a multiplication function (func). Figure 3B
illustrates a dataflow graph 300 for the program source of Figure 3A according to embodiments of the disclosure. Dataflow graph 300 includes a pick node 304,
switch node 306, and multiplication node 308. A buffer may optionally be included along one or more of the communication paths. Depicted dataflow graph 300
may perform an operation of selecting input X with pick node 304, multiplying X by Y (e.g., multiplication node 308), and then outputting the result from the left
output of the switch node 306. Figure 3C illustrates an accelerator (e.g., CSA) with a plurality of processing elements 301 configured to execute the dataflow
graph of Figure 3B according to embodiments of the disclosure. More particularly, the dataflow graph 300 is overlaid into the array of processing elements 301
(e.g., and the (e.g., interconnect) network(s) therebetween), for example, such that each node of the dataflow graph 300 is represented as a dataflow operator in
the array of processing elements 301. For example, certain dataflow operations may be achieved with a processing element and/or certain dataflow operations
may be achieved with a communications network (e.g., a network dataflow endpoint circuit thereof). For example, a Pick, PickSingleLeg, PickAny, Switch, and/or
SwitchAny operation may be achieved with one or more components of a communications network (e.g., a network dataflow endpoint circuit thereof), e.g., in
contrast to a processing element.
[0032] In one embodiment, one or more of the processing elements in the array of processing elements 301 is to access memory through memory interface 302. In one
embodiment, pick node 304 of dataflow graph 300 thus corresponds (e.g., is represented by) to pick operator 304A, switch node 306 of dataflow graph 300 thus
corresponds (e.g., is represented by) to switch operator 306A, and multiplier node 308 of dataflow graph 300 thus corresponds (e.g., is represented by) to
multiplier operator 308A. Another processing element and/or a flow control path network may provide the control signals (e.g., control tokens) to the pick
operator 304A and switch operator 306A to perform the operation in Figure 3A. In one embodiment, array of processing elements 301 is configured to execute
the dataflow graph 300 of Figure 3B before execution begins. In one embodiment, compiler performs the conversion from Figure 3A-3B. In one embodiment, the
input of the dataflow graph nodes into the array of processing elements logically embeds the dataflow graph into the array of processing elements, e.g., as
discussed further below, such that the input/output paths are configured to produce the desired result.
1.2 Latency Insensitive Channels
[0033] Communications arcs are the second major component of the dataflow graph. Certain embodiments of a CSA describes these arcs as latency insensitive
channels, for example, in-order, back-pressured (e.g., not producing or sending output until there is a place to store the output), point-to-point communications
channels. As with dataflow operators, latency insensitive channels are fundamentally asynchronous, giving the freedom to compose many types of networks to
implement the channels of a particular graph. Latency insensitive channels may have arbitrarily long latencies and still faithfully implement the CSA architecture.
However, in certain embodiments there is strong incentive in terms of performance and energy to make latencies as small as possible. Section 2.2 herein
discloses a network microarchitecture in which dataflow graph channels are implemented in a pipelined fashion with no more than one cycle of latency.
Embodiments of latency-insensitive channels provide a critical abstraction layer which may be leveraged with the CSA architecture to provide a number of
runtime services to the applications programmer. For example, a CSA may leverage latency-insensitive channels in the implementation of the CSA configuration
(the loading of a program onto the CSA array).
[0034] Figure 4 illustrates an example execution of a dataflow graph 400 according to embodiments of the disclosure. At step 1, input values (e.g., 1 for X in Figure 3B
and 2 for Y in Figure 3B) may be loaded in dataflow graph 400 to perform a 1 ∗ 2 multiplication operation. One or more of the data input values may be static
(e.g., constant) in the operation (e.g., 1 for X and 2 for Y in reference to Figure 3B) or updated during the operation. At step 2, a processing element (e.g., on a
flow control path network) or other circuit outputs a zero to control input (e.g., multiplexer control signal) of pick node 404 (e.g., to source a one from port "0" to
its output) and outputs a zero to control input (e.g., multiplexer control signal) of switch node 406 (e.g., to provide its input out of port "0" to a destination (e.g., a
downstream processing element). At step 3, the data value of 1 is output from pick node 404 (e.g., and consumes its control signal "0" at the pick node 404) to
multiplier node 408 to be multiplied with the data value of 2 at step 4. At step 4, the output of multiplier node 408 arrives at switch node 406, e.g., which causes
switch node 406 to consume a control signal "0" to output the value of 2 from port "0" of switch node 406 at step 5. The operation is then complete. A CSA may
thus be programmed accordingly such that a corresponding dataflow operator for each node performs the operations in Figure 4. Although execution is
serialized in this example, in principle all dataflow operations may execute in parallel. Steps are used in Figure 4 to differentiate dataflow execution from any
physical microarchitectural manifestation. In one embodiment a downstream processing element is to send a signal (or not send a ready signal) (for example,
on a flow control path network) to the switch 406 to stall the output from the switch 406, e.g., until the downstream processing element is ready (e.g., has
storage room) for the output.
1.3 Memory
[0035] Dataflow architectures generally focus on communication and data manipulation with less attention paid to state. However, enabling real software, especially
programs written in legacy sequential languages, requires significant attention to interfacing with memory. Certain embodiments of a CSA use architectural
memory operations as their primary interface to (e.g., large) stateful storage. From the perspective of the dataflow graph, memory operations are similar to other
dataflow operations, except that they have the side effect of updating a shared store. In particular, memory operations of certain embodiments herein have the
same semantics as every other dataflow operator, for example, they "execute" when their operands, e.g., an address, are available and, after some latency, a
response is produced. Certain embodiments herein explicitly decouple the operand input and result output such that memory operators are naturally pipelined
and have the potential to produce many simultaneous outstanding requests, e.g., making them exceptionally well suited to the latency and bandwidth
characteristics of a memory subsystem. Embodiments of a CSA provide basic memory operations such as load, which takes an address channel and populates
a response channel with the values corresponding to the addresses, and a store. Embodiments of a CSA may also provide more advanced operations such as inmemory atomics and consistency operators. These operations may have similar semantics to their von Neumann counterparts. Embodiments of a CSA may
accelerate existing programs described using sequential languages such as C and Fortran. A consequence of supporting these language models is addressing
program memory order, e.g., the serial ordering of memory operations typically prescribed by these languages.
[0036] Figure 5 illustrates a program source (e.g., C code) 500 according to embodiments of the disclosure. According to the memory semantics of the C programming
language, memory copy (memcpy) should be serialized. However, memcpy may be parallelized with an embodiment of the CSA if arrays A and B are known to be
disjoint. Figure 5 further illustrates the problem of program order. In general, compilers cannot prove that array A is different from array B, e.g., either for the
same value of index or different values of index across loop bodies. This is known as pointer or memory aliasing. Since compilers are to generate statically
correct code, they are usually forced to serialize memory accesses. Typically, compilers targeting sequential von Neumann architectures use instruction ordering
as a natural means of enforcing program order. However, embodiments of the CSA have no notion of instruction or instruction-based program ordering as
defined by a program counter. In certain embodiments, incoming dependency tokens, e.g., which contain no architecturally visible information, are like all other
dataflow tokens and memory operations may not execute until they have received a dependency token. In certain embodiments, memory operations produce an
outgoing dependency token once their operation is visible to all logically subsequent, dependent memory operations. In certain embodiments, dependency
tokens are similar to other dataflow tokens in a dataflow graph. For example, since memory operations occur in conditional contexts, dependency tokens may
also be manipulated using control operators described in Section 1.1, e.g., like any other tokens. Dependency tokens may have the effect of serializing memory
accesses, e.g., providing the compiler a means of architecturally defining the order of memory accesses.
1.4 Runtime Services
[0037] A primary architectural considerations of embodiments of the CSA involve the actual execution of user-level programs, but it may also be desirable to provide
several support mechanisms which underpin this execution. Chief among these are configuration (in which a dataflow graph is loaded into the CSA), extraction
(in which the state of an executing graph is moved to memory), and exceptions (in which mathematical, soft, and other types of errors in the fabric are detected

https://patents.google.com/patent/EP3726389B1/en?q=(high+radix)&oq=high+radix&page=1

8/76

2/14/26, 2:00 PM

EP3726389B1 - Apparatuses, methods, and systems for memory interface circuit allocation in a configurable spatial accelera…

and handled, possibly by an external entity). Section 2.9 below discusses the properties of a latency-insensitive dataflow architecture of an embodiment of a
CSA to yield efficient, largely pipelined implementations of these functions. Conceptually, configuration may load the state of a dataflow graph into the
interconnect (and/or communications network (e.g., a network dataflow endpoint circuit thereof)) and processing elements (e.g., fabric), e.g., generally from
memory. During this step, all structures in the CSA may be loaded with a new dataflow graph and any dataflow tokens live in that graph, for example, as a
consequence of a context switch. The latency-insensitive semantics of a CSA may permit a distributed, asynchronous initialization of the fabric, e.g., as soon as
PEs are configured, they may begin execution immediately. Unconfigured PEs may backpressure their channels until they are configured, e.g., preventing
communications between configured and unconfigured elements. The CSA configuration may be partitioned into privileged and user-level state. Such a two-level
partitioning may enable primary configuration of the fabric to occur without invoking the operating system. During one embodiment of extraction, a logical view
of the dataflow graph is captured and committed into memory, e.g., including all live control and dataflow tokens and state in the graph.
[0038] Extraction may also play a role in providing reliability guarantees through the creation of fabric checkpoints. Exceptions in a CSA may generally be caused by the
same events that cause exceptions in processors, such as illegal operator arguments or reliability, availability, and serviceability (RAS) events. In certain
embodiments, exceptions are detected at the level of dataflow operators, for example, checking argument values or through modular arithmetic schemes. Upon
detecting an exception, a dataflow operator (e.g., circuit) may halt and emit an exception message, e.g., which contains both an operation identifier and some
details of the nature of the problem that has occurred. In one embodiment, the dataflow operator will remain halted until it has been reconfigured. The exception
message may then be communicated to an associated processor (e.g., core) for service, e.g., which may include extracting the graph for software analysis.
1.5 Tile-level Architecture
[0039] Embodiments of the CSA computer architectures (e.g., targeting HPC and datacenter uses) are tiled. Figures 6 and 9 show tile-level deployments of a CSA.
Figure 9 shows a full-tile implementation of a CSA, e.g., which may be an accelerator of a processor with a core. A main advantage of this architecture is may be
reduced design risk, e.g., such that the CSA and core are completely decoupled in manufacturing. In addition to allowing better component reuse, this may allow
the design of components like the CSA Cache to consider only the CSA, e.g., rather than needing to incorporate the stricter latency requirements of the core.
Finally, separate tiles may allow for the integration of CSA with small or large cores. One embodiment of the CSA captures most vector-parallel workloads such
that most vector-style workloads run directly on the CSA, but in certain embodiments vector-style instructions in the core may be included, e.g., to support legacy
binaries.
2. MICROARCHITECTURE
[0040] In one embodiment, the goal of the CSA microarchitecture is to provide a high quality implementation of each dataflow operator specified by the CSA
architecture. Embodiments of the CSA microarchitecture provide that each processing element (and/or communications network (e.g., a network dataflow
endpoint circuit thereof)) of the microarchitecture corresponds to approximately one node (e.g., entity) in the architectural dataflow graph. In one embodiment, a
node in the dataflow graph is distributed in multiple network dataflow endpoint circuits. In certain embodiments, this results in microarchitectural elements that
are not only compact, resulting in a dense computation array, but also energy efficient, for example, where processing elements (PEs) are both simple and largely
unmultiplexed, e.g., executing a single dataflow operator for a configuration (e.g., programming) of the CSA. To further reduce energy and implementation area, a
CSA may include a configurable, heterogeneous fabric style in which each PE thereof implements only a subset of dataflow operators (e.g., with a separate
subset of dataflow operators implemented with network dataflow endpoint circuit(s)). Peripheral and support subsystems, such as the CSA cache, may be
provisioned to support the distributed parallelism incumbent in the main CSA processing fabric itself. Implementation of CSA microarchitectures may utilize
dataflow and latency-insensitive communications abstractions present in the architecture. In certain embodiments, there is (e.g., substantially) a one-to-one
correspondence between nodes in the compiler generated graph and the dataflow operators (e.g., dataflow operator compute elements) in a CSA.
[0041] Below is a discussion of an example CSA, followed by a more detailed discussion of the microarchitecture. Certain embodiments herein provide a CSA that
allows for easy compilation, e.g., in contrast to an existing FPGA compilers that handle a small subset of a programming language (e.g., C or C++) and require
many hours to compile even small programs.
[0042] Certain embodiments of a CSA architecture admits of heterogeneous coarse-grained operations, like double precision floating point. Programs may be
expressed in fewer coarse grained operations, e.g., such that the disclosed compiler runs faster than traditional spatial compilers. Certain embodiments include
a fabric with new processing elements to support sequential concepts like program ordered memory accesses. Certain embodiments implement hardware to
support coarse-grained dataflow-style communication channels. This communication model is abstract, and very close to the control-dataflow representation
used by the compiler. Certain embodiments herein include a network implementation that supports single-cycle latency communications, e.g., utilizing (e.g.,
small) PEs which support single control-dataflow operations. In certain embodiments, not only does this improve energy efficiency and performance, it simplifies
compilation because the compiler makes a one-to-one mapping between high-level dataflow constructs and the fabric. Certain embodiments herein thus
simplify the task of compiling existing (e.g., C, C++, or Fortran) programs to a CSA (e.g., fabric).
[0043] Energy efficiency may be a first order concern in modern computer systems. Certain embodiments herein provide a new schema of energy-efficient spatial
architectures. In certain embodiments, these architectures form a fabric with a unique composition of a heterogeneous mix of small, energy-efficient, data-flow
oriented processing elements (PEs) (and/or a packet switched communications network (e.g., a network dataflow endpoint circuit thereof)) with a lightweight
circuit switched communications network (e.g., interconnect), e.g., with hardened support for flow control. Due to the energy advantages of each, the
combination of these components may form a spatial accelerator (e.g., as part of a computer) suitable for executing compiler-generated parallel programs in an
extremely energy efficient manner. Since this fabric is heterogeneous, certain embodiments may be customized for different application domains by introducing
new domain-specific PEs. For example, a fabric for high-performance computing might include some customization for double-precision, fused multiply-add,
while a fabric targeting deep neural networks might include low-precision floating point operations.
[0044] An embodiment of a spatial architecture schema, e.g., as exemplified in Figure 6, is the composition of light-weight processing elements (PE) connected by an
inter-PE network. Generally, PEs may comprise dataflow operators, e.g., where once (e.g., all) input operands arrive at the dataflow operator, some operation
(e.g., micro-instruction or set of micro-instructions) is executed, and the results are forwarded to downstream operators. Control, scheduling, and data storage
may therefore be distributed amongst the PEs, e.g., removing the overhead of the centralized structures that dominate classical processors.
[0045] Programs may be converted to dataflow graphs that are mapped onto the architecture by configuring PEs and the network to express the control-dataflow graph
of the program. Communication channels may be flow-controlled and fully back-pressured, e.g., such that PEs will stall if either source communication channels
have no data or destination communication channels are full. In one embodiment, at runtime, data flow through the PEs and channels that have been configured
to implement the operation (e.g., an accelerated algorithm). For example, data may be streamed in from memory, through the fabric, and then back out to
memory.
[0046] Embodiments of such an architecture may achieve remarkable performance efficiency relative to traditional multicore processors: compute (e.g., in the form of
PEs) may be simpler, more energy efficient, and more plentiful than in larger cores, and communications may be direct and mostly short-haul, e.g., as opposed to
occurring over a wide, full-chip network as in typical multicore processors. Moreover, because embodiments of the architecture are extremely parallel, a number
of powerful circuit and device level optimizations are possible without seriously impacting throughput, e.g., low leakage devices and low operating voltage.
These lower-level optimizations may enable even greater performance advantages relative to traditional cores. The combination of efficiency at the architectural,
circuit, and device levels yields of these embodiments are compelling. Embodiments of this architecture may enable larger active areas as transistor density
continues to increase.
[0047] Embodiments herein offer a unique combination of dataflow support and circuit switching to enable the fabric to be smaller, more energy-efficient, and provide
higher aggregate performance as compared to previous architectures. FPGAs are generally tuned towards fine-grained bit manipulation, whereas embodiments

https://patents.google.com/patent/EP3726389B1/en?q=(high+radix)&oq=high+radix&page=1

9/76

2/14/26, 2:00 PM

EP3726389B1 - Apparatuses, methods, and systems for memory interface circuit allocation in a configurable spatial accelera…

herein are tuned toward the double-precision floating point operations found in HPC applications. Certain embodiments herein may include a FPGA in addition to
a CSA according to this disclosure.
[0048] Certain embodiments herein combine a light-weight network with energy efficient dataflow processing elements (and/or communications network (e.g., a
network dataflow endpoint circuit thereof)) to form a high-throughput, low-latency, energy-efficient HPC fabric. This low-latency network may enable the building
of processing elements (and/or communications network (e.g., a network dataflow endpoint circuit thereof)) with fewer functionalities, for example, only one or
two instructions and perhaps one architecturally visible register, since it is efficient to gang multiple PEs together to form a complete program.
[0049] Relative to a processor core, CSA embodiments herein may provide for more computational density and energy efficiency. For example, when PEs are very small
(e.g., compared to a core), the CSA may perform many more operations and have much more computational parallelism than a core, e.g., perhaps as many as 16
times the number of FMAs as a vector processing unit (VPU). To utilize all of these computational elements, the energy per operation is very low in certain
embodiments.
[0050] The energy advantages our embodiments of this dataflow architecture are many. Parallelism is explicit in dataflow graphs and embodiments of the CSA
architecture spend no or minimal energy to extract it, e.g., unlike out-of-order processors which must re-discover parallelism each time an instruction is
executed. Since each PE is responsible for a single operation in one embodiment, the register files and ports counts may be small, e.g., often only one, and
therefore use less energy than their counterparts in core. Certain CSAs include many PEs, each of which holds live program values, giving the aggregate effect of
a huge register file in a traditional architecture, which dramatically reduces memory accesses. In embodiments where the memory is multi-ported and
distributed, a CSA may sustain many more outstanding memory requests and utilize more bandwidth than a core. These advantages may combine to yield an
energy level per watt that is only a small percentage over the cost of the bare arithmetic circuitry. For example, in the case of an integer multiply, a CSA may
consume no more than 25% more energy than the underlying multiplication circuit. Relative to one embodiment of a core, an integer operation in that CSA fabric
consumes less than 1/30th of the energy per integer operation.
[0051] From a programming perspective, the application-specific malleability of embodiments of the CSA architecture yields significant advantages over a vector
processing unit (VPU). In traditional, inflexible architectures, the number of functional units, like floating divide or the various transcendental mathematical
functions, must be chosen at design time based on some expected use case. In embodiments of the CSA architecture, such functions may be configured (e.g.,
by a user and not a manufacturer) into the fabric based on the requirement of each application. Application throughput may thereby be further increased.
Simultaneously, the compute density of embodiments of the CSA improves by avoiding hardening such functions, and instead provision more instances of
primitive functions like floating multiplication. These advantages may be significant in HPC workloads, some of which spend 75% of floating execution time in
transcendental functions.
[0052] Certain embodiments of the CSA represents a significant advance as a dataflow-oriented spatial architectures, e.g., the PEs of this disclosure may be smaller,
but also more energy-efficient. These improvements may directly result from the combination of dataflow-oriented PEs with a lightweight, circuit switched
interconnect, for example, which has single-cycle latency, e.g., in contrast to a packet switched network (e.g., with, at a minimum, a 300% higher latency). Certain
embodiments of PEs support 32-bit or 64-bit operation. Certain embodiments herein permit the introduction of new application-specific PEs, for example, for
machine learning or security, and not merely a homogeneous combination. Certain embodiments herein combine lightweight dataflow-oriented processing
elements with a lightweight, low-latency network to form an energy efficient computational fabric.
[0053] In order for certain spatial architectures to be successful, programmers are to configure them with relatively little effort, e.g., while obtaining significant power
and performance superiority over sequential cores. Certain embodiments herein provide for a CSA (e.g., spatial fabric) that is easily programmed (e.g., by a
compiler), power efficient, and highly parallel. Certain embodiments herein provide for a (e.g., interconnect) network that achieves these three goals. From a
programmability perspective, certain embodiments of the network provide flow controlled channels, e.g., which correspond to the control-dataflow graph (CDFG)
model of execution used in compilers. Certain network embodiments utilize dedicated, circuit switched links, such that program performance is easier to reason
about, both by a human and a compiler, because performance is predictable. Certain network embodiments offer both high bandwidth and low latency. Certain
network embodiments (e.g., static, circuit switching) provides a latency of 0 to 1 cycle (e.g., depending on the transmission distance.) Certain network
embodiments provide for a high bandwidth by laying out several networks in parallel, e.g., and in low-level metals. Certain network embodiments communicate
in low-level metals and over short distances, and thus are very power efficient.
[0054] Certain embodiments of networks include architectural support for flow control. For example, in spatial accelerators composed of small processing elements
(PEs), communications latency and bandwidth may be critical to overall program performance. Certain embodiments herein provide for a light-weight, circuit
switched network which facilitates communication between PEs in spatial processing arrays, such as the spatial array shown in Figure 6, and the microarchitectural control features necessary to support this network. Certain embodiments of a network enable the construction of point-to-point, flow controlled
communications channels which support the communications of the dataflow oriented processing elements (PEs). In addition to point-to-point
communications, certain networks herein also support multicast communications. Communications channels may be formed by statically configuring the
network to from virtual circuits between PEs. Circuit switching techniques herein may decrease communications latency and commensurately minimize network
buffering, e.g., resulting in both high performance and high energy efficiency. In certain embodiments of a network, inter-PE latency may be as low as a zero
cycles, meaning that the downstream PE may operate on data in the cycle after it is produced. To obtain even higher bandwidth, and to admit more programs,
multiple networks may be laid out in parallel, e.g., as shown in Figure 6.
[0055] Spatial architectures, such as the one shown in Figure 6, may be the composition of lightweight processing elements connected by an inter-PE network (and/or
communications network (e.g., a network dataflow endpoint circuit thereof)). Programs, viewed as dataflow graphs, may be mapped onto the architecture by
configuring PEs and the network. Generally, PEs may be configured as dataflow operators, and once (e.g., all) input operands arrive at the PE, some operation
may then occur, and the result are forwarded to the desired downstream PEs. PEs may communicate over dedicated virtual circuits which are formed by
statically configuring a circuit switched communications network. These virtual circuits may be flow controlled and fully back-pressured, e.g., such that PEs will
stall if either the source has no data or the destination is full. At runtime, data may flow through the PEs implementing the mapped algorithm. For example, data
may be streamed in from memory, through the fabric, and then back out to memory. Embodiments of this architecture may achieve remarkable performance
efficiency relative to traditional multicore processors: for example, where compute, in the form of PEs, is simpler and more numerous than larger cores and
communication are direct, e.g., as opposed to an extension of the memory system.
[0056] Figure 6 illustrates an accelerator tile 600 comprising an array of processing elements (PEs) according to embodiments of the disclosure. The interconnect
network is depicted as circuit switched, statically configured communications channels. For example, a set of channels coupled together by a switch (e.g.,
switch 610 in a first network and switch 611 in a second network). The first network and second network may be separate or coupled together. For example,
switch 610 may couple one or more of the four data paths (612, 614, 616, 618) together, e.g., as configured to perform an operation according to a dataflow
graph. In one embodiment, the number of data paths is any plurality. Processing element (e.g., processing element 604) may be as disclosed herein, for example,
as in Figure 10. Accelerator tile 600 includes a memory/cache hierarchy interface 602, e.g., to interface the accelerator tile 600 with a memory and/or cache. A
data path (e.g., 618) may extend to another tile or terminate, e.g., at the edge of a tile. A processing element may include an input buffer (e.g., buffer 606) and an
output buffer (e.g., buffer 608).
[0057] Operations may be executed based on the availability of their inputs and the status of the PE. A PE may obtain operands from input channels and write results to
output channels, although internal register state may also be used. Certain embodiments herein include a configurable dataflow-friendly PE. Figure 10 shows a
detailed block diagram of one such PE: the integer PE. This PE consists of several I/O buffers, an ALU, a storage register, some instruction registers, and a
scheduler. Each cycle, the scheduler may select an instruction for execution based on the availability of the input and output buffers and the status of the PE.
The result of the operation may then be written to either an output buffer or to a (e.g., local to the PE) register. Data written to an output buffer may be

https://patents.google.com/patent/EP3726389B1/en?q=(high+radix)&oq=high+radix&page=1

10/76

2/14/26, 2:00 PM

EP3726389B1 - Apparatuses, methods, and systems for memory interface circuit allocation in a configurable spatial accelera…

transported to a downstream PE for further processing. This style of PE may be extremely energy efficient, for example, rather than reading data from a complex,
multi-ported register file, a PE reads the data from a register. Similarly, instructions may be stored directly in a register, rather than in a virtualized instruction
cache.
[0058] Instruction registers may be set during a special configuration step. During this step, auxiliary control wires and state, in addition to the inter-PE network, may be
used to stream in configuration across the several PEs comprising the fabric. As result of parallelism, certain embodiments of such a network may provide for
rapid reconfiguration, e.g., a tile sized fabric may be configured in less than about 10 microseconds.
[0059] Figure 10 represents one example configuration of a processing element, e.g., in which all architectural elements are minimally sized. In other embodiments,
each of the components of a processing element is independently scaled to produce new PEs. For example, to handle more complicated programs, a larger
number of instructions that are executable by a PE may be introduced. A second dimension of configurability is in the function of the PE arithmetic logic unit
(ALU). In Figure 10, an integer PE is depicted which may support addition, subtraction, and various logic operations. Other kinds of PEs may be created by
substituting different kinds of functional units into the PE. An integer multiplication PE, for example, might have no registers, a single instruction, and a single
output buffer. Certain embodiments of a PE decompose a fused multiply add (FMA) into separate, but tightly coupled floating multiply and floating add units to
improve support for multiply-add-heavy workloads. PEs are discussed further below.
[0060] Figure 7A illustrates a configurable data path network QAG00 (e.g., of network one or network two discussed in reference to Figure 6) according to
embodiments of the disclosure. Network QAG00 includes a plurality of multiplexers (e.g., multiplexers QAG02, QAG04, QAG06) that may be configured (e.g., via
their respective control signals) to connect one or more data paths (e.g., from PEs) together. Figure 7B illustrates a configurable flow control path network
QAG01 (e.g., network one or network two discussed in reference to Figure 6) according to embodiments of the disclosure. A network may be a light-weight PEto-PE network. Certain embodiments of a network may be thought of as a set of composable primitives for the construction of distributed, point-to-point data
channels. Figure 7A shows a network that has two channels enabled, the bold black line and the dotted black line. The bold black line channel is multicast, e.g., a
single input is sent to two outputs. Note that channels may cross at some points within a single network, even though dedicated circuit switched paths are
formed between channel endpoints. Furthermore, this crossing may not introduce a structural hazard between the two channels, so that each operates
independently and at full bandwidth.
[0061] Implementing distributed data channels may include two paths, illustrated in Figures 7A-7B. The forward, or data path, carries data from a producer to a
consumer. Multiplexors may be configured to steer data and valid bits from the producer to the consumer, e.g., as in Figure 7A. In the case of multicast, the data
will be steered to multiple consumer endpoints. The second portion of this embodiment of a network is the flow control or backpressure path, which flows in
reverse of the forward data path, e.g., as in Figure 7B. Consumer endpoints may assert when they are ready to accept new data. These signals may then be
steered back to the producer using configurable logical conjunctions, labelled as (e.g., backflow) flowcontrol function in Figure 7B. In one embodiment, each
flowcontrol function circuit may be a plurality of switches (e.g., muxes), for example, similar to Figure 7A. The flow control path may handle returning control
data from consumer to producer. Conjunctions may enable multicast, e.g., where each consumer is ready to receive data before the producer assumes that it
has been received. In one embodiment, a PE is a PE that has a dataflow operator as its architectural interface. Additionally or alternatively, in one embodiment a
PE may be any kind of PE (e.g., in the fabric), for example, but not limited to, a PE that has an instruction pointer, triggered instruction, or state machine based
architectural interface.
[0062] The network may be statically configured, e.g., in addition to PEs being statically configured. During the configuration step, configuration bits may be set at each
network component. These bits control, for example, the multiplexer selections and flow control functions. A network may comprise a plurality of networks, e.g.,
a data path network and a flow control path network. A network or plurality of networks may utilize paths of different widths (e.g., a first width, and a narrower or
wider width). In one embodiment, a data path network has a wider (e.g., bit transport) width than the width of a flow control path network. In one embodiment,
each of a first network and a second network includes their own data path network and flow control path network, e.g., data path network A and flow control
path network A and wider data path network B and flow control path network B.
[0063] Certain embodiments of a network are bufferless, and data is to move between producer and consumer in a single cycle. Certain embodiments of a network are
also boundless, that is, the network spans the entire fabric. In one embodiment, one PE is to communicate with any other PE in a single cycle. In one
embodiment, to improve routing bandwidth, several networks may be laid out in parallel between rows of PEs.
[0064] Relative to FPGAs, certain embodiments of networks herein have three advantages: area, frequency, and program expression. Certain embodiments of networks
herein operate at a coarse grain, e.g., which reduces the number configuration bits, and thereby the area of the network. Certain embodiments of networks also
obtain area reduction by implementing flow control logic directly in circuitry (e.g., silicon). Certain embodiments of hardened network implementations also
enjoys a frequency advantage over FPGA. Because of an area and frequency advantage, a power advantage may exist where a lower voltage is used at
throughput parity. Finally, certain embodiments of networks provide better high-level semantics than FPGA wires, especially with respect to variable timing, and
thus those certain embodiments are more easily targeted by compilers. Certain embodiments of networks herein may be thought of as a set of composable
primitives for the construction of distributed, point-to-point data channels.
[0065] In certain embodiments, a multicast source may not assert its data valid unless it receives a ready signal from each sink. Therefore, an extra conjunction and
control bit may be utilized in the multicast case.
[0066] Like certain PEs, the network may be statically configured. During this step, configuration bits are set at each network component. These bits control, for
example, the multiplexer selection and flow control function. The forward path of our network requires some bits to swing its muxes. In the example shown in
Figure 7A, four bits per hop are required: the east and west muxes utilize one bit each, while the southbound multiplexer utilize two bits. In this embodiment, four
bits may be utilized for the data path, but 7 bits may be utilized for the flow control function (e.g., in the flow control path network). Other embodiments may
utilize more bits, for example, if a CSA further utilizes a north-south direction. The flow control function may utilize a control bit for each direction from which
flow control can come. This may enables the setting of the sensitivity of the flow control function statically. The table 1 below summarizes the Boolean algebraic
implementation of the flow control function for the network in Figure 7B, with configuration bits capitalized. In this example, seven bits are utilized. Table 1: Flow
Implementation
readyToEast

(EAST_WEST_SENSITIVE+readyFromWest) *
(EAST_SOUTH_SENSITIVE+readyFromSouth)

readyToWest

(WEST_EAST_SENSITIVE+readyFromEast) *
(WEST_SOUTH_SENSITIVE+readyFromSouth)

readyToNorth

(NORTH_WEST_SENSITIVE+readyFromWest) *
(NORTH_EAST_SENSITIVE+readyFromEast) *
(NORTH_SOUTH_SENSITIVE+readyFromSouth)

For the third flow control box from the left in Figure 7B, EAST_WEST_SENSITIVE and NORTH_SOUTH_SENSITIVE are depicted as set to implement the flow
control for the bold line and dotted line channels, respectively.
[0067] Figure 8 illustrates a circuit switched network 800 according to embodiments of the disclosure. Circuit switched network 800 is coupled to a CSA component
(e.g., a processing element (PE)) 802, and may likewise couple to other CSA component(s) (e.g., PEs), for example, over one or more channels that are created
from switches (e.g., multiplexers) 804-828. This may include horizontal (H) switches and/or vertical (V) switches. Depicted switches may be switches in Figure
6. Switches may include one or more registers 804A-828A to store the control values (e.g., configuration bits) to control the selection of input(s) and/or output(s)
of the switch to allow values to pass from an input(s) to an output(s). In one embodiment, the switches are selectively coupled to one or more of networks 830

https://patents.google.com/patent/EP3726389B1/en?q=(high+radix)&oq=high+radix&page=1

11/76

2/14/26, 2:00 PM

EP3726389B1 - Apparatuses, methods, and systems for memory interface circuit allocation in a configurable spatial accelera…

(e.g., sending data to the right (east (E))), 832 (e.g., sending data downwardly (south (S))), 834 (e.g., sending data to the left (west (W))), and/or 836 (e.g.,
sending data upwardly (north (N))). Networks 830, 832, 834, and/or 836 may be coupled to another instance of the components (or a subset of the components)
in Figure 8, for example, to create flow controlled communications channels (e.g., paths) which support communications between components (e.g., PEs) of a
configurable spatial accelerator (e.g., a CSA as discussed herein). In one embodiment, a network (e.g., networks 830, 832, 834, and/or 836 or a separate
network) receive a control value (e.g., configuration bits) from a source (e.g., a core) and cause that control value (e.g., configuration bits) to be stored in
registers 804A-828A to cause the corresponding switches 804-828 to form the desired channels (e.g., according to a dataflow graph). Processing element 802
may also include control register(s) 802A, for example, as operation configuration register 919 in Figure 9. Switches and other components may thus be set in
certain embodiments to create data path or data paths between processing elements and/or backpressure paths for those data paths, e.g., as discussed herein.
In one embodiment, the values (e.g., configuration bits) in these (control) registers 804A-828A are depicted with variables names that refer to the mux selection
for the inputs, for example, with the values having a number which refers to the port number, and a letter which refers to the direction or PE output the data is
coming from, e.g., where E1 in 806A refers to port number 1 coming from the east side of the network.
[0068] The network(s) may be statically configured, e.g., in addition to PEs being statically configured during configuration for a dataflow graph. During the
configuration step, configuration bits may be set at each network component. These bits may control, for example, the multiplexer selections to control the flow
of a dataflow token (e.g., on a data path network) and its corresponding backpressure token (e.g., on a flow control path network). A network may comprise a
plurality of networks, e.g., a data path network and a flow control path network. A network or plurality of networks may utilize paths of different widths (e.g., a
first width, and a narrower or wider second width). In one embodiment, a data path network has a wider (e.g., bit transport) width than the width of a flow control
path network. In one embodiment, each of a first network and a second network includes their own data paths and flow control paths, e.g., data path A and flow
control path A and wider data path B and flow control path B. For example, a data path and flow control path for a single output buffer of a producer PE that
couples to a plurality of input buffers of consumer PEs. In one embodiment, to improve routing bandwidth, several networks are laid out in parallel between rows
of PEs. Like certain PEs, the network may be statically configured. During this step, configuration bits may be set at each network component. These bits control,
for example, the data path (e.g., multiplexer created data path) and/or flow control path (e.g., multiplexer created flow control path). The forward (e.g., data) path
may utilize control bits to swing its switches and/or logic gates.
[0069] Figure 9 illustrates a hardware processor tile 900 comprising an accelerator 902 according to embodiments of the disclosure. Accelerator 902 may be a CSA
according to this disclosure. Tile 900 includes a plurality of cache banks (e.g., cache bank 908). Request address file (RAF) circuits 910 may be included, e.g., as
discussed below in Section 2.2. ODI may refer to an On Die Interconnect, e.g., an interconnect stretching across an entire die connecting up all the tiles. OTI may
refer to an On Tile Interconnect, for example, stretching across a tile, e.g., connecting cache banks on the tile together.
2.1 Processing Elements
[0070] In certain embodiments, a CSA includes an array of heterogeneous PEs, in which the fabric is composed of several types of PEs each of which implement only a
subset of the dataflow operators. By way of example, Figure 10 shows a provisional implementation of a PE capable of implementing a broad set of the integer
and control operations. Other PEs, including those supporting floating point addition, floating point multiplication, buffering, and certain control operations may
have a similar implementation style, e.g., with the appropriate (dataflow operator) circuitry substituted for the ALU. PEs (e.g., dataflow operators) of a CSA may
be configured (e.g., programmed) before the beginning of execution to implement a particular dataflow operation from among the set that the PE supports. A
configuration may include one or two control words which specify an opcode controlling the ALU, steer the various multiplexors within the PE, and actuate
dataflow into and out of the PE channels. Dataflow operators may be implemented by microcoding these configurations bits. The depicted integer PE 1000 in
Figure 10 is organized as a single-stage logical pipeline flowing from top to bottom. Data enters PE 1000 from one of set of local networks, where it is registered
in an input buffer for subsequent operation. Each PE may support a number of wide, data-oriented and narrow, control-oriented channels. The number of
provisioned channels may vary based on PE functionality, but one embodiment of an integer-oriented PE has 2 wide and 1-2 narrow input and output channels.
Although the integer PE is implemented as a single-cycle pipeline, other pipelining choices may be utilized. For example, multiplication PEs may have multiple
pipeline stages.
[0071] PE execution may proceed in a dataflow style. Based on the configuration microcode, the scheduler may examine the status of the PE ingress and egress
buffers, and, when all the inputs for the configured operation have arrived and the egress buffer of the operation is available, orchestrates the actual execution of
the operation by a dataflow operator (e.g., on the ALU). The resulting value may be placed in the configured egress buffer. Transfers between the egress buffer of
one PE and the ingress buffer of another PE may occur asynchronously as buffering becomes available. In certain embodiments, PEs are provisioned such that
at least one dataflow operation completes per cycle. Section 2 discussed dataflow operator encompassing primitive operations, such as add, xor, or pick.
Certain embodiments may provide advantages in energy, area, performance, and latency. In one embodiment, with an extension to a PE control path, more fused
combinations may be enabled. In one embodiment, the width of the processing elements is 64 bits, e.g., for the heavy utilization of double-precision floating
point computation in HPC and to support 64-bit memory addressing.
2.2 Communications Networks
[0072] Embodiments of the CSA microarchitecture provide a hierarchy of networks which together provide an implementation of the architectural abstraction of
latency-insensitive channels across multiple communications scales. The lowest level of CSA communications hierarchy may be the local network. The local
network may be statically circuit switched, e.g., using configuration registers to swing multiplexor(s) in the local network data-path to form fixed electrical paths
between communicating PEs. In one embodiment, the configuration of the local network is set once per dataflow graph, e.g., at the same time as the PE
configuration. In one embodiment, static, circuit switching optimizes for energy, e.g., where a large majority (perhaps greater than 95%) of CSA communications
traffic will cross the local network. A program may include terms which are used in multiple expressions. To optimize for this case, embodiments herein provide
for hardware support for multicast within the local network. Several local networks may be ganged together to form routing channels, e.g., which are
interspersed (as a grid) between rows and columns of PEs. As an optimization, several local networks may be included to carry control tokens. In comparison to
a FPGA interconnect, a CSA local network may be routed at the granularity of the data-path, and another difference may be a CSA's treatment of control. One
embodiment of a CSA local network is explicitly flow controlled (e.g., back-pressured). For example, for each forward data-path and multiplexor set, a CSA is to
provide a backward-flowing flow control path that is physically paired with the forward data-path. The combination of the two microarchitectural paths may
provide a low-latency, low-energy, low-area, point-to-point implementation of the latency-insensitive channel abstraction. In one embodiment, a CSA's flow control
lines are not visible to the user program, but they may be manipulated by the architecture in service of the user program. For example, the exception handling
mechanisms described in Section 1.2 may be achieved by pulling flow control lines to a "not present" state upon the detection of an exceptional condition. This
action may not only gracefully stalls those parts of the pipeline which are involved in the offending computation, but may also preserve the machine state
leading up the exception, e.g., for diagnostic analysis. A second network layer, e.g., the mezzanine network, may be a shared, packet switched network.
Mezzanine network may include a plurality of distributed network controllers, network dataflow endpoint circuits. The mezzanine network (e.g., the network
schematically indicated by the dotted box in Figure 48) may provide more general, long range communications, e.g., at the cost of latency, bandwidth, and
energy. In some programs, most communications may occur on the local network, and thus mezzanine network provisioning will be considerably reduced in
comparison, for example, each PE may connects to multiple local networks, but the CSA will provision only one mezzanine endpoint per logical neighborhood of
PEs. Since the mezzanine is effectively a shared network, each mezzanine network may carry multiple logically independent channels, e.g., and be provisioned
with multiple virtual channels. In one embodiment, the main function of the mezzanine network is to provide wide-range communications in-between PEs and
between PEs and memory. In addition to this capability, the mezzanine may also include network dataflow endpoint circuit(s), for example, to perform certain
dataflow operations. In addition to this capability, the mezzanine may also operate as a runtime support network, e.g., by which various services may access the
complete fabric in a user-program-transparent manner. In this capacity, the mezzanine endpoint may function as a controller for its local neighborhood, for

https://patents.google.com/patent/EP3726389B1/en?q=(high+radix)&oq=high+radix&page=1

12/76

2/14/26, 2:00 PM

EP3726389B1 - Apparatuses, methods, and systems for memory interface circuit allocation in a configurable spatial accelera…

example, during CSA configuration. To form channels spanning a CSA tile, three subchannels and two local network channels (which carry traffic to and from a
single channel in the mezzanine network) may be utilized. In one embodiment, one mezzanine channel is utilized, e.g., one mezzanine and two local = 3 total
network hops.
[0073] The composability of channels across network layers may be extended to higher level network layers at the inter-tile, inter-die, and fabric granularities.
[0074] Figure 10 illustrates a processing element 1000 according to embodiments of the disclosure. In one embodiment, operation configuration register 1019 is
loaded during configuration (e.g., mapping) and specifies the particular operation (or operations) this processing (e.g., compute) element is to perform. Register
1020 activity may be controlled by that operation (an output of multiplexer 1016, e.g., controlled by the scheduler 1014). Scheduler 1014 may schedule an
operation or operations of processing element 1000, for example, when input data and control input arrives. Control input buffer 1022 is connected to local
network 1002 (e.g., and local network 1002 may include a data path network as in Figure 7A and a flow control path network as in Figure 7B) and is loaded with
a value when it arrives (e.g., the network has a data bit(s) and valid bit(s)). Control output buffer 1032, data output buffer 1034, and/or data output buffer 1036
may receive an output of processing element 1000, e.g., as controlled by the operation (an output of multiplexer 1016). Status register 1038 may be loaded
whenever the ALU 1018 executes (also controlled by output of multiplexer 1016). Data in control input buffer 1022 and control output buffer 1032 may be a
single bit. Multiplexer 1021 (e.g., operand A) and multiplexer 1023 (e.g., operand B) may source inputs.
[0075] For example, suppose the operation of this processing (e.g., compute) element is (or includes) what is called call a pick in Figure 3B. The processing element
1000 then is to select data from either data input buffer 1024 or data input buffer 1026, e.g., to go to data output buffer 1034 (e.g., default) or data output buffer
1036. The control bit in 1022 may thus indicate a 0 if selecting from data input buffer 1024 or a 1 if selecting from data input buffer 1026.
[0076] For example, suppose the operation of this processing (e.g., compute) element is (or includes) what is called call a switch in Figure 3B. The processing element
1000 is to output data to data output buffer 1034 or data output buffer 1036, e.g., from data input buffer 1024 (e.g., default) or data input buffer 1026. The
control bit in 1022 may thus indicate a 0 if outputting to data output buffer 1034 or a 1 if outputting to data output buffer 1036.
[0077] Multiple networks (e.g., interconnects) may be connected to a processing element, e.g., (input) networks 1002, 1004, 1006 and (output) networks 1008, 1010,
1012. The connections may be switches, e.g., as discussed in reference to Figures 7A and 7B. In one embodiment, each network includes two sub-networks (or
two channels on the network), e.g., one for the data path network in Figure 7A and one for the flow control (e.g., backpressure) path network in Figure 7B. As one
example, local network 1002 (e.g., set up as a control interconnect) is depicted as being switched (e.g., connected) to control input buffer 1022. In this
embodiment, a data path (e.g., network as in Figure 7A) may carry the control input value (e.g., bit or bits) (e.g., a control token) and the flow control path (e.g.,
network) may carry the backpressure signal (e.g., backpressure or no-backpressure token) from control input buffer 1022, e.g., to indicate to the upstream
producer (e.g., PE) that a new control input value is not to be loaded into (e.g., sent to) control input buffer 1022 until the backpressure signal indicates there is
room in the control input buffer 1022 for the new control input value (e.g., from a control output buffer of the upstream producer). In one embodiment, the new
control input value may not enter control input buffer 1022 until both (i) the upstream producer receives the "space available" backpressure signal from "control
input" buffer 1022 and (ii) the new control input value is sent from the upstream producer, e.g., and this may stall the processing element 1000 until that happens
(and space in the target, output buffer(s) is available).
[0078] Data input buffer 1024 and data input buffer 1026 may perform similarly, e.g., local network 1004 (e.g., set up as a data (as opposed to control) interconnect) is
depicted as being switched (e.g., connected) to data input buffer 1024. In this embodiment, a data path (e.g., network as in Figure 7A) may carry the data input
value (e.g., bit or bits) (e.g., a dataflow token) and the flow control path (e.g., network) may carry the backpressure signal (e.g., backpressure or no-backpressure
token) from data input buffer 1024, e.g., to indicate to the upstream producer (e.g., PE) that a new data input value is not to be loaded into (e.g., sent to) data
input buffer 1024 until the backpressure signal indicates there is room in the data input buffer 1024 for the new data input value (e.g., from a data output buffer
of the upstream producer). In one embodiment, the new data input value may not enter data input buffer 1024 until both (i) the upstream producer receives the
"space available" backpressure signal from "data input" buffer 1024 and (ii) the new data input value is sent from the upstream producer, e.g., and this may stall
the processing element 1000 until that happens (and space in the target, output buffer(s) is available). A control output value and/or data output value may be
stalled in their respective output buffers (e.g., 1032, 1034, 1036) until a backpressure signal indicates there is available space in the input buffer for the
downstream processing element(s).
[0079] A processing element 1000 may be stalled from execution until its operands (e.g., a control input value and its corresponding data input value or values) are
received and/or until there is room in the output buffer(s) of the processing element 1000 for the data that is to be produced by the execution of the operation on
those operands.
[0080] In certain embodiments, a significant source of area and energy reduction is the customization of the dataflow operations supported by each type of processing
element. In one embodiment, a proper subset (e.g., most) processing elements support only a few operations (e.g., one, two, three, or four operation types), for
example, an implementation choice where a floating point PE only supports one of floating point multiply or floating point add, but not both.
2.3 Memory Interface
[0081] In certain embodiments, data requests (e.g., a load request or a store request) are sent and received by memory interface circuits (e.g., RAF circuits) of a
configurable spatial accelerator. In one embodiment, data corresponding to a request (e.g., a load request or a store request) is returned to the same memory
interface circuit (e.g., RAF circuit) that issued the request. In another embodiment, data corresponding to a request (e.g., a load request or a store request) from
a first memory interface circuit (e.g., RAF circuit) is sent to another memory interface circuit (e.g., RAF circuit) that did not issue the request but is to receive the
corresponding data for the request. A request address file (RAF) circuit, versions of which are shown in Figure 11 and Figure 12, may be responsible for
executing memory operations and serve as an intermediary between the CSA fabric and the memory hierarchy. As such, the main microarchitectural task of the
RAF may be to rationalize the out-of-order memory subsystem with the in-order semantics of CSA fabric. In this capacity, the RAF circuit may be provisioned with
completion buffers, e.g., data storage structures that re-order memory responses and return them to the fabric in the request order. The second major
functionality of the RAF circuit may be to provide support in the form of address translation and a page walker. Incoming virtual addresses may be translated to
physical addresses using a (e.g., channel-associative) translation lookaside buffer (TLB). To provide ample memory bandwidth, each CSA tile may include
multiple RAF circuits. Like the various PEs of the fabric, the RAF circuits may operate in a dataflow-style by checking for the availability of input arguments and
output buffering, if required, before selecting a memory operation to execute. In certain embodiments, a single RAF circuit (e.g., its port into memory) is
multiplexed among several co-located memory operations (e.g., as indicated by a value stored in a memory operation register for the RAF circuit). A multiplexed
RAF circuit may be used to minimize the area overhead of its various subcomponents, for example, to share the Accelerator Cache Interconnect (ACI) network
(e.g., as described in more detail below), shared virtual memory (SVM) support hardware, mezzanine network interface, and/or other hardware management
facilities. However, there are some program characteristics that may also motivate this choice. In one embodiment, a (e.g., valid) dataflow graph is to poll
memory in a shared virtual memory system. Memory-latency-bound programs, like graph traversals, may utilize many separate memory operations to saturate
memory bandwidth due to memory-dependent control flow. Although each RAF may be multiplexed, a CSA may include multiple (e.g., between 8 and 32) RAFs at
a tile granularity to ensure adequate cache bandwidth. RAFs may communicate with the rest of the fabric via both a local network and a mezzanine network.
Where RAFs are multiplexed, each RAF may be provisioned with several ports into the local network. These ports may serve as a minimum-latency, highlydeterministic path to memory for use by latency-sensitive or high-bandwidth memory operations. In addition, a RAF may be provisioned with a mezzanine
network endpoint, e.g., which provides memory access to runtime services and distant user-level memory accessors.
[0082] Figure 11 illustrates a request address file (RAF) circuit 1100 according to embodiments of the disclosure. In one embodiment, at configuration time, the
memory load and store operations that were in a dataflow graph are specified in register(s) 1110. The arcs to those memory operations in the dataflow graphs
may then be connected to the input queues 1122, 1124, and 1126. The arcs from those memory operations are thus to leave completion buffers 1128, 1130, or
1132 in certain embodiments. Dependency tokens (which may be single bits) arrive into queues 1118 and 1120 in certain embodiments. Dependency tokens are

https://patents.google.com/patent/EP3726389B1/en?q=(high+radix)&oq=high+radix&page=1

13/76

2/14/26, 2:00 PM

EP3726389B1 - Apparatuses, methods, and systems for memory interface circuit allocation in a configurable spatial accelera…

to leave from queue 1116 in certain embodiments. Dependency token counter 1114 may be a compact representation of a queue and track a number of
dependency tokens used for any given input queue. If the dependency token counters 1114 saturate, no additional dependency tokens may be generated for new
memory operations in certain embodiments. Accordingly, a memory ordering circuit (e.g., a RAF in Figure 13) may stall scheduling new memory operations until
the dependency token counters 1114 becomes unsaturated. Optionally, an allocator circuit 1250 may be included, for example, as discussed in Section 2.4. In
certain embodiments, allocator circuit 1250 is to allocate components of the RAF circuit 1100 to a particular memory operation (e.g., a store operation or a load
operation), for example, as indicated by a value stored in register(s) 1110. In one embodiment, the components for an operation are (i) a single input queue 1122,
1124, or 1126 (e.g., to receive address data from a PE for a load operation requested by a PE from memory (e.g., cache) via port 1101) and a corresponding
completion buffer 1128, 1130, or 1132 (e.g., to receive an indication that the load operation has been completed from memory) or (ii) a pair of input queues from
1122, 1124, or 1126 (e.g., one to receive data-to-be-stored (e.g., payload data) and one to receive an address indicating where to store that data from a PE into
memory (e.g., cache) via port 1101) and a corresponding completion buffer 1128, 1130, or 1132 (e.g., to receive an indication that the store operation has been
completed in memory). As an example for a load, an address arrives into queue 1122 which the scheduler 1112 matches up in register 1110 as being
programmed to be a load operation. In certain embodiments, a completion buffer slot for this load is assigned, e.g., in the order the address arrived. Assuming
this particular load in the graph has no dependencies specified, the address and completion buffer slot are sent off to the memory system by the scheduler (e.g.,
via memory command 1142) in certain embodiments. When the result returns to multiplexer 1140 (shown schematically), it is stored into the completion buffer
slot it specifies (e.g., as it carried the target slot all along though the memory system) in certain embodiments. The completion buffer sends results back into
local network (e.g., local network 1102, 1104, 1106, or 1108) in the order the addresses arrived in certain embodiments.
[0083] Stores may be similar, for example, except both address and data have to arrive (e.g., from one or more PEs) before any operation is sent off to the memory
system in certain embodiments.
[0084] Local network 1102, 1104, 1106, or 1108 may be a circuit switched network, e.g., as discussed in reference to Figures 6-8. In certain embodiments, RAF circuit
1100 is to send a backpressure value via a network to a producer (e.g., transmitter) component (e.g., PE) when an input queue of the RAF circuit 1100 is full. The
backpressure value may cause a stall of the producing component (e.g., PE) from issuing or sending an additional memory request (e.g., to that particularly input
queue) until storage space is available in the input queue of the RAF circuit. In certain embodiments, a receiving component (e.g., PE) is to send a backpressure
value via a network to RAF circuit 1100 to stall the sending of data from a completion buffer 1128, 1130, or 1132 until storage space is available in the input
queue of the receiving component (e.g., PE).
[0085] Optionally, a translation lookaside buffer (TLB) 1146 may be included to convert a logical address received from an input queue 1122, 1124, or 1126 into a
physical address of the memory (e.g., cache). In one embodiment, the memory accessed is one or more of the cache banks discussed herein.
[0086] Figure 12 illustrates a request address file (RAF) circuit 1200 according to embodiments of the disclosure. In one embodiment, at configuration time, the
memory load and store operations that were in a dataflow graph are specified in register(s) 1210. The arcs to those memory operations in the dataflow graphs
may then be connected to the input queues 1222, 1224, and 1226. The arcs from those memory operations are thus to leave completion buffers 1228, 1230, or
1232 in certain embodiments. Dependency tokens (which may be single bits) arrive into queues 1218 and 1220 in certain embodiments. Dependency tokens are
to leave from queue 1216 in certain embodiments. Dependency token counter 1214 may be a compact representation of a queue and track a number of
dependency tokens used for any given input queue. If the dependency token counters 1214 saturate, no additional dependency tokens may be generated for new
memory operations in certain embodiments. Accordingly, a memory ordering circuit (e.g., a RAF in Figure 13) may stall scheduling new memory operations until
the dependency token counters 1214 becomes unsaturated. In certain embodiments, ALU 1248 is provided in the RAF circuit 1200 to permit memory address
calculations to be directly performed in the RAF, e.g., where use of the ALU is optionally specified as part of the configuration value in register 1210. Example
uses include: address displacement calculations in which a base address is added to an offset and/or stateful calculations in which an address may be
repeatedly incremented at the RAF, such as streaming load (sld).
[0087] Optionally, an allocator circuit 1250 may be included, for example, as discussed in Section 2.4. In certain embodiments, allocator circuit 1250 is to allocate
components of the RAF circuit 1200 to a particular memory operation (e.g., a store operation or a load operation), for example, as indicated by a value stored in
register(s) 1210. In one embodiment, the components for an operation are (i) a single input queue 1222, 1224, or 1226 (e.g., to receive address data from a PE
for a load operation requested by a PE from memory (e.g., cache) via port 1201) and a corresponding completion buffer 1228, 1230, or 1232 (e.g., to receive an
indication that the load operation has been completed from memory) or (ii) a pair of input queues from 1222, 1224, or 1226 (e.g., one to receive data-to-bestored (e.g., payload data) and one to receive an address indicating where to store that data from a PE into memory (e.g., cache) via port 1201) and a
corresponding completion buffer 1228, 1230, or 1232 (e.g., to receive an indication that the store operation has been completed in memory).As an example for a
load, an address arrives into queue 1222 which the scheduler 1212 matches up in register 1210 as being programmed to be a load operation. In certain
embodiments, a completion buffer slot for this load is assigned, e.g., in the order the address arrived. Assuming this particular load in the graph has no
dependencies specified, the address and completion buffer slot are sent off to the memory system by the scheduler (e.g., via memory command 1242) in certain
embodiments. When the result returns to demultiplexer 1240 (shown schematically), it is stored into the completion buffer slot specified by the control
information provided by control demultiplexer 1244 (e.g., as it carried the target slot all along though the memory system) in certain embodiments. The
completion buffer sends results back into CSA network (e.g., mezzanine network 1202) in the order the addresses arrived in certain embodiments.
[0088] Stores may be similar, for example, except both address and data have to arrive (e.g., from one or more PEs) before any operation is sent off to the memory
system in certain embodiments.
[0089] Network 1202 may be a packet switched network, e.g., a mezzanine network as discussed in reference to Figures 48-55. In certain embodiments, RAF circuit
1200 is to send a backpressure value via a network to a producer (e.g., transmitter) component (e.g., PE) when an input queue of the RAF circuit 1200 is full. The
backpressure value may cause a stall of the producing component (e.g., PE) from issuing or sending an additional memory request (e.g., to that particularly input
queue) until storage space is available in the input queue of the RAF circuit. In certain embodiments, a receiving component (e.g., PE) is to send a backpressure
value via a network to RAF circuit 1200 to stall the sending of data from a completion buffer 1228, 1230, or 1232 until storage space is available in the input
queue of the receiving component (e.g., PE). Data sent from and into network 1220 may include a channel identification value that identifies which input queue
1222, 1224, or 1226 that data is to be stored into (e.g., data less the bits that for the identification value). For example, an identification value may include a first
field that identifies a particular input queue of a RAF circuit (e.g., and a second field that identifies that particular RAF circuit from a plurality of RAF circuits).
[0090] Optionally, a translation lookaside buffer (TLB) 1246 may be included to convert a logical address received from an input queue 1222, 1224, or 1226 into a
physical address of the memory (e.g., cache). In one embodiment, the memory accessed is one or more of the cache banks discussed herein.
[0091] Dataflow graphs may be capable of generating a profusion of (e.g., word granularity) requests in parallel. Thus, certain embodiments of the CSA provide a cache
subsystem with sufficient bandwidth to service the CSA. A heavily banked cache microarchitecture, e.g., as shown in Figure 13 may be utilized. Figure 13
illustrates a circuit 1300 with a plurality of request address file (RAF) circuits (e.g., RAF circuit (1)) coupled between a plurality of accelerator tiles (1308, 1310,
1312, 1314) and a plurality of cache banks (e.g., cache bank 1302) according to embodiments of the disclosure. In one embodiment, the number of RAFs and
cache banks may be in a ratio of either 1:1 or 1:2. Cache banks may contain full cache lines (e.g., as opposed to sharding by word), with each line having exactly
one home in the cache. Cache lines may be mapped to cache banks via a pseudo-random function. The CSA may adopt the shared virtual memory (SVM) model
to integrate with other tiled architectures. Certain embodiments include an Accelerator Cache Interconnect (ACI) network 1340 connecting the RAFs to the
cache banks. This network may carry address and data between the RAFs and the cache. The topology of the ACI may be a cascaded crossbar, e.g., as a
compromise between latency and implementation complexity. Allocator 1330 (e.g., allocator circuit) may be included to allocate components of RAF circuit(s)
for particular operations or instances of particular operations, e.g., as discussed below in Section 2.4. Allocator may be coupled to a core of a processor, e.g.,

https://patents.google.com/patent/EP3726389B1/en?q=(high+radix)&oq=high+radix&page=1

14/76

2/14/26, 2:00 PM

EP3726389B1 - Apparatuses, methods, and systems for memory interface circuit allocation in a configurable spatial accelera…

one of the cores in Figure 2. A core may send an indication to an allocator to begin allocation. PEs may communicate with RAF circuits via a circuit switched
network, e.g., as discussed herein.
[0092] In certain embodiments, accelerator-cache network is further coupled to circuitry 1320 that includes a cache home agent and/or next level cache. In certain
embodiments, accelerator-cache network (e.g., interconnect) is separate from any (for example, circuit switched or packet switched) network of an accelerator
(e.g., accelerator tile), e.g., RAF is the interface between the processing elements and the cache home agent and/or next level cache. In one embodiment, a
cache home agent is to connect to a memory (e.g., separate from the cache banks) to access data from that memory (e.g., memory 202 in Figure 2), e.g., to
move data between the cache banks and the (e.g., system) memory. In one embodiment, a next level cache is a (e.g., single) higher level cache, for example,
such that the next level cache (e.g., higher level cache) is checked for data that was not found (e.g., a miss) in a lower level cache (e.g., cache banks). In one
embodiment, this data is payload data. In another embodiment, this data is a physical address to virtual address mapping. In one embodiment, a CHA is to
perform a search of (e.g., system) memory for a miss (e.g., a miss in the higher level cache) and not perform a search for a hit (e.g., the data being requested is
in the cache being searched).
[0093] Figure 14 illustrates a circuit 1400 with a plurality of request address file (RAF) circuits (e.g., RAF circuit (1)) coupled between a plurality of accelerator tiles
(1408, 1410, 1412, 1414) and a plurality of cache banks (e.g., cache bank 1402) according to embodiments of the disclosure. In one embodiment, the number of
RAFs and cache banks may be in a ratio of either 1:1 or 1:2. Cache banks may contain full cache lines (e.g., as opposed to sharding by word), with each line
having exactly one home in the cache. Cache lines may be mapped to cache banks via a pseudo-random function. The CSA may adopt the shared virtual
memory (SVM) model to integrate with other tiled architectures. Certain embodiments include an Accelerator Cache Interconnect (ACI) network 1440
connecting the RAFs to the cache banks. This network may carry address and data between the RAFs and the cache. The topology of the ACI may be a
cascaded crossbar, e.g., as a compromise between latency and implementation complexity. Allocator 1430 (e.g., allocator circuit) may be included to allocate
components of RAF circuit(s) for particular operations or instances of particular operations, e.g., as discussed below in Section 2.4. Allocator may be coupled to
a core of a processor, e.g., one of the cores in Figure 2. A core may send an indication to an allocator to begin allocation. PEs may communicate with RAF
circuits via a packed switched network, e.g., mezzanine network as discussed herein. In the depicted embodiment, each RAF circuit (1) through (8) includes its
own respective mezzanine network 1442 (1) through (8) as shown with a dotted box to couple to one or more (e.g., any) of the PEs in Figure 14.
[0094] In certain embodiments, accelerator-cache network is further coupled to circuitry 1420 that includes a cache home agent and/or next level cache. In certain
embodiments, accelerator-cache network (e.g., interconnect) is separate from any (for example, circuit switched or packet switched) network of an accelerator
(e.g., accelerator tile), e.g., RAF is the interface between the processing elements and the cache home agent and/or next level cache. In one embodiment, a
cache home agent is to connect to a memory (e.g., separate from the cache banks) to access data from that memory (e.g., memory 202 in Figure 2), e.g., to
move data between the cache banks and the (e.g., system) memory. In one embodiment, a next level cache is a (e.g., single) higher level cache, for example,
such that the next level cache (e.g., higher level cache) is checked for data that was not found (e.g., a miss) in a lower level cache (e.g., cache banks). In one
embodiment, this data is payload data. In another embodiment, this data is a physical address to virtual address mapping. In one embodiment, a CHA is to
perform a search of (e.g., system) memory for a miss (e.g., a miss in the higher level cache) and not perform a search for a hit (e.g., the data being requested is
in the cache being searched).
2.4 Allocation of Memory Interface Circuits and/or Components Thereof
[0095] To meet nowadays increasingly advanced performance target and aggressive energy goal, a parallel, distributed dataflow architecture, named Configurable
Spatial Architecture (CSA), has been developed. A CSA may include processing elements, request address file (RAF) circuits, network between the processing
elements and RAF circuits, cache(s), network between RAF circuits and cache(s), other memory (e.g., as shown in Figure 16), network between caches and
memory, or any combination thereof.
[0096] As a dataflow architecture, embodiments of CSA have a unique memory architecture, for example, where memory accesses are decoupled into an explicit
request and response phase allowing pipelining through memory. This architecture permits the address generation portions of the dataflow graphs to typically
produce a large number of address accesses, and allows these embodiments of CSA to drive high memory bandwidth, and also gives the CSA memory interface
microarchitecture a number of opportunities to improve program performance by observing the memory access stream. Certain embodiments herein leverage
these improvements to improve performance of dataflow graphs, e.g., by allowing more data accesses in a given period of time. In one embodiment, the
characteristics of parallel computing determine the intensive memory accesses and therefore the CSA's memory sub-system is critical for the ultimate
performance levels, e.g., to minimize the execution time within the memory sub-system. Certain embodiments herein provide for improved memory sub-system
design via allocation and the improvements to allocation discussed herein.
[0097] Figure 15 illustrates a flow diagram 1500 according to embodiments of the disclosure. Depicted flow 1500 includes: coupling a spatial array of processing
elements to a first memory interface circuit comprising a first port into a cache, a first plurality of input queues to store data for memory requests from the
spatial array of processing elements, and a first memory operation register, and to a second memory interface circuit comprising a second port into the cache, a
second plurality of input queues to store data for memory requests from the spatial array of processing elements, and a second memory operation register
1502; setting respective first values into the first memory operation register and the second memory operation register according to a first allocation mode to
couple the first port to a first input queue of the first plurality of input queues that stores data for memory requests from a first processing element of the spatial
array of processing elements, couple the second port to a first input queue of the second plurality of input queues that stores data for memory requests from a
second processing element of the spatial array of processing elements, and couple the first port to a second input queue of the first plurality of input queues that
stores data for memory requests from a third processing element of the spatial array of processing elements 1504; and setting respective second values into
the first memory operation register and the second memory operation register according to a second allocation mode to couple the first port to the first input
queue of the first plurality of input queues that stores data for memory requests from the first processing element of the spatial array of processing elements,
couple the second port to the first input queue of the second plurality of input queues that stores data for memory requests from the second processing element
of the spatial array of processing elements, and couple the second port to a second input queue of the second plurality of input queues that stores data for
memory requests from the third processing element of the spatial array of processing elements 1506.
[0098] Figure 16 illustrates a high level view of a configurable spatial accelerator and its memory sub-systems (including RAF circuits 1606, Accelerator Cache Interface
(ACI) network 1604, cache banks 1602, memory (e.g., coherency) network 1616, cache home agent 1618, and main memory 1620) according to embodiments of
the disclosure. The depicted CSA memory sub-systems are distributed, with multiple interfaces (e.g., RAF circuits) capable of injecting requests. In certain
embodiments, this distributed dataflow architecture and microarchitecture improves the functioning of a computer (e.g., including RAF circuits).
[0099] In one embodiment, each RAF circuit includes multiple input queues (e.g., and multiple completion buffers) that are to be programmed (e.g., via storing a
respective value into a memory operation register(s)) to (e.g., repeatedly) perform a certain operation. For example, a given processing element may request
data from memory (e.g., cache) and do so via sending a memory request (e.g., sending a value that indicates a memory address) to a particular input queue(s)
of a RAF circuit, and that RAF circuit (e.g., its memory operation register(s)) was preprogrammed to cause issuance of those requests to memory. As another
example, each RAF circuit includes a plurality of input queues (e.g., input queues 1622 in RAF circuit 1) and a plurality of slots in a completion buffer (e.g.,
completion buffer 1624 in RAF circuit 1) that are assigned to a particular memory operation (e.g., for a particular data requesting PE and/or data receiving PE,
which may be the same PE in one embodiment). RAF circuits may receive memory operation requests from accelerator tile(s) 1608 (e.g., an accelerator tile
having a plurality of processing elements as discussed herein).
[0100] In Figure 16, an allocator 1610 (e.g., an allocator circuit) is to send the appropriate control values (e.g., into memory operation register(s)) to assign memory
operation to a particular RAF circuit (e.g., and more particularly, to a particular subset of the components of that particular RAF circuit) according to a desired
allocation mode of a plurality of allocation modes. Having a plurality of allocation modes is an improvement to the functioning of a computer because it allows

https://patents.google.com/patent/EP3726389B1/en?q=(high+radix)&oq=high+radix&page=1

15/76

2/14/26, 2:00 PM

EP3726389B1 - Apparatuses, methods, and systems for memory interface circuit allocation in a configurable spatial accelera…

allocation flexibility based on static and dynamic execution properties of a dataflow graph. Certain embodiments herein improve the performance of a CSA with
an optimized mapping of memory operations and RAF circuit resources (e.g., input buffers and/or completion buffers).
[0101] One or more of the allocation modes herein allow for optimizing RAF circuit resources in order to match each application's requirements and thus reach the
optimal performance goal. In embodiments with a statically multiplexed and distributed structure, allocation of RAF circuits (e.g., and proper subsets of the
components therein) allows for memory operations of a dataflow graph to be bound to the RAF circuits in a way that achieves the maximal performance. Several
considerations may weigh into this decision, including static and dynamic execution properties of the dataflow graph.
[0102] In certain embodiments, a memory operation is (or a proper subset of the memory operations of a dataflow graph are) statically assigned to a particular RAF
circuit for the lifetime of graph execution. In one embodiment, a plurality of respective memory operations are assigned to each respective RAF circuit, and, at
runtime, a RAF circuit (e.g., hardware) will select (e.g., each cycle) one memory operation among the ready-to-execute memory operations (e.g., ready because
their data is available in an input queue(s) and there is space available in their completion buffer for their data and/or completion indication). In certain
embodiments, a memory operation is (or a proper subset of the memory operations of a dataflow graph are) dynamically assigned to a particular RAF circuit
during the lifetime of graph execution.
[0103] In certain embodiments, the RAF circuit is responsible for executing memory operations and serves as an interface between the CSA fabric (e.g., PEs) and the
memory hierarchy. Certain embodiments herein include an allocator (e.g., allocator circuit) that assigns memory operations to RAF channels. In one
embodiment, the allocator is to achieve the best performance and maximal bandwidth. The allocator may take as inputs: (i) the number of memory operations to
be performed in executing a dataflow graph (e.g., "N" number of memory operations (OP) in the graph: OP0, OP1, OP2, ... OPN-1, (ii) the number of RAF circuits,
and/or (iii) the number of input queues (and/or completion buffers) for each RAF circuit (e.g., "R" number of RAF circuits (e.g., per CSA tile) and "C" number of
input queues per RAF circuit): RAF(0)_InputQueue0, RAF(0)_InputQueue1, ... RAF(0)_InputQueueC-1, RAF(1)_InputQueue0, RAF(1)_InputQueue1, ...
RAF(1)_InputQueueC-1, and RAF(R-1)_InputQueue0, RAF(R-1)_InputQueue1, ... RAF(R-1)_InputQueueC-1). An InputQueue as discussed above may be a single
input queue or a pair of input queues that are bound together (e.g., in a store operation, a first input queue to receive the data to be stored and a second input
queue to receive the address for the data that is to be stored). In one embodiment, a single memory request is issued (e.g., arbitrated) by each RAF circuit per
cycle into cache (e.g., via an ACI network) and thus the allocation of memory operations to RAF circuits is critical for performance. In one embodiment, the
allocation is to select between multiple allocation modes to achieve a mapping of N memory operations to (e.g., R × C) RAF resources (e.g., input queues) that
achieves the best performance and/or lowest power consumption. In one embodiment, each RAF circuit has the same number of input queues (and/or
completion buffers) as any other RAF circuits utilized.
[0104] In one embodiment, a random allocation of memory operations to RAF components (e.g., input queues) is done, but such an allocation may have extreme load
imbalance, and make the performance unacceptable and unpredictable.
[0105] The below includes twelve examples of (e.g., non-random) allocation modes for an allocator to utilize to assign memory operations to RAF circuits in order to
reach a desired goal (e.g., optimal performance level). An allocation mode may select from one or more of the following goals: (1) high-bandwidth memory
operations are more critical to performance, (2) high-latency memory operations are more critical to performance, (3) load-balancing across all RAF circuits
leads to better performance, (4) assignments based on bidding groups help improve performance, (5) assignments based on operation types help improve
performance, or (6) assignments based on operation bins with bandwidth ranges help improve performance. Further, the below includes two allocation modes to
allocate completion buffers.
[0106] In certain embodiments, an allocator (e.g., allocator circuit) collects run time data (or accesses previously collected run time data). The run time data may
include memory issue counts for a dataflow graph. In one embodiment, memory issues counts may be any integer, e.g., a certain dataflow graph may include
10s, 100s, 1000s, 10,000s, 100,000s, millions, or even billions of memory issuances for that dataflow graph.
[0107] In one embodiment, each of the RAF circuits are an instance of RAF circuit 1100 of Figure 11 or RAF circuit 1200 of Figure 12. In certain embodiments, the
setting of a value into a memory operation register (e.g., into register(s) 1110 in Figure 11 or into register(s) 1210 in Figure 12) sets that RAF circuit into the
desired allocation mode, for example, to cause the assigning of input queues for a respective, particular memory operation according to the allocation mode. In
certain embodiments, the assigning includes coupling input queues to a port of a RAF circuit into memory (e.g., into cache).
I. LOAD-BALANCING LONGEST-JOB-FIRST (LB-LJF) ALLOCATION MODE
[0108] In certain embodiments, memory operations that are issued more times are more critical to performance. In one embodiment, for a particular dataflow graph
and input data set, each memory operation will execute a fixed number of times. If frequently issued memory operations are given less issue bandwidth, the
minimum execution time will increase in certain embodiments.
[0109] Thus, giving high-issue count operations the higher arbitration priority may help improve the performance and achieve higher bandwidth in these embodiments.
In addition, better performance may be achieved in these embodiments if the total number of issue times across all RAF circuits is balanced. The first allocation
mode provides those two improvements by sorting the memory operations (e.g., of a single dataflow graph) according to their issue counts and assigning the
memory operations to RAF components (e.g., input queues) in a balanced way. This may be referred to as Load-Balancing Longest-Job-First (LB-LJF) allocation
mode.
[0110] An example of this allocation mode is described below:
1) Sort all memory operations from the highest issue count (e.g., the number of times a particular memory operation executes) to the lowest issue count.
Issue count may be obtained either through feedback (e.g., profile-guided optimization) or from compiler generated relative issue count estimations;
2) Start assigning the sorted memory operations to RAF(0)_InputQueue0, RAF(1)_InputQueue0, ... RAF(R-1)_InputQueue0 until every RAF circuit's first
component (e.g., input queue) is allocated;
3) Count all RAF circuits' issue counts (e.g., loads and stores) that have been assigned so far, e.g., add all issue counts of all components (e.g., input
queues) in each RAF circuit;
4) Assigning the next sorted memory operation to the next available component (e.g., input queue) of the RAF circuit with the minimal load;
5) Repeat 3) and 4) until the last of the memory operations is allocated to a RAF circuit; and
6) Note if two memory operations' issue counts are the same, this allocation mode handles them in the opposite of the lexical program order (or in lexical
program order in another embodiment).
[0111] In one embodiment, LB-LJF RAF allocation mode assigns RAF components (e.g., input queues) to memory operations statically before the application starts to
run and relies on the bandwidth information whose estimation (e.g., the issue counts) is assumed to be provided by a compiler (e.g., that generates a dataflow
graph) and/or late tools (e.g., any of the elaboration, buffer insertion, fusion, and place and route software modules in Figure 19). In certain embodiments, the
LB-LJF allocation mode is able to balance memory operations (e.g., issue counts) across multiple RAF circuits.
II. LOAD-BALANCING (LB) ALLOCATION MODE
[0112] The second allocation mode is referred to as Load-Balancing (LB) allocation mode. Similar to the first allocation mode, this allocation mode gives high-issue
count operations the higher arbitration priority and balances the total number of issue times across all RAFs. One difference from the first allocation mode is
that this allocation mode does not sort memory operations according to their issue counts, but instead allocates them to RAF components (e.g., input queues)
according to their lexical program order.
[0113] An example of this allocation mode is described below:
1) Start assigning, in lexical program (e.g., unsorted), the memory operations to RAF(0)_InputQueueO, RAF(1)_InputQueue0, ... RAF(R-1)_InputQueue0 until
every RAF circuit's first component (e.g., input queue) is allocated;

https://patents.google.com/patent/EP3726389B1/en?q=(high+radix)&oq=high+radix&page=1

16/76

2/14/26, 2:00 PM

EP3726389B1 - Apparatuses, methods, and systems for memory interface circuit allocation in a configurable spatial accelera…

2) Count all RAF circuits' issue counts (e.g., loads and stores) that have been assigned so far, e.g., add all issue counts of all components (e.g., input
queues) in each RAF circuit. In one embodiment, issue count is obtained either through feedback (e.g., profile-guided optimization) or from compiler
generated relative issue count estimations;
3) Assigning the next (e.g., unsorted) memory operation to the next available component (e.g., input queue) of the RAF circuit with the minimal load; and
4) Repeat 2) and 3) until the last of the memory operations is allocated to a RAF circuit.
[0114] In one embodiment, LB RAF allocation mode assigns RAF components (e.g., input queues) to memory operations statically before the application starts to run
and relies on the bandwidth information whose estimation (e.g., the issue counts) is assumed to be provided by a compiler (e.g., that generates a dataflow
graph) and/or late tools (e.g., any of the elaboration, buffer insertion, fusion, and place and route software modules in Figure 19). In certain embodiments, the LB
allocation mode is able to balance memory operations (e.g., issue counts) across multiple RAF circuits.
III. BIN-BASED LOAD-BALANCING LONGEST-JOB-FIRST (BB-LB-LJF)
[0115] The third RAF allocation mode is referred to as Bin-Based Load-Balancing Longest-Job-First (BB-LB-LJF) allocation mode. Similar to the first allocation mode,
this allocation mode gives high-issue count operations the higher arbitration priority and balances the total number of issue times across all RAFs. One
difference from the first allocation mode is that this allocation mode places a proper subset of memory operations that are issued substantially the same
amount of times in the same bin and treats those in each bin equally in order to optimize the performance further. The detailed allocation mode is described.
[0116] An example of this allocation mode is described below:
1) Sort all memory operations from the highest issue count (e.g., the number of times a particular memory operation executes) to the lowest issue
count. Issue count may be obtained either through feedback (e.g., profile-guided optimization) or from compiler generated relative issue count
estimations;
2) Additionally, the memory operations with close issue counts (for example, plus or minus about 2%, 3%, 4%, 5%, 6%, 7%, 8%, 9%, 10%, or any other
integer) are considered part of a single bin so that each bin of a plurality of bins covers a small range of issue counts instead of a single issue count;
3) All bins are sorted from the highest issue count range to the lowest issue count range; but inside each bin, memory operations remain in the lexical
program order (for example, a bin for issue counts between 100 and 1000 may include a first (in program order) memory operation having an issue
count of 850 and a second (in program order) memory operation having an issue count of 950);
4) Start assigning the memory operations beginning with the first of the sorted bins to RAF(0)_InputQueue0, RAF(1)_InputQueue0, ... RAF(R1)_InputQueue0 until every RAF circuit's first component (e.g., input queue) is allocated;
4) Count all RAF circuits' issue counts (e.g., loads and stores) that have been assigned so far, e.g., add all issue counts of all components (e.g., input
queues) in each RAF circuit;
5) Assigning the next (in program order) memory operation from a bin to the next available component (e.g., input queue) of the RAF circuit with the
minimal load; and 4) Repeat 4) and 5) until the last of the memory operations is allocated to a RAF circuit.
[0117] In one embodiment, BB-LB-LJF RAF allocation mode assigns RAF components (e.g., input queues) to memory operations statically before the application starts
to run and relies on the bandwidth information whose estimation (e.g., the issue counts) is assumed to be provided by a compiler (e.g., that generates a dataflow
graph) and/or late tools (e.g., any of the elaboration, buffer insertion, fusion, and place and route software modules in Figure 19). In certain embodiments, the
BB-LB-LJF allocation mode is able to balance memory operations (e.g., issue counts) across multiple RAF circuits.
IV. TYPE-BASED LOAD-BALANCING LONGEST-JOB-FIRST (TB-LB-LJF) ALLOCATION MODE
[0118] The fourth RAF allocation mode is referred to as Type-Based Load-Balancing Longest-Job-First (TB-LB-LJF) allocation mode. This allocation mode uses
bandwidth as a key decider of RAF component (e.g., input queue) resource allocation among competing memory operations (e.g., with the number of issue
times as the bandwidth indicator) to provide a load-balancing allocation mode which also applies a longest-job-first strategy. Certain embodiments of this mode
also prioritize different types of memory requests to further optimize the performance.
[0119] In certain embodiments, there are three main types of memory operations handled by a CSA memory subsystem, i.e., loads, stores, and prefetches. However, in
certain dataflow graphs, it may not be desirable to treat all of those three main types with the same level of priority when allocating RAF circuit resources. In
certain embodiments, prefetches are not as critical as load and store requests where prefetches aim to boost the performance by fetching data well before it is
actually needed. In certain embodiments, allocating loads and stores before prefetches leads to a better performing CSA.
[0120] In certain embodiments, in the fourth allocation mode, memory operations in the order of load-store-prefetch are sorted according to their issue counts and
assigned to RAF components (e.g., input queues) in a balanced way.
[0121] An example of this allocation mode is described below:
1) Sort all load operations from the highest issue count (e.g., the number of times a particular memory operation executes) to the lowest issue count.
Issue count may be obtained either through feedback (e.g., profile-guided optimization) or from compiler generated relative issue count estimations;
2) Sort all store operations from the highest issue count (e.g., the number of times a particular memory operation executes) to the lowest issue count.
Issue count may be obtained either through feedback (e.g., profile-guided optimization) or from compiler generated relative issue count estimations;
3) Sort all pre-fetch operations from the highest issue count (e.g., the number of times a particular memory operation executes) to the lowest issue count.
Issue count may be obtained either through feedback (e.g., profile-guided optimization) or from compiler generated relative issue count estimations;
4) Start assigning the sorted memory operations (e.g., in the following order as needed: sorted load operations, then sorted store operations, and finally,
sorted prefetch operations) to RAF(0)_InputQueue0, RAF(1)_InputQueue0, ... RAF(R-1)_InputQueue0 until every RAF circuit's first component (e.g., input
queue) is allocated;
5) Count all RAF circuits' issue counts (e.g., loads and stores) that have been assigned so far, e.g., add all issue counts of all components (e.g., input
queues) in each RAF circuit;
6) Assigning the next memory operation to the next available component (e.g., input queue) of the RAF circuit with the minimal load for the sorted
operations (e.g., in the following order as needed: sorted load operations, then sorted store operations, and finally, sorted prefetch operations);
7) Repeat 5) and 6) until the last of the memory operations is allocated to a RAF circuit; and
8) Note if two memory operations' issue counts are the same, this allocation mode handles them in the opposite of the lexical program order (or in lexical
program order in another embodiment).
[0122] In one embodiment, TB-LB-LJF RAF allocation mode assigns RAF components (e.g., input queues) to memory operations statically before the application starts
to run and relies on the bandwidth information whose estimation (e.g., the issue counts) is assumed to be provided by a compiler (e.g., that generates a dataflow
graph) and/or late tools (e.g., any of the elaboration, buffer insertion, fusion, and place and route software modules in Figure 19). In certain embodiments, the
TB-LB-LJF allocation mode is able to balance memory operations (e.g., issue counts) across multiple RAF circuits and takes the types of memory operations in
consideration.
V. RANDOMIZED LOAD-BALANCING LONGEST-JOB-FIRST (RANDOMIZED-LB-LJF) ALLOCATION MODE
[0123] The fifth RAF allocation mode is referred to as Randomized Load-Balancing Longest-Job-First (Randomized-LB-LJF) allocation mode. This allocation mode
applies a randomized strategy to make the RAF allocations more coarse-grained. In one embodiment of this mode, memory operations are sorted according to
their issue counts and assigned to RAF components (e.g., input queues) in a balanced way by applying a randomized methodology.
[0124] An example of this allocation mode is described below:
1) Sort all memory operations from the highest issue count (e.g., the number of times a particular memory operation executes) to the lowest issue count.
Issue count may be obtained either through feedback (e.g., profile-guided optimization) or from compiler generated relative issue count estimations;

https://patents.google.com/patent/EP3726389B1/en?q=(high+radix)&oq=high+radix&page=1

17/76

2/14/26, 2:00 PM

EP3726389B1 - Apparatuses, methods, and systems for memory interface circuit allocation in a configurable spatial accelera…

2) Start assigning the sorted memory operations to RAF(0)_InputQueue0, RAF(1)_InputQueue0, ... RAF(R-1)_InputQueue0 until every RAF circuit's first
component (e.g., input queue) is allocated;
3) Count all RAF circuits' issue counts (e.g., loads and stores) that have been assigned so far, e.g., add all issue counts of all components (e.g., input
queues) in each RAF circuit;
4) Pick "M" number (e.g., a proper subset of all the RAF circuits) of RAF circuits below an (e.g., minimal) issue count value;
5) Randomly choose one RAF circuit from the M number (e.g., more than one and less than all of) RAF circuits;
6) Assigning the next sorted memory operation to that one chosen RAF circuit;
7) Repeat 3)-6) until the last of memory operation is allocated; and
8) Note if two memory operations' issue counts are the same, this allocation mode handles them in the opposite of the lexical program order (or in lexical
program order in another embodiment).
[0125] In one embodiment, Randomized-LB-LJF RAF allocation mode assigns RAF components (e.g., input queues) to memory operations statically before the
application starts to run and relies on the bandwidth information whose estimation (e.g., the issue counts) is assumed to be provided by a compiler (e.g., that
generates a dataflow graph) and/or late tools (e.g., any of the elaboration, buffer insertion, fusion, and place and route software modules in Figure 19). In certain
embodiments, the Randomized-LB-LJF allocation mode is able to balance memory operations (e.g., issue counts) across multiple RAF circuits and also utilizes
advantages of a randomized strategy.
VI. RANDOMIZED BIN-BASED LOAD-BALANCING LONGEST-JOB-FIRST (RANDOMIZED-BB-LB-LJF) ALLOCATION MODE
[0126] The sixth RAF allocation mode is referred to as Randomized Bin-Based Load-Balancing Longest-Job-First (Randomized-BB-LB-LJF) allocation mode. This
allocation mode uses bandwidth as a key decider of RAF channel (e.g., input queue) resource allocation among competing memory operations (e.g., with the
number of issue times as the bandwidth indicator). In one embodiment, this mode also applies both Longest-Job-First and randomized strategies. Certain
embodiments of this mode also put all memory operations that are issued (e.g., substantially or roughly) the same amount of times in the same bin and treats
them equally in order to optimize the performance further. Thus, certain embodiments herein utilize memory operations sorted according to their issue counts,
puts those into the bins, and assigns them to RAF components (e.g., input queues) in a balanced way by applying a randomized strategy.
[0127] An example of this allocation mode is described below:
1) Sort all memory operations from the highest issue count (e.g., the number of times a particular memory operation executes) to the lowest issue
count. Issue count may be obtained either through feedback (e.g., profile-guided optimization) or from compiler generated relative issue count
estimations;
2) Additionally, the memory operations with close issue counts (for example, plus or minus about 2%, 3%, 4%, 5%, 6%, 7%, 8%, 9%, 10%, or any other
integer) are considered part of a single bin so that each bin of a plurality of bins covers a small range of issue counts instead of a single issue count;
3) All bins are sorted from the highest issue count range to the lowest issue count range; but inside each bin, memory operations remain in the lexical
program order (for example, a bin for issue counts between 100 and 1000 may include a first (in program order) memory operation having an issue
count of 850 and a second (in program order) memory operation having an issue count of 950);
4) Start assigning the memory operations beginning with the first of the sorted bins to RAF(0)_InputQueue0, RAF(1)_InputQueue0, ... RAF(R1)_InputQueue0 until every RAF circuit's first component (e.g., input queue) is allocated;
4) Count all RAF circuits' issue counts (e.g., loads and stores) that have been assigned so far, e.g., add all issue counts of all components (e.g., input
queues) in each RAF circuit;
5) Pick "M" number (e.g., a proper subset of all the RAF circuits) of RAF circuits below an (e.g., minimal) issue count value;
6) Randomly choose one RAF circuit from the M number (e.g., more than one and less than all of) RAF circuits;
7) Assigning the next sorted memory operation to that one chosen RAF circuit; and
8) Repeat 4)-7) until the last of memory operation is allocated.
[0128] In one embodiment, Randomized-BB-LB-LJF allocation mode assigns RAF components (e.g., input queues) to memory operations statically before the
application starts to run and relies on the bandwidth information whose estimation (e.g., the issue counts) is assumed to be provided by a compiler (e.g., that
generates a dataflow graph) and/or late tools (e.g., any of the elaboration, buffer insertion, fusion, and place and route software modules in Figure 19). In certain
embodiments, the Randomized-LB-LJF allocation mode is able to balance memory operations (e.g., issue counts) across multiple RAF circuits and also take
advantage of randomized and bin-based strategies.
VII. BIN-BASED RANDOMIZED LOAD-BALANCING LONGEST-JOB-FIRST (BB-RANDOMIZED-LB-LJF) ALLOCATION MODE
[0129] The seventh RAF allocation mode is referred to as Bin-Based Randomized Load-Balancing Longest-Job-First (BB-Randomized-LB-LJF) allocation mode. In
certain embodiments, memory operations that are issued more times are more critical to performance, there is better performance if the total number of issue
times across all RAF circuits is balanced, a randomized strategy makes the allocations more coarse-grained, and memory operations with similar (e.g., very
close) issue counts are put in the same bin and are treated similarly. In certain embodiments of this mode, memory operations are sorted according to their
issue counts, put into the bins, and assigned to RAF components (e.g., input queues) in a balanced way by applying a randomized strategy.
[0130] An example of this allocation mode is described below:
1) Sort all memory operations from the highest issue count (e.g., the number of times a particular memory operation executes) to the lowest issue
count. Issue count may be obtained either through feedback (e.g., profile-guided optimization) or from compiler generated relative issue count
estimations;
2) Additionally, the memory operations with close issue counts (for example, plus or minus about 2%, 3%, 4%, 5%, 6%, 7%, 8%, 9%, 10%, or any other
integer) are considered part of a single bin so that each bin of a plurality of bins covers a small range of issue counts instead of a single issue count;
3) All bins are sorted from the highest issue count range to the lowest issue count range; but inside each bin, memory operations remain in the lexical
program order (for example, a bin for issue counts between 100 and 1000 may include a first (in program order) memory operation having an issue
count of 850 and a second (in program order) memory operation having an issue count of 950);
4) Start assigning the memory operations randomly from the first of the sorted bins (e..g, then, when done, randomly from the second bin, etc.) to
RAF(0)_InputQueue0, RAF(1)_InputQueue0, ... RAF(R-1)_InputQueue0 until every RAF circuit's first component (e.g., input queue) is allocated. Note that
the memory operations in the same bin are allocated to RAF circuits in the random order;
5) Count all RAF circuits' issue counts (e.g., loads and stores) that have been assigned so far, e.g., add all issue counts of all components (e.g., input
queues) in each RAF circuit;
6) Assigning the next (in random order) memory operation from a highest issue count bin to the next available component (e.g., input queue) of the RAF
circuit with the minimal load; and
4) Repeat 5) and 6) until the last of the memory operations is allocated to a RAF circuit.
[0131] In one embodiment, BB-Randomized-LB-LJF allocation mode assigns RAF components (e.g., input queues) to memory operations statically before the
application starts to run and relies on the bandwidth information whose estimation (e.g., the issue counts) is assumed to be provided by a compiler (e.g., that
generates a dataflow graph) and/or late tools (e.g., any of the elaboration, buffer insertion, fusion, and place and route software modules in Figure 19). In certain
embodiments, the BB-Randomized-LB-LJF allocation mode is able to balance memory operations (e.g., issue counts) across multiple RAF circuits and also take
advantage of randomized and bin-based strategies.
VIII. BANDWIDTH-BALANCING LONGEST-JOB-FIRST (BANDWIDTH-BALANCING LJF) ALLOCATION MODE

https://patents.google.com/patent/EP3726389B1/en?q=(high+radix)&oq=high+radix&page=1

18/76

2/14/26, 2:00 PM

EP3726389B1 - Apparatuses, methods, and systems for memory interface circuit allocation in a configurable spatial accelera…

[0132] The eighth RAF allocation mode is referred to as Bandwidth-Balancing Longest-Job-First (Bandwidth-Balancing LJF) allocation mode. This allocation mode uses
request and response bandwidth as a key decider of RAF component (e.g., input queue) resource allocation among competing memory operations. In one
embodiment, this mode is a load-balancing allocation mode which also applies a Longest-Job-First strategy. In certain embodiments of this mode, memory
operations are sorted according to their issue counts and assigned to RAF components (e.g., input queues) in a request/response bandwidth balanced way.
[0133] An example of this allocation mode is described below:
1) Sort all memory operations from the highest issue count (e.g., the number of times a particular memory operation executes) to the lowest issue count.
Issue count may be obtained either through feedback (e.g., profile-guided optimization) or from compiler generated relative issue count estimations;
2) Start assigning the sorted memory operations to RAF(0)_InputQueue0, RAF(1)_InputQueue0, ... RAF(R-1)_InputQueue0 until every RAF circuit's first
component (e.g., input queue) is allocated;
3) Individually track for each RAF circuit ("RAFi"): its total request bandwidth ("Req_BW") and total response bandwidth ("Res_BW"), for example, RAFi:
Req_BWi and Res_BWi for i = 0, 1, 2, ... R-1;
4) Assigning the next sorted memory operation to the next available component (e.g., input queue) of the RAF circuit that satisfies Minimum(Maximum
(Req_BWi + Req_BWNext, Res_BWi + Res_BWNext), for i=0...R-1); and
5) Repeat 3) and 4) until the last of the memory operations is allocated to a RAF circuit.
[0134] In one embodiment, Bandwidth-Balancing LJF allocation mode assigns RAF components (e.g., input queues) to memory operations statically before the
application starts to run and relies on the bandwidth information whose estimation (e.g., the issue counts) is assumed to be provided by a compiler (e.g., that
generates a dataflow graph) and/or late tools (e.g., any of the elaboration, buffer insertion, fusion, and place and route software modules in Figure 19). In certain
embodiments, the Bandwidth-Balancing LJF allocation mode is able to balance request/response bandwidth across multiple RAF circuits while giving the higher
arbitration priority to the longer memory operation.
IX. BANDWIDTH-BALANCING ALLOCATION MODE
[0135] The ninth RAF allocation mode is referred to as Bandwidth-Balancing allocation mode. This allocation mode uses request and response bandwidth as a key
decider of RAF component (e.g., input queue) resource allocation among competing memory operations. In one embodiment, this mode is a bandwidth
balancing allocation mode to optimize the performance by so that each RAF circuit's request/response bandwidth is balanced.
[0136] An example of this allocation mode is described below:
1) Start assigning, in lexical program order, (e.g., unsorted) memory operations to RAF(0)_InputQueue0, RAF(1)_InputQueue0, ... RAF(R-1)_InputQueue0
until every RAF circuit's first component (e.g., input queue) is allocated;
2) Individually track for each RAF circuit ("RAFi"): its total request bandwidth ("Req_BW") and total response bandwidth ("Res_BW"), for example, RAFi:
Req_BWi and Res_BWi for i = 0, 1, 2, ... R-1;
3) Assigning the next, in lexical program order, (e.g., unsorted) memory operation to the next available component (e.g., input queue) of the RAF circuit
that satisfies Minimum(Maximum (Req_BWi + Req_BWNext, Res_BWi + Res_BWNext), for i=0...R-1); and
4) Repeat 2) and 3) until the last of the memory operations is allocated to a RAF circuit.
[0137] In one embodiment, Bandwidth-Balancing allocation mode assigns RAF components (e.g., input queues) to memory operations statically before the application
starts to run and relies on the bandwidth information whose estimation (e.g., the issue counts) is assumed to be provided by a compiler (e.g., that generates a
dataflow graph) and/or late tools (e.g., any of the elaboration, buffer insertion, fusion, and place and route software modules in Figure 19). In certain
embodiments, the Bandwidth-Balancing allocation mode is able to balance request/response bandwidth across the RAF circuits.
X. LATENCY-AWARE LOAD-BALANCING LONGEST-JOB-FIRST (LA-LB-LJF) ALLOCATION MODE
[0138] The tenth RAF allocation mode is referred to as Latency-Aware Load-Balancing Longest-Job-First (LA-LB-LJF) allocation mode. This allocation mode uses both
the bandwidth and latency as a key decider of RAF component (e.g., input queue) resource allocation among competing memory operations. This allocation
mode uses the number of issue times as the bandwidth indicator and uses the multiplication of the bandwidth and latency as the load indicator, as well as a
longest-job-first strategy. In one embodiment, giving high-bandwidth operations the higher arbitration priority helps improve the performance and achieve higher
bandwidth, but memory operations with longer latency may be more likely important and giving them the higher priority during the RAF resource allocations help
reduce the latency and lead to the better performance in certain embodiments. Certain embodiments herein provide an allocation mode that considers both the
bandwidth and latency (e.g., where the load is a multiplication of the bandwidth and latency), and balances the loads across all RAF circuit in a balanced
manner.. In one embodiment, memory operations are sorted according to their loads (e.g., bandwidth value multiplied by latency value) and assigns the memory
operations to RAF components (e.g., input queues) in a balanced way.
[0139] An example of this allocation mode is described below:
1) In certain embodiments, a memory operation's load is represented as the multiplication of its issue count and memory latency. The issue count and/or
latency may be obtained either through feedback (e.g., profile-guided optimization) or from compiler generated relative issue count estimations;
2) Sort all memory operations from the highest load (e.g., load as represented above) to the lowest load (e.g., load as represented above);
3) Start assigning the sorted memory operations to RAF(0)_InputQueue0, RAF(1)_InputQueue0, ... RAF(R-1)_InputQueue0 until every RAF circuit's first
component (e.g., input queue) is allocated;
4) Individually track for each RAF circuit its loads thus far;
5) Assigning the next memory operation to the next available component (e.g., input queue) of the RAF circuit with the minimal load;
6) Repeat 4) and 35 until the last of the memory operations is allocated to a RAF circuit; and
7) Note if two memory operations' issue counts are the same, this allocation mode handles them in the opposite of the lexical program order (or in lexical
program order in another embodiment).
[0140] In one embodiment, LA-LB-LJF RAF allocation mode assigns RAF components (e.g., input queues) to memory operations statically before the application starts
to run and relies on the bandwidth information whose estimation (e.g., the issue counts) is assumed to be provided by a compiler (e.g., that generates a dataflow
graph) and/or late tools (e.g., any of the elaboration, buffer insertion, fusion, and place and route software modules in Figure 19). In certain embodiments, the
LA-LB-LJF allocation mode is able to balance memory operations (e.g., issue counts) across multiple RAF circuits by taking both the bandwidth and latency into
consideration.
XI. BIDDING-GROUP-BASED LOAD-BALANCING LONGEST-JOB-FIRST (BGB-LB-LJF) ALLOCATION MODE
[0141] The eleventh RAF allocation mode is referred to as Bidding-Group-Based Load-Balancing Longest-Job-First (BGB-LB-LJF) allocation mode. Certain embodiments
of this allocation mode includes two parts, (i) first allocating the memory operations to RAF circuit bidding groups described (e.g., as discussed below in
reference to Figure 17) and then allocating them to individual RAF circuits in each bidding group, e.g., where in both parts, a load-balancing allocation mode also
applies a longest-job-first strategy. This allocation mode uses bandwidth as a key decider of RAF component (e.g., input queue) resource allocation among
competing memory operations (e.g., using the number of issue times as the bandwidth indicator). In certain embodiments, this mode gives high-issue count
operations the higher arbitration priority (e.g., to improve the performance and achieve the higher bandwidth) and balances bandwidth across RAF circuits.
[0142] An example of this allocation mode is described below:
1) Sort all memory operations from the highest issue count (e.g., the number of times a particular memory operation executes) to the lowest issue count.
Issue count may be obtained either through feedback (e.g., profile-guided optimization) or from compiler generated relative issue count estimations;
2) Divide a proper subset or RAF circuits into M (e.g., where M is a integer greater than one) number of bidding groups (e.g., Group1, Group2, ... GroupM);
3) As a first part, assign memory operations to RAF bidding groups by:

https://patents.google.com/patent/EP3726389B1/en?q=(high+radix)&oq=high+radix&page=1

19/76

2/14/26, 2:00 PM

EP3726389B1 - Apparatuses, methods, and systems for memory interface circuit allocation in a configurable spatial accelera…
a) Walking through the sorted memory operations and assigning memory operations to Group 1, Group2, ... GroupM until every bidding group is
allocated a memory operation;
b) Count all groups' loads (e.g., issue counts) thus far (e.g., add all issue counts of memory operations in each bidding group);
c) Continue to walk through the sorted memory operations and allocate the next memory operation to the group with the minimal load;
d) Loop back to 3b) until the last memory operation is allocated to a bidding group;
e) Note if two memory operations' issue counts are the same, this allocation mode handles them in the opposite of the lexical program order (or in
lexical program order in another embodiment);

4) As a second part: assign memory operations to RAF circuits within each bidding group. For example, in the first part i.e., 3) above, some number of
memory operations may be assigned to the bidding group Groupi (where i is the group number from 1 to M) with X RAF circuits (RAFXi, RAFXi+1, ...
RAFXi+X-1)
a) Walk through the sorted memory operations assigned to Groupi and assign them to RAFXi, RAFXi+1, ... RAFXi+X-1 until every RAF circuit's first
component (e.g., input queue) is allocated;
b) Count all X RAF circuits' loads so far;
c) Continue to walk through the sorted memory operations and allocate the next memory operation to the next available component (e.g., input
queue) of the RAF circuit with the minimal load;
d) Loop back to b) until the last memory operation is allocated; and
e) Note if two memory operations' issue counts are the same, this allocation mode handles them in the opposite of the lexical program order (or in
lexical program order in another embodiment)
[0143] In one embodiment, BGB-LB-LJF RAF allocation mode assigns RAF components (e.g., input queues) to memory operations statically before the application
starts to run and relies on the bandwidth information whose estimation (e.g., the issue counts) is assumed to be provided by a compiler (e.g., that generates a
dataflow graph) and/or late tools (e.g., any of the elaboration, buffer insertion, fusion, and place and route software modules in Figure 19). In certain
embodiments, the BGB-LB-LJF allocation mode is able to balance memory operations (e.g., issue counts) across RAF bidding groups and within each group.
XII. LOAD-BALANCING LONGEST-JOB-FIRST BIDDING-GROUP-BASED (LB-LJF-BGB) ALLOCATION MODE
[0144] The twelfth RAF allocation mode is referred to as Load-Balancing Longest-Job-First Bidding-Group-Based (LB-LJFBGB) allocation mode. In contrast to 1) ― 4) in
the example discussed above for the eleventh allocation mode, an embodiment of the twelfth allocation mode swaps the order of two parts (e.g., part 3 and part
4) in that example of the eleventh allocation mode. For example, an embodiment of the twelfth allocation mode first allocates the memory operations to RAF
circuits and then groups RAF circuits into bidding groups.
[0145] This allocation mode uses bandwidth as a key decider of RAF component (e.g., input queue) resource allocation among competing memory operations (e.g.,
using the number of issue times as the bandwidth indicator). In certain embodiments, memory operations are sorted according to their issue counts and
assigned to RAF circuits, and then groups RAF circuits into respective bidding groups in a balanced way.
[0146] An example of this allocation mode is described below:
1) Sort all memory operations from the highest issue count (e.g., the number of times a particular memory operation executes) to the lowest issue count.
Issue count may be obtained either through feedback (e.g., profile-guided optimization) or from compiler generated relative issue count estimations;
2) Divide a proper subset or RAF circuits into M (e.g., where M is a integer greater than one) number of bidding groups (e.g., Group1, Group2, ... GroupM),
for example, with "R" number of RAF circuits each having "C" number of components (e.g., input queues) per RAF circuit;
3) Assign memory operations to R virtual RAF circuits and:
a) Walk through the sorted memory operations and assign them to virtual RAF(0), virtual RAF(1), ... virtual RAF(R-1) until every virtual RAF circuit's
first component (e.g., input queue) is allocated;
b) Count all virtual RAF circuits' loads so far (e.g., add all issue counts of memory operations assigned for each virtual RAF);
c) Continue to walk through the sorted memory operations and allocate the next memory operation to the next available component (e.g., input
queue) of the virtual RAF with the minimal load;
d) Loop back to b) until the last memory operation is allocated;
e) Note if two memory operations' issue counts are the same, this allocation mode handles them in the opposite of the lexical program order (or in
lexical program order in another embodiment);
4) Assign R number of virtual RAFs to number M bidding groups while there are X number of virtual RAFs per group (e.g., where M × X = R) and:
a) Calculate all R virtual RAF circuits' total loads (e.g., issue counts) and sort them accordingly;
b) Walk through the sorted virtual RAFs from the highest load to the lowest load and
assign them to Group 1, Group2, ... GroupM until every bidding group is allocated with
a first RAF circuit;
c) Count all bidding groups' loads (e.g., issue counts) so far (e.g., add all issue counts of memory operations for each bidding group);
d) Continue to walk through the sorted virtual RAFs and allocate the next virtual RAF to the group with the minimal load;
e) Loop back to c) until the last virtual RAF is allocated to a bidding group, if a bidding group has X virtual RAFs assigned the bidding group is
removed from consideration and no further RAFs will be assigned to it;
f) Note if two memory operations' issue counts are the same, this allocation mode handles them in the opposite of the lexical program order (or in
lexical program order in another embodiment);
5) Map R virtual RAFs in M bidding groups to R physical (e.g., actual) RAFs in the following way:
X virtual RAFs in Group1 are RAF(0), RAF(1), ... RAFX-1
X virtual RAFs in Group2 are RAFX, RAFX+1, ... RAF2X-1
X virtual RAFs in GroupM are RAFR-X, RAFR-X+1, ... RAF(R-1).
[0147] In one embodiment, LB-LJF-BGB RAF allocation mode assigns RAF components (e.g., input queues) to memory operations statically before the application
starts to run and relies on the bandwidth information whose estimation (e.g., the issue counts) is assumed to be provided by a compiler (e.g., that generates a
dataflow graph) and/or late tools (e.g., any of the elaboration, buffer insertion, fusion, and place and route software modules in Figure 19). In certain
embodiments, the LB-LJF-BGB allocation mode is able to balance memory operations (e.g., issue counts) across RAF bidding groups and in each group.
[0148] Thus in certain embodiments of hardware, there is not a single allocation mode that can help each workload achieve the optimal performance level. Different
workloads benefit from different allocation modes, and thus a single hardware design may be used for multiple dataflow graphs (e.g., having different
workloads) by switching to a different (e.g., of the twelve above) allocation mode to optimize the performance.
[0149] Figure 17 illustrates a network 1704 (e.g., an ACI network) between RAF circuits 1706 and cache banks 1702 that utilizes bid groups for RAF circuit allocation
according to embodiments of the disclosure. To improve ACI network bandwidth, certain embodiments herein adopt an approach based on bid groups. In this
approach, each ACI network endpoint (e.g., RAF circuits or cache banks) is provisioned with a plurality of request storage (e.g., input) queues. Groups of (e.g.,
nearby) RAF circuits (or cache banks) share one of these input queues into each of the cache banks. Figure 17 shows an ACI network 1704 in which the RAF
circuits have been arranged into two groups of four RAF circuits each. In certain embodiments, the ACI network thus allows collisions among the bid groups to
be simultaneously buffered by partitioning requestors into bid groups, e.g., to smooth collisions across time and improve the realizable ACI network bandwidth.

https://patents.google.com/patent/EP3726389B1/en?q=(high+radix)&oq=high+radix&page=1

20/76

2/14/26, 2:00 PM

EP3726389B1 - Apparatuses, methods, and systems for memory interface circuit allocation in a configurable spatial accelera…

[0150] Additionally or alternatively to the RAF circuit allocation modes discussed herein, completion buffers of a single RAF circuit may be allocated according to a
plurality of allocation modes. In certain embodiments, a RAF circuit is responsible for executing memory operations and serves as an interface between
processing elements of a CSA fabric and the memory hierarchy. In one embodiment, a RAF circuit is to rationalize an out-of-order memory subsystem with inorder semantics of the CSA fabric. In this capacity, a RAF circuit may be provisioned with completion buffers (CB), e.g., queue-like structures that can re-order
memory responses and return them to the CSA fabric in the original request order. Therefore, each load or store request is to allocate a slot in the completion
buffer before it can issue into the memory subsystem in certain embodiments.
[0151] However, in one embodiment a plurality (e.g., all) of memory operations that are handled by a single RAF circuit share CB entries (e.g., slots), and thus they
complete for usage of those CB entries(e.g., slots). Without enough CB entries, memory operations may have to stall. In addition, all memory operations handled
by each RAF circuit may compete for issuance (e.g., arbitration) into the (e.g., ACI) network in order to be issued to cache banks.
[0152] In certain embodiments, completion buffer slots are statically partitioned among the memory operations as a part of the program/graph configuration. However,
in other embodiments, it may be desirable that more CB entries are be assigned to memory operations that are more critical for the performance and less CB
entries should be assigned to trivial memory operations that are not on the critical paths. Further, in some cases it is critical to not over provision completion
buffer storage. As noted previously, dataflow graphs often generate large volumes of memory requests, which can lead to inefficient request bandwidth
allocation and, thereby, performance degradation in the case of low bandwidth memory operations. Thus, careful CB allocation acts as a bandwidth balancer and
improves performance.
[0153] Certain embodiments herein optimize CB resources in order to match each dataflow graph's static and dynamic requirements to achieve the optimal
performance goal (e.g., the shortest execution time of a dataflow graph). In one embodiment, "N" (e.g., greater than one) memory operations, op0, op1, op2,...,
opN-1, are assigned to RAFM (e.g., where the value "M" identifies the particular RAF circuit). Note that in certain embodiments, (e.g., in each cycle) only one of
memory operation can be arbitrated to an ACI network in a given time period (e.g., heading to the cache) so that those N number of memory operations are
competing with each other.
[0154] Assuming there are "B" number (e.g., greater than one) of CB entries in RAFM that are shared by N memory operations, in one embodiment, the allocation of
completion buffers is to select between multiple allocation modes to achieve an optimal mapping of the "B" number of CB entries for N memory operations, op0,
op1, op2,..., opN-1, that can achieve the best performance according to Bi number of CB entries are assigned so that:
B = ΣBi

( for i = 0 to N − 1

[0155] In some embodiments, a random allocation causes an extreme imbalance of resource utilization, making the performance unacceptable and unpredictable. In
some embodiments, equal weight allocation can be done, but that may be not flexible and thus not achieve the optimal performance desired. Certain
embodiments herein allocate completion buffer (CB) slots according to bandwidth/latency as a key decider among competing memory operations. In one
embodiment, each memory operation's load indicator is based on bandwidth/latency. In certain embodiments, a CB allocation mode assign some portions of
total CB entries based on the load indicator and applies the equal weight allocation for the rest of the total CB entries to help (e.g., every) memory operations
received the desired amount of resources.
[0156] Figure 18 illustrates a request address file (RAF) circuit 1800 according to embodiments of the disclosure. In the depicted embodiment, RAF circuit 1800
includes a unified physical completion buffer 1827 (e.g., with a respective port of multiple ports coupled to a proper subset of slots 1828, 1830, and 1832) (e.g.,
buffer reserved only for load operations) and unified physical completion buffer 1833 (e.g., with a respective port of multiple ports coupled to a proper subset of
slots1834, 1836, and 1838) (e.g., buffer reserved only for store operations). In one embodiment, at configuration time, the memory load and store operations that
were in a dataflow graph are specified in register(s) 1810. The arcs to those memory operations in the dataflow graphs may then be connected to the input
queues 1822, 1824, and 1826. The arcs from those memory operations are thus to leave completion buffers 1827 or 1833 in certain embodiments. In one
embodiment, set of completion buffer slots 1828, 1830, and 1832 are each a part of single (e.g., unified) completion buffer hardware 1827 that is logically
divided (e.g., assigned) into logical buffers (e.g., logical load buffer 1831 that is assigned (e.g., less than all) slots in unified physical completion buffer 1827)
that are assignable to particular memory operations) and/or set of completion buffer slots 1834, 1836, and 1838 are each a part of another single (e.g., unified)
completion buffer hardware 1833 that is logically divided into logical buffers (e.g., logical store buffer 1835 that is assigned (e.g., less than all) slots in unified
physical completion buffer 1835) that are assignable to particular memory operations). In one embodiment, set of completion buffer slots 1828, 1830, 1832,
1834, 1836, and 1838 are each a part of single (e.g., unified) completion buffer hardware that is logically divided into slots that are assignable to particular
memory operations), e.g., a single completion buffer is formed from a load completion buffer 1827 and a store completion buffer 1833. In certain embodiments,
the assignment of a completion buffer (e.g., a certain number of slots of a unified completion buffer) is according to any of the disclosure herein.
[0157] Dependency tokens (which may be single bits) arrive into queues 1818 and 1820 in certain embodiments. Dependency tokens are to leave from queue 1816 in
certain embodiments. Dependency token counter 1814 may be a compact representation of a queue and track a number of dependency tokens used for any
given input queue. If the dependency token counters 1814 saturate, no additional dependency tokens may be generated for new memory operations in certain
embodiments. Accordingly, a memory ordering circuit (e.g., a RAF circuit) may stall scheduling new memory operations until the dependency token counters
1814 becomes unsaturated. In certain embodiments, ALU 1848 is provided in the RAF circuit 1800 to permit memory address calculations to be directly
performed in the RAF, e.g., where use of the ALU is optionally specified as part of the configuration value (e.g., stored into register 1810). Example uses include:
address displacement calculations in which a base address is added to an offset and/or stateful calculations in which an address may be repeatedly
incremented at the RAF, such as streaming load (sld).
[0158] Optionally, an allocator circuit 1850 may be included, for example, as discussed in Section 2.4. In certain embodiments, allocator circuit 1850 is to allocate
components (e.g., one or more (but less than all) of slots of completion buffer) of the RAF circuit 1800 to a particular memory operation (e.g., a store operation
or a load operation), for example, as indicated by a value stored in register(s) 1810. In one embodiment, the components for an operation are (i) a single input
queue 1822, 1824, or 1826 (e.g., to receive address data from a PE for a load operation requested by a PE from memory (e.g., cache) via port 1801) and a
corresponding completion buffer (e.g., a logical buffer from unified physical completion buffer 1827) (e.g., to receive an indication that the load operation has
been completed from memory) or (ii) a pair of input queues from 1822, 1824, or 1826 (e.g., one to receive data-to-be-stored (e.g., payload data) and one to
receive an address indicating where to store that data from a PE into memory (e.g., cache) via port 1801) and a corresponding completion buffer (logical buffer
from unified physical completion buffer 1833) (e.g., to receive an indication that the store operation has been completed in memory). As an example for a load,
an address arrives into queue 1822 which the scheduler 1812 matches up in register 1810 as being programmed to be a load operation. In certain embodiments,
a completion buffer slot for this load is assigned, e.g., in the order the address arrived. Assuming this particular load in the graph has no dependencies specified,
the address and completion buffer slot are sent off to the memory system by the scheduler (e.g., via memory command 1842) in certain embodiments. When
the result returns to multiplexer 1840 (shown schematically), it is stored into the completion buffer slot it specifies (e.g., as it carried the target slot all along
though the memory system) in certain embodiments. The completion buffer sends results back into CSA network (e.g., mezzanine network 1802) in the order
the addresses arrived in certain embodiments.
[0159] Stores may be similar, for example, except both address and data have to arrive (e.g., from one or more PEs) before any operation is sent off to the memory
system in certain embodiments.
[0160] Memory command 1842 (e.g., ACI message) may include one or more (e.g., any combination) of the following: operation code (opcode), data, physical address,
RAF circuit identification (ID), cache ID (e.g., cache bank ID), or completion buffer slot ID.
[0161] Network 1802 may be a packet switched network, e.g., a mezzanine network as discussed in reference to Figures 48-55. In certain embodiments, RAF circuit
1800 is to send a backpressure value via a network to a producer (e.g., transmitter) component (e.g., PE) when an input queue of the RAF circuit 1800 is full. The

https://patents.google.com/patent/EP3726389B1/en?q=(high+radix)&oq=high+radix&page=1

21/76

2/14/26, 2:00 PM

EP3726389B1 - Apparatuses, methods, and systems for memory interface circuit allocation in a configurable spatial accelera…

backpressure value may cause a stall of the producing component (e.g., PE) from issuing or sending an additional memory request (e.g., to that particularly input
queue) until storage space is available in the input queue of the RAF circuit. In certain embodiments, a receiving component (e.g., PE) is to send a backpressure
value via a network to RAF circuit 1800 to stall the sending of data from completion buffer slots 1828, 1830, or 1832 until storage space is available in the input
queue of the receiving component (e.g., PE). Data sent from and into network 1820 may include a channel identification value that identifies which input queue
1822, 1824, or 1826 that data is to be stored into (e.g., data less the bits that for the identification value). For example, an identification value may include a first
field that identifies a particular input queue of a RAF circuit (e.g., and a second field that identifies that particular RAF circuit from a plurality of RAF circuits).
[0162] Optionally, a translation lookaside buffer (TLB) 1846 may be included to convert a logical address received from an input queue 1822, 1824, or 1826 into a
physical address of the memory (e.g., cache). In one embodiment, the memory accessed is one or more of the cache banks discussed herein.
[0163] In one embodiment, all memory operations in a single RAF circuit share the completion buffer entries and moreover only one memory operation in each RAF
circuit can be granted into memory (e.g., by ACI arbitration circuitry). When the completion buffer entries assigned to a memory operation are all occupied, the
memory request has to stall and waits until some completion buffer entries are cleared out and become available again in certain embodiments. In one
embodiment buffer usage is represented by bandwidth multiplied by latency), and that resultant indicates which buffer(s) should be balanced with respect to
memory operation's bandwidth and/or memory operation's latency in order to optimize throughput. It has been determined that completion buffer resource
usage is a major performance bottleneck for certain dataflow graphs and thus certain modes of allocation of those resources are desirable for certain
situations. The below includes two examples of (e.g., non-random) CB allocation modes for an allocator to utilize to assign CB slot(s) to memory operations in a
RAF circuit in order to reach a desired goal (e.g., optimal performance level). In certain embodiments, the setting of a value into a memory operation register
(e.g., into register(s) 1810 in Figure 18) sets that RAF circuit into the desired CB allocation mode, for example, to cause the reserving of any proper subset (e.g.,
less than all) of the number of slots of load completion buffers or store completion buffers for a particular memory operation. In certain embodiments, a
completion buffer (e.g., a slot or slots reserved for a particular memory operation) receives the result of an instance of a memory operation (e.g., an indication of
success for a store operation or a retrieved data value for a load operation).
I. BANDWIDTH AWARE MODE
[0164] In certain embodiments, more critical memory operations are to be assigned more CB entries (e.g., slots) to minimize the time that those critical memory
operations are stalled because they impact the final performance more. Certain embodiments herein identify those critical memory operations by identifying
memory operations with the highest bandwidth, and assigns more hardware resources in order to achieve a higher performance level. This CB allocation mode is
a bandwidth aware completion buffer assignment (e.g., where the higher the bandwidth for a memory operation, the more CB entries that are assigned). In one
embodiment, the number of issue times of each memory operation for particular dataflow graph is used as the indication of the bandwidth. This issue count
may be obtained either through feedback (e.g., profile-guided optimization) or from compiler generated relative issue count estimations. In one embodiment, in
order to guarantee that all memory operations have the desired completion buffer resources, some portions of completion buffer resources are evenly
distributed to other memory operations.
[0165] An example of this Bandwidth Aware allocation mode is described below:
1) "R" number of RAF circuits (e.g., per CSA tile) and "C" memory operations (e.g., input queues) per RAF circuit: RAF0_InputQueue0, ...,
RAF0_InputQueueC-1, RAF1_InputQueue0, ..., RAF1_InputQueueC-1, ..., RAFR-1_InputQueue0, ..., RAFR-1_InputQueueC-1;
2) N memory operations, op0, op1, op2 ,... opN-1, are assigned to RAFM (e.g., where the value "M" identifies the particular RAF circuit) and there are totally
"B" number of CB entries in RAFM;
3) Calculate the total bandwidth of RAFM: Total_Bandwidth_M = ∑ IssueCount_opi (for i = 0 to (C-1));
4) Memory operation opi is assigned according to:
{[(B × P% ÷ N) + (B × (1 ― P%) × (Issue_Count_opi/Total_Bandwidth_M)]} CB entries, where the first part is the even distribution and the second part is the
proportional distribution. In this embodiment, each memory operation is to receive the completion buffer resources proportional to its bandwidth (e.g.,
estimated by the issue count). To further improve the performance and to guarantee that each memory operation gets the minimal amount of completion
buffer resources in order to run smoothly, certain embodiments divide the whole completion buffer resources into two pools. For example, with the first
pool assigned to all memory operations evenly (named even pool, i.e., P% of total CB entries) and the second pool assigned according to the bandwidth
as described above (named proportional pool or bandwidth-aware pool, i.e., 1-P% of total CB entries). The division may be different for different graphs
and may be dynamic.
2. LATENCY AND BANDWIDTH AWARE MODE
[0166] Though in certain embodiments, memory operations with the higher bandwidth are more critical to performance, in other embodiments, memory operations with
the longer latency are more critical to performance and assigning them more completion buffer improves the performance.
[0167] This CB allocation mode is a bandwidth and latency aware completion buffer assignment (e.g., with a load indication value being the resultant of the
multiplication of the bandwidth and latency for each memory operation). For example, with the highest loads being assigned the most CB entries. This issue
count and latency may be obtained either through feedback (e.g., profile-guided optimization) or from compiler generated relative issue count estimations. In
one embodiment, to guarantee that all memory operations have the minimal completion buffer resources, some portions of completion buffer resources are
evenly distributed to all memory operations. Thus in certain embodiments, CB slots are allocated based on the bandwidth-delay product of the various memory
operations assigned to the particular RAF circuit.
[0168] An example of this Latency and Bandwidth Aware allocation mode is described below:
1) "R" number of RAF circuits (e.g., per CSA tile) and "C" memory operations (e.g., input queues) per RAF circuit: RAF0_InputQueue0, ...,
RAF0_InputQueueC-1, RAF1_InputQueue0, ..., RAF1_InputQueueC-1, ..., RAFR-1_InputQueue0, ..., RAFR-1 InputQueueC-1;
2) N memory operations, op0, op1, op2 ,... opN-1, are assigned to RAFM (e.g., where the value "M" identifies the particular RAF circuit) and there are totally
"B" number of CB entries in RAFM;
3) Calculate the total load of RAFM: Total_Load_M = ∑ (IssueCount_opi × Latency_opi ) (for i = 0 to (C-1));
[0169] 4) Memory operation opi is assigned according to:
{[(B × P% ÷ N) + (B × (1 ― P%) × ((Issue_Count_opi x Latency_opi )/ Total_Load_M)]} CB entries, where the first part is the even distribution and the second part is
the proportional distribution. In this embodiment, each memory operation is to receive the completion buffer resources proportional to its load, estimated by the
multiplication of the issue count and the latency. To further improve the performance and to guarantee that each memory operation gets the minimal amount of
completion buffer resources in order to run smoothly, certain embodiments divide the whole completion buffer resources into two pools. For example, with the
first pool assigned to all memory operations evenly (named even pool, i.e., P% of total CB entries) and the second pool is assigned according to the load as
described above (named proportional pool or load-aware pool, i.e., 1-P% of total CB entries). The division may be different for different dataflow graphs and may
be dynamic.
[0170] Figure 19 illustrates a software flow 1900 for allocation according to embodiments of the disclosure. The depicted flow 1900 indicates a path by which highlevel languages (e.g., C, C++, and Fortran) can be translated into machine media executable by embodiments of CSA software. In particular, communications
paths between the phases represent not only the communication of such media between phases of the translation but also represent the communication of
metadata, such as bandwidth consumption, execution frequency, and loop membership. Compilation begins from source code 1902, which may include
annotations 1918 to be consumed by later stages of the translator (e.g. RAF allocator 1914). The compiler 1906 translates the source code into an abstract

https://patents.google.com/patent/EP3726389B1/en?q=(high+radix)&oq=high+radix&page=1

22/76

2/14/26, 2:00 PM

EP3726389B1 - Apparatuses, methods, and systems for memory interface circuit allocation in a configurable spatial accelera…

dataflow graph operating set architecture. The abstract operating set architecture is then mapped to a specific CSA instance (e.g., hardware fabric) by the
elaboration phase 1908 which replaces certain complex dataflow operators with machine-specific implementations. Optionally, abstract dataflow graph
operating set architecture sources 1904, arising, for example, by hand coding, may be introduced as an input for elaboration. The elaborated dataflow graph is
further modified by buffer insertion 1910, which adds storage to the graph, and fusion 1912, which collapses some operations into single processing elements.
These phases may be run multiple times, in a looping fashion. Finally, the graph is placed and routed 1916 in the actual CSA instance (e.g., hardware fabric). The
RAF allocation 1914 considered previously in Section 2.4 may participate in place and route, to assist the translation mechanism in providing a solution that will
improve program performance subject to the behaviors of RAF allocation.
[0171] At least some embodiments of the disclosed technologies can be described in view of the following examples:
Example 1. An apparatus (e.g., a processor) comprising: a spatial array of processing elements; a cache; a first memory interface circuit comprising a
first port into the cache, a first plurality of input queues to store data for memory requests from the spatial array of processing elements, and a first
memory operation register; a second memory interface circuit comprising a second port into the cache, a second plurality of input queues to store data
for memory requests from the spatial array of processing elements, and a second memory operation register; and an allocator circuit to: set respective
first values into the first memory operation register and the second memory operation register according to a first allocation mode to couple the first
port to a first input queue of the first plurality of input queues that stores data for memory requests from a first processing element of the spatial array
of processing elements, couple the second port to a first input queue of the second plurality of input queues that stores data for memory requests from
a second processing element of the spatial array of processing elements, and couple the first port to a second input queue of the first plurality of input
queues that stores data for memory requests from a third processing element of the spatial array of processing elements, and set respective second
values into the first memory operation register and the second memory operation register according to a second allocation mode to couple the first port
to the first input queue of the first plurality of input queues that stores data for memory requests from the first processing element of the spatial array of
processing elements, couple the second port to the first input queue of the second plurality of input queues that stores data for memory requests from
the second processing element of the spatial array of processing elements, and couple the second port to a second input queue of the second plurality
of input queues that stores data for memory requests from the third processing element of the spatial array of processing elements.
Example 2. The apparatus of example 1, wherein the respective first values set in the first memory operation register and the second memory operation
register causes a first completion buffer of the first memory interface circuit to receive a completion indication from the cache for memory requests
from the first processing element, a first completion buffer of the second memory interface circuit to receive a completion indication from the cache for
memory requests from the second processing element, and a second completion buffer of the first memory interface circuit to receive a completion
indication from the cache for memory requests from the third processing element.
Example 3. The apparatus of example 2, wherein the first completion buffer of the first memory interface circuit is a first proper subset of slots of a
unified completion buffer of the first memory interface circuit, the second completion buffer of the first memory interface circuit is a second proper
subset of slots of the unified completion buffer of the first memory interface circuit, and the allocator circuit assigns a largest number of buffer slots of
the unified completion buffer to the one of the first processing element or the third processing element that issues a largest number of memory
requests for a dataflow graph.
Example 4. The apparatus of example 2, wherein the first completion buffer of the first memory interface circuit is a first proper subset of slots of a
unified completion buffer of the first memory interface circuit, the second completion buffer of the first memory interface circuit is a second proper
subset of slots of the unified completion buffer of the first memory interface circuit, and the allocator circuit assigns a largest number of buffer slots of
the unified completion buffer to the one of the first processing element or the third processing element that has a longest latency for memory requests
for a dataflow graph.
Example 5. The apparatus of example 1, wherein the second allocation mode allocates input queues based on issuance by the first processing element
of a largest number of memory requests for a dataflow graph, the second processing element of a next largest number of memory requests for the
dataflow graph, and the third processing element of a smaller number of memory requests for the dataflow graph than the next largest number of
memory requests.
Example 6. The apparatus of example 1, wherein the allocator circuit allocates a next input queue of the first memory interface circuit or the second
memory interface circuit in program order to the one of the first memory interface circuit or the second memory interface circuit with a fewest number
of memory requests assigned to its input queues for a dataflow graph.
Example 7. The apparatus of example 1, wherein the allocator circuit switches from the first allocation mode to the second allocation mode in runtime
for a dataflow graph.
Example 8. The apparatus of example 1, wherein the first memory interface circuit, when in the first allocation mode, sends a first backpressure value to
stall the first processing element from issuing an additional memory request when the first input queue of the first memory interface circuit is not
available for data for the additional memory request, the second memory interface circuit, when in the first allocation mode, sends a second
backpressure value to stall the second processing element from issuing an additional memory request when the first input queue of the second
memory interface circuit is not available for data for the additional memory request, and the first memory interface circuit, when in the first allocation
mode, sends a third backpressure value to stall the third processing element from issuing an additional memory request when the second input queue
of the first memory interface circuit is not available for data for the additional memory request.
Example 9. A method comprising: coupling a spatial array of processing elements to a first memory interface circuit comprising a first port into a cache,
a first plurality of input queues to store data for memory requests from the spatial array of processing elements, and a first memory operation register,
and to a second memory interface circuit comprising a second port into the cache, a second plurality of input queues to store data for memory requests
from the spatial array of processing elements, and a second memory operation register; setting respective first values into the first memory operation
register and the second memory operation register according to a first allocation mode to couple the first port to a first input queue of the first plurality
of input queues that stores data for memory requests from a first processing element of the spatial array of processing elements, couple the second
port to a first input queue of the second plurality of input queues that stores data for memory requests from a second processing element of the spatial
array of processing elements, and couple the first port to a second input queue of the first plurality of input queues that stores data for memory
requests from a third processing element of the spatial array of processing elements; and setting respective second values into the first memory
operation register and the second memory operation register according to a second allocation mode to couple the first port to the first input queue of
the first plurality of input queues that stores data for memory requests from the first processing element of the spatial array of processing elements,
couple the second port to the first input queue of the second plurality of input queues that stores data for memory requests from the second processing
element of the spatial array of processing elements, and couple the second port to a second input queue of the second plurality of input queues that
stores data for memory requests from the third processing element of the spatial array of processing elements.
Example 10. The method of example 9, wherein setting the respective first values in the first memory operation register and the second memory
operation register causes a first completion buffer of the first memory interface circuit to receive a completion indication from the cache for memory
requests from the first processing element, a first completion buffer of the second memory interface circuit to receive a completion indication from the
cache for memory requests from the second processing element, and a second completion buffer of the first memory interface circuit to receive a
completion indication from the cache for memory requests from the third processing element.

https://patents.google.com/patent/EP3726389B1/en?q=(high+radix)&oq=high+radix&page=1

23/76

2/14/26, 2:00 PM

EP3726389B1 - Apparatuses, methods, and systems for memory interface circuit allocation in a configurable spatial accelera…
Example 11. The method of example 10, wherein the first completion buffer of the first memory interface circuit is a first proper subset of slots of a
unified completion buffer of the first memory interface circuit, the second completion buffer of the first memory interface circuit is a second proper
subset of slots of the unified completion buffer of the first memory interface circuit, and the setting of the respective first values or the respective
second values comprises assigning a largest number of buffer slots of the unified completion buffer to the one of the first processing element or the
third processing element that issues a largest number of memory requests for a dataflow graph.
Example 12. The method of example 10, wherein the first completion buffer of the first memory interface circuit is a first proper subset of slots of a
unified completion buffer of the first memory interface circuit, the second completion buffer of the first memory interface circuit is a second proper
subset of slots of the unified completion buffer of the first memory interface circuit, and the setting of the respective first values or the respective
second values comprises assigning a largest number of buffer slots of the unified completion buffer to the one of the first processing element or the
third processing element that has a longest latency for memory requests for a dataflow graph.
Example 13. The method of example 9, wherein the second allocation mode allocates input queues based on issuance by the first processing element
of a largest number of memory requests for a dataflow graph, the second processing element of a next largest number of memory requests for the
dataflow graph, and the third processing element of a smaller number of memory requests for the dataflow graph than the next largest number of
memory requests.
Example 14. The method of example 9, wherein the setting of the respective first values or the respective second values comprises allocating a next
input queue of the first memory interface circuit or the second memory interface circuit in program order to the one of the first memory interface circuit
or the second memory interface circuit with a fewest number of memory requests assigned to its input queues for a dataflow graph.
Example 15. The method of example 9, wherein the method comprises switching from the first allocation mode to the second allocation mode in
runtime for a dataflow graph.
Example 16. The method of example 9, wherein the first memory interface circuit, when in the first allocation mode, sends a first backpressure value to
stall the first processing element from issuing an additional memory request when the first input queue of the first memory interface circuit is not
available for data for the additional memory request, the second memory interface circuit, when in the first allocation mode, sends a second
backpressure value to stall the second processing element from issuing an additional memory request when the first input queue of the second
memory interface circuit is not available for data for the additional memory request, and the first memory interface circuit, when in the first allocation
mode, sends a third backpressure value to stall the third processing element from issuing an additional memory request when the second input queue
of the first memory interface circuit is not available for data for the additional memory request.
Example 17. A non-transitory machine readable medium that stores code that when executed by a machine causes the machine to perform a method
comprising: coupling a spatial array of processing elements to a first memory interface circuit comprising a first port into a cache, a first plurality of
input queues to store data for memory requests from the spatial array of processing elements, and a first memory operation register, and to a second
memory interface circuit comprising a second port into the cache, a second plurality of input queues to store data for memory requests from the spatial
array of processing elements, and a second memory operation register; setting respective first values into the first memory operation register and the
second memory operation register according to a first allocation mode to couple the first port to a first input queue of the first plurality of input queues
that stores data for memory requests from a first processing element of the spatial array of processing elements, couple the second port to a first input
queue of the second plurality of input queues that stores data for memory requests from a second processing element of the spatial array of
processing elements, and couple the first port to a second input queue of the first plurality of input queues that stores data for memory requests from a
third processing element of the spatial array of processing elements; and setting respective second values into the first memory operation register and
the second memory operation register according to a second allocation mode to couple the first port to the first input queue of the first plurality of input
queues that stores data for memory requests from the first processing element of the spatial array of processing elements, couple the second port to
the first input queue of the second plurality of input queues that stores data for memory requests from the second processing element of the spatial
array of processing elements, and couple the second port to a second input queue of the second plurality of input queues that stores data for memory
requests from the third processing element of the spatial array of processing elements.
Example 18. The non-transitory machine readable medium that of example 17, wherein setting the respective first values in the first memory operation
register and the second memory operation register causes a first completion buffer of the first memory interface circuit to receive a completion
indication from the cache for memory requests from the first processing element, a first completion buffer of the second memory interface circuit to
receive a completion indication from the cache for memory requests from the second processing element, and a second completion buffer of the first
memory interface circuit to receive a completion indication from the cache for memory requests from the third processing element.
Example 19. The non-transitory machine readable medium that of example 18, wherein the first completion buffer of the first memory interface circuit is
a first proper subset of slots of a unified completion buffer of the first memory interface circuit, the second completion buffer of the first memory
interface circuit is a second proper subset of slots of the unified completion buffer of the first memory interface circuit, and the setting of the respective
first values or the respective second values comprises assigning a largest number of buffer slots of the unified completion buffer to the one of the first
processing element or the third processing element that issues a largest number of memory requests for a dataflow graph.
Example 20. The non-transitory machine readable medium that of example 18, wherein the first completion buffer of the first memory interface circuit is
a first proper subset of slots of a unified completion buffer of the first memory interface circuit, the second completion buffer of the first memory
interface circuit is a second proper subset of slots of the unified completion buffer of the first memory interface circuit, and the setting of the respective
first values or the respective second values comprises assigning a largest number of buffer slots of the unified completion buffer to the one of the first
processing element or the third processing element that has a longest latency for memory requests for a dataflow graph.
Example 21. The non-transitory machine readable medium that of example 17, wherein the second allocation mode allocates input queues based on
issuance by the first processing element of a largest number of memory requests for a dataflow graph, the second processing element of a next largest
number of memory requests for the dataflow graph, and the third processing element of a smaller number of memory requests for the dataflow graph
than the next largest number of memory requests.
Example 22. The non-transitory machine readable medium that of example 17, wherein the setting of the respective first values or the respective second
values comprises allocating a next input queue of the first memory interface circuit or the second memory interface circuit in program order to the one
of the first memory interface circuit or the second memory interface circuit with a fewest number of memory requests assigned to its input queues for a
dataflow graph.
Example 23. The non-transitory machine readable medium that of example 17, wherein the method comprises switching from the first allocation mode
to the second allocation mode in runtime for a dataflow graph.
Example 24. The non-transitory machine readable medium that of example 17, wherein the first memory interface circuit, when in the first allocation
mode, sends a first backpressure value to stall the first processing element from issuing an additional memory request when the first input queue of the
first memory interface circuit is not available for data for the additional memory request, the second memory interface circuit, when in the first
allocation mode, sends a second backpressure value to stall the second processing element from issuing an additional memory request when the first
input queue of the second memory interface circuit is not available for data for the additional memory request, and the first memory interface circuit,
when in the first allocation mode, sends a third backpressure value to stall the third processing element from issuing an additional memory request
when the second input queue of the first memory interface circuit is not available for data for the additional memory request.

https://patents.google.com/patent/EP3726389B1/en?q=(high+radix)&oq=high+radix&page=1

24/76

2/14/26, 2:00 PM

EP3726389B1 - Apparatuses, methods, and systems for memory interface circuit allocation in a configurable spatial accelera…

2.5 Network Resources, e.g., Circuitry, to Perform (e.g., Dataflow) Operations
[0172] In certain embodiments, processing elements (PEs) communicate using dedicated virtual circuits which are formed by statically configuring a (e.g., circuit
switched) communications network. These virtual circuits may be flow controlled and fully back-pressured, e.g., such that a PE will stall if either the source has
no data or its destination is full. At runtime, data may flow through the PEs implementing the mapped dataflow graph (e.g., mapped algorithm). For example,
data may be streamed in from memory, through the (e.g., fabric area of a) spatial array of processing elements, and then back out to memory.
[0173] Such an architecture may achieve remarkable performance efficiency relative to traditional multicore processors: compute, e.g., in the form of PEs, may be
simpler and more numerous than cores and communications may be direct, e.g., as opposed to an extension of the memory system. However, the (e.g., fabric
area of) spatial array of processing elements may be tuned for the implementation of compiler-generated expression trees, which may feature little multiplexing
or demultiplexing. Certain embodiments herein extend (for example, via network resources, such as, but not limited to, network dataflow endpoint circuits) the
architecture to support (e.g., high-radix) multiplexing and/or demultiplexing, for example, especially in the context of function calls.
[0174] Spatial arrays, such as the spatial array of processing elements 101 in Figure 1, may use (e.g., packet switched) networks for communications. Certain
embodiments herein provide circuitry to overlay high-radix dataflow operations on these networks for communications. For example, certain embodiments
herein utilize the existing network for communications (e.g., interconnect network 104 described in reference to Figure 1) to provide data routing capabilities
between processing elements and other components of the spatial array, but also augment the network (e.g., network endpoints) to support the performance
and/or control of some (e.g., less than all) of dataflow operations (e.g., without utilizing the processing elements to perform those dataflow operations). In one
embodiment, (e.g., high radix) dataflow operations are supported with special hardware structures (e.g. network dataflow endpoint circuits) within a spatial array,
for example, without consuming processing resources or degrading performance (e.g., of the processing elements).
[0175] In one embodiment, a circuit switched network between two points (e.g., between a producer and consumer of data) includes a dedicated communication line
between those two points, for example, with (e.g., physical) switches between the two points set to create a (e.g., exclusive) physical circuit between the two
points. In one embodiment, a circuit switched network between two points is set up at the beginning of use of the connection between the two points and
maintained throughout the use of the connection. In another embodiment, a packet switched network includes a shared communication line (e.g., channel)
between two (e.g., or more) points, for example, where packets from different connections share that communication line (for example, routed according to data
of each packet, e.g., in the header of a packet including a header and a payload). An example of a packet switched network is discussed below, e.g., in reference
to a mezzanine network.
[0176] Figure 20 illustrates a data flow graph 2000 of a pseudocode function call 2001 according to embodiments of the disclosure. Function call 2001 is to load two
input data operands (e.g., indicated by pointers ∗a and ∗b, respectively), and multiply them together, and return the resultant data. This or other functions may
be performed multiple times (e.g., in a dataflow graph). The dataflow graph in Figure 20 illustrates a PickAny dataflow operator 2002 to perform the operation of
selecting a control data (e.g., an index) (for example, from call sites 2002A) and copying with copy dataflow operator 2004 that control data (e.g., index) to each
of the first Pick dataflow operator 2006, second Pick dataflow operator 2006, and Switch dataflow operator 2016. In one embodiment, an index (e.g., from the
PickAny thus inputs and outputs data to the same index position, e.g., of [0, 1...M], where M is an integer. First Pick dataflow operator 2006 may then pull one
input data element of a plurality of input data elements 2006A according to the control data, and use the one input data element as (∗a) to then load the input
data value stored at ∗a with load dataflow operator 2010. Second Pick dataflow operator 2008 may then pull one input data element of a plurality of input data
elements 2008A according to the control data, and use the one input data element as (∗b) to then load the input data value stored at ∗b with load dataflow
operator 2012. Those two input data values may then be multiplied by multiplication dataflow operator 2014 (e.g., as a part of a processing element). The
resultant data of the multiplication may then be routed (e.g., to a downstream processing element or other component) by Switch dataflow operator 2016, e.g.,
to call sites 2016A, for example, according to the control data (e.g., index) to Switch dataflow operator 2016.
[0177] Figure 20 is an example of a function call where the number of dataflow operators used to manage the steering of data (e.g., tokens) may be significant, for
example, to steer the data to and/or from call sites. In one example, one or more of PickAny dataflow operator 2002, first Pick dataflow operator 2006, second
Pick dataflow operator 2006, and Switch dataflow operator 2016 may be utilized to route (e.g., steer) data, for example, when there are multiple (e.g., many) call
sites. In an embodiment where a (e.g., main) goal of introducing a multiplexed and/or demultiplexed function call is to reduce the implementation area of a
particular dataflow graph, certain embodiments herein (e.g., of microarchitecture) reduce the area overhead of such multiplexed and/or demultiplexed (e.g.,
portions) of dataflow graphs.
[0178] Figure 21 illustrates a spatial array 2101 of processing elements (PEs) with a plurality of network dataflow endpoint circuits (2102, 2104, 2106) according to
embodiments of the disclosure. Spatial array 2101 of processing elements may include a communications (e.g., interconnect) network in between components,
for example, as discussed herein. In one embodiment, communications network is one or more (e.g., channels of a) packet switched communications network.
In one embodiment, communications network is one or more circuit switched, statically configured communications channels. For example, a set of channels
coupled together by a switch (e.g., switch 2110 in a first network and switch 2111 in a second network). The first network and second network may be separate
or coupled together. For example, switch 2110 may couple one or more of a plurality (e.g., four) data paths therein together, e.g., as configured to perform an
operation according to a dataflow graph. In one embodiment, the number of data paths is any plurality. Processing element (e.g., processing element 2108) may
be as disclosed herein, for example, as in Figure 10. Accelerator tile 2100 includes a memory/cache hierarchy interface 2112, e.g., to interface the accelerator
tile 2100 with a memory and/or cache. A data path may extend to another tile or terminate, e.g., at the edge of a tile. A processing element may include an input
buffer (e.g., buffer 2109) and an output buffer.
[0179] Operations may be executed based on the availability of their inputs and the status of the PE. A PE may obtain operands from input channels and write results to
output channels, although internal register state may also be used. Certain embodiments herein include a configurable dataflow-friendly PE. Figure 10 shows a
detailed block diagram of one such PE: the integer PE. This PE consists of several I/O buffers, an ALU, a storage register, some instruction registers, and a
scheduler. Each cycle, the scheduler may select an instruction for execution based on the availability of the input and output buffers and the status of the PE.
The result of the operation may then be written to either an output buffer or to a (e.g., local to the PE) register. Data written to an output buffer may be
transported to a downstream PE for further processing. This style of PE may be extremely energy efficient, for example, rather than reading data from a complex,
multi-ported register file, a PE reads the data from a register. Similarly, instructions may be stored directly in a register, rather than in a virtualized instruction
cache.
[0180] Instruction registers may be set during a special configuration step. During this step, auxiliary control wires and state, in addition to the inter-PE network, may be
used to stream in configuration across the several PEs comprising the fabric. As result of parallelism, certain embodiments of such a network may provide for
rapid reconfiguration, e.g., a tile sized fabric may be configured in less than about 10 microseconds.
[0181] Further, depicted accelerator tile 2100 includes packet switched communications network 2114, for example, as part of a mezzanine network, e.g., as described
below. Certain embodiments herein allow for (e.g., a distributed) dataflow operations (e.g., operations that only route data) to be performed on (e.g., within) the
communications network (e.g., and not in the processing element(s)). As an example, a distributed Pick dataflow operation of a dataflow graph is depicted in
Figure 21. Particularly, distributed pick is implemented using three separate configurations on three separate network (e.g., global) endpoints (e.g., network
dataflow endpoint circuits (2102, 2104, 2106)). Dataflow operations may be distributed, e.g., with several endpoints to be configured in a coordinated manner.
For example, a compilation tool may understand the need for coordination. Endpoints (e.g., network dataflow endpoint circuits) may be shared among several
distributed operations, for example, a dataflow operation (e.g., pick) endpoint may be collated with several sends related to the dataflow operation (e.g., pick). A
distributed dataflow operation (e.g., pick) may generate the same result the same as a non-distributed dataflow operation (e.g., pick). In certain embodiment, a
difference between distributed and non-distributed dataflow operations is that in the distributed dataflow operations have their data (e.g., data to be routed, but

https://patents.google.com/patent/EP3726389B1/en?q=(high+radix)&oq=high+radix&page=1

25/76

2/14/26, 2:00 PM

EP3726389B1 - Apparatuses, methods, and systems for memory interface circuit allocation in a configurable spatial accelera…

which may not include control data) over a packet switched communications network, e.g., with associated flow control and distributed coordination. Although
different sized processing elements (PE) are shown, in one embodiment, each processing element is of the same size (e.g., silicon area). In one embodiment, a
buffer element to buffer data may also be included, e.g., separate from a processing element.
[0182] As one example, a pick dataflow operation may have a plurality of inputs and steer (e.g., route) one of them as an output, e.g., as in Figure 20. Instead of utilizing
a processing element to perform the pick dataflow operation, it may be achieved with one or more of network communication resources (e.g., network dataflow
endpoint circuits). Additionally or alternatively, the network dataflow endpoint circuits may route data between processing elements, e.g., for the processing
elements to perform processing operations on the data. Embodiments herein may thus utilize to the communications network to perform (e.g., steering)
dataflow operations. Additionally or alternatively, the network dataflow endpoint circuits may perform as a mezzanine network discussed below.
[0183] In the depicted embodiment, packet switched communications network 2114 may handle certain (e.g., configuration) communications, for example, to program
the processing elements and/or circuit switched network (e.g., network 2113, which may include switches). In one embodiment, a circuit switched network is
configured (e.g., programmed) to perform one or more operations (e.g., dataflow operations of a dataflow graph).
[0184] Packet switched communications network 2114 includes a plurality of endpoints (e.g., network dataflow endpoint circuits (2102, 2104, 2106). In one
embodiment, each endpoint includes an address or other indicator value to allow data to be routed to and/or from that endpoint, e.g., according to (e.g., a header
of) a data packet.
[0185] Additionally or alternatively to performing one or more of the above, packet switched communications network 2114 may perform dataflow operations. Network
dataflow endpoint circuits (2102, 2104, 2106) may be configured (e.g., programmed) to perform a (e.g., distributed pick) operation of a dataflow graph.
Programming of components (e.g., a circuit) are described herein. An embodiment of configuring a network dataflow endpoint circuit (e.g., an operation
configuration register thereof) is discussed in reference to Figure 22.
[0186] As an example of a distributed pick dataflow operation, network dataflow endpoint circuits (2102, 2104, 2106) in Figure 21 may be configured (e.g.,
programmed) to perform a distributed pick operation of a dataflow graph. An embodiment of configuring a network dataflow endpoint circuit (e.g., an operation
configuration register thereof) is discussed in reference to Figure 22. Additionally or alternatively to configuring remote endpoint circuits, local endpoint circuits
may also be configured according to this disclosure.
[0187] Network dataflow endpoint circuit 2102 may be configured to receive input data from a plurality of sources (e.g., network dataflow endpoint circuit 2104 and
network dataflow endpoint circuit 2106), and to output resultant data, e.g., as in Figure 20), for example, according to control data. Network dataflow endpoint
circuit 2104 may be configured to provide (e.g., send) input data to network dataflow endpoint circuit 2102, e.g., on receipt of the input data from processing
element 2122. This may be referred to as Input 0 in Figure 21. In one embodiment, circuit switched network is configured (e.g., programmed) to provide a
dedicated communication line between processing element 2122 and network dataflow endpoint circuit 2104 along path 2124. Network dataflow endpoint
circuit 2106 may be configured to provide (e.g., send) input data to network dataflow endpoint circuit 2102, e.g., on receipt of the input data from processing
element 2120. This may be referred to as Input 1 in Figure 21. In one embodiment, circuit switched network is configured (e.g., programmed) to provide a
dedicated communication line between processing element 2120 and network dataflow endpoint circuit 2106 along path 2116.
[0188] When network dataflow endpoint circuit 2104 is to transmit input data to network dataflow endpoint circuit 2102 (e.g., when network dataflow endpoint circuit
2102 has available storage room for the data and/or network dataflow endpoint circuit 2104 has its input data), network dataflow endpoint circuit 2104 may
generate a packet (e.g., including the input data and a header to steer that data to network dataflow endpoint circuit 2102 on the packet switched
communications network 2114 (e.g., as a stop on that (e.g., ring) network 2114). This is illustrated schematically with dashed line 2126 in Figure 21. Although
the example shown in Figure 21 utilizes two sources (e.g., two inputs) a single or any plurality (e.g., greater than two) of sources (e.g., inputs) may be utilized.
[0189] When network dataflow endpoint circuit 2106 is to transmit input data to network dataflow endpoint circuit 2102 (e.g., when network dataflow endpoint circuit
2102 has available storage room for the data and/or network dataflow endpoint circuit 2106 has its input data), network dataflow endpoint circuit 2104 may
generate a packet (e.g., including the input data and a header to steer that data to network dataflow endpoint circuit 2102 on the packet switched
communications network 2114 (e.g., as a stop on that (e.g., ring) network 2114). This is illustrated schematically with dashed line 2118 in Figure 21. Though a
mesh network is shown, other network topologies may be used.
[0190] Network dataflow endpoint circuit 2102 (e.g., on receipt of the Input 0 from network dataflow endpoint circuit 2104, Input 1 from network dataflow endpoint
circuit 2106, and/or control data) may then perform the programmed dataflow operation (e.g., a Pick operation in this example). The network dataflow endpoint
circuit 2102 may then output the according resultant data from the operation, e.g., to processing element 2108 in Figure 21. In one embodiment, circuit switched
network is configured (e.g., programmed) to provide a dedicated communication line between processing element 2108 (e.g., a buffer thereof) and network
dataflow endpoint circuit 2102 along path 2128. A further example of a distributed Pick operation is discussed below in reference to Figure 34-36.
[0191] In one embodiment, the control data to perform an operation (e.g., pick operation) comes from other components of the spatial array, e.g., a processing element
or through network. An example of this is discussed below in reference to Figure 22. Note that Pick operator is shown schematically in endpoint 2102, and may
not be a multiplexer circuit, for example, see the discussion below of network dataflow endpoint circuit 2200 in Figure 22.
[0192] In certain embodiments, a dataflow graph may have certain operations performed by a processing element and certain operations performed by a
communication network (e.g., network dataflow endpoint circuit or circuits).
[0193] Figure 22 illustrates a network dataflow endpoint circuit 2200 according to embodiments of the disclosure. Although multiple components are illustrated in
network dataflow endpoint circuit 2200, one or more instances of each component may be utilized in a single network dataflow endpoint circuit. An embodiment
of a network dataflow endpoint circuit may include any (e.g., not all) of the components in Figure 22.
[0194] Figure 22 depicts the microarchitecture of a (e.g., mezzanine) network interface showing embodiments of main data (solid line) and control data (dotted) paths.
This microarchitecture provides a configuration storage and scheduler to enable (e.g., high-radix) dataflow operators. Certain embodiments herein include data
paths to the scheduler to enable leg selection and description. Figure 22 shows a high-level microarchitecture of a network (e.g., mezzanine) endpoint (e.g.,
stop), which may be a member of a ring network for context. To support (e.g., high-radix) dataflow operations, the configuration of the endpoint (e.g., operation
configuration storage 2226) to include configurations that examine multiple network (e.g., virtual) channels (e.g., as opposed to single virtual channels in a
baseline implementation). Certain embodiments of network dataflow endpoint circuit 2200 include data paths from ingress and to egress to control the
selection of (e.g., pick and switch types of operations), and/or to describe the choice made by the scheduler in the case of PickAny dataflow operators or
SwitchAny dataflow operators. Flow control and backpressure behavior may be utilized in each communication channel, e.g., in a (e.g., packet switched
communications) network and (e.g., circuit switched) network (e.g., fabric of a spatial array of processing elements).
[0195] As one description of an embodiment of the microarchitecture, a pick dataflow operator may function to pick one output of resultant data from a plurality of
inputs of input data, e.g., based on control data. A network dataflow endpoint circuit 2200 may be configured to consider one of the spatial array ingress
buffer(s) 2202 of the circuit 2200 (e.g., data from the fabric being control data) as selecting among multiple input data elements stored in network ingress
buffer(s) 2224 of the circuit 2200 to steer the resultant data to the spatial array egress buffer 2208 of the circuit 2200. Thus, the network ingress buffer(s) 2224
may be thought of as inputs to a virtual mux, the spatial array ingress buffer 2202 as the multiplexer select, and the spatial array egress buffer 2208 as the
multiplexer output. In one embodiment, when a (e.g., control data) value is detected and/or arrives in the spatial array ingress buffer 2202, the scheduler 2228
(e.g., as programmed by an operation configuration in storage 2226) is sensitized to examine the corresponding network ingress channel. When data is available
in that channel, it is removed from the network ingress buffer 2224 and moved to the spatial array egress buffer 2208. The control bits of both ingresses and
egress may then be updated to reflect the transfer of data. This may result in control flow tokens or credits being propagated in the associated network. In
certain embodiment, all inputs (e.g., control or data) may arise locally or over the network.

https://patents.google.com/patent/EP3726389B1/en?q=(high+radix)&oq=high+radix&page=1

26/76

2/14/26, 2:00 PM

EP3726389B1 - Apparatuses, methods, and systems for memory interface circuit allocation in a configurable spatial accelera…

[0196] Initially, it may seem that the use of packet switched networks to implement the (e.g., high-radix staging) operators of multiplexed and/or demultiplexed codes
hampers performance. For example, in one embodiment, a packet-switched network is generally shared and the caller and callee dataflow graphs may be distant
from one another. Recall, however, that in certain embodiments, the intention of supporting multiplexing and/or demultiplexing is to reduce the area consumed
by infrequent code paths within a dataflow operator (e.g., by the spatial array). Thus, certain embodiments herein reduce area and avoid the consumption of
more expensive fabric resources, for example, like PEs, e.g., without (substantially) affecting the area and efficiency of individual PEs to supporting those (e.g.,
infrequent) operations.
[0197] Turning now to further detail of Figure 22, depicted network dataflow endpoint circuit 2200 includes a spatial array (e.g., fabric) ingress buffer 2202, for example,
to input data (e.g., control data) from a (e.g., circuit switched) network. As noted above, although a single spatial array (e.g., fabric) ingress buffer 2202 is
depicted, a plurality of spatial array (e.g., fabric) ingress buffers may be in a network dataflow endpoint circuit. In one embodiment, spatial array (e.g., fabric)
ingress buffer 2202 is to receive data (e.g., control data) from a communications network of a spatial array (e.g., a spatial array of processing elements), for
example, from one or more of network 2204 and network 2206. In one embodiment, network 2204 is part of network 2113 in Figure 21.
[0198] Depicted network dataflow endpoint circuit 2200 includes a spatial array (e.g., fabric) egress buffer 2208, for example, to output data (e.g., control data) to a
(e.g., circuit switched) network. As noted above, although a single spatial array (e.g., fabric) egress buffer 2208 is depicted, a plurality of spatial array (e.g.,
fabric) egress buffers may be in a network dataflow endpoint circuit. In one embodiment, spatial array (e.g., fabric) egress buffer 2208 is to send (e.g., transmit)
data (e.g., control data) onto a communications network of a spatial array (e.g., a spatial array of processing elements), for example, onto one or more of
network 2210 and network 2212. In one embodiment, network 2210 is part of network 2113 in Figure 21.
[0199] Additionally or alternatively, network dataflow endpoint circuit 2200 may be coupled to another network 2214, e.g., a packet switched network. Another network
2214, e.g., a packet switched network, may be used to transmit (e.g., send or receive) (e.g., input and/or resultant) data to processing elements or other
components of a spatial array and/or to transmit one or more of input data or resultant data. In one embodiment, network 2214 is part of the packet switched
communications network 2114 in Figure 21, e.g., a time multiplexed network.
[0200] Network buffer 2218 (e.g., register(s)) may be a stop on (e.g., ring) network 2214, for example, to receive data from network 2214.
[0201] Depicted network dataflow endpoint circuit 2200 includes a network egress buffer 2222, for example, to output data (e.g., resultant data) to a (e.g., packet
switched) network. As noted above, although a single network egress buffer 2222 is depicted, a plurality of network egress buffers may be in a network dataflow
endpoint circuit. In one embodiment, network egress buffer 2222 is to send (e.g., transmit) data (e.g., resultant data) onto a communications network of a
spatial array (e.g., a spatial array of processing elements), for example, onto network 2214. In one embodiment, network 2214 is part of packet switched network
2114 in Figure 21. In certain embodiments, network egress buffer 2222 is to output data (e.g., from spatial array ingress buffer 2202) to (e.g., packet switched)
network 2214, for example, to be routed (e.g., steered) to other components (e.g., other network dataflow endpoint circuit(s)).
[0202] Depicted network dataflow endpoint circuit 2200 includes a network ingress buffer 2222, for example, to input data (e.g., inputted data) from a (e.g., packet
switched) network. As noted above, although a single network ingress buffer 2224 is depicted, a plurality of network ingress buffers may be in a network
dataflow endpoint circuit. In one embodiment, network ingress buffer 2224 is to receive (e.g., transmit) data (e.g., input data) from a communications network of
a spatial array (e.g., a spatial array of processing elements), for example, from network 2214. In one embodiment, network 2214 is part of packet switched
network 2114 in Figure 21. In certain embodiments, network ingress buffer 2224 is to input data (e.g., from spatial array ingress buffer 2202) from (e.g., packet
switched) network 2214, for example, to be routed (e.g., steered) there (e.g., into spatial array egress buffer 2208) from other components (e.g., other network
dataflow endpoint circuit(s)).
[0203] In one embodiment, the data format (e.g., of the data on network 2214) includes a packet having data and a header (e.g., with the destination of that data). In
one embodiment, the data format (e.g., of the data on network 2204 and/or 2206) includes only the data (e.g., not a packet having data and a header (e.g., with
the destination of that data)). Network dataflow endpoint circuit 2200 may add (e.g., data output from circuit 2200) or remove (e.g., data input into circuit 2200)
a header (or other data) to or from a packet. Coupling 2220 (e.g., wire) may send data received from network 2214 (e.g., from network buffer 2218) to network
ingress buffer 2224 and/or multiplexer 2216. Multiplexer 2216 may (e.g., via a control signal from the scheduler 2228) output data from network buffer 2218 or
from network egress buffer 2222. In one embodiment, one or more of multiplexer 2216 or network buffer 2218 are separate components from network dataflow
endpoint circuit 2200. A buffer may include a plurality of (e.g., discrete) entries, for example, a plurality of registers.
[0204] In one embodiment, operation configuration storage 2226 (e.g., register or registers) is loaded during configuration (e.g., mapping) and specifies the particular
operation (or operations) this network dataflow endpoint circuit 2200 (e.g., not a processing element of a spatial array) is to perform (e.g., data steering
operations in contrast to logic and/or arithmetic operations). Buffer(s) (e.g., 2202, 2208, 2222, and/or 2224) activity may be controlled by that operation (e.g.,
controlled by the scheduler 2228). Scheduler 2228 may schedule an operation or operations of network dataflow endpoint circuit 2200, for example, when (e.g.,
all) input (e.g., payload) data and/or control data arrives. Dotted lines to and from scheduler 2228 indicate paths that may be utilized for control data, e.g., to
and/or from scheduler 2228. Scheduler may also control multiplexer 2216, e.g., to steer data to and/or from network dataflow endpoint circuit 2200 and network
2214.
[0205] In reference to the distributed pick operation in Figure 21 above, network dataflow endpoint circuit 2102 may be configured (e.g., as an operation in its operation
configuration register 2226 as in Figure 22) to receive (e.g., in (two storage locations in) its network ingress buffer 2224 as in Figure 22) input data from each of
network dataflow endpoint circuit 2104 and network dataflow endpoint circuit 2106, and to output resultant data (e.g., from its spatial array egress buffer 2208
as in Figure 22), for example, according to control data (e.g., in its spatial array ingress buffer 2202 as in Figure 22). Network dataflow endpoint circuit 2104 may
be configured (e.g., as an operation in its operation configuration register 2226 as in Figure 22) to provide (e.g., send via circuit 2104's network egress buffer
2222 as in Figure 22) input data to network dataflow endpoint circuit 2102, e.g., on receipt (e.g., in circuit 2104's spatial array ingress buffer 2202 as in Figure 22)
of the input data from processing element 2122. This may be referred to as Input 0 in Figure 21. In one embodiment, circuit switched network is configured (e.g.,
programmed) to provide a dedicated communication line between processing element 2122 and network dataflow endpoint circuit 2104 along path 2124.
Network dataflow endpoint circuit 2104 may include (e.g., add) a header packet with the received data (e.g., in its network egress buffer 2222 as in Figure 22) to
steer the packet (e.g., input data) to network dataflow endpoint circuit 2102. Network dataflow endpoint circuit 2106 may be configured (e.g., as an operation in
its operation configuration register 2226 as in Figure 22) to provide (e.g., send via circuit 2106's network egress buffer 2222 as in Figure 22) input data to
network dataflow endpoint circuit 2102, e.g., on receipt (e.g., in circuit 2106's spatial array ingress buffer 2202 as in Figure 22) of the input data from processing
element 2120. This may be referred to as Input 1 in Figure 21. In one embodiment, circuit switched network is configured (e.g., programmed) to provide a
dedicated communication line between processing element 2120 and network dataflow endpoint circuit 2106 along path 2116. Network dataflow endpoint
circuit 2106 may include (e.g., add) a header packet with the received data (e.g., in its network egress buffer 2222 as in Figure 22) to steer the packet (e.g., input
data) to network dataflow endpoint circuit 2102.
[0206] When network dataflow endpoint circuit 2104 is to transmit input data to network dataflow endpoint circuit 2102 (e.g., when network dataflow endpoint circuit
2102 has available storage room for the data and/or network dataflow endpoint circuit 2104 has its input data), network dataflow endpoint circuit 2104 may
generate a packet (e.g., including the input data and a header to steer that data to network dataflow endpoint circuit 2102 on the packet switched
communications network 2114 (e.g., as a stop on that (e.g., ring) network). This is illustrated schematically with dashed line 2126 in Figure 21. Network 2114 is
shown schematically with multiple dotted boxes in Figure 21. Network 2114 may include a network controller 2114A, e.g., to manage the ingress and/or egress
of data on network 2114A.
[0207] When network dataflow endpoint circuit 2106 is to transmit input data to network dataflow endpoint circuit 2102 (e.g., when network dataflow endpoint circuit
2102 has available storage room for the data and/or network dataflow endpoint circuit 2106 has its input data), network dataflow endpoint circuit 2104 may

https://patents.google.com/patent/EP3726389B1/en?q=(high+radix)&oq=high+radix&page=1

27/76

2/14/26, 2:00 PM

EP3726389B1 - Apparatuses, methods, and systems for memory interface circuit allocation in a configurable spatial accelera…

generate a packet (e.g., including the input data and a header to steer that data to network dataflow endpoint circuit 2102 on the packet switched
communications network 2114 (e.g., as a stop on that (e.g., ring) network). This is illustrated schematically with dashed line 2118 in Figure 21.
[0208] Network dataflow endpoint circuit 2102 (e.g., on receipt of the Input 0 from network dataflow endpoint circuit 2104 in circuit 2102's network ingress buffer(s),
Input 1 from network dataflow endpoint circuit 2106 in circuit 2102's network ingress buffer(s), and/or control data from processing element 2108 in circuit
2102's spatial array ingress buffer) may then perform the programmed dataflow operation (e.g., a Pick operation in this example). The network dataflow
endpoint circuit 2102 may then output the according resultant data from the operation, e.g., to processing element 2108 in Figure 21. In one embodiment, circuit
switched network is configured (e.g., programmed) to provide a dedicated communication line between processing element 2108 (e.g., a buffer thereof) and
network dataflow endpoint circuit 2102 along path 2128. A further example of a distributed Pick operation is discussed below in reference to Figure 34-36.
Buffers in Figure 21 may be the small, unlabeled boxes in each PE.
[0209] Figures 23-8 below include example data formats, but other data formats may be utilized. One or more fields may be included in a data format (e.g., in a packet).
Data format may be used by network dataflow endpoint circuits, e.g., to transmit (e.g., send and/or receive) data between a first component (e.g., between a first
network dataflow endpoint circuit and a second network dataflow endpoint circuit, component of a spatial array, etc.).
[0210] Figure 23 illustrates data formats for a send operation 2302 and a receive operation 2304 according to embodiments of the disclosure. In one embodiment,
send operation 2302 and receive operation 2304 are data formats of data transmitted on a packed switched communication network. Depicted send operation
2302 data format includes a destination field 2302A (e.g., indicating which component in a network the data is to be sent to), a channel field 2302B (e.g.
indicating which channel on the network the data is to be sent on), and an input field 2302C (e.g., the payload or input data that is to be sent). Depicted receive
operation 2304 includes an output field, e.g., which may also include a destination field (not depicted). These data formats may be used (e.g., for packet(s)) to
handle moving data in and out of components. These configurations may be separable and/or happen in parallel. These configurations may use separate
resources. The term channel may generally refer to the communication resources (e.g., in management hardware) associated with the request. Association of
configuration and queue management hardware may be explicit.
[0211] Figure 24 illustrates another data format for a send operation 2402 according to embodiments of the disclosure. In one embodiment, send operation 2402 is a
data format of data transmitted on a packed switched communication network. Depicted send operation 2402 data format includes a type field (e.g., used to
annotate special control packets, such as, but not limited to, configuration, extraction, or exception packets), destination field 2402B (e.g., indicating which
component in a network the data is to be sent to), a channel field 2402C (e.g. indicating which channel on the network the data is to be sent on), and an input
field 2402D (e.g., the payload or input data that is to be sent).
[0212] Figure 25 illustrates configuration data formats to configure a circuit element (e.g., network dataflow endpoint circuit) for a send (e.g., switch) operation 2502
and a receive (e.g., pick) operation 2504 according to embodiments of the disclosure. In one embodiment, send operation 2502 and receive operation 2504 are
configuration data formats for data to be transmitted on a packed switched communication network, for example, between network dataflow endpoint circuits.
Depicted send operation configuration data format 2502includes a destination field 2502A (e.g., indicating which component(s) in a network the (input) data is
to be sent to), a channel field 2502B (e.g. indicating which channel on the network the (input) data is to be sent on), an input field 2502C (for example, an
identifier of the component(s) that is to send the input data, e.g., the set of inputs in the (e.g., fabric ingress) buffer that this element is sensitive to), and an
operation field 2502D (e.g., indicating which of a plurality of operations are to be performed). In one embodiment, the (e.g., outbound) operation is one of a
Switch or SwitchAny dataflow operation, e.g., corresponding to a (e.g., same) dataflow operator of a dataflow graph.
[0213] Depicted receive operation configuration data format 2504 includes an output field 2504A (e.g., indicating which component(s) in a network the (resultant) data
is to be sent to), an input field 2504B (e.g., an identifier of the component(s) that is to send the input data), and an operation field 2504C (e.g., indicating which of
a plurality of operations are to be performed). In one embodiment, the (e.g., inbound) operation is one of a Pick, PickSingleLeg, PickAny, or Merge dataflow
operation, e.g., corresponding to a (e.g., same) dataflow operator of a dataflow graph. In one embodiment, a merge dataflow operation is a pick that requires and
dequeues all operands (e.g., with the egress endpoint receiving control).
[0214] A configuration data format utilized herein may include one or more of the fields described herein, e.g., in any order.
[0215] Figure 26 illustrates a configuration data format 2602 to configure a circuit element (e.g., network dataflow endpoint circuit) for a send operation with its input,
output, and control data annotated on a circuit 2600 according to embodiments of the disclosure. Depicted send operation configuration data format 2602
includes a destination field 2602A (e.g., indicating which component in a network the data is to be sent to), a channel field 2602B (e.g. indicating which channel
on the (packet switched) network the data is to be sent on), and an input field 2302C (e.g., an identifier of the component(s) that is to send the input data). In one
embodiment, circuit 2600 (e.g., network dataflow endpoint circuit) is to receive packet of data in the data format of send operation configuration data format
2602, for example, with the destination indicating which circuit of a plurality of circuits the resultant is to be sent to, the channel indicating which channel of the
(packet switched) network the data is to be sent on, and the input being which circuit of a plurality of circuits the input data is to be received from. The AND gate
2604 is to allow the operation to be performed when both the input data is available and the credit status is a yes (for example, the dependency token indicates)
indicating there is room for the output data to be stored, e.g., in a buffer of the destination. In certain embodiments, each operation is annotated with its
requirements (e.g., inputs, outputs, and control) and if all requirements are met, the configuration is 'performable' by the circuit (e.g., network dataflow endpoint
circuit).
[0216] Figure 27 illustrates a configuration data format 2702 to configure a circuit element (e.g., network dataflow endpoint circuit) for a selected (e.g., send) operation
with its input, output, and control data annotated on a circuit 2700 according to embodiments of the disclosure. Depicted (e.g., send) operation configuration
data format 2702 includes a destination field 2702A (e.g., indicating which component(s) in a network the (input) data is to be sent to), a channel field 2702B
(e.g. indicating which channel on the network the (input) data is to be sent on), an input field 2702C (e.g., an identifier of the component(s) that is to send the
input data), and an operation field 2702D (e.g., indicating which of a plurality of operations are to be performed and/or the source of the control data for that
operation). In one embodiment, the (e.g., outbound) operation is one of a send, Switch, or SwitchAny dataflow operation, e.g., corresponding to a (e.g., same)
dataflow operator of a dataflow graph.
[0217] In one embodiment, circuit 2700 (e.g., network dataflow endpoint circuit) is to receive packet of data in the data format of (e.g., send) operation configuration
data format 2702, for example, with the input being the source(s) of the payload (e.g., input data) and the operation field indicating which operation is to be
performed (e.g., shown schematically as Switch or SwitchAny). Depicted multiplexer 2704 may select the operation to be performed from a plurality of available
operations, e.g., based on the value in operation field 2702D. In one embodiment, circuit 2700 is to perform that operation when both the input data is available
and the credit status is a yes (for example, the dependency token indicates) indicating there is room for the output data to be stored, e.g., in a buffer of the
destination.
[0218] In one embodiment, the send operation does not utilize control beyond checking its input(s) are available for sending. This may enable switch to perform the
operation without credit on all legs. In one embodiment, the Switch and/or SwitchAny operation includes a multiplexer controlled by the value stored in the
operation field 2702D to select the correct queue management circuitry.
[0219] Value stored in operation field 2702D may select among control options, e.g., with different control (e.g., logic) circuitry for each operation, for example, as in
Figures 28-31. In some embodiments, credit (e.g., credit on a network) status is another input (e.g., as depicted in Figures 28-29 here).
[0220] Figure 28 illustrates a configuration data format to configure a circuit element (e.g., network dataflow endpoint circuit) for a Switch operation configuration data
format 2802 with its input, output, and control data annotated on a circuit 2800 according to embodiments of the disclosure. In one embodiment, the (e.g.,
outbound) operation value stored in the operation field 2702D is for a Switch operation, e.g., corresponding to a Switch dataflow operator of a dataflow graph. In
one embodiment, circuit 2800 (e.g., network dataflow endpoint circuit) is to receive a packet of data in the data format of Switch operation 2802, for example,
with the input in input field 2802A being what component(s) are to be sent the data and the operation field 2802B indicating which operation is to be performed

https://patents.google.com/patent/EP3726389B1/en?q=(high+radix)&oq=high+radix&page=1

28/76

2/14/26, 2:00 PM

EP3726389B1 - Apparatuses, methods, and systems for memory interface circuit allocation in a configurable spatial accelera…

(e.g., shown schematically as Switch). Depicted circuit 2800 may select the operation to be executed from a plurality of available operations based on the
operation field 2802B. In one embodiment, circuit 2700 is to perform that operation when both the input data (for example, according to the input status, e.g.,
there is room for the data in the destination(s)) is available and the credit status (e.g., selection operation (OP) status) is a yes (for example, the network credit
indicates that there is availability on the network to send that data to the destination(s)). For example, multiplexers 2810, 2812, 2814 may be used with a
respective input status and credit status for each input (e.g., where the output data is to be sent to in the switch operation), e.g., to prevent an input from
showing as available until both the input status (e.g., room for data in the destination) and the credit status (e.g., there is room on the network to get to the
destination) are true (e.g., yes). In one embodiment, input status is an indication there is or is not room for the (output) data to be stored, e.g., in a buffer of the
destination. In certain embodiments, AND gate 2806 is to allow the operation to be performed when both the input data is available (e.g., as output from
multiplexer 2804) and the selection operation (e.g., control data) status is a yes, for example, indicating the selection operation (e.g., which of a plurality of
outputs an input is to be sent to, see., e.g., Figure 20). In certain embodiments, the performance of the operation with the control data (e.g., selection op) is to
cause input data from one of the inputs to be output on one or more (e.g., a plurality of) outputs (e.g., as indicated by the control data), e.g., according to the
multiplexer selection bits from multiplexer 2808. In one embodiment, selection op chooses which leg of the switch output will be used and/or selection decoder
creates multiplexer selection bits.
[0221] Figure 29 illustrates a configuration data format to configure a circuit element (e.g., network dataflow endpoint circuit) for a SwitchAny operation configuration
data format 2902 with its input, output, and control data annotated on a circuit 2900 according to embodiments of the disclosure. In one embodiment, the (e.g.,
outbound) operation value stored in the operation field 2702D is for a SwitchAny operation, e.g., corresponding to a SwitchAny dataflow operator of a dataflow
graph. In one embodiment, circuit 2900 (e.g., network dataflow endpoint circuit) is to receive a packet of data in the data format of SwitchAny operation
configuration data format 2902, for example, with the input in input field 2902A being what component(s) are to be sent the data and the operation field 2902B
indicating which operation is to be performed (e.g., shown schematically as SwitchAny) and/or the source of the control data for that operation. In one
embodiment, circuit 2700 is to perform that operation when any of the input data (for example, according to the input status, e.g., there is room for the data in
the destination(s)) is available and the credit status is a yes (for example, the network credit indicates that there is availability on the network to send that data
to the destination(s)). For example, multiplexers 2910, 2912, 2914 may be used with a respective input status and credit status for each input (e.g., where the
output data is to be sent to in the SwitchAny operation), e.g., to prevent an input from showing as available until both the input status (e.g., room for data in the
destination) and the credit status (e.g., there is room on the network to get to the destination) are true (e.g., yes). In one embodiment, input status is an
indication there is room or is not room for the (output) data to be stored, e.g., in a buffer of the destination. In certain embodiments, OR gate 2904 is to allow the
operation to be performed when any one of the outputs are available. In certain embodiments, the performance of the operation is to cause the first available
input data from one of the inputs to be output on one or more (e.g., a plurality of) outputs, e.g., according to the multiplexer selection bits from multiplexer 2906.
In one embodiment, SwitchAny occurs as soon as any output credit is available (e.g., as opposed to a Switch that utilizes a selection op). Multiplexer select bits
may be used to steer an input to an (e.g., network) egress buffer of a network dataflow endpoint circuit.
[0222] Figure 30 illustrates a configuration data format to configure a circuit element (e.g., network dataflow endpoint circuit) for a Pick operation configuration data
format 3002 with its input, output, and control data annotated on a circuit 3000 according to embodiments of the disclosure. In one embodiment, the (e.g.,
inbound) operation value stored in the operation field 3002C is for a Pick operation, e.g., corresponding to a Pick dataflow operator of a dataflow graph. In one
embodiment, circuit 3000 (e.g., network dataflow endpoint circuit) is to receive a packet of data in the data format of Pick operation configuration data format
3002, for example, with the data in input field 3002B being what component(s) are to send the input data, the data in output field 3002A being what
component(s) are to be sent the input data, and the operation field 3002C indicating which operation is to be performed (e.g., shown schematically as Pick)
and/or the source of the control data for that operation. Depicted circuit 3000 may select the operation to be executed from a plurality of available operations
based on the operation field 3002C. In one embodiment, circuit 3000 is to perform that operation when both the input data (for example, according to the input
(e.g., network ingress buffer) status, e.g., all the input data has arrived) is available, the credit status (e.g., output status) is a yes (for example, the spatial array
egress buffer) indicating there is room for the output data to be stored, e.g., in a buffer of the destination(s), and the selection operation (e.g., control data)
status is a yes. In certain embodiments, AND gate 3006 is to allow the operation to be performed when both the input data is available (e.g., as output from
multiplexer 3004), an output space is available, and the selection operation (e.g., control data) status is a yes, for example, indicating the selection operation
(e.g., which of a plurality of outputs an input is to be sent to, see., e.g., Figure 20). In certain embodiments, the performance of the operation with the control data
(e.g., selection op) is to cause input data from one of a plurality of inputs (e.g., indicated by the control data) to be output on one or more (e.g., a plurality of)
outputs, e.g., according to the multiplexer selection bits from multiplexer 3008. In one embodiment, selection op chooses which leg of the pick will be used
and/or selection decoder creates multiplexer selection bits.
[0223] Figure 31 illustrates a configuration data format to configure a circuit element (e.g., network dataflow endpoint circuit) for a PickAny operation 3102 with its
input, output, and control data annotated on a circuit 3100 according to embodiments of the disclosure. In one embodiment, the (e.g., inbound) operation value
stored in the operation field 3102C is for a PickAny operation, e.g., corresponding to a PickAny dataflow operator of a dataflow graph. In one embodiment, circuit
3100 (e.g., network dataflow endpoint circuit) is to receive a packet of data in the data format of PickAny operation configuration data format 3102, for example,
with the data in input field 3102B being what component(s) are to send the input data, the data in output field 3102A being what component(s) are to be sent the
input data, and the operation field 3102C indicating which operation is to be performed (e.g., shown schematically as PickAny). Depicted circuit 3100 may select
the operation to be executed from a plurality of available operations based on the operation field 3102C. In one embodiment, circuit 3100 is to perform that
operation when any (e.g., a first arriving of) the input data (for example, according to the input (e.g., network ingress buffer) status, e.g., any of the input data has
arrived) is available and the credit status (e.g., output status) is a yes (for example, the spatial array egress bufferindicates) indicating there is room for the
output data to be stored, e.g., in a buffer of the destination(s). In certain embodiments, AND gate 3106 is to allow the operation to be performed when any of the
input data is available (e.g., as output from multiplexer 3104) and an output space is available. In certain embodiments, the performance of the operation is to
cause the (e.g., first arriving) input data from one of a plurality of inputs to be output on one or more (e.g., a plurality of) outputs, e.g., according to the
multiplexer selection bits from multiplexer 3108.
[0224] In one embodiment, PickAny executes on the presence of any data and/or selection decoder creates multiplexer selection bits.
[0225] Figure 32 illustrates selection of an operation (3202, 3204, 3206) by a network dataflow endpoint circuit 3200 for performance according to embodiments of the
disclosure. Pending operations storage 3201 (e.g., in scheduler 2228 in Figure 22) may store one or more dataflow operations, e.g., according to the format(s)
discussed herein. Scheduler (for example, based on a fixed priority or the oldest of the operations, e.g., that have all of their operands) may schedule an
operation for performance. For example, scheduler may select operation 3202, and according to a value stored in operation field, send the corresponding control
signals from multiplexer 3208 and/or multiplexer 3210. As an example, several operations may be simultaneously executeable in a single network dataflow
endpoint circuit. Assuming all data is there, the "performable" signal (e.g., as shown in Figures 26-31) may be input as a signal into multiplexer 3212. Multiplexer
3212 may send as an output control signals for a selected operation (e.g., one of operation 3202, 3204, and 3206) that cause multiplexer 3208 to configure the
connections in a network dataflow endpoint circuit to perform the selected operation (e.g., to source from or send data to buffer(s)). Multiplexer 3212 may send
as an output control signals for a selected operation (e.g., one of operation 3202, 3204, and 3206) that cause multiplexer 3210 to configure the connections in a
network dataflow endpoint circuit to remove data from the queue(s), e.g., consumed data. As an example, see the discussion herein about having data (e.g.,
token) removed. The "PE status" in Figure 32 may be the control data coming from a PE, for example, the empty indicator and full indicators of the queues (e.g.,
backpressure signals and/or network credit). In one embodiment, the PE status may include the empty or full bits for all the buffers and/or datapaths, e.g., in
Figure 22 herein. Figure 32 illustrates generalized scheduling for embodiments herein, e.g., with specialized scheduling for embodiments discussed in reference
to Figures 28-31.

https://patents.google.com/patent/EP3726389B1/en?q=(high+radix)&oq=high+radix&page=1

29/76

2/14/26, 2:00 PM

EP3726389B1 - Apparatuses, methods, and systems for memory interface circuit allocation in a configurable spatial accelera…

[0226] In one embodiment, (e.g., as with scheduling) the choice of dequeue is determined by the operation and its dynamic behavior, e.g., to dequeue the operation after
performance. In one embodiment, a circuit is to use the operand selection bits to dequeue data (e.g., input, output and/or control data).
[0227] Figure 33 illustrates a network dataflow endpoint circuit 3300 according to embodiments of the disclosure. In comparison to Figure 22, network dataflow
endpoint circuit 3300 has split the configuration and control into two separate schedulers. In one embodiment, egress scheduler 3328A is to schedule an
operation on data that is to enter (e.g., from a circuit switched communication network coupled to) the dataflow endpoint circuit 3300 (e.g., at argument queue
3302, for example, spatial array ingress buffer 2202 as in Figure 22) and output (e.g., from a packet switched communication network coupled to) the dataflow
endpoint circuit 3300 (e.g., at network egress buffer 3322, for example, network egress buffer 2222 as in Figure 22). In one embodiment, ingress scheduler
3328B is to schedule an operation on data that is to enter (e.g., from a packet switched communication network coupled to) the dataflow endpoint circuit 3300
(e.g., at network ingress buffer 3324, for example, network ingress buffer 3224 as in Figure 22) and output (e.g., from a circuit switched communication network
coupled to) the dataflow endpoint circuit 3300 (e.g., at output buffer 3308, for example, spatial array egress buffer 3208 as in Figure 22). Scheduler 3328A
and/or scheduler 3328B may include as an input the (e.g., operating) status of circuit 3300, e.g., fullness level of inputs (e.g., buffers 3302A, 3302), fullness level
of outputs (e.g., buffers 3308), values (e.g., value in 3302A), etc. Scheduler 3328B may include a credit return circuit, for example, to denote that credit is
returned to sender, e.g., after receipt in network ingress buffer 3324 of circuit 3300.
[0228] Network 3314 may be a circuit switched network, e.g., as discussed herein. Additionally or alternatively, a packet switched network (e.g., as discussed herein)
may also be utilized, for example, coupled to network egress buffer 3322, network ingress buffer 3324, or other components herein. Argument queue 3302 may
include a control buffer 3302A, for example, to indicate when a respective input queue (e.g., buffer) includes a (new) item of data, e.g., as a single bit. Turning
now to Figures 34-36, in one embodiment, these cumulatively show the configurations to create a distributed pick.
[0229] Figure 34 illustrates a network dataflow endpoint circuit 3400 receiving input zero (0) while performing a pick operation according to embodiments of the
disclosure, for example, as discussed above in reference to Figure 21. In one embodiment, egress configuration 3426A is loaded (e.g., during a configuration
step) with a portion of a pick operation that is to send data to a different network dataflow endpoint circuit (e.g., circuit 3600 in Figure 36). In one embodiment,
egress scheduler 3428A is to monitor the argument queue 3402 (e.g., data queue) for input data (e.g., from a processing element). According to an embodiment
of the depicted data format, the "send" (e.g., a binary value therefor) indicates data is to be sent according to fields X, Y, with X being the value indicating a
particular target network dataflow endpoint circuit (e.g., 0 being network dataflow endpoint circuit 3600 in Figure 36) and Y being the value indicating which
network ingress buffer (e.g., buffer 3624) location the value is to be stored. In one embodiment, Y is the value indicating a particular channel of a multiple
channel (e.g., packet switched) network (e.g., 0 being channel 0 and/or buffer element 0 of network dataflow endpoint circuit 3600 in Figure 36). When the input
data arrives, it is then to be sent (e.g., from network egress buffer 3422) by network dataflow endpoint circuit 3400 to a different network dataflow endpoint
circuit (e.g., network dataflow endpoint circuit 3600 in Figure 36).
[0230] Figure 35 illustrates a network dataflow endpoint circuit 3500 receiving input one (1) while performing a pick operation according to embodiments of the
disclosure, for example, as discussed above in reference to Figure 21. In one embodiment, egress configuration 3526A is loaded (e.g., during a configuration
step) with a portion of a pick operation that is to send data to a different network dataflow endpoint circuit (e.g., circuit 3600 in Figure 36). In one embodiment,
egress scheduler 3528A is to monitor the argument queue 3520 (e.g., data queue 3502B) for input data (e.g., from a processing element). According to an
embodiment of the depicted data format, the "send" (e.g., a binary value therefor) indicates data is to be sent according to fields X, Y, with X being the value
indicating a particular target network dataflow endpoint circuit (e.g., 0 being network dataflow endpoint circuit 3600 in Figure 36) and Y being the value
indicating which network ingress buffer (e.g., buffer 3624) location the value is to be stored. In one embodiment, Y is the value indicating a particular channel of
a multiple channel (e.g., packet switched) network (e.g., 1 being channel 1 and/or buffer element 1 of network dataflow endpoint circuit 3600 in Figure 36). When
the input data arrives, it is then to be sent (e.g., from network egress buffer 3422) by network dataflow endpoint circuit 3500 to a different network dataflow
endpoint circuit (e.g., network dataflow endpoint circuit 3600 in Figure 36).
[0231] Figure 36 illustrates a network dataflow endpoint circuit 3600 outputting the selected input while performing a pick operation according to embodiments of the
disclosure, for example, as discussed above in reference to Figure 21. In one embodiment, other network dataflow endpoint circuits (e.g., circuit 3400 and circuit
3500) are to send their input data to network ingress buffer 3624 of circuit 3600. In one embodiment, ingress configuration 3626B is loaded (e.g., during a
configuration step) with a portion of a pick operation that is to pick the data sent to network dataflow endpoint circuit 3600, e.g., according to a control value. In
one embodiment, control value is to be received in ingress control 3632 (e.g., buffer). In one embodiment, ingress scheduler 3528A is to monitor the receipt of
the control value and the input values (e.g., in network ingress buffer 3624). For example, if the control value says pick from buffer element A (e.g., 0 or 1 in this
example) (e.g., from channel A) of network ingress buffer 3624, the value stored in that buffer element A is then output as a resultant of the operation by circuit
3600, for example, into an output buffer 3608, e.g., when output buffer has storage space (e.g., as indicated by a backpressure signal). In one embodiment,
circuit 3600's output data is sent out when the egress buffer has a token (e.g., input data and control data) and the receiver asserts that it has buffer (e.g.,
indicating storage is available, although other assignments of resources are possible, this example is simply illustrative).
[0232] Figure 37 illustrates a flow diagram 3700 according to embodiments of the disclosure. Depicted flow 3700 includes providing a spatial array of processing
elements 3702; routing, with a packet switched communications network, data within the spatial array between processing elements according to a dataflow
graph 3704; performing a first dataflow operation of the dataflow graph with the processing elements 3706; and performing a second dataflow operation of the
dataflow graph with a plurality of network dataflow endpoint circuits of the packet switched communications network 3708.
[0233] Referring again to Figure 9, accelerator (e.g., CSA) 902 may perform (e.g., or request performance of) an access (e.g., a load and/or store) of data to one or more
of plurality of cache banks (e.g., cache bank 908). A memory interface circuit (e.g., request address file (RAF) circuit(s)) may be included, e.g., as discussed
herein, to provide access between memory (e.g., cache banks) and the accelerator 902. Referring again to Figure 13, a requesting circuit (e.g., a processing
element) may perform (e.g., or request performance of) an access (e.g., a load and/or store) of data to one or more of plurality of cache banks (e.g., cache bank
1302). A memory interface circuit (e.g., request address file (RAF) circuit(s)) may be included, e.g., as discussed herein, to provide access between memory (e.g.,
one or more banks of the cache memory) and the accelerator (e.g., one or more of accelerator tiles (1308, 1310, 1312, 1314)). Referring again to Figures 21
and/or 22, a requesting circuit (e.g., a processing element) may perform (e.g., or request performance of) an access (e.g., a load and/or store) of data to one or
more of a plurality of cache banks. A memory interface circuit (for example, request address file (RAF) circuit(s), e.g., RAF/cache interface 2112) may be
included, e.g., as discussed herein, to provide access between memory (e.g., one or more banks of the cache memory) and the accelerator (e.g., one or more of
the processing elements and/or network dataflow endpoint circuits (e.g., circuits 2102, 2104, 2106)).
[0234] In certain embodiments, an accelerator (e.g., a PE thereof) couples to a RAF circuit or a plurality of RAF circuits through (i) a circuit switched network (for
example, as discussed herein, e.g., in reference to Figures 6-13) or (ii) through a packet switched network (for example, as discussed herein, e.g., in reference to
Figures 20-37).
[0235] In certain embodiments, a circuit (e.g., a request address file (RAF) circuit) (e.g., each of a plurality of RAF circuits) includes a translation lookaside buffer (TLB)
(e.g., TLB circuit). TLB may receive an input of a virtual address and output a physical address corresponding to the mapping (e.g., address mapping) of the
virtual address to the physical address (e.g., different than any mapping of a dataflow graph to hardware). A virtual address may be an address as seen by a
program running on circuitry (e.g., on an accelerator and/or processor). A physical address may be an (e.g., different than the virtual) address in memory
hardware. A TLB may include a data structure (e.g., table) to store (e.g., recently used) virtual-to-physical memory address translations, e.g., such that the
translation does not have to be performed on each virtual address present to obtain the physical memory address corresponding to that virtual address. If the
virtual address entry is not in the TLB, a circuit (e.g., a TLB manager circuit) may perform a page walk to determine the virtual-to-physical memory address
translation. In one embodiment, a circuit (e.g., a RAF circuit) is to receive an input of a virtual address for translation in a TLB (e.g., TLB in RAF circuit) from a
requesting entity (e.g., a PE or other hardware component) via a circuit switched network, e.g., as in Figures 6-13. Additionally or alternatively, a circuit (e.g., a

https://patents.google.com/patent/EP3726389B1/en?q=(high+radix)&oq=high+radix&page=1

30/76

2/14/26, 2:00 PM

EP3726389B1 - Apparatuses, methods, and systems for memory interface circuit allocation in a configurable spatial accelera…

RAF circuit) may receive an input of a virtual address for translation in a TLB (e.g., TLB in RAF circuit) from a requesting entity (e.g., a PE, network dataflow
endpoint circuit, or other hardware component) via a packet switched network, e.g., as in Figures 20-37. In certain embodiments, data received for a memory
(e.g., cache) access request is a memory command. A memory command may include the virtual address to-be-accessed, operation to be performed (e.g., a
load or a store), and/or payload data (e.g., for a store).
2.6 Floating Point Support
[0236] Certain HPC applications are characterized by their need for significant floating point bandwidth. To meet this need, embodiments of a CSA may be provisioned
with multiple (e.g., between 128 and 256 each) of floating add and multiplication PEs, e.g., depending on tile configuration. A CSA may provide a few other
extended precision modes, e.g., to simplify math library implementation. CSA floating point PEs may support both single and double precision, but lower
precision PEs may support machine learning workloads. A CSA may provide an order of magnitude more floating point performance than a processor core. In
one embodiment, in addition to increasing floating point bandwidth, in order to power all of the floating point units, the energy consumed in floating point
operations is reduced. For example, to reduce energy, a CSA may selectively gate the low-order bits of the floating point multiplier array. In examining the
behavior of floating point arithmetic, the low order bits of the multiplication array may often not influence the final, rounded product. Figure 38 illustrates a
floating point multiplier 3800 partitioned into three regions (the result region, three potential carry regions (3802, 3804, 3806), and the gated region) according to
embodiments of the disclosure. In certain embodiments, the carry region is likely to influence the result region and the gated region is unlikely to influence the
result region. Considering a gated region of g bits, the maximum carry may be:
𝑔

carry

𝑔

≤

1
2

𝑔

𝑖−1

∑ 𝑖2
1
𝑔

𝑔

≤ ∑
1

𝑖
𝑔

2

− ∑
1

1
𝑔

+1

2

≤ 𝑔−1

Given this maximum carry, if the result of the carry region is less than 2c - g, where the carry region is c bits wide, then the gated region may be ignored since it
does not influence the result region. Increasing g means that it is more likely the gated region will be needed, while increasing c means that, under random
assumption, the gated region will be unused and may be disabled to avoid energy consumption. In embodiments of a CSA floating multiplication PE, a two stage
pipelined approach is utilized in which first the carry region is determined and then the gated region is determined if it is found to influence the result. If more
information about the context of the multiplication is known, a CSA more aggressively tune the size of the gated region. In FMA, the multiplication result may be
added to an accumulator, which is often much larger than either of the multiplicands. In this case, the addend exponent may be observed in advance of
multiplication and the CSDA may adjust the gated region accordingly. One embodiment of the CSA includes a scheme in which a context value, which bounds
the minimum result of a computation, is provided to related multipliers, in order to select minimum energy gating configurations.
2.7 Runtime Services
[0237] In certain embodiment, a CSA includes a heterogeneous and distributed fabric, and consequently, runtime service implementations are to accommodate several
kinds of PEs in a parallel and distributed fashion. Although runtime services in a CSA may be critical, they may be infrequent relative to user-level computation.
Certain implementations, therefore, focus on overlaying services on hardware resources. To meet these goals, CSA runtime services may be cast as a hierarchy,
e.g., with each layer corresponding to a CSA network. At the tile level, a single external-facing controller may accepts or sends service commands to an
associated core with the CSA tile. A tile-level controller may serve to coordinate regional controllers at the RAFs, e.g., using the ACI network. In turn, regional
controllers may coordinate local controllers at certain mezzanine network stops (e.g., network dataflow endpoint circuits). At the lowest level, service specific
micro-protocols may execute over the local network, e.g., during a special mode controlled through the mezzanine controllers. The micro-protocols may permit
each PE (e.g., PE class by type) to interact with the runtime service according to its own needs. Parallelism is thus implicit in this hierarchical organization, and
operations at the lowest levels may occur simultaneously. This parallelism may enables the configuration of a CSA tile in between hundreds of nanoseconds to a
few microseconds, e.g., depending on the configuration size and its location in the memory hierarchy. Embodiments of the CSA thus leverage properties of
dataflow graphs to improve implementation of each runtime service. One key observation is that runtime services may need only to preserve a legal logical view
of the dataflow graph, e.g., a state that can be produced through some ordering of dataflow operator executions. Services may generally not need to guarantee a
temporal view of the dataflow graph, e.g., the state of a dataflow graph in a CSA at a specific point in time. This may permit the CSA to conduct most runtime
services in a distributed, pipelined, and parallel fashion, e.g., provided that the service is orchestrated to preserve the logical view of the dataflow graph. The local
configuration micro-protocol may be a packet-based protocol overlaid on the local network. Configuration targets may be organized into a configuration chain,
e.g., which is fixed in the microarchitecture. Fabric (e.g., PE) targets may be configured one at a time, e.g., using a single extra register per target to achieve
distributed coordination. To start configuration, a controller may drive an out-of-band signal which places all fabric targets in its neighborhood into an
unconfigured, paused state and swings multiplexors in the local network to a pre-defined conformation. As the fabric (e.g., PE) targets are configured, that is
they completely receive their configuration packet, they may set their configuration microprotocol registers, notifying the immediately succeeding target (e.g.,
PE) that it may proceed to configure using the subsequent packet. There is no limitation to the size of a configuration packet, and packets may have dynamically
variable length. For example, PEs configuring constant operands may have a configuration packet that is lengthened to include the constant field (e.g., X and Y in
Figures 3B-3C). Figure 39 illustrates an in-flight configuration of an accelerator 3900 with a plurality of processing elements (e.g., PEs 3902, 3904, 3906, 3908)
according to embodiments of the disclosure. Once configured, PEs may execute subject to dataflow constraints. However, channels involving unconfigured PEs
may be disabled by the microarchitecture, e.g., preventing any undefined operations from occurring. These properties allow embodiments of a CSA to initialize
and execute in a distributed fashion with no centralized control whatsoever. From an unconfigured state, configuration may occur completely in parallel, e.g., in
perhaps as few as 200 nanoseconds. However, due to the distributed initialization of embodiments of a CSA, PEs may become active, for example sending
requests to memory, well before the entire fabric is configured. Extraction may proceed in much the same way as configuration. The local network may be
conformed to extract data from one target at a time, and state bits used to achieve distributed coordination. A CSA may orchestrate extraction to be nondestructive, that is, at the completion of extraction each extractable target has returned to its starting state. In this implementation, all state in the target may be
circulated to an egress register tied to the local network in a scan-like fashion. Although in-place extraction may be achieved by introducing new paths at the
register-transfer level (RTL), or using existing lines to provide the same functionalities with lower overhead. Like configuration, hierarchical extraction is achieved
in parallel.
[0238] Figure 40 illustrates a snapshot 4000 of an in-flight, pipelined extraction according to embodiments of the disclosure. In some use cases of extraction, such as
checkpointing, latency may not be a concern so long as fabric throughput is maintained. In these cases, extraction may be orchestrated in a pipelined fashion.
This arrangement, shown in Figure 40, permits most of the fabric to continue executing, while a narrow region is disabled for extraction. Configuration and

https://patents.google.com/patent/EP3726389B1/en?q=(high+radix)&oq=high+radix&page=1

31/76

2/14/26, 2:00 PM

EP3726389B1 - Apparatuses, methods, and systems for memory interface circuit allocation in a configurable spatial accelera…

extraction may be coordinated and composed to achieve a pipelined context switch. Exceptions may differ qualitatively from configuration and extraction in that,
rather than occurring at a specified time, they arise anywhere in the fabric at any point during runtime. Thus, in one embodiment, the exception micro-protocol
may not be overlaid on the local network, which is occupied by the user program at runtime, and utilizes its own network. However, by nature, exceptions are rare
and insensitive to latency and bandwidth. Thus certain embodiments of CSA utilize a packet switched network to carry exceptions to the local mezzanine stop,
e.g., where they are forwarded up the service hierarchy (e.g., as in Figure 55). Packets in the local exception network may be extremely small. In many cases, a
PE identification (ID) of only two to eight bits suffices as a complete packet, e.g., since the CSA may create a unique exception identifier as the packet traverses
the exception service hierarchy. Such a scheme may be desirable because it also reduces the area overhead of producing exceptions at each PE.
3. COMPILATION
[0239] The ability to compile programs written in high-level languages onto a CSA may be essential for industry adoption. This section gives a high-level overview of
compilation strategies for embodiments of a CSA. First is a proposal for a CSA software framework that illustrates the desired properties of an ideal productionquality toolchain. Next, a prototype compiler framework is discussed. A "control-to-dataflow conversion" is then discussed, e.g., to converts ordinary sequential
control-flow code into CSA dataflow assembly code.
3.1 Example Production Framework
[0240] Figure 41 illustrates a compilation toolchain 4100 for an accelerator according to embodiments of the disclosure. This toolchain compiles high-level languages
(such as C, C++, and Fortran) into a combination of host code (LLVM) intermediate representation (IR) for the specific regions to be accelerated. The CSAspecific portion of this compilation toolchain takes LLVM IR as its input, optimizes and compiles this IR into a CSA assembly, e.g., adding appropriate buffering
on latency-insensitive channels for performance. It then places and routes the CSA assembly on the hardware fabric, and configures the PEs and network for
execution. In one embodiment, the toolchain supports the CSA-specific compilation as a just-in-time (JIT), incorporating potential runtime feedback from actual
executions. One of the key design characteristics of the framework is compilation of (LLVM) IR for the CSA, rather than using a higher-level language as input.
While a program written in a high-level programming language designed specifically for the CSA might achieve maximal performance and/or energy efficiency,
the adoption of new high-level languages or programming frameworks may be slow and limited in practice because of the difficulty of converting existing code
bases. Using (LLVM) IR as input enables a wide range of existing programs to potentially execute on a CSA, e.g., without the need to create a new language or
significantly modify the front-end of new languages that want to run on the CSA.
3.2 Prototype Compiler
[0241] Figure 42 illustrates a compiler 4200 for an accelerator according to embodiments of the disclosure. Compiler 4200 initially focuses on ahead-of-time
compilation of C and C++ through the (e.g., Clang) front-end. To compile (LLVM) IR, the compiler implements a CSA back-end target within LLVM with three main
stages. First, the CSA back-end lowers LLVM IR into a target-specific machine instructions for the sequential unit, which implements most CSA operations
combined with a traditional RISC-like control-flow architecture (e.g., with branches and a program counter). The sequential unit in the toolchain may serve as a
useful aid for both compiler and application developers, since it enables an incremental transformation of a program from control flow (CF) to dataflow (DF),
e.g., converting one section of code at a time from control-flow to dataflow and validating program correctness. The sequential unit may also provide a model
for handling code that does not fit in the spatial array. Next, the compiler converts these control-flow instructions into dataflow operators (e.g., code) for the
CSA. This phase is described later in Section 3.3. Then, the CSA back-end may run its own optimization passes on the dataflow instructions. Finally, the compiler
may dump the instructions in a CSA assembly format. This assembly format is taken as input to late-stage tools which place and route the dataflow instructions
on the actual CSA hardware.
3.3 Control to Dataflow Conversion
[0242] A key portion of the compiler may be implemented in the control-to-dataflow conversion pass, or dataflow conversion pass for short. This pass takes in a
function represented in control flow form, e.g., a control-flow graph (CFG) with sequential machine instructions operating on virtual registers, and converts it into
a dataflow function that is conceptually a graph of dataflow operations (instructions) connected by latency-insensitive channels (LICs). This section gives a highlevel description of this pass, describing how it conceptually deals with memory operations, branches, and loops in certain embodiments.

Straight-Line Code
[0243] Figure 43A illustrates sequential assembly code 4302 according to embodiments of the disclosure. Figure 43B illustrates dataflow assembly code 4304 for the
sequential assembly code 4302 of Figure 43A according to embodiments of the disclosure. Figure 43C illustrates a dataflow graph 4306 for the dataflow
assembly code 4304 of Figure 43B for an accelerator according to embodiments of the disclosure.
[0244] First, consider the simple case of converting straight-line sequential code to dataflow. The dataflow conversion pass may convert a basic block of sequential
code, such as the code shown in Figure 43A into CSA assembly code, shown in Figure 43B. Conceptually, the CSA assembly in Figure 43B represents the
dataflow graph shown in Figure 43C. In this example, each sequential instruction is translated into a matching CSA assembly. The .lie statements (e.g., for data)
declare latency-insensitive channels which correspond to the virtual registers in the sequential code (e.g., Rdata). In practice, the input to the dataflow
conversion pass may be in numbered virtual registers. For clarity, however, this section uses descriptive register names. Note that load and store operations are
supported in the CSA architecture in this embodiment, allowing for many more programs to run than an architecture supporting only pure dataflow. Since the
sequential code input to the compiler is in SSA (singlestatic assignment) form, for a simple basic block, the control-to-dataflow pass may convert each virtual
register definition into the production of a single value on a latency-insensitive channel. The SSA form allows multiple uses of a single definition of a virtual
register, such as in Rdata2). To support this model, the CSA assembly code supports multiple uses of the same LIC (e.g., data2), with the simulator implicitly
creating the necessary copies of the LICs. One key difference between sequential code and dataflow code is in the treatment of memory operations. The code in
Figure 43A is conceptually serial, which means that the load32 (ld32) of addr3 should appear to happen after the st32 of addr, in case that addr and addr3
addresses overlap.

Branches
[0245] To convert programs with multiple basic blocks and conditionals to dataflow, the compiler generates special dataflow operators to replace the branches. More
specifically, the compiler uses switch operators to steer outgoing data at the end of a basic block in the original CFG, and pick operators to select values from
the appropriate incoming channel at the beginning of a basic block. As a concrete example, consider the code and corresponding dataflow graph in Figures 44A44C, which conditionally computes a value of y based on several inputs: a i, x, and n. After computing the branch condition test, the dataflow code uses a switch
operator (e.g., see Figures 3B-3C) steers the value in channel x to channel xF if test is 0, or channel xT if test is 1. Similarly, a pick operator (e.g., see Figures 3B3C) is used to send channel yF to y if test is 0, or send channel yT to y if test is 1. In this example, it turns out that even though the value of a is only used in the
true branch of the conditional, the CSA is to include a switch operator which steers it to channel aT when test is 1, and consumes (eats) the value when test is 0.
This latter case is expressed by setting the false output of the switch to %ign. It may not be correct to simply connect channel a directly to the true path, because
in the cases where execution actually takes the false path, this value of "a" will be left over in the graph, leading to incorrect value of a for the next execution of
the function. This example highlights the property of control equivalence, a key property in embodiments of correct dataflow conversion.
[0246] Control Equivalence: Consider a single-entry-single-exit control flow graph G with two basic blocks A and B. A and B are control-equivalent if all complete control
flow paths through G visit A and B the same number of times.
[0247] LIC Replacement: In a control flow graph G, suppose an operation in basic block A defines a virtual register x, and an operation in basic block B that uses x. Then
a correct control-to-dataflow transformation can replace x with a latency-insensitive channel only if A and B are control equivalent. The control-equivalence
relation partitions the basic blocks of a CFG into strong control-dependence regions. Figure 44A illustrates C source code 4402 according to embodiments of
the disclosure. Figure 44B illustrates dataflow assembly code 4404 for the C source code 4402 of Figure 44A according to embodiments of the disclosure.
Figure 44C illustrates a dataflow graph 4406 for the dataflow assembly code 4404 of Figure 44B for an accelerator according to embodiments of the disclosure.

https://patents.google.com/patent/EP3726389B1/en?q=(high+radix)&oq=high+radix&page=1

32/76

2/14/26, 2:00 PM

EP3726389B1 - Apparatuses, methods, and systems for memory interface circuit allocation in a configurable spatial accelera…

In the example in Figures 44A-44C, the basic block before and after the conditionals are control-equivalent to each other, but the basic blocks in the true and
false paths are each in their own control dependence region. One correct algorithm for converting a CFG to dataflow is to have the compiler insert (1) switches
to compensate for the mismatch in execution frequency for any values that flow between basic blocks which are not control equivalent, and (2) picks at the
beginning of basic blocks to choose correctly from any incoming values to a basic block. Generating the appropriate control signals for these picks and switches
may be the key part of dataflow conversion.

Loops
[0248] Another important class of CFGs in dataflow conversion are CFGs for single-entry-single-exit loops, a common form of loop generated in (LLVM) IR. These loops
may be almost acyclic, except for a single back edge from the end of the loop back to a loop header block. The dataflow conversion pass may use same highlevel strategy to convert loops as for branches, e.g., it inserts switches at the end of the loop to direct values out of the loop (either out the loop exit or around
the back-edge to the beginning of the loop), and inserts picks at the beginning of the loop to choose between initial values entering the loop and values coming
through the back edge. Figure 45A illustrates C source code 4502 according to embodiments of the disclosure. Figure 45B illustrates dataflow assembly code
4504 for the C source code 4502 of Figure 45A according to embodiments of the disclosure. Figure 45C illustrates a dataflow graph 4506 for the dataflow
assembly code 4504 of Figure 45B for an accelerator according to embodiments of the disclosure. Figures 45A-45C shows C and CSA assembly code for an
example do-while loop that adds up values of a loop induction variable i, as well as the corresponding dataflow graph. For each variable that conceptually cycles
around the loop (i and sum), this graph has a corresponding pick/switch pair that controls the flow of these values. Note that this example also uses a
pick/switch pair to cycle the value of n around the loop, even though n is loop-invariant. This repetition of n enables conversion of n's virtual register into a LIC,
since it matches the execution frequencies between a conceptual definition of n outside the loop and the one or more uses of n inside the loop. In general, for a
correct dataflow conversion, registers that are live-in into a loop are to be repeated once for each iteration inside the loop body when the register is converted
into a LIC. Similarly, registers that are updated inside a loop and are live-out from the loop are to be consumed, e.g., with a single final value sent out of the loop.
Loops introduce a wrinkle into the dataflow conversion process, namely that the control for a pick at the top of the loop and the switch for the bottom of the loop
are offset. For example, if the loop in Figure 44A executes three iterations and exits, the control to picker should be 0, 1, 1, while the control to switcher should be
1, 1, 0. This control is implemented by starting the picker channel with an initial extra 0 when the function begins on cycle 0 (which is specified in the assembly
by the directives .value 0 and .avail 0), and then copying the output switcher into picker. Note that the last 0 in switcher restores a final 0 into picker, ensuring that
the final state of the dataflow graph matches its initial state.
[0249] Figure 46A illustrates a flow diagram 4600 according to embodiments of the disclosure. Depicted flow 4600 includes decoding an instruction with a decoder of
a core of a processor into a decoded instruction 4602; executing the decoded instruction with an execution unit of the core of the processor to perform a first
operation 4604; receiving an input of a dataflow graph comprising a plurality of nodes 4606; overlaying the dataflow graph into a plurality of processing elements
of the processor and an interconnect network between the plurality of processing elements of the processor with each node represented as a dataflow operator
in the plurality of processing elements 4608; and performing a second operation of the dataflow graph with the interconnect network and the plurality of
processing elements by a respective, incoming operand set arriving at each of the dataflow operators of the plurality of processing elements 4610.
[0250] Figure 46B illustrates a flow diagram 4601 according to embodiments of the disclosure. Depicted flow 4601 includes receiving an input of a dataflow graph
comprising a plurality of nodes 4603; and overlaying the dataflow graph into a plurality of processing elements of a processor, a data path network between the
plurality of processing elements, and a flow control path network between the plurality of processing elements with each node represented as a dataflow
operator in the plurality of processing elements 4605.
[0251] In one embodiment, the core writes a command into a memory queue and a CSA (e.g., the plurality of processing elements) monitors the memory queue and
begins executing when the command is read. In one embodiment, the core executes a first part of a program and a CSA (e.g., the plurality of processing
elements) executes a second part of the program. In one embodiment, the core does other work while the CSA is executing its operations.
4. CSA ADVANTAGES
[0252] In certain embodiments, the CSA architecture and microarchitecture provides profound energy, performance, and usability advantages over roadmap processor
architectures and FPGAs. In this section, these architectures are compared to embodiments of the CSA and highlights the superiority of CSA in accelerating
parallel dataflow graphs relative to each.
4.1 Processors
[0253] Figure 47 illustrates a throughput versus energy per operation graph 4700 according to embodiments of the disclosure. As shown in Figure 47, small cores are
generally more energy efficient than large cores, and, in some workloads, this advantage may be translated to absolute performance through higher core counts.
The CSA microarchitecture follows these observations to their conclusion and removes (e.g., most) energy-hungry control structures associated with von
Neumann architectures, including most of the instruction-side microarchitecture. By removing these overheads and implementing simple, single operation PEs,
embodiments of a CSA obtains a dense, efficient spatial array. Unlike small cores, which are usually quite serial, a CSA may gang its PEs together, e.g., via the
circuit switched local network, to form explicitly parallel aggregate dataflow graphs. The result is performance in not only parallel applications, but also serial
applications as well. Unlike cores, which may pay dearly for performance in terms area and energy, a CSA is already parallel in its native execution model. In
certain embodiments, a CSA neither requires speculation to increase performance nor does it need to repeatedly re-extract parallelism from a sequential
program representation, thereby avoiding two of the main energy taxes in von Neumann architectures. Most structures in embodiments of a CSA are distributed,
small, and energy efficient, as opposed to the centralized, bulky, energy hungry structures found in cores. Consider the case of registers in the CSA: each PE may
have a few (e.g., 10 or less) storage registers. Taken individually, these registers may be more efficient that traditional register files. In aggregate, these registers
may provide the effect of a large, in-fabric register file. As a result, embodiments of a CSA avoids most of stack spills and fills incurred by classical architectures,
while using much less energy per state access. Of course, applications may still access memory. In embodiments of a CSA, memory access request and
response are architecturally decoupled, enabling workloads to sustain many more outstanding memory accesses per unit of area and energy. This property
yields substantially higher performance for cache-bound workloads and reduces the area and energy needed to saturate main memory in memory-bound
workloads. Embodiments of a CSA expose new forms of energy efficiency which are unique to non-von Neumann architectures. One consequence of executing a
single operation (e.g., instruction) at a (e.g., most) PEs is reduced operand entropy. In the case of an increment operation, each execution may result in a handful
of circuit-level toggles and little energy consumption, a case examined in detail in Section 5.2. In contrast, von Neumann architectures are multiplexed, resulting
in large numbers of bit transitions. The asynchronous style of embodiments of a CSA also enables microarchitectural optimizations, such as the floating point
optimizations described in Section 2.6 that are difficult to realize in tightly scheduled core pipelines. Because PEs may be relatively simple and their behavior in a
particular dataflow graph be statically known, clock gating and power gating techniques may be applied more effectively than in coarser architectures. The
graph-execution style, small size, and malleability of embodiments of CSA PEs and the network together enable the expression many kinds of parallelism:
instruction, data, pipeline, vector, memory, thread, and task parallelism may all be implemented. For example, in embodiments of a CSA, one application may use
arithmetic units to provide a high degree of address bandwidth, while another application may use those same units for computation. In many cases, multiple
kinds of parallelism may be combined to achieve even more performance. Many key HPC operations may be both replicated and pipelined, resulting in orders-ofmagnitude performance gains. In contrast, von Neumann-style cores typically optimize for one style of parallelism, carefully chosen by the architects, resulting in
a failure to capture all important application kernels. Just as embodiments of a CSA expose and facilitates many forms of parallelism, it does not mandate a
particular form of parallelism, or, worse, a particular subroutine be present in an application in order to benefit from the CSA. Many applications, including singlestream applications, may obtain both performance and energy benefits from embodiments of a CSA, e.g., even when compiled without modification. This
reverses the long trend of requiring significant programmer effort to obtain a substantial performance gain in singlestream applications. Indeed, in some

https://patents.google.com/patent/EP3726389B1/en?q=(high+radix)&oq=high+radix&page=1

33/76

2/14/26, 2:00 PM

EP3726389B1 - Apparatuses, methods, and systems for memory interface circuit allocation in a configurable spatial accelera…

applications, embodiments of a CSA obtain more performance from functionally equivalent, but less "modern" codes than from their convoluted, contemporary
cousins which have been tortured to target vector instructions.
4.2 Comparison of CSA Embodiments and FGPAs
[0254] The choice of dataflow operators as the fundamental architecture of embodiments of a CSA differentiates those CSAs from a FGPA, and particularly the CSA is
as superior accelerator for HPC dataflow graphs arising from traditional programming languages. Dataflow operators are fundamentally asynchronous. This
enables embodiments of a CSA not only to have great freedom of implementation in the microarchitecture, but it also enables them to simply and succinctly
accommodate abstract architectural concepts. For example, embodiments of a CSA naturally accommodate many memory microarchitectures, which are
essentially asynchronous, with a simple load-store interface. One need only examine an FPGA DRAM controller to appreciate the difference in complexity.
Embodiments of a CSA also leverage asynchrony to provide faster and more-fully-featured runtime services like configuration and extraction, which are believed
to be four to six orders of magnitude faster than an FPGA. By narrowing the architectural interface, embodiments of a CSA provide control over most timing
paths at the microarchitectural level. This allows embodiments of a CSA to operate at a much higher frequency than the more general control mechanism
offered in a FPGA. Similarly, clock and reset, which may be architecturally fundamental to FPGAs, are microarchitectural in the CSA, e.g., obviating the need to
support them as programmable entities. Dataflow operators may be, for the most part, coarse-grained. By only dealing in coarse operators, embodiments of a
CSA improve both the density of the fabric and its energy consumption: CSA executes operations directly rather than emulating them with look-up tables. A
second consequence of coarseness is a simplification of the place and route problem. CSA dataflow graphs are many orders of magnitude smaller than FPGA
net-lists and place and route time are commensurately reduced in embodiments of a CSA. The significant differences between embodiments of a CSA and a
FPGA make the CSA superior as an accelerator, e.g., for dataflow graphs arising from traditional programming languages.
5. EVALUATION
[0255] The CSA is a novel computer architecture with the potential to provide enormous performance and energy advantages relative to roadmap processors. Consider
the case of computing a single strided address for walking across an array. This case may be important in HPC applications, e.g., which spend significant integer
effort in computing address offsets. In address computation, and especially strided address computation, one argument is constant and the other varies only
slightly per computation. Thus, only a handful of bits per cycle toggle in the majority of cases. Indeed, it may be shown, using a derivation similar to the bound on
floating point carry bits described in Section 2.6, that less than two bits of input toggle per computation in average for a stride calculation, reducing energy by
50% over a random toggle distribution. Were a time-multiplexed approach used, much of this energy savings may be lost. In one embodiment, the CSA achieves
approximately 3x energy efficiency over a core while delivering an 8x performance gain. The parallelism gains achieved by embodiments of a CSA may result in
reduced program run times, yielding a proportionate, substantial reduction in leakage energy. At the PE level, embodiments of a CSA are extremely energy
efficient. A second important question for the CSA is whether the CSA consumes a reasonable amount of energy at the tile level. Since embodiments of a CSA
are capable of exercising every floating point PE in the fabric at every cycle, it serves as a reasonable upper bound for energy and power consumption, e.g., such
that most of the energy goes into floating point multiply and add.
6. FURTHER CSA DETAILS
[0256] This section discusses further details for configuration and exception handling.
6.1 Microarchitecture for Configuring a CSA
[0257] This section discloses examples of how to configure a CSA (e.g., fabric), how to achieve this configuration quickly, and how to minimize the resource overhead
of configuration. Configuring the fabric quickly may be of preeminent importance in accelerating small portions of a larger algorithm, and consequently in
broadening the applicability of a CSA. The section further discloses features that allow embodiments of a CSA to be programmed with configurations of
different length.
[0258] Embodiments of a CSA (e.g., fabric) may differ from traditional cores in that they make use of a configuration step in which (e.g., large) parts of the fabric are
loaded with program configuration in advance of program execution. An advantage of static configuration may be that very little energy is spent at runtime on
the configuration, e.g., as opposed to sequential cores which spend energy fetching configuration information (an instruction) nearly every cycle. The previous
disadvantage of configuration is that it was a coarse-grained step with a potentially large latency, which places an under-bound on the size of program that can
be accelerated in the fabric due to the cost of context switching. This disclosure describes a scalable microarchitecture for rapidly configuring a spatial array in
a distributed fashion, e.g., that avoids the previous disadvantages.
[0259] As discussed above, a CSA may include light-weight processing elements connected by an inter-PE network. Programs, viewed as control-dataflow graphs, are
then mapped onto the architecture by configuring the configurable fabric elements (CFEs), for example PEs and the interconnect (fabric) networks. Generally,
PEs may be configured as dataflow operators and once all input operands arrive at the PE, some operation occurs, and the results are forwarded to another PE
or PEs for consumption or output. PEs may communicate over dedicated virtual circuits which are formed by statically configuring the circuit switched
communications network. These virtual circuits may be flow controlled and fully back-pressured, e.g., such that PEs will stall if either the source has no data or
destination is full. At runtime, data may flow through the PEs implementing the mapped algorithm. For example, data may be streamed in from memory, through
the fabric, and then back out to memory. Such a spatial architecture may achieve remarkable performance efficiency relative to traditional multicore processors:
compute, in the form of PEs, may be simpler and more numerous than larger cores and communications may be direct, as opposed to an extension of the
memory system.
[0260] Embodiments of a CSA may not utilize (e.g., software controlled) packet switching, e.g., packet switching that requires significant software assistance to realize,
which slows configuration. Embodiments of a CSA include out-of-band signaling in the network (e.g., of only 2-3 bits, depending on the feature set supported)
and a fixed configuration topology to avoid the need for significant software support.
[0261] One key difference between embodiments of a CSA and the approach used in FPGAs is that a CSA approach may use a wide data word, is distributed, and
includes mechanisms to fetch program data directly from memory. Embodiments of a CSA may not utilize JTAG-style single bit communications in the interest
of area efficiency, e.g., as that may require milliseconds to completely configure a large FPGA fabric.
[0262] Embodiments of a CSA include a distributed configuration protocol and microarchitecture to support this protocol. Initially, configuration state may reside in
memory. Multiple (e.g., distributed) local configuration controllers (boxes) (LCCs) may stream portions of the overall program into their local region of the spatial
fabric, e.g., using a combination of a small set of control signals and the fabric-provided network. State elements may be used at each CFE to form configuration
chains, e.g., allowing individual CFEs to self-program without global addressing.
[0263] Embodiments of a CSA include specific hardware support for the formation of configuration chains, e.g., not software establishing these chains dynamically at
the cost of increasing configuration time. Embodiments of a CSA are not purely packet switched and do include extra out-of-band control wires (e.g., control is
not sent through the data path requiring extra cycles to strobe this information and reserialize this information). Embodiments of a CSA decreases configuration
latency by fixing the configuration ordering and by providing explicit out-of-band control (e.g., by at least a factor of two), while not significantly increasing
network complexity.
[0264] Embodiments of a CSA do not use a serial mechanism for configuration in which data is streamed bit by bit into the fabric using a JTAG-like protocol.
Embodiments of a CSA utilize a coarse-grained fabric approach. In certain embodiments, adding a few control wires or state elements to a 64 or 32-bit-oriented
CSA fabric has a lower cost relative to adding those same control mechanisms to a 4 or 6 bit fabric.
[0265] Figure 48 illustrates an accelerator tile 4800 comprising an array of processing elements (PE) and a local configuration controller (4802, 4806) according to
embodiments of the disclosure. Each PE, each network controller (e.g., network dataflow endpoint circuit), and each switch may be a configurable fabric
elements (CFEs), e.g., which are configured (e.g., programmed) by embodiments of the CSA architecture.

https://patents.google.com/patent/EP3726389B1/en?q=(high+radix)&oq=high+radix&page=1

34/76

2/14/26, 2:00 PM

EP3726389B1 - Apparatuses, methods, and systems for memory interface circuit allocation in a configurable spatial accelera…

[0266] Embodiments of a CSA include hardware that provides for efficient, distributed, low-latency configuration of a heterogeneous spatial fabric. This may be
achieved according to four techniques. First, a hardware entity, the local configuration controller (LCC) is utilized, for example, as in Figures 48-50. An LCC may
fetch a stream of configuration information from (e.g., virtual) memory. Second, a configuration data path may be included, e.g., that is as wide as the native
width of the PE fabric and which may be overlaid on top of the PE fabric. Third, new control signals may be received into the PE fabric which orchestrate the
configuration process. Fourth, state elements may be located (e.g., in a register) at each configurable endpoint which track the status of adjacent CFEs, allowing
each CFE to unambiguously self-configure without extra control signals. These four microarchitectural features may allow a CSA to configure chains of its CFEs.
To obtain low configuration latency, the configuration may be partitioned by building many LCCs and CFE chains. At configuration time, these may operate
independently to load the fabric in parallel, e.g., dramatically reducing latency. As a result of these combinations, fabrics configured using embodiments of a CSA
architecture, may be completely configured (e.g., in hundreds of nanoseconds). In the following, the detailed the operation of the various components of
embodiments of a CSA configuration network are disclosed.
[0267] Figures 49A-49C illustrate a local configuration controller 4902 configuring a data path network according to embodiments of the disclosure. Depicted network
includes a plurality of multiplexers (e.g., multiplexers 4906, 4908, 4910) that may be configured (e.g., via their respective control signals) to connect one or more
data paths (e.g., from PEs) together. Figure 49A illustrates the network 4900 (e.g., fabric) configured (e.g., set) for some previous operation or program. Figure
49B illustrates the local configuration controller 4902 (e.g., including a network interface circuit 4904 to send and/or receive signals) strobing a configuration
signal and the local network is set to a default configuration (e.g., as depicted) that allows the LCC to send configuration data to all configurable fabric elements
(CFEs), e.g., muxes. Figure 49C illustrates the LCC strobing configuration information across the network, configuring CFEs in a predetermined (e.g., silicondefined) sequence. In one embodiment, when CFEs are configured they may begin operation immediately. In another embodiments, the CFEs wait to begin
operation until the fabric has been completely configured (e.g., as signaled by configuration terminator (e.g., configuration terminator 5104 and configuration
terminator 5108 in Figure 51) for each local configuration controller). In one embodiment, the LCC obtains control over the network fabric by sending a special
message, or driving a signal. It then strobes configuration data (e.g., over a period of many cycles) to the CFEs in the fabric. In these figures, the multiplexor
networks are analogues of the "Switch" shown in certain Figures (e.g., Figure 6).

Local Configuration Controller
[0268] Figure 50 illustrates a (e.g., local) configuration controller 5002 according to embodiments of the disclosure. A local configuration controller (LCC) may be the
hardware entity which is responsible for loading the local portions (e.g., in a subset of a tile or otherwise) of the fabric program, interpreting these program
portions, and then loading these program portions into the fabric by driving the appropriate protocol on the various configuration wires. In this capacity, the LCC
may be a special-purpose, sequential microcontroller.
[0269] LCC operation may begin when it receives a pointer to a code segment. Depending on the LCB microarchitecture, this pointer (e.g., stored in pointer register
5006) may come either over a network (e.g., from within the CSA (fabric) itself) or through a memory system access to the LCC When it receives such a pointer,
the LCC optionally drains relevant state from its portion of the fabric for context storage, and then proceeds to immediately reconfigure the portion of the fabric
for which it is responsible. The program loaded by the LCC may be a combination of configuration data for the fabric and control commands for the LCC, e.g.,
which are lightly encoded. As the LCC streams in the program portion, it may interprets the program as a command stream and perform the appropriate encoded
action to configure (e.g., load) the fabric.
[0270] Two different microarchitectures for the LCC are shown in Figure 48, e.g., with one or both being utilized in a CSA. The first places the LCC 4802 at the memory
interface. In this case, the LCC may make direct requests to the memory system to load data. In the second case the LCC 4806 is placed on a memory network,
in which it may make requests to the memory only indirectly. In both cases, the logical operation of the LCB is unchanged. In one embodiment, an LCCs is
informed of the program to load, for example, by a set of (e.g., OS-visible) control-status-registers which will be used to inform individual LCCs of new program
pointers, etc.

Extra Out-of-band Control Channels (e.g., Wires)
[0271] In certain embodiments, configuration relies on 2-8 extra, out-of-band control channels to improve configuration speed, as defined below. For example,
configuration controller 5002 may include the following control channels, e.g., CFG START control channel 5008, CFG_VALID control channel 5010, and CFG
DONE control channel 5012, with examples of each discussed in Table 2 below. Table 2: Control Channels
CFG_STA
RT
CFG_VALI
D

Asserted at beginning of configuration. Sets configuration state at each CFE and sets the configuration bus.

Denotes validity of values on configuration bus.

CFG_DON Optional. Denotes completion of the configuration of a particular CFE. This allows configuration to be short circuited in case a CFE does not require
E

additional configuration

[0272] Generally, the handling of configuration information may be left to the implementer of a particular CFE. For example, a selectable function CFE may have a
provision for setting registers using an existing data path, while a fixed function CFE might simply set a configuration register.
[0273] Due to long wire delays when programming a large set of CFEs, the CFG_VALID signal may be treated as a clock/latch enable for CFE components. Since this
signal is used as a clock, in one embodiment the duty cycle of the line is at most 50%. As a result, configuration throughput is approximately halved. Optionally, a
second CFG VALID signal may be added to enable continuous programming.
[0274] In one embodiment, only CFG_START is strictly communicated on an independent coupling (e.g., wire), for example, CFG VALID and CFG_DONE may be overlaid
on top of other network couplings.

Reuse of Network Resources
[0275] To reduce the overhead of configuration, certain embodiments of a CSA make use of existing network infrastructure to communicate configuration data. A LCC
may make use of both a chip-level memory hierarchy and a fabric-level communications networks to move data from storage into the fabric. As a result, in
certain embodiments of a CSA, the configuration infrastructure adds no more than 2% to the overall fabric area and power.
[0276] Reuse of network resources in certain embodiments of a CSA may cause a network to have some hardware support for a configuration mechanism. Circuit
switched networks of embodiments of a CSA cause an LCC to set their multiplexors in a specific way for configuration when the 'CFG START' signal is asserted.
Packet switched networks do not require extension, although LCC endpoints (e.g., configuration terminators) use a specific address in the packet switched
network. Network reuse is optional, and some embodiments may find dedicated configuration buses to be more convenient.

Per CFE State
[0277] Each CFE may maintain a bit denoting whether or not it has been configured (see, e.g., Figure 39). This bit may be de-asserted when the configuration start
signal is driven, and then asserted once the particular CFE has been configured. In one configuration protocol, CFEs are arranged to form chains with the CFE
configuration state bit determining the topology of the chain. A CFE may read the configuration state bit of the immediately adjacent CFE. If this adjacent CFE is
configured and the current CFE is not configured, the CFE may determine that any current configuration data is targeted at the current CFE. When the 'CFG DONE'
signal is asserted, the CFE may set its configuration bit, e.g., enabling upstream CFEs to configure. As a base case to the configuration process, a configuration
terminator (e.g., configuration terminator 4804 for LCC 4802 or configuration terminator 4808 for LCC 4806 in Figure 48) which asserts that it is configured may
be included at the end of a chain.
[0278] Internal to the CFE, this bit may be used to drive flow control ready signals. For example, when the configuration bit is de-asserted, network control signals may
automatically be clamped to a values that prevent data from flowing, while, within PEs, no operations or other actions will be scheduled.

https://patents.google.com/patent/EP3726389B1/en?q=(high+radix)&oq=high+radix&page=1

35/76

2/14/26, 2:00 PM

EP3726389B1 - Apparatuses, methods, and systems for memory interface circuit allocation in a configurable spatial accelera…

Dealing with High-delay Configuration Paths
[0279] One embodiment of an LCC may drive a signal over a long distance, e.g., through many multiplexors and with many loads. Thus, it may be difficult for a signal to
arrive at a distant CFE within a short clock cycle. In certain embodiments, configuration signals are at some division (e.g., fraction of) of the main (e.g., CSA)
clock frequency to ensure digital timing discipline at configuration. Clock division may be utilized in an out-of-band signaling protocol, and does not require any
modification of the main clock tree.

Ensuring Consistent Fabric Behavior During Configuration
[0280] Since certain configuration schemes are distributed and have non-deterministic timing due to program and memory effects, different portions of the fabric may
be configured at different times. As a result, certain embodiments of a CSA provide mechanisms to prevent inconsistent operation among configured and
unconfigured CFEs. Generally, consistency is viewed as a property required of and maintained by CFEs themselves, e.g., using the internal CFE state. For
example, when a CFE is in an unconfigured state, it may claim that its input buffers are full, and that its output is invalid. When configured, these values will be
set to the true state of the buffers. As enough of the fabric comes out of configuration, these techniques may permit it to begin operation. This has the effect of
further reducing context switching latency, e.g., if long-latency memory requests are issued early.

Variable-width Configuration
[0281] Different CFEs may have different configuration word widths. For smaller CFE configuration words, implementers may balance delay by equitably assigning CFE
configuration loads across the network wires. To balance loading on network wires, one option is to assign configuration bits to different portions of network
wires to limit the net delay on any one wire. Wide data words may be handled by using serialization/deserialization techniques. These decisions may be taken on
a per-fabric basis to optimize the behavior of a specific CSA (e.g., fabric). Network controller (e.g., one or more of network controller 4810 and network controller
4812 may communicate with each domain (e.g., subset) of the CSA (e.g., fabric), for example, to send configuration information to one or more LCCs. Network
controller may be part of a communications network (e.g., separate from circuit switched network). Network controller may include a network dataflow endpoint
circuit.
6.2 Microarchitecture for Low Latency Configuration of a CSA and for Timely Fetching of Configuration Data for a CSA
[0282] Embodiments of a CSA may be an energy-efficient and high-performance means of accelerating user applications. When considering whether a program (e.g., a
dataflow graph thereof) may be successfully accelerated by an accelerator, both the time to configure the accelerator and the time to run the program may be
considered. If the run time is short, then the configuration time may play a large role in determining successful acceleration. Therefore, to maximize the domain
of accelerable programs, in some embodiments the configuration time is made as short as possible. One or more configuration caches may be includes in a
CSA, e.g., such that the high bandwidth, low-latency store enables rapid reconfiguration. Next is a description of several embodiments of a configuration cache.
[0283] In one embodiment, during configuration, the configuration hardware (e.g., LCC) optionally accesses the configuration cache to obtain new configuration
information. The configuration cache may operate either as a traditional address based cache, or in an OS managed mode, in which configurations are stored in
the local address space and addressed by reference to that address space. If configuration state is located in the cache, then no requests to the backing store
are to be made in certain embodiments. In certain embodiments, this configuration cache is separate from any (e.g., lower level) shared cache in the memory
hierarchy.
[0284] Figure 51 illustrates an accelerator tile 5100 comprising an array of processing elements, a configuration cache (e.g., 5118 or 5120), and a local configuration
controller (e.g., 5102 or 5106) according to embodiments of the disclosure. In one embodiment, configuration cache 5114 is co-located with local configuration
controller 5102. In one embodiment, configuration cache 5118 is located in the configuration domain of local configuration controller 5106, e.g., with a first
domain ending at configuration terminator 5104 and a second domain ending at configuration terminator 5108). A configuration cache may allow a local
configuration controller may refer to the configuration cache during configuration, e.g., in the hope of obtaining configuration state with lower latency than a
reference to memory. A configuration cache (storage) may either be dedicated or may be accessed as a configuration mode of an in-fabric storage element, e.g.,
local cache 5116.

CachingModes
[0285]

1. Demand Caching - In this mode, the configuration cache operates as a true cache. The configuration controller issues address-based requests, which
are checked against tags in the cache. Misses are loaded into the cache and then may be re-referenced during future reprogramming.
2. In-Fabric Storage (Scratchpad) Caching - In this mode the configuration cache receives a reference to a configuration sequence in its own, small
address space, rather than the larger address space of the host. This may improve memory density since the portion of cache used to store tags may
instead be used to store configuration.

[0286] In certain embodiments, a configuration cache may have the configuration data preloaded into it, e.g., either by external direction or internal direction. This may
allow reduction in the latency to load programs. Certain embodiments herein provide for an interface to a configuration cache which permits the loading of new
configuration state into the cache, e.g., even if a configuration is running in the fabric already. The initiation of this load may occur from either an internal or
external source. Embodiments of a pre-loading mechanism further reduce latency by removing the latency of cache loading from the configuration path.

Pre-fetching modes
[0287]

1. Explicit Prefetching - A configuration path is augmented with a new command, ConfigurationCachePrefetch. Instead of programming the fabric, this
command simply cause a load of the relevant program configuration into a configuration cache, without programming the fabric. Since this mechanism
piggybacks on the existing configuration infrastructure, it is exposed both within the fabric and externally, e.g., to cores and other entities accessing the
memory space.
2. Implicit prefetching -A global configuration controller may maintain a prefetch predictor, and use this to initiate the explicit prefetching to a
configuration cache, e.g., in an automated fashion.
6.3 Hardware for Rapid Reconfiguration of a CSA in Response to an Exception

[0288] Certain embodiments of a CSA (e.g., a spatial fabric) include large amounts of instruction and configuration state, e.g., which is largely static during the
operation of the CSA. Thus, the configuration state may be vulnerable to soft errors. Rapid and error-free recovery of these soft errors may be critical to the longterm reliability and performance of spatial systems.
[0289] Certain embodiments herein provide for a rapid configuration recovery loop, e.g., in which configuration errors are detected and portions of the fabric
immediately reconfigured. Certain embodiments herein include a configuration controller, e.g., with reliability, availability, and serviceability (RAS) reprogramming
features. Certain embodiments of CSA include circuitry for high-speed configuration, error reporting, and parity checking within the spatial fabric. Using a
combination of these three features, and optionally, a configuration cache, a configuration/exception handling circuit may recover from soft errors in
configuration. When detected, soft errors may be conveyed to a configuration cache which initiates an immediate reconfiguration of (e.g., that portion of) the
fabric. Certain embodiments provide for a dedicated reconfiguration circuit, e.g., which is faster than any solution that would be indirectly implemented in the
fabric. In certain embodiments, co-located exception and configuration circuit cooperates to reload the fabric on configuration error detection.
[0290] Figure 52 illustrates an accelerator tile 5200 comprising an array of processing elements and a configuration and exception handling controller (5202, 5206) with
a reconfiguration circuit (5218, 5222) according to embodiments of the disclosure. In one embodiment, when a PE detects a configuration error through its local
RAS features, it sends a (e.g., configuration error or reconfiguration error) message by its exception generator to the configuration and exception handling
controller (e.g., 5202 or 5206). On receipt of this message, the configuration and exception handling controller (e.g., 5202 or 5206) initiates the co-located
reconfiguration circuit (e.g., 5218 or 5222, respectively) to reload configuration state. The configuration microarchitecture proceeds and reloads (e.g., only)
configurations state, and in certain embodiments, only the configuration state for the PE reporting the RAS error. Upon completion of reconfiguration, the fabric
may resume normal operation. To decrease latency, the configuration state used by the configuration and exception handling controller (e.g., 5202 or 5206) may

https://patents.google.com/patent/EP3726389B1/en?q=(high+radix)&oq=high+radix&page=1

36/76

2/14/26, 2:00 PM

EP3726389B1 - Apparatuses, methods, and systems for memory interface circuit allocation in a configurable spatial accelera…

be sourced from a configuration cache. As a base case to the configuration or reconfiguration process, a configuration terminator (e.g., configuration terminator
5204 for configuration and exception handling controller 5202 or configuration terminator 5208 for configuration and exception handling controller 5206) in
Figure 52) which asserts that it is configured (or reconfigures) may be included at the end of a chain.
[0291] Figure 53 illustrates a reconfiguration circuit 5318 according to embodiments of the disclosure. Reconfiguration circuit 5318 includes a configuration state
register 5320 to store the configuration state (or a pointer thereto).
7.4 Hardware for Fabric-Initiated Reconfiguration of a CSA
[0292] Some portions of an application targeting a CSA (e.g., spatial array) may be run infrequently or may be mutually exclusive with other parts of the program. To
save area, to improve performance, and/or reduce power, it may be useful to time multiplex portions of the spatial fabric among several different parts of the
program dataflow graph. Certain embodiments herein include an interface by which a CSA (e.g., via the spatial program) may request that part of the fabric be
reprogrammed. This may enable the CSA to dynamically change itself according to dynamic control flow. Certain embodiments herein allow for fabric initiated
reconfiguration (e.g., reprogramming). Certain embodiments herein provide for a set of interfaces for triggering configuration from within the fabric. In some
embodiments, a PE issues a reconfiguration request based on some decision in the program dataflow graph. This request may travel a network to our new
configuration interface, where it triggers reconfiguration. Once reconfiguration is completed, a message may optionally be returned notifying of the completion.
Certain embodiments of a CSA thus provide for a program (e.g., dataflow graph) directed reconfiguration capability.
[0293] Figure 54 illustrates an accelerator tile 5400 comprising an array of processing elements and a configuration and exception handling controller 5406 with a
reconfiguration circuit 5418 according to embodiments of the disclosure. Here, a portion of the fabric issues a request for (re)configuration to a configuration
domain, e.g., of configuration and exception handling controller 5406 and/or reconfiguration circuit 5418. The domain (re)configures itself, and when the request
has been satisfied, the configuration and exception handling controller 5406 and/or reconfiguration circuit 5418 issues a response to the fabric, to notify the
fabric that (re)configuration is complete. In one embodiment, configuration and exception handling controller 5406 and/or reconfiguration circuit 5418 disables
communication during the time that (re)configuration is ongoing, so the program has no consistency issues during operation.

Configuration Modes
[0294] Configure-by-address - In this mode, the fabric makes a direct request to load configuration data from a particular address.
[0295] Configure-by-reference - In this mode the fabric makes a request to load a new configuration, e.g., by a pre-determined reference ID. This may simplify the
determination of the code to load, since the location of the code has been abstracted.

Configuring Multiple Domains
[0296] A CSA may include a higher level configuration controller to support a multicast mechanism to cast (e.g., via network indicated by the dotted box) configuration
requests to multiple (e.g., distributed or local) configuration controllers. This may enable a single configuration request to be replicated across larger portions of
the fabric, e.g., triggering a broad reconfiguration.
6.5 Exception Aggregators
[0297] Certain embodiments of a CSA may also experience an exception (e.g., exceptional condition), for example, floating point underflow. When these conditions
occur, a special handlers may be invoked to either correct the program or to terminate it. Certain embodiments herein provide for a system-level architecture for
handling exceptions in spatial fabrics. Since certain spatial fabrics emphasize area efficiency, embodiments herein minimize total area while providing a general
exception mechanism. Certain embodiments herein provides a low area means of signaling exceptional conditions occurring in within a CSA (e.g., a spatial
array). Certain embodiments herein provide an interface and signaling protocol for conveying such exceptions, as well as a PE-level exception semantics. Certain
embodiments herein are dedicated exception handling capabilities, e.g., and do not require explicit handling by the programmer.
[0298] One embodiments of a CSA exception architecture consists of four portions, e.g., shown in Figures 55-56. These portions may be arranged in a hierarchy, in
which exceptions flow from the producer, and eventually up to the tile-level exception aggregator (e.g., handler), which may rendezvous with an exception
servicer, e.g., of a core. The four portions may be:
1. PE Exception Generator
2. Local Exception Network
3. Mezzanine Exception Aggregator
4. Tile-Level Exception Aggregator
[0299] Figure 55 illustrates an accelerator tile 5500 comprising an array of processing elements and a mezzanine exception aggregator 5502 coupled to a tile-level
exception aggregator 5504 according to embodiments of the disclosure. Figure 56 illustrates a processing element 5600 with an exception generator 5644
according to embodiments of the disclosure.

PE Exception Generator
[0300] Processing element 5600 may include processing element 1000 from Figure 10, for example, with similar numbers being similar components, e.g., local network
1002 and local network 5602. Additional network 5613 (e.g., channel) may be an exception network. A PE may implement an interface to an exception network
(e.g., exception network 5613 (e.g., channel) on Figure 56). For example, Figure 56 shows the microarchitecture of such an interface, wherein the PE has an
exception generator 5644 (e.g., initiate an exception finite state machine (FSM) 5640 to strobe an exception packet (e.g., BOXID 5642) out on to the exception
network. BOXID 5642 may be a unique identifier for an exception producing entity (e.g., a PE or box) within a local exception network. When an exception is
detected, exception generator 5644 senses the exception network and strobes out the BOXID when the network is found to be free. Exceptions may be caused
by many conditions, for example, but not limited to, arithmetic error, failed ECC check on state, etc. however, it may also be that an exception dataflow operation
is introduced, with the idea of support constructs like breakpoints.
[0301] The initiation of the exception may either occur explicitly, by the execution of a programmer supplied instruction, or implicitly when a hardened error condition
(e.g., a floating point underflow) is detected. Upon an exception, the PE 5600 may enter a waiting state, in which it waits to be serviced by the eventual exception
handler, e.g., external to the PE 5600. The contents of the exception packet depend on the implementation of the particular PE, as described below.

Local Exception Network
[0302] A (e.g., local) exception network steers exception packets from PE 5600 to the mezzanine exception network. Exception network (e.g., 5613) may be a serial,
packet switched network consisting of a (e.g., single) control wire and one or more data wires, e.g., organized in a ring or tree topology, e.g., for a subset of PEs.
Each PE may have a (e.g., ring) stop in the (e.g., local) exception network, e.g., where it can arbitrate to inject messages into the exception network.
[0303] PE endpoints needing to inject an exception packet may observe their local exception network egress point. If the control signal indicates busy, the PE is to wait
to commence inject its packet. If the network is not busy, that is, the downstream stop has no packet to forward, then the PE will proceed commence injection.
[0304] Network packets may be of variable or fixed length. Each packet may begin with a fixed length header field identifying the source PE of the packet. This may be
followed by a variable number of PE-specific field containing information, for example, including error codes, data values, or other useful status information.

Mezzanine Exception Aggregator
[0305] The mezzanine exception aggregator 5504 is responsible for assembling local exception network into larger packets and sending them to the tile-level exception
aggregator 5502. The mezzanine exception aggregator 5504 may pre-pend the local exception packet with its own unique ID, e.g., ensuring that exception
messages are unambiguous. The mezzanine exception aggregator 5504 may interface to a special exception-only virtual channel in the mezzanine network, e.g.,
ensuring the deadlock-freedom of exceptions.
[0306] The mezzanine exception aggregator 5504 may also be able to directly service certain classes of exception. For example, a configuration request from the fabric
may be served out of the mezzanine network using caches local to the mezzanine network stop.

Tile-Level Exception Aggregator

https://patents.google.com/patent/EP3726389B1/en?q=(high+radix)&oq=high+radix&page=1

37/76

2/14/26, 2:00 PM

EP3726389B1 - Apparatuses, methods, and systems for memory interface circuit allocation in a configurable spatial accelera…

[0307] The final stage of the exception system is the tile-level exception aggregator 5502. The tile-level exception aggregator 5502 is responsible for collecting
exceptions from the various mezzanine-level exception aggregators (e.g., 5504) and forwarding them to the appropriate servicing hardware (e.g., core). As such,
the tile-level exception aggregator 5502 may include some internal tables and controller to associate particular messages with handler routines. These tables
may be indexed either directly or with a small state machine in order to steer particular exceptions.
[0308] Like the mezzanine exception aggregator, the tile-level exception aggregator may service some exception requests. For example, it may initiate the
reprogramming of a large portion of the PE fabric in response to a specific exception.
6.6 Extraction Controllers
[0309] Certain embodiments of a CSA include an extraction controller(s) to extract data from the fabric. The below discusses embodiments of how to achieve this
extraction quickly and how to minimize the resource overhead of data extraction. Data extraction may be utilized for such critical tasks as exception handling
and context switching. Certain embodiments herein extract data from a heterogeneous spatial fabric by introducing features that allow extractable fabric
elements (EFEs) (for example, PEs, network controllers, and/or switches) with variable and dynamically variable amounts of state to be extracted.
[0310] Embodiments of a CSA include a distributed data extraction protocol and microarchitecture to support this protocol. Certain embodiments of a CSA include
multiple local extraction controllers (LECs) which stream program data out of their local region of the spatial fabric using a combination of a (e.g., small) set of
control signals and the fabric-provided network. State elements may be used at each extractable fabric element (EFE) to form extraction chains, e.g., allowing
individual EFEs to self-extract without global addressing.
[0311] Embodiments of a CSA do not use a local network to extract program data. Embodiments of a CSA include specific hardware support (e.g., an extraction
controller) for the formation of extraction chains, for example, and do not rely on software to establish these chains dynamically, e.g., at the cost of increasing
extraction time. Embodiments of a CSA are not purely packet switched and do include extra out-of-band control wires (e.g., control is not sent through the data
path requiring extra cycles to strobe and reserialize this information). Embodiments of a CSA decrease extraction latency by fixing the extraction ordering and by
providing explicit out-of-band control (e.g., by at least a factor of two), while not significantly increasing network complexity.
[0312] Embodiments of a CSA do not use a serial mechanism for data extraction, in which data is streamed bit by bit from the fabric using a JTAG-like protocol.
Embodiments of a CSA utilize a coarse-grained fabric approach. In certain embodiments, adding a few control wires or state elements to a 64 or 32-bit-oriented
CSA fabric has a lower cost relative to adding those same control mechanisms to a 4 or 6 bit fabric.
[0313] Figure 57 illustrates an accelerator tile 5700 comprising an array of processing elements and a local extraction controller (5702, 5706) according to
embodiments of the disclosure. Each PE, each network controller, and each switch may be an extractable fabric elements (EFEs), e.g., which are configured (e.g.,
programmed) by embodiments of the CSA architecture.
[0314] Embodiments of a CSA include hardware that provides for efficient, distributed, low-latency extraction from a heterogeneous spatial fabric. This may be achieved
according to four techniques. First, a hardware entity, the local extraction controller (LEC) is utilized, for example, as in Figures 57-59. A LEC may accept
commands from a host (for example, a processor core), e.g., extracting a stream of data from the spatial array, and writing this data back to virtual memory for
inspection by the host. Second, a extraction data path may be included, e.g., that is as wide as the native width of the PE fabric and which may be overlaid on top
of the PE fabric. Third, new control signals may be received into the PE fabric which orchestrate the extraction process. Fourth, state elements may be located
(e.g., in a register) at each configurable endpoint which track the status of adjacent EFEs, allowing each EFE to unambiguously export its state without extra
control signals. These four microarchitectural features may allow a CSA to extract data from chains of EFEs. To obtain low data extraction latency, certain
embodiments may partition the extraction problem by including multiple (e.g., many) LECs and EFE chains in the fabric. At extraction time, these chains may
operate independently to extract data from the fabric in parallel, e.g., dramatically reducing latency. As a result of these combinations, a CSA may perform a
complete state dump (e.g., in hundreds of nanoseconds).
[0315] Figures 58A-58C illustrate a local extraction controller 5802 configuring a data path network according to embodiments of the disclosure. Depicted network
includes a plurality of multiplexers (e.g., multiplexers 5806, 5808, 5810) that may be configured (e.g., via their respective control signals) to connect one or more
data paths (e.g., from PEs) together. Figure 58A illustrates the network 5800 (e.g., fabric) configured (e.g., set) for some previous operation or program. Figure
58B illustrates the local extraction controller 5802 (e.g., including a network interface circuit 5804 to send and/or receive signals) strobing an extraction signal
and all PEs controlled by the LEC enter into extraction mode. The last PE in the extraction chain (or an extraction terminator) may master the extraction channels
(e.g., bus) and being sending data according to either (1) signals from the LEC or (2) internally produced signals (e.g., from a PE). Once completed, a PE may set
its completion flag, e.g., enabling the next PE to extract its data. Figure 58C illustrates the most distant PE has completed the extraction process and as a result
it has set its extraction state bit or bits, e.g., which swing the muxes into the adjacent network to enable the next PE to begin the extraction process. The
extracted PE may resume normal operation. In some embodiments, the PE may remain disabled until other action is taken. In these figures, the multiplexor
networks are analogues of the "Switch" shown in certain Figures (e.g., Figure 6).
[0316] The following sections describe the operation of the various components of embodiments of an extraction network.

Local Extraction Controller
[0317] Figure 59 illustrates an extraction controller 5902 according to embodiments of the disclosure. A local extraction controller (LEC) may be the hardware entity
which is responsible for accepting extraction commands, coordinating the extraction process with the EFEs, and/or storing extracted data, e.g., to virtual
memory. In this capacity, the LEC may be a special-purpose, sequential microcontroller.
[0318] LEC operation may begin when it receives a pointer to a buffer (e.g., in virtual memory) where fabric state will be written, and, optionally, a command controlling
how much of the fabric will be extracted. Depending on the LEC microarchitecture, this pointer (e.g., stored in pointer register 5904) may come either over a
network or through a memory system access to the LEC. When it receives such a pointer (e.g., command), the LEC proceeds to extract state from the portion of
the fabric for which it is responsible. The LEC may stream this extracted data out of the fabric into the buffer provided by the external caller.
[0319] Two different microarchitectures for the LEC are shown in Figure 57. The first places the LEC 5702 at the memory interface. In this case, the LEC may make
direct requests to the memory system to write extracted data. In the second case the LEC 5706 is placed on a memory network, in which it may make requests
to the memory only indirectly. In both cases, the logical operation of the LEC may be unchanged. In one embodiment, LECs are informed of the desire to extract
data from the fabric, for example, by a set of (e.g., OS-visible) control-status-registers which will be used to inform individual LECs of new commands.

Extra Out-of-band Control Channels (e.g., Wires)
[0320] In certain embodiments, extraction relies on 2-8 extra, out-of-band signals to improve configuration speed, as defined below. Signals driven by the LEC may be
labelled LEC. Signals driven by the EFE (e.g., PE) may be labelled EFE. Configuration controller 5902 may include the following control channels, e.g.,
LEC_EXTRACT control channel 6006, LEC_START control channel 5908, LEC_STROBE control channel 5910, and EFE_COMPLETE control channel 5912, with
examples of each discussed in Table 3 below. TABLE 3: Extraction Channels
LEC_EXTRACT

Optional signal asserted by the LEC during extraction process. Lowering this signal causes normal operation to resume.

LEC_START

Signal denoting start of extraction, allowing setup of local EFE state

LEC_STROBE
EFE_COMPLET
E

Optional strobe signal for controlling extraction related state machines at EFEs. EFEs may generate this signal internally in some
implementations.
Optional signal strobed when EFE has completed dumping state. This helps LEC identify the completion of individual EFE dumps.

[0321] Generally, the handling of extraction may be left to the implementer of a particular EFE. For example, selectable function EFE may have a provision for dumping
registers using an existing data path, while a fixed function EFE might simply have a multiplexor.

https://patents.google.com/patent/EP3726389B1/en?q=(high+radix)&oq=high+radix&page=1

38/76

2/14/26, 2:00 PM

EP3726389B1 - Apparatuses, methods, and systems for memory interface circuit allocation in a configurable spatial accelera…

[0322] Due to long wire delays when programming a large set of EFEs, the LEC_STROBE signal may be treated as a clock/latch enable for EFE components. Since this
signal is used as a clock, in one embodiment the duty cycle of the line is at most 50%. As a result, extraction throughput is approximately halved. Optionally, a
second LEC_STROBE signal may be added to enable continuous extraction.
[0323] In one embodiment, only LEC_START is strictly communicated on an independent coupling (e.g., wire), for example, other control channels may be overlayed on
existing network (e.g., wires).

Reuse of Network Resources
[0324] To reduce the overhead of data extraction, certain embodiments of a CSA make use of existing network infrastructure to communicate extraction data. A LEC
may make use of both a chip-level memory hierarchy and a fabric-level communications networks to move data from the fabric into storage. As a result, in
certain embodiments of a CSA, the extraction infrastructure adds no more than 2% to the overall fabric area and power.
[0325] Reuse of network resources in certain embodiments of a CSA may cause a network to have some hardware support for an extraction protocol. Circuit switched
networks require of certain embodiments of a CSA cause a LEC to set their multiplexors in a specific way for configuration when the 'LEC_START' signal is
asserted. Packet switched networks may not require extension, although LEC endpoints (e.g., extraction terminators) use a specific address in the packet
switched network. Network reuse is optional, and some embodiments may find dedicated configuration buses to be more convenient.

Per EFE State
[0326] Each EFE may maintain a bit denoting whether or not it has exported its state. This bit may de-asserted when the extraction start signal is driven, and then
asserted once the particular EFE finished extraction. In one extraction protocol, EFEs are arranged to form chains with the EFE extraction state bit determining
the topology of the chain. A EFE may read the extraction state bit of the immediately adjacent EFE. If this adjacent EFE has its extraction bit set and the current
EFE does not, the EFE may determine that it owns the extraction bus. When an EFE dumps its last data value, it may drives the 'EFE_DONE' signal and sets its
extraction bit, e.g., enabling upstream EFEs to configure for extraction. The network adjacent to the EFE may observe this signal and also adjust its state to
handle the transition. As a base case to the extraction process, an extraction terminator (e.g., extraction terminator 5704 for LEC 5702 or extraction terminator
5708 for LEC 5706 in Figure 48) which asserts that extraction is complete may be included at the end of a chain.
[0327] Internal to the EFE, this bit may be used to drive flow control ready signals. For example, when the extraction bit is de-asserted, network control signals may
automatically be clamped to a values that prevent data from flowing, while, within PEs, no operations or actions will be scheduled.

Dealing with High-delay Paths
[0328] One embodiment of a LEC may drive a signal over a long distance, e.g., through many multiplexors and with many loads. Thus, it may be difficult for a signal to
arrive at a distant EFE within a short clock cycle. In certain embodiments, extraction signals are at some division (e.g., fraction of) of the main (e.g., CSA) clock
frequency to ensure digital timing discipline at extraction. Clock division may be utilized in an out-of-band signaling protocol, and does not require any
modification of the main clock tree.

Ensuring Consistent Fabric Behavior During Extraction
[0329] Since certain extraction scheme are distributed and have non-deterministic timing due to program and memory effects, different members of the fabric may be
under extraction at different times. While LEC_EXTRACT is driven, all network flow control signals may be driven logically low, e.g., thus freezing the operation of
a particular segment of the fabric.
[0330] An extraction process may be non-destructive. Therefore a set of PEs may be considered operational once extraction has completed. An extension to an
extraction protocol may allow PEs to optionally be disabled post extraction. Alternatively, beginning configuration during the extraction process will have similar
effect in embodiments.

Single PE Extraction
[0331] In some cases, it may be expedient to extract a single PE. In this case, an optional address signal may be driven as part of the commencement of the extraction
process. This may enable the PE targeted for extraction to be directly enabled. Once this PE has been extracted, the extraction process may cease with the
lowering of the LEC EXTRACT signal. In this way, a single PE may be selectively extracted, e.g., by the local extraction controller.

Handling Extraction Backpressure
[0332] In an embodiment where the LEC writes extracted data to memory (for example, for post-processing, e.g., in software), it may be subject to limitted memory
bandwidth. In the case that the LEC exhausts its buffering capacity, or expects that it will exhaust its buffering capacity, it may stops strobing the LEC_STROBE
signal until the buffering issue has resolved.
[0333] Note that in certain figures (e.g., Figures 48, 51, 52, 54, 55, and 57) communications are shown schematically. In certain embodiments, those communications
may occur over the (e.g., interconnect) network.
6.7 Flow Diagrams
[0334] Figure 60 illustrates a flow diagram 6000 according to embodiments of the disclosure. Depicted flow 6000 includes decoding an instruction with a decoder of a
core of a processor into a decoded instruction 6002; executing the decoded instruction with an execution unit of the core of the processor to perform a first
operation 6004; receiving an input of a dataflow graph comprising a plurality of nodes 6006; overlaying the dataflow graph into an array of processing elements
of the processor with each node represented as a dataflow operator in the array of processing elements 6008; and performing a second operation of the
dataflow graph with the array of processing elements when an incoming operand set arrives at the array of processing elements 6010.
[0335] Figure 61 illustrates a flow diagram 6100 according to embodiments of the disclosure. Depicted flow 6100 includes decoding an instruction with a decoder of a
core of a processor into a decoded instruction 6102; executing the decoded instruction with an execution unit of the core of the processor to perform a first
operation 6104; receiving an input of a dataflow graph comprising a plurality of nodes 6106; overlaying the dataflow graph into a plurality of processing elements
of the processor and an interconnect network between the plurality of processing elements of the processor with each node represented as a dataflow operator
in the plurality of processing elements 6108; and performing a second operation of the dataflow graph with the interconnect network and the plurality of
processing elements when an incoming operand set arrives at the plurality of processing elements 6110.
6.8 Memory
[0336] Figure 62A is a block diagram of a system 6200 that employs a memory ordering circuit 6205 interposed between a memory subsystem 6210 and acceleration
hardware 6202, according to an embodiment of the present disclosure. The memory subsystem 6210 may include known memory components, including cache,
memory, and one or more memory controller(s) associated with a processor-based architecture. The acceleration hardware 6202 may be coarse-grained spatial
architecture made up of lightweight processing elements (or other types of processing components) connected by an inter-processing element (PE) network or
another type of inter-component network.
[0337] In one embodiment, programs, viewed as control data flow graphs, are mapped onto the spatial architecture by configuring PEs and a communications network.
Generally, PEs are configured as dataflow operators, similar to functional units in a processor: once the input operands arrive at the PE, some operation occurs,
and results are forwarded to downstream PEs in a pipelined fashion. Dataflow operators (or other types of operators) may choose to consume incoming data on
a per-operator basis. Simple operators, like those handling the unconditional evaluation of arithmetic expressions often consume all incoming data. It is
sometimes useful, however, for operators to maintain state, for example, in accumulation.
[0338] The PEs communicate using dedicated virtual circuits, which are formed by statically configuring a circuit-switched communications network. These virtual
circuits are flow controlled and fully back pressured, such that PEs will stall if either the source has no data or the destination is full. At runtime, data flows
through the PEs implementing a mapped algorithm according to a dataflow graph, also referred to as a subprogram herein. For example, data may be streamed
in from memory, through the acceleration hardware 6202, and then back out to memory. Such an architecture can achieve remarkable performance efficiency
relative to traditional multicore processors: compute, in the form of PEs, is simpler and more numerous than larger cores and communication is direct, as

https://patents.google.com/patent/EP3726389B1/en?q=(high+radix)&oq=high+radix&page=1

39/76

2/14/26, 2:00 PM

EP3726389B1 - Apparatuses, methods, and systems for memory interface circuit allocation in a configurable spatial accelera…

opposed to an extension of the memory subsystem 6210. Memory system parallelism, however, helps to support parallel PE computation. If memory accesses
are serialized, high parallelism is likely unachievable. To facilitate parallelism of memory accesses, the disclosed memory ordering circuit 6205 includes memory
ordering architecture and microarchitecture, as will be explained in detail. In one embodiment, the memory ordering circuit 6205 is a request address file circuit
(or "RAF") or other memory request circuitry.
[0339] Figure 62B is a block diagram of the system 6200 of Figure 62A but which employs multiple memory ordering circuits 6205, according to an embodiment of the
present disclosure. Each memory ordering circuit 6205 may function as an interface between the memory subsystem 6210 and a portion of the acceleration
hardware 6202 (e.g., spatial array of processing elements or tile). The memory subsystem 6210 may include a plurality of cache slices 12 (e.g., cache slices 12A,
12B, 12C, and 12D in the embodiment of Figure 62B), and a certain number of memory ordering circuits 6205 (four in this embodiment) may be used for each
cache slice 12. A crossbar 6204 (e.g., RAF circuit) may connect the memory ordering circuits 6205 to banks of cache that make up each cache slice 12A, 12B,
12C, and 12D. For example, there may be eight banks of memory in each cache slice in one embodiment. The system 6200 may be instantiated on a single die,
for example, as a system on a chip (SoC). In one embodiment, the SoC includes the acceleration hardware 6202. In an alternative embodiment, the acceleration
hardware 6202 is an external programmable chip such as an FPGA or CGRA, and the memory ordering circuits 6205 interface with the acceleration hardware
6202 through an input/output hub or the like.
[0340] Each memory ordering circuit 6205 may accept read and write requests to the memory subsystem 6210. The requests from the acceleration hardware 6202
arrive at the memory ordering circuit 6205 in a separate channel for each node of the dataflow graph that initiates read or write accesses, also referred to as
load or store accesses herein. Buffering is provided so that the processing of loads will return the requested data to the acceleration hardware 6202 in the order
it was requested. In other words, iteration six data is returned before iteration seven data, and so forth. Furthermore, note that the request channel from a
memory ordering circuit 6205 to a particular cache bank may be implemented as an ordered channel and any first request that leaves before a second request
will arrive at the cache bank before the second request.
[0341] Figure 63 is a block diagram 6300 illustrating general functioning of memory operations into and out of the acceleration hardware 6202, according to an
embodiment of the present disclosure. The operations occurring out the top of the acceleration hardware 6202 are understood to be made to and from a
memory of the memory subsystem 6210. Note that two load requests are made, followed by corresponding load responses. While the acceleration hardware
6202 performs processing on data from the load responses, a third load request and response occur, which trigger additional acceleration hardware processing.
The results of the acceleration hardware processing for these three load operations are then passed into a store operation, and thus a final result is stored back
to memory.
[0342] By considering this sequence of operations, it may be evident that spatial arrays more naturally map to channels. Furthermore, the acceleration hardware 6202 is
latency-insensitive in terms of the request and response channels, and inherent parallel processing that may occur. The acceleration hardware may also
decouple execution of a program from implementation of the memory subsystem 6210 (Figure 62A), as interfacing with the memory occurs at discrete
moments separate from multiple processing steps taken by the acceleration hardware 6202. For example, a load request to and a load response from memory
are separate actions, and may be scheduled differently in different circumstances depending on dependency flow of memory operations. The use of spatial
fabric, for example, for processing instructions facilitates spatial separation and distribution of such a load request and a load response.
[0343] Figure 64 is a block diagram 6400 illustrating a spatial dependency flow for a store operation 6401, according to an embodiment of the present disclosure.
Reference to a store operation is exemplary, as the same flow may apply to a load operation (but without incoming data), or to other operators such as a fence. A
fence is an ordering operation for memory subsystems that ensures that all prior memory operations of a type (such as all stores or all loads) have completed.
The store operation 6401 may receive an address 6402 (of memory) and data 6404 received from the acceleration hardware 6202. The store operation 6401
may also receive an incoming dependency token 6408, and in response to the availability of these three items, the store operation 6401may generate an
outgoing dependency token 6412. The incoming dependency token, which may, for example, be an initial dependency token of a program, may be provided in a
compiler-supplied configuration for the program, or may be provided by execution of memory-mapped input/output (I/O). Alternatively, if the program has already
been running, the incoming dependency token 6408 may be received from the acceleration hardware 6202, e.g., in association with a preceding memory
operation from which the store operation 6401 depends. The outgoing dependency token 6412 may be generated based on the address 6402 and data 6404
being required by a program-subsequent memory operation.
[0344] Figure 65 is a detailed block diagram of the memory ordering circuit 6205 of Figure 62A, according to an embodiment of the present disclosure. The memory
ordering circuit 6205 may be coupled to an out-of-order memory subsystem 6210, which as discussed, may include cache 12 and memory 18, and associated
out-of-order memory controller(s). The memory ordering circuit 6205 may include, or be coupled to, a communications network interface 20 that may be either
an inter-tile or an intra-tile network interface, and may be a circuit switched network interface (as illustrated), and thus include circuit-switched interconnects.
Alternatively, or additionally, the communications network interface 20 may include packet-switched interconnects.
[0345] The memory ordering circuit 6205 may further include, but not be limited to, a memory interface 6510, an operations queue 6512, input queue(s) 6516, a
completion queue 6520, an operation configuration data structure 6524, and an operations manager circuit 6530 that may further include a scheduler circuit
6532 and an execution circuit 6534. In one embodiment, the memory interface 6510 may be circuit-switched, and in another embodiment, the memory interface
6510 may be packet-switched, or both may exist simultaneously. The operations queue 6512 may buffer memory operations (with corresponding arguments)
that are being processed for request, and may, therefore, correspond to addresses and data coming into the input queues 6516.
[0346] More specifically, the input queues 6516 may be an aggregation of at least the following: a load address queue, a store address queue, a store data queue, and a
dependency queue. When implementing the input queue 6516 as aggregated, the memory ordering circuit 6205 may provide for sharing of logical queues, with
additional control logic to logically separate the queues, which are individual channels with the memory ordering circuit. This may maximize input queue usage,
but may also require additional complexity and space for the logic circuitry to manage the logical separation of the aggregated queue. Alternatively, as will be
discussed with reference to Figure 66, the input queues 6516 may be implemented in a segregated fashion, with a separate hardware queue for each. Whether
aggregated (Figure 65) or disaggregated (Figure 66), implementation for purposes of this disclosure is substantially the same, with the former using additional
logic to logically separate the queues within a single, shared hardware queue.
[0347] When shared, the input queues 6516 and the completion queue 6520 may be implemented as ring buffers of a fixed size. A ring buffer is an efficient
implementation of a circular queue that has a first-in-first-out (FIFO) data characteristic. These queues may, therefore, enforce a semantical order of a program
for which the memory operations are being requested. In one embodiment, a ring buffer (such as for the store address queue) may have entries corresponding
to entries flowing through an associated queue (such as the store data queue or the dependency queue) at the same rate. In this way, a store address may
remain associated with corresponding store data.
[0348] More specifically, the load address queue may buffer an incoming address of the memory 18 from which to retrieve data. The store address queue may buffer an
incoming address of the memory 18 to which to write data, which is buffered in the store data queue. The dependency queue may buffer dependency tokens in
association with the addresses of the load address queue and the store address queue. Each queue, representing a separate channel, may be implemented with
a fixed or dynamic number of entries. When fixed, the more entries that are available, the more efficient complicated loop processing may be made. But, having
too many entries costs more area and energy to implement. In some cases, e.g., with the aggregated architecture, the disclosed input queue 6516 may share
queue slots. Use of the slots in a queue may be statically allocated.
[0349] The completion queue 6520 may be a separate set of queues to buffer data received from memory in response to memory commands issued by load
operations. The completion queue 6520 may be used to hold a load operation that has been scheduled but for which data has not yet been received (and thus
has not yet completed). The completion queue 6520, may therefore, be used to reorder data and operation flow.

https://patents.google.com/patent/EP3726389B1/en?q=(high+radix)&oq=high+radix&page=1

40/76

2/14/26, 2:00 PM

EP3726389B1 - Apparatuses, methods, and systems for memory interface circuit allocation in a configurable spatial accelera…

[0350] The operations manager circuit 6530, which will be explained in more detail with reference to Figures 66 through 30, may provide logic for scheduling and
executing queued memory operations when taking into account dependency tokens used to provide correct ordering of the memory operations. The operation
manager 6530 may access the operation configuration data structure 6524 to determine which queues are grouped together to form a given memory operation.
For example, the operation configuration data structure 6524 may include that a specific dependency counter (or queue), input queue, output queue, and
completion queue are all grouped together for a particular memory operation. As each successive memory operation may be assigned a different group of
queues, access to varying queues may be interleaved across a sub-program of memory operations. Knowing all of these queues, the operations manager circuit
6530 may interface with the operations queue 6512, the input queue(s) 6516, the completion queue(s) 6520, and the memory subsystem 6210 to initially issue
memory operations to the memory subsystem 6210 when successive memory operations become "executable," and to next complete the memory operation
with some acknowledgement from the memory subsystem. This acknowledgement may be, for example, data in response to a load operation command or an
acknowledgement of data being stored in the memory in response to a store operation command.
[0351] Figure 66 is a flow diagram of a microarchitecture 6600 of the memory ordering circuit 6205 of Figure 62A, according to an embodiment of the present
disclosure. The memory subsystem 6210 may allow illegal execution of a program in which ordering of memory operations is wrong, due to the semantics of C
language (and other object-oriented program languages). The microarchitecture 6600 may enforce the ordering of the memory operations (sequences of loads
from and stores to memory) so that results of instructions that the acceleration hardware 6202 executes are properly ordered. A number of local networks 50
are illustrated to represent a portion of the acceleration hardware 6202 coupled to the microarchitecture 6600.
[0352] From an architectural perspective, there are at least two goals: first, to run general sequential codes correctly, and second, to obtain high performance in the
memory operations performed by the microarchitecture 6600. To ensure program correctness, the compiler expresses the dependency between the store
operation and the load operation to an array, p, in some fashion, which are expressed via dependency tokens as will be explained. To improve performance, the
microarchitecture 6600 finds and issues as many load commands of an array in parallel as is legal with respect to program order.
[0353] In one embodiment, the microarchitecture 6600 may include the operations queue 6512, the input queues 6516, the completion queues 6520, and the operations
manager circuit 6530 discussed with reference to Figure 65, above, where individual queues may be referred to as channels. The microarchitecture 6600 may
further include a plurality of dependency token counters 6614 (e.g., one per input queue), a set of dependency queues 6618 (e.g., one each per input queue), an
address multiplexer 6632, a store data multiplexer 6634, a completion queue index multiplexer 6636, and a load data multiplexer 6638. The operations manager
circuit 6530, in one embodiment, may direct these various multiplexers in generating a memory command 6650 (to be sent to the memory subsystem 6210) and
in receipt of responses of load commands back from the memory subsystem 6210, as will be explained.
[0354] The input queues 6516, as mentioned, may include a load address queue 6622, a store address queue 6624, and a store data queue 6626. (The small numbers 0,
1, 2 are channel labels and will be referred to later in Figure 69 and Figure 72A.) In various embodiments, these input queues may be multiplied to contain
additional channels, to handle additional parallelization of memory operation processing. Each dependency queue 6618 may be associated with one of the input
queues 6516. More specifically, the dependency queue 6618 labeled BO may be associated with the load address queue 6622 and the dependency queue
labeled B1 may be associated with the store address queue 6624. If additional channels of the input queues 6516 are provided, the dependency queues 6618
may include additional, corresponding channels.
[0355] In one embodiment, the completion queues 6520 may include a set of output buffers 6644 and 6646 for receipt of load data from the memory subsystem 6210
and a completion queue 6642 to buffer addresses and data for load operations according to an index maintained by the operations manager circuit 6530. The
operations manager circuit 6530 can manage the index to ensure in-order execution of the load operations, and to identify data received into the output buffers
6644 and 6646 that may be moved to scheduled load operations in the completion queue 6642.
[0356] More specifically, because the memory subsystem 6210 is out of order, but the acceleration hardware 6202 completes operations in order, the microarchitecture
6600 may reorder memory operations with use of the completion queue 6642. Three different sub-operations may be performed in relation to the completion
queue 6642, namely to allocate, enqueue, and dequeue. For allocation, the operations manager circuit 6530 may allocate an index into the completion queue
6642 in an in-order next slot of the completion queue. The operations manager circuit may provide this index to the memory subsystem 6210, which may then
know the slot to which to write data for a load operation. To enqueue, the memory subsystem 6210 may write data as an entry to the indexed, in-order next slot
in the completion queue 6642 like random access memory (RAM), setting a status bit of the entry to valid. To dequeue, the operations manager circuit 6530 may
present the data stored in this in-order next slot to complete the load operation, setting the status bit of the entry to invalid. Invalid entries may then be available
for a new allocation.
[0357] In one embodiment, the status signals 6548 may refer to statuses of the input queues 6516, the completion queues 6520, the dependency queues 6618, and the
dependency token counters 6614. These statuses, for example, may include an input status, an output status, and a control status, which may refer to the
presence or absence of a dependency token in association with an input or an output. The input status may include the presence or absence of addresses and
the output status may include the presence or absence of store values and available completion buffer slots. The dependency token counters 6614 may be a
compact representation of a queue and track a number of dependency tokens used for any given input queue. If the dependency token counters 6614 saturate,
no additional dependency tokens may be generated for new memory operations. Accordingly, the memory ordering circuit 6205 may stall scheduling new
memory operations until the dependency token counters 6614 becomes unsaturated.
[0358] With additional reference to Figure 67, Figure 67 is a block diagram of an executable determiner circuit 6700, according to an embodiment of the present
disclosure. The memory ordering circuit 6205 may be set up with several different kinds of memory operations, for example a load and a store:
IdNo[d,x] result.outN, addr.in64, order.in0, order.out0
stNo[d,x] addr.in64, data.inN, order.in0, order.out0
[0359] The executable determiner circuit 6700 may be integrated as a part of the scheduler circuit 6532 and which may perform a logical operation to determine
whether a given memory operation is executable, and thus ready to be issued to memory. A memory operation may be executed when the queues corresponding
to its memory arguments have data and an associated dependency token is present. These memory arguments may include, for example, an input queue
identifier 6710 (indicative of a channel of the input queue 6516), an output queue identifier 6720 (indicative of a channel of the completion queues 6520), a
dependency queue identifier 6730 (e.g., what dependency queue or counter should be referenced), and an operation type indicator 6740 (e.g., load operation or
store operation). A field (e.g., of a memory request) may be included, e.g., in the above format, that stores a bit or bits to indicate to use the hazard checking
hardware.
[0360] These memory arguments may be queued within the operations queue 6512, and used to schedule issuance of memory operations in association with incoming
addresses and data from memory and the acceleration hardware 6202. (See Figure 68.) Incoming status signals 6548 may be logically combined with these
identifiers and then the results may be added (e.g., through an AND gate 6750) to output an executable signal, e.g., which is asserted when the memory
operation is executable. The incoming status signals 6548 may include an input status 6712 for the input queue identifier 6710, an output status 6722 for the
output queue identifier 6720, and a control status 6732 (related to dependency tokens) for the dependency queue identifier 6730.
[0361] For a load operation, and by way of example, the memory ordering circuit 6205 may issue a load command when the load operation has an address (input
status) and room to buffer the load result in the completion queue 6642 (output status). Similarly, the memory ordering circuit 6205 may issue a store command
for a store operation when the store operation has both an address and data value (input status). Accordingly, the status signals 6548 may communicate a level
of emptiness (or fullness) of the queues to which the status signals pertain. The operation type may then dictate whether the logic results in an executable
signal depending on what address and data should be available.
[0362] To implement dependency ordering, the scheduler circuit 6532 may extend memory operations to include dependency tokens as underlined above in the
example load and store operations. The control status 6732 may indicate whether a dependency token is available within the dependency queue identified by the

https://patents.google.com/patent/EP3726389B1/en?q=(high+radix)&oq=high+radix&page=1

41/76

2/14/26, 2:00 PM

EP3726389B1 - Apparatuses, methods, and systems for memory interface circuit allocation in a configurable spatial accelera…

dependency queue identifier 6730, which could be one of the dependency queues 6618 (for an incoming memory operation) or a dependency token counter
6614 (for a completed memory operation). Under this formulation, a dependent memory operation requires an additional ordering token to execute and
generates an additional ordering token upon completion of the memory operation, where completion means that data from the result of the memory operation
has become available to program-subsequent memory operations.
[0363] In one embodiment, with further reference to Figure 66, the operations manager circuit 6530 may direct the address multiplexer 6632 to select an address
argument that is buffered within either the load address queue 6622 or the store address queue 6624, depending on whether a load operation or a store
operation is currently being scheduled for execution. If it is a store operation, the operations manager circuit 6530 may also direct the store data multiplexer
6634 to select corresponding data from the store data queue 6626. The operations manager circuit 6530 may also direct the completion queue index multiplexer
6636 to retrieve a load operation entry, indexed according to queue status and/or program order, within the completion queues 6520, to complete a load
operation. The operations manager circuit 6530 may also direct the load data multiplexer 6638 to select data received from the memory subsystem 6210 into
the completion queues 6520 for a load operation that is awaiting completion. In this way, the operations manager circuit 6530 may direct selection of inputs that
go into forming the memory command 6650, e.g., a load command or a store command, or that the execution circuit 6534 is waiting for to complete a memory
operation.
[0364] Figure 68 is a block diagram the execution circuit 6534 that may include a priority encoder 6806 and selection circuitry 6808 and which generates output control
line(s) 6810, according to one embodiment of the present disclosure. In one embodiment, the execution circuit 6534 may access queued memory operations (in
the operations queue 6512) that have been determined to be executable (Figure 67). The execution circuit 6534 may also receive the schedules 6804A, 6804B,
6804C for multiple of the queued memory operations that have been queued and also indicated as ready to issue to memory. The priority encoder 6806 may
thus receive an identity of the executable memory operations that have been scheduled and execute certain rules (or follow particular logic) to select the
memory operation from those coming in that has priority to be executed first. The priority encoder 6806 may output a selector signal 6807 that identifies the
scheduled memory operation that has a highest priority, and has thus been selected.
[0365] The priority encoder 6806, for example, may be a circuit (such as a state machine or a simpler converter) that compresses multiple binary inputs into a smaller
number of outputs, including possibly just one output. The output of a priority encoder is the binary representation of the original number starting from zero of
the most significant input bit. So, in one example, when memory operation 0 ("zero"), memory operation one ("1"), and memory operation two ("2") are executable
and scheduled, corresponding to 6804A, 6804B, and 6804C, respectively. The priority encoder 6806 may be configured to output the selector signal 6807 to the
selection circuitry 6808 indicating the memory operation zero as the memory operation that has highest priority. The selection circuitry 6808 may be a
multiplexer in one embodiment, and be configured to output its selection (e.g., of memory operation zero) onto the control lines 6810, as a control signal, in
response to the selector signal from the priority encoder 6806 (and indicative of selection of memory operation of highest priority). This control signal may go to
the multiplexers 6632, 6634, 6636, and/or 6638, as discussed with reference to Figure 66, to populate the memory command 6650 that is next to issue (be sent)
to the memory subsystem 6210. The transmittal of the memory command may be understood to be issuance of a memory operation to the memory subsystem
6210.
[0366] Figure 69 is a block diagram of an exemplary load operation 6900, both logical and in binary form, according to an embodiment of the present disclosure.
Referring back to Figure 67, the logical representation of the load operation 6900 may include channel zero ("0") (corresponding to the load address queue 6622)
as the input queue identifier 6710 and completion channel one ("1") (corresponding to the output buffer 6644) as the output queue identifier 6720. The
dependency queue identifier 6730 may include two identifiers, channel BO (corresponding to the first of the dependency queues 6618) for incoming dependency
tokens and counter C0 for outgoing dependency tokens. The operation type 6740 has an indication of "Load," which could be a numerical indicator as well, to
indicate the memory operation is a load operation. Below the logical representation of the logical memory operation is a binary representation for exemplary
purposes, e.g., where a load is indicated by "00." The load operation of Figure 69 may be extended to include other configurations such as a store operation
(Figure 71A) or other type of memory operations, such as a fence.
[0367] An example of memory ordering by the memory ordering circuit 6205 will be illustrated with a simplified example for purposes of explanation with relation to
Figures 70A-70B, 71A-71B, and 72A-72G. For this example, the following code includes an array, p, which is accessed by indices i and i+2:

for(i) {
temp = p[i];
p[i+2] = temp;
}
8] Assume, for this example, that array p contains 0,1,2,3,4,5,6, and at the end of loop execution, array p will contain 0,1,0,1,0,1,0. This code may be transformed by unrolling
the loop, as illustrated in Figures 70A and 70B. True address dependencies are annotated by arrows in Figure 70A, which in each case, a load operation is dependent on a
store operation to the same address. For example, for the first of such dependencies, a store (e.g., a write) to p[2] needs to occur before a load (e.g., a read) from p[2],
and second of such dependencies, a store to p[3] needs to occur before a load from p[3], and so forth. As a compiler is to be pessimistic, the compiler annotates
dependencies between two memory operations, load p[i] and store p[i+2]. Note that only sometimes do reads and writes conflict. The micro-architecture 6600 is
designed to extract memory-level parallelism where memory operations may move forward at the same time when there are no conflicts to the same address. This is
especially the case for load operations, which expose latency in code execution due to waiting for preceding dependent store operations to complete. In the example
code in Figure 70B, safe reorderings are noted by the arrows on the left of the unfolded code.
9] The way the microarchitecture may perform this reordering is discussed with reference to Figures 71A-71B and 72A-72G. Note that this approach is not as optimal as
possible because the microarchitecture 6600 may not send a memory command to memory every cycle. However, with minimal hardware, the microarchitecture
supports dependency flows by executing memory operations when operands (e.g., address and data, for a store, or address for a load) and dependency tokens are
available.
0] Figure 71A is a block diagram of exemplary memory arguments for a load operation 7102 and for a store operation 7104, according to an embodiment of the present
disclosure. These, or similar, memory arguments were discussed with relation to Figure 69 and will not be repeated here. Note, however, that the store operation 7104 has
no indicator for the output queue identifier because no data is being output to the acceleration hardware 6202. Instead, the store address in channel 1 and the data in
channel 2 of the input queues 6516, as identified in the input queue identifier memory argument, are to be scheduled for transmission to the memory subsystem 6210 in
a memory command to complete the store operation 7104. Furthermore, the input channels and output channels of the dependency queues are both implemented with
counters. Because the load operations and the store operations as displayed in Figures 70A and 70B are interdependent, the counters may be cycled between the load
operations and the store operations within the flow of the code.
1] Figure 71B is a block diagram illustrating flow of the load operations and store operations, such as the load operation 7102 and the store 7104 operation of Figure 70A,
through the microarchitecture 6600 of the memory ordering circuit of Figure 66, according to an embodiment of the present disclosure. For simplicity of explanation, not
all of the components are displayed, but reference may be made back to the additional components displayed in Figure 66. Various ovals indicating "Load" for the load
operation 7102 and "Store" for the store operation 7104 are overlaid on some of the components of the microarchitecture 6600 as indication of how various channels of
the queues are being used as the memory operations are queued and ordered through the microarchitecture 6600.
2] Figures 72A, 72B, 72C, 72D, 72E, 72F, 72G, and 72H are block diagrams illustrating functional flow of load operations and store operations for the exemplary program of
Figures 70A and 70B through queues of the microarchitecture of Figure 71B, according to an embodiment of the present disclosure. Each figure may correspond to a
next cycle of processing by the microarchitecture 6600. Values that are italicized are incoming values (into the queues) and values that are bolded are outgoing values
(out of the queues). All other values with normal fonts are retained values already existing in the queues.

https://patents.google.com/patent/EP3726389B1/en?q=(high+radix)&oq=high+radix&page=1

42/76

2/14/26, 2:00 PM

EP3726389B1 - Apparatuses, methods, and systems for memory interface circuit allocation in a configurable spatial accelera…

3] In Figure 72A, the address p[0] is incoming into the load address queue 6622, and the address p[2] is incoming into the store address queue 6624, starting the control
flow process. Note that counter C0, for dependency input for the load address queue, is "1" and counter C1, for dependency output, is zero. In contrast, the "1" of C0
indicates a dependency out value for the store operation. This indicates an incoming dependency for the load operation of p[0] and an outgoing dependency for the store
operation of p[2]. These values, however, are not yet active, but will become active, in this way, in Figure 72B.
4] In Figure 72B, address p[0] is bolded to indicate it is outgoing in this cycle. A new address p[1] is incoming into the load address queue and a new address p[3] is
incoming into the store address queue. A zero ("0")-valued bit in the completion queue 6642 is also incoming, which indicates any data present for that indexed entry is
invalid. As mentioned, the values for the counters C0 and C1 are now indicated as incoming, and are thus now active this cycle.
5] In Figure 72C, the outgoing address p[0] has now left the load address queue and a new address p[2] is incoming into the load address queue. And, the data ("0") is
incoming into the completion queue for address p[0]. The validity bit is set to "1" to indicate that the data in the completion queue is valid. Furthermore, a new address
p[4] is incoming into the store address queue. The value for counter C0 is indicated as outgoing and the value for counter C1 is indicated as incoming. The value of "1" for
C1 indicates an incoming dependency for store operation to address p[4].
6] Note that the address p[2] for the newest load operation is dependent on the value that first needs to be stored by the store operation for address p[2], which is at the top
of the store address queue. Later, the indexed entry in the completion queue for the load operation from address p[2] may remain buffered until the data from the store
operation to the address p[2] is completed (see Figures 72F-72H).
7] In Figure 72D, the data ("0") is outgoing from the completion queue for address p[0], which is therefore being sent out to the acceleration hardware 6202. Furthermore, a
new address p[3] is incoming into the load address queue and a new address p[5] is incoming into the store address queue. The values for the counters C0 and C1
remain unchanged.
8] In Figure 72E, the value ("0") for the address p[2] is incoming into the store data queue, while a new address p[4] comes into the load address queue and a new address
p[6] comes into the store address queue. The counter values for C0 and C1 remain unchanged.
9] In Figure 72F, the value ("0") for the address p[2] in the store data queue, and the address p[2] in the store address queue are both outgoing values. Likewise, the value for
the counter C1 is indicated as outgoing, while the value ("0") for counter C0 remain unchanged. Furthermore, a new address p[5] is incoming into the load address queue
and a new address p[7] is incoming into the store address queue.
0] In Figure 72G, the value ("0") is incoming to indicate the indexed value within the completion queue 6642 is invalid. The address p[1] is bolded to indicate it is outgoing
from the load address queue while a new address p[6] is incoming into the load address queue. A new address p[8] is also incoming into the store address queue. The
value of counter C0 is incoming as a "1," corresponding to an incoming dependency for the load operation of address p[6] and an outgoing dependency for the store
operation of address p[8]. The value of counter C1 is now "0," and is indicated as outgoing.
1] In Figure 72H, a data value of "1" is incoming into the completion queue 6642 while the validity bit is also incoming as a "1," meaning that the buffered data is valid. This is
the data needed to complete the load operation for p[2]. Recall that this data had to first be stored to address p[2], which happened in Figure 72F. The value of "0" for
counter C0 is outgoing, and a value of "1," for counter C1 is incoming. Furthermore, a new address p[7] is incoming into the load address queue and a new address p[9] is
incoming into the store address queue.
2] In the present embodiment, the process of executing the code of Figures 70A and 70B may continue on with bouncing dependency tokens between "0" and "1" for the
load operations and the store operations. This is due to the tight dependencies between p[i] and p[i+2]. Other code with less frequent dependencies may generate
dependency tokens at a slower rate, and thus reset the counters C0 and C1 at a slower rate, causing the generation of tokens of higher values (corresponding to further
semantically-separated memory operations).
3] Figure 73 is a flow chart of a method 7300 for ordering memory operations between acceleration hardware and an out-of-order memory subsystem, according to an
embodiment of the present disclosure. The method 7300 may be performed by a system that may include hardware (e.g., circuitry, dedicated logic, and/or programmable
logic), software (e.g., instructions executable on a computer system to perform hardware simulation), or a combination thereof. In an illustrative example, the method
7300 may be performed by the memory ordering circuit 6205 and various subcomponents of the memory ordering circuit 6205.
4] More specifically, referring to Figure 73, the method 7300 may start with the memory ordering circuit queuing memory operations in an operations queue of the memory
ordering circuit (7310). Memory operation and control arguments may make up the memory operations, as queued, where the memory operation and control arguments
are mapped to certain queues within the memory ordering circuit as discussed previously. The memory ordering circuit may work to issue the memory operations to a
memory in association with acceleration hardware, to ensure the memory operations complete in program order. The method 7300 may continue with the memory
ordering circuit receiving, in set of input queues, from the acceleration hardware, an address of the memory associated with a second memory operation of the memory
operations (7320). In one embodiment, a load address queue of the set of input queues is the channel to receive the address. In another embodiment, a store address
queue of the set of input queues is the channel to receive the address. The method 7300 may continue with the memory ordering circuit receiving, from the acceleration
hardware, a dependency token associated with the address, wherein the dependency token indicates a dependency on data generated by a first memory operation, of the
memory operations, which precedes the second memory operation (7330). In one embodiment, a channel of a dependency queue is to receive the dependency token.
The first memory operation may be either a load operation or a store operation.
5] The method 7300 may continue with the memory ordering circuit scheduling issuance of the second memory operation to the memory in response to receiving the
dependency token and the address associated with the dependency token (7340). For example, when the load address queue receives the address for an address
argument of a load operation and the dependency queue receives the dependency token for a control argument of the load operation, the memory ordering circuit may
schedule issuance of the second memory operation as a load operation. The method 7300 may continue with the memory ordering circuit issuing the second memory
operation (e.g., in a command) to the memory in response to completion of the first memory operation (7350). For example, if the first memory operation is a store,
completion may be verified by acknowledgement that the data in a store data queue of the set of input queues has been written to the address in the memory. Similarly, if
the first memory operation is a load operation, completion may be verified by receipt of data from the memory for the load operation.
7. SUMMARY
6] Supercomputing at the ExaFLOP scale may be a challenge in high-performance computing, a challenge which is not likely to be met by conventional von Neumann
architectures. To achieve ExaFLOPs, embodiments of a CSA provide a heterogeneous spatial array that targets direct execution of (e.g., compiler-produced) dataflow
graphs. In addition to laying out the architectural principles of embodiments of a CSA, the above also describes and evaluates embodiments of a CSA which showed
performance and energy of larger than 10x over existing products. Compiler-generated code may have significant performance and energy gains over roadmap
architectures. As a heterogeneous, parametric architecture, embodiments of a CSA may be readily adapted to all computing uses. For example, a mobile version of CSA
might be tuned to 32-bits, while a machine-learning focused array might feature significant numbers of vectorized 8-bit multiplication units. The main advantages of
embodiments of a CSA are high performance and extreme energy efficiency, characteristics relevant to all forms of computing ranging from supercomputing and
datacenter to the internet-of-things.
7] In one embodiment, a processor includes a spatial array of processing elements; and a packet switched communications network to route data within the spatial array
between processing elements according to a dataflow graph to perform a first dataflow operation of the dataflow graph, wherein the packet switched communications
network further comprises a plurality of network dataflow endpoint circuits to perform a second dataflow operation of the dataflow graph. A network dataflow endpoint
circuit of the plurality of network dataflow endpoint circuits may include a network ingress buffer to receive input data from the packet switched communications
network; and a spatial array egress buffer to output resultant data to the spatial array of processing elements according to the second dataflow operation on the input
data. The spatial array egress buffer may output the resultant data based on a scheduler within the network dataflow endpoint circuit monitoring the packet switched
communications network. The spatial array egress buffer may output the resultant data based on the scheduler within the network dataflow endpoint circuit monitoring a
selected channel of multiple network virtual channels of the packet switched communications network. A network dataflow endpoint circuit of the plurality of network
dataflow endpoint circuits may include a spatial array ingress buffer to receive control data from the spatial array that causes a network ingress buffer of the network

https://patents.google.com/patent/EP3726389B1/en?q=(high+radix)&oq=high+radix&page=1

43/76

2/14/26, 2:00 PM

EP3726389B1 - Apparatuses, methods, and systems for memory interface circuit allocation in a configurable spatial accelera…

dataflow endpoint circuit that received input data from the packet switched communications network to output resultant data to the spatial array of processing elements
according to the second dataflow operation on the input data and the control data. A network dataflow endpoint circuit of the plurality of network dataflow endpoint
circuits may stall an output of resultant data of the second dataflow operation from a spatial array egress buffer of the network dataflow endpoint circuit when a
backpressure signal from a downstream processing element of the spatial array of processing elements indicates that storage in the downstream processing element is
not available for the output of the network dataflow endpoint circuit. A network dataflow endpoint circuit of the plurality of network dataflow endpoint circuits may send a
backpressure signal to stall a source from sending input data on the packet switched communications network into a network ingress buffer of the network dataflow
endpoint circuit when the network ingress buffer is not available. The spatial array of processing elements may include a plurality of processing elements; and an
interconnect network between the plurality of processing elements to receive an input of the dataflow graph comprising a plurality of nodes, wherein the dataflow graph
is to be overlaid into the interconnect network, the plurality of processing elements, and the plurality of network dataflow endpoint circuits with each node represented as
a dataflow operator in either of the plurality of processing elements and the plurality of network dataflow endpoint circuits, and the plurality of processing elements and
the plurality of network dataflow endpoint circuits are to perform an operation by an incoming operand set arriving at each of the dataflow operators of the plurality of
processing elements and the plurality of network dataflow endpoint circuits. The spatial array of processing elements may include a circuit switched network to transport
the data within the spatial array between processing elements according to the dataflow graph.
8] In another embodiment, a method includes providing a spatial array of processing elements; routing, with a packet switched communications network, data within the
spatial array between processing elements according to a dataflow graph; performing a first dataflow operation of the dataflow graph with the processing elements; and
performing a second dataflow operation of the dataflow graph with a plurality of network dataflow endpoint circuits of the packet switched communications network.
The performing the second dataflow operation may include receiving input data from the packet switched communications network with a network ingress buffer of a
network dataflow endpoint circuit of the plurality of network dataflow endpoint circuits; and outputting resultant data from a spatial array egress buffer of the network
dataflow endpoint circuit to the spatial array of processing elements according to the second dataflow operation on the input data. The outputting may include outputting
the resultant data based on a scheduler within the network dataflow endpoint circuit monitoring the packet switched communications network. The outputting may
include outputting the resultant data based on the scheduler within the network dataflow endpoint circuit monitoring a selected channel of multiple network virtual
channels of the packet switched communications network. The performing the second dataflow operation may include receiving control data, with a spatial array ingress
buffer of a network dataflow endpoint circuit of the plurality of network dataflow endpoint circuits, from the spatial array; and configuring the network dataflow endpoint
circuit to cause a network ingress buffer of the network dataflow endpoint circuit that received input data from the packet switched communications network to output
resultant data to the spatial array of processing elements according to the second dataflow operation on the input data and the control data. The performing the second
dataflow operation may include stalling an output of the second dataflow operation from a spatial array egress buffer of a network dataflow endpoint circuit of the
plurality of network dataflow endpoint circuits when a backpressure signal from a downstream processing element of the spatial array of processing elements indicates
that storage in the downstream processing element is not available for the output of the network dataflow endpoint circuit. The performing the second dataflow
operation may include sending a backpressure signal from a network dataflow endpoint circuit of the plurality of network dataflow endpoint circuits to stall a source from
sending input data on the packet switched communications network into a network ingress buffer of the network dataflow endpoint circuit when the network ingress
buffer is not available. The routing, performing the first dataflow operation, and performing the second dataflow operation may include receiving an input of a dataflow
graph comprising a plurality of nodes; overlaying the dataflow graph into the spatial array of processing elements and the plurality of network dataflow endpoint circuits
with each node represented as a dataflow operator in either of the processing elements and the plurality of network dataflow endpoint circuits; and performing the first
dataflow operation with the processing elements and performing the second dataflow operation with the plurality of network dataflow endpoint circuits when an
incoming operand set arrives at each of the dataflow operators of the processing elements and the plurality of network dataflow endpoint circuits. The method may
include transporting the data within the spatial array between processing elements according to the dataflow graph with a circuit switched network of the spatial array.
9] In yet another embodiment, a non-transitory machine readable medium that stores code that when executed by a machine causes the machine to perform a method
including providing a spatial array of processing elements; routing, with a packet switched communications network, data within the spatial array between processing
elements according to a dataflow graph; performing a first dataflow operation of the dataflow graph with the processing elements; and performing a second dataflow
operation of the dataflow graph with a plurality of network dataflow endpoint circuits of the packet switched communications network. The performing the second
dataflow operation may include receiving input data from the packet switched communications network with a network ingress buffer of a network dataflow endpoint
circuit of the plurality of network dataflow endpoint circuits; and outputting resultant data from a spatial array egress buffer of the network dataflow endpoint circuit to
the spatial array of processing elements according to the second dataflow operation on the input data. The outputting may include outputting the resultant data based on
a scheduler within the network dataflow endpoint circuit monitoring the packet switched communications network. The outputting may include outputting the resultant
data based on the scheduler within the network dataflow endpoint circuit monitoring a selected channel of multiple network virtual channels of the packet switched
communications network. The performing the second dataflow operation may include receiving control data, with a spatial array ingress buffer of a network dataflow
endpoint circuit of the plurality of network dataflow endpoint circuits, from the spatial array; and configuring the network dataflow endpoint circuit to cause a network
ingress buffer of the network dataflow endpoint circuit that received input data from the packet switched communications network to output resultant data to the spatial
array of processing elements according to the second dataflow operation on the input data and the control data. The performing the second dataflow operation may
include stalling an output of the second dataflow operation from a spatial array egress buffer of a network dataflow endpoint circuit of the plurality of network dataflow
endpoint circuits when a backpressure signal from a downstream processing element of the spatial array of processing elements indicates that storage in the
downstream processing element is not available for the output of the network dataflow endpoint circuit. The performing the second dataflow operation may include
sending a backpressure signal from a network dataflow endpoint circuit of the plurality of network dataflow endpoint circuits to stall a source from sending input data on
the packet switched communications network into a network ingress buffer of the network dataflow endpoint circuit when the network ingress buffer is not available. The
routing, performing the first dataflow operation, and performing the second dataflow operation may include receiving an input of a dataflow graph comprising a plurality
of nodes; overlaying the dataflow graph into the spatial array of processing elements and the plurality of network dataflow endpoint circuits with each node represented
as a dataflow operator in either of the processing elements and the plurality of network dataflow endpoint circuits; and performing the first dataflow operation with the
processing elements and performing the second dataflow operation with the plurality of network dataflow endpoint circuits when an incoming operand set arrives at
each of the dataflow operators of the processing elements and the plurality of network dataflow endpoint circuits. The method may include transporting the data within
the spatial array between processing elements according to the dataflow graph with a circuit switched network of the spatial array.
0] In another embodiment, a processor includes a spatial array of processing elements; and a packet switched communications network to route data within the spatial
array between processing elements according to a dataflow graph to perform a first dataflow operation of the dataflow graph, wherein the packet switched
communications network further comprises means to perform a second dataflow operation of the dataflow graph.
1] In one embodiment, a processor includes a core with a decoder to decode an instruction into a decoded instruction and an execution unit to execute the decoded
instruction to perform a first operation; a plurality of processing elements; and an interconnect network between the plurality of processing elements to receive an input
of a dataflow graph comprising a plurality of nodes, wherein the dataflow graph is to be overlaid into the interconnect network and the plurality of processing elements
with each node represented as a dataflow operator in the plurality of processing elements, and the plurality of processing elements are to perform a second operation by
a respective, incoming operand set arriving at each of the dataflow operators of the plurality of processing elements. A processing element of the plurality of processing
elements may stall execution when a backpressure signal from a downstream processing element indicates that storage in the downstream processing element is not
available for an output of the processing element. The processor may include a flow control path network to carry the backpressure signal according to the dataflow
graph. A dataflow token may cause an output from a dataflow operator receiving the dataflow token to be sent to an input buffer of a particular processing element of the
plurality of processing elements. The second operation may include a memory access and the plurality of processing elements comprises a memory-accessing dataflow

https://patents.google.com/patent/EP3726389B1/en?q=(high+radix)&oq=high+radix&page=1

44/76

2/14/26, 2:00 PM

EP3726389B1 - Apparatuses, methods, and systems for memory interface circuit allocation in a configurable spatial accelera…

operator that is not to perform the memory access until receiving a memory dependency token from a logically previous dataflow operator. The plurality of processing
elements may include a first type of processing element and a second, different type of processing element.
2] In another embodiment, a method includes decoding an instruction with a decoder of a core of a processor into a decoded instruction; executing the decoded instruction
with an execution unit of the core of the processor to perform a first operation; receiving an input of a dataflow graph comprising a plurality of nodes; overlaying the
dataflow graph into a plurality of processing elements of the processor and an interconnect network between the plurality of processing elements of the processor with
each node represented as a dataflow operator in the plurality of processing elements; and performing a second operation of the dataflow graph with the interconnect
network and the plurality of processing elements by a respective, incoming operand set arriving at each of the dataflow operators of the plurality of processing elements.
The method may include stalling execution by a processing element of the plurality of processing elements when a backpressure signal from a downstream processing
element indicates that storage in the downstream processing element is not available for an output of the processing element. The method may include sending the
backpressure signal on a flow control path network according to the dataflow graph. A dataflow token may cause an output from a dataflow operator receiving the
dataflow token to be sent to an input buffer of a particular processing element of the plurality of processing elements. The method may include not performing a memory
access until receiving a memory dependency token from a logically previous dataflow operator, wherein the second operation comprises the memory access and the
plurality of processing elements comprises a memory-accessing dataflow operator. The method may include providing a first type of processing element and a second,
different type of processing element of the plurality of processing elements.
3] In yet another embodiment, an apparatus includes a data path network between a plurality of processing elements; and a flow control path network between the plurality
of processing elements, wherein the data path network and the flow control path network are to receive an input of a dataflow graph comprising a plurality of nodes, the
dataflow graph is to be overlaid into the data path network, the flow control path network, and the plurality of processing elements with each node represented as a
dataflow operator in the plurality of processing elements, and the plurality of processing elements are to perform a second operation by a respective, incoming operand
set arriving at each of the dataflow operators of the plurality of processing elements. The flow control path network may carry backpressure signals to a plurality of
dataflow operators according to the dataflow graph. A dataflow token sent on the data path network to a dataflow operator may cause an output from the dataflow
operator to be sent to an input buffer of a particular processing element of the plurality of processing elements on the data path network. The data path network may be
a static, circuit switched network to carry the respective, input operand set to each of the dataflow operators according to the dataflow graph. The flow control path
network may transmit a backpressure signal according to the dataflow graph from a downstream processing element to indicate that storage in the downstream
processing element is not available for an output of the processing element. At least one data path of the data path network and at least one flow control path of the flow
control path network may form a channelized circuit with backpressure control. The flow control path network may pipeline at least two of the plurality of processing
elements in series.
4] In another embodiment, a method includes receiving an input of a dataflow graph comprising a plurality of nodes; and overlaying the dataflow graph into a plurality of
processing elements of a processor, a data path network between the plurality of processing elements, and a flow control path network between the plurality of
processing elements with each node represented as a dataflow operator in the plurality of processing elements. The method may include carrying backpressure signals
with the flow control path network to a plurality of dataflow operators according to the dataflow graph. The method may include sending a dataflow token on the data
path network to a dataflow operator to cause an output from the dataflow operator to be sent to an input buffer of a particular processing element of the plurality of
processing elements on the data path network. The method may include setting a plurality of switches of the data path network and/or a plurality of switches of the flow
control path network to carry the respective, input operand set to each of the dataflow operators according to the dataflow graph, wherein the data path network is a
static, circuit switched network. The method may include transmitting a backpressure signal with the flow control path network according to the dataflow graph from a
downstream processing element to indicate that storage in the downstream processing element is not available for an output of the processing element. The method
may include forming a channelized circuit with backpressure control with at least one data path of the data path network and at least one flow control path of the flow
control path network.
5] In yet another embodiment, a processor includes a core with a decoder to decode an instruction into a decoded instruction and an execution unit to execute the decoded
instruction to perform a first operation; a plurality of processing elements; and a network means between the plurality of processing elements to receive an input of a
dataflow graph comprising a plurality of nodes, wherein the dataflow graph is to be overlaid into the network means and the plurality of processing elements with each
node represented as a dataflow operator in the plurality of processing elements, and the plurality of processing elements are to perform a second operation by a
respective, incoming operand set arriving at each of the dataflow operators of the plurality of processing elements.
6] In another embodiment, an apparatus includes a data path means between a plurality of processing elements; and a flow control path means between the plurality of
processing elements, wherein the data path means and the flow control path means are to receive an input of a dataflow graph comprising a plurality of nodes, the
dataflow graph is to be overlaid into the data path means, the flow control path means, and the plurality of processing elements with each node represented as a
dataflow operator in the plurality of processing elements, and the plurality of processing elements are to perform a second operation by a respective, incoming operand
set arriving at each of the dataflow operators of the plurality of processing elements.
7] In one embodiment, a processor includes a core with a decoder to decode an instruction into a decoded instruction and an execution unit to execute the decoded
instruction to perform a first operation; and an array of processing elements to receive an input of a dataflow graph comprising a plurality of nodes, wherein the dataflow
graph is to be overlaid into the array of processing elements with each node represented as a dataflow operator in the array of processing elements, and the array of
processing elements is to perform a second operation when an incoming operand set arrives at the array of processing elements. The array of processing element may
not perform the second operation until the incoming operand set arrives at the array of processing elements and storage in the array of processing elements is available
for output of the second operation. The array of processing elements may include a network (or channel(s)) to carry dataflow tokens and control tokens to a plurality of
dataflow operators. The second operation may include a memory access and the array of processing elements may include a memory-accessing dataflow operator that
is not to perform the memory access until receiving a memory dependency token from a logically previous dataflow operator. Each processing element may perform only
one or two operations of the dataflow graph.
8] In another embodiment, a method includes decoding an instruction with a decoder of a core of a processor into a decoded instruction; executing the decoded instruction
with an execution unit of the core of the processor to perform a first operation; receiving an input of a dataflow graph comprising a plurality of nodes; overlaying the
dataflow graph into an array of processing elements of the processor with each node represented as a dataflow operator in the array of processing elements; and
performing a second operation of the dataflow graph with the array of processing elements when an incoming operand set arrives at the array of processing elements.
The array of processing elements may not perform the second operation until the incoming operand set arrives at the array of processing elements and storage in the
array of processing elements is available for output of the second operation. The array of processing elements may include a network carrying dataflow tokens and
control tokens to a plurality of dataflow operators. The second operation may include a memory access and the array of processing elements comprises a memoryaccessing dataflow operator that is not to perform the memory access until receiving a memory dependency token from a logically previous dataflow operator. Each
processing element may performs only one or two operations of the dataflow graph.
9] In yet another embodiment, a non-transitory machine readable medium that stores code that when executed by a machine causes the machine to perform a method
including decoding an instruction with a decoder of a core of a processor into a decoded instruction; executing the decoded instruction with an execution unit of the core
of the processor to perform a first operation; receiving an input of a dataflow graph comprising a plurality of nodes; overlaying the dataflow graph into an array of
processing elements of the processor with each node represented as a dataflow operator in the array of processing elements; and performing a second operation of the
dataflow graph with the array of processing elements when an incoming operand set arrives at the array of processing elements. The array of processing element may
not perform the second operation until the incoming operand set arrives at the array of processing elements and storage in the array of processing elements is available
for output of the second operation. The array of processing elements may include a network carrying dataflow tokens and control tokens to a plurality of dataflow
operators. The second operation may include a memory access and the array of processing elements comprises a memory-accessing dataflow operator that is not to

https://patents.google.com/patent/EP3726389B1/en?q=(high+radix)&oq=high+radix&page=1

45/76

2/14/26, 2:00 PM

EP3726389B1 - Apparatuses, methods, and systems for memory interface circuit allocation in a configurable spatial accelera…

perform the memory access until receiving a memory dependency token from a logically previous dataflow operator. Each processing element may performs only one or
two operations of the dataflow graph.
0] In another embodiment, a processor includes a core with a decoder to decode an instruction into a decoded instruction and an execution unit to execute the decoded
instruction to perform a first operation; and means to receive an input of a dataflow graph comprising a plurality of nodes, wherein the dataflow graph is to be overlaid
into the means with each node represented as a dataflow operator in the means, and the means is to perform a second operation when an incoming operand set arrives
at the means.
1] In one embodiment, a processor includes a core with a decoder to decode an instruction into a decoded instruction and an execution unit to execute the decoded
instruction to perform a first operation; a plurality of processing elements; and an interconnect network between the plurality of processing elements to receive an input
of a dataflow graph comprising a plurality of nodes, wherein the dataflow graph is to be overlaid into the interconnect network and the plurality of processing elements
with each node represented as a dataflow operator in the plurality of processing elements, and the plurality of processing elements is to perform a second operation
when an incoming operand set arrives at the plurality of processing elements. The processor may further comprise a plurality of configuration controllers, each
configuration controller is coupled to a respective subset of the plurality of processing elements, and each configuration controller is to load configuration information
from storage and cause coupling of the respective subset of the plurality of processing elements according to the configuration information. The processor may include
a plurality of configuration caches, and each configuration controller is coupled to a respective configuration cache to fetch the configuration information for the
respective subset of the plurality of processing elements. The first operation performed by the execution unit may prefetch configuration information into each of the
plurality of configuration caches. Each of the plurality of configuration controllers may include a reconfiguration circuit to cause a reconfiguration for at least one
processing element of the respective subset of the plurality of processing elements on receipt of a configuration error message from the at least one processing
element. Each of the plurality of configuration controllers may a reconfiguration circuit to cause a reconfiguration for the respective subset of the plurality of processing
elements on receipt of a reconfiguration request message, and disable communication with the respective subset of the plurality of processing elements until the
reconfiguration is complete. The processor may include a plurality of exception aggregators, and each exception aggregator is coupled to a respective subset of the
plurality of processing elements to collect exceptions from the respective subset of the plurality of processing elements and forward the exceptions to the core for
servicing. The processor may include a plurality of extraction controllers, each extraction controller is coupled to a respective subset of the plurality of processing
elements, and each extraction controller is to cause state data from the respective subset of the plurality of processing elements to be saved to memory.
2] In another embodiment, a method includes decoding an instruction with a decoder of a core of a processor into a decoded instruction; executing the decoded instruction
with an execution unit of the core of the processor to perform a first operation; receiving an input of a dataflow graph comprising a plurality of nodes; overlaying the
dataflow graph into a plurality of processing elements of the processor and an interconnect network between the plurality of processing elements of the processor with
each node represented as a dataflow operator in the plurality of processing elements; and performing a second operation of the dataflow graph with the interconnect
network and the plurality of processing elements when an incoming operand set arrives at the plurality of processing elements. The method may include loading
configuration information from storage for respective subsets of the plurality of processing elements and causing coupling for each respective subset of the plurality of
processing elements according to the configuration information. The method may include fetching the configuration information for the respective subset of the plurality
of processing elements from a respective configuration cache of a plurality of configuration caches. The first operation performed by the execution unit may be
prefetching configuration information into each of the plurality of configuration caches. The method may include causing a reconfiguration for at least one processing
element of the respective subset of the plurality of processing elements on receipt of a configuration error message from the at least one processing element. The
method may include causing a reconfiguration for the respective subset of the plurality of processing elements on receipt of a reconfiguration request message; and
disabling communication with the respective subset of the plurality of processing elements until the reconfiguration is complete. The method may include collecting
exceptions from a respective subset of the plurality of processing elements; and forwarding the exceptions to the core for servicing. The method may include causing
state data from a respective subset of the plurality of processing elements to be saved to memory.
3] In yet another embodiment, a non-transitory machine readable medium that stores code that when executed by a machine causes the machine to perform a method
including decoding an instruction with a decoder of a core of a processor into a decoded instruction; executing the decoded instruction with an execution unit of the core
of the processor to perform a first operation; receiving an input of a dataflow graph comprising a plurality of nodes; overlaying the dataflow graph into a plurality of
processing elements of the processor and an interconnect network between the plurality of processing elements of the processor with each node represented as a
dataflow operator in the plurality of processing elements; and performing a second operation of the dataflow graph with the interconnect network and the plurality of
processing elements when an incoming operand set arrives at the plurality of processing elements. The method may include loading configuration information from
storage for respective subsets of the plurality of processing elements and causing coupling for each respective subset of the plurality of processing elements according
to the configuration information. The method may include fetching the configuration information for the respective subset of the plurality of processing elements from a
respective configuration cache of a plurality of configuration caches. The first operation performed by the execution unit may be prefetching configuration information
into each of the plurality of configuration caches. The method may include causing a reconfiguration for at least one processing element of the respective subset of the
plurality of processing elements on receipt of a configuration error message from the at least one processing element. The method may include causing a
reconfiguration for the respective subset of the plurality of processing elements on receipt of a reconfiguration request message; and disabling communication with the
respective subset of the plurality of processing elements until the reconfiguration is complete. The method may include collecting exceptions from a respective subset of
the plurality of processing elements; and forwarding the exceptions to the core for servicing. The method may include causing state data from a respective subset of the
plurality of processing elements to be saved to memory.
4] In another embodiment, a processor includes a core with a decoder to decode an instruction into a decoded instruction and an execution unit to execute the decoded
instruction to perform a first operation; a plurality of processing elements; and means between the plurality of processing elements to receive an input of a dataflow
graph comprising a plurality of nodes, wherein the dataflow graph is to be overlaid into the m and the plurality of processing elements with each node represented as a
dataflow operator in the plurality of processing elements, and the plurality of processing elements is to perform a second operation when an incoming operand set
arrives at the plurality of processing elements.
5] In one embodiment, an apparatus (e.g., a processor) includes: a spatial array of processing elements comprising a communications network to receive an input of a
dataflow graph comprising a plurality of nodes, wherein the dataflow graph is to be overlaid into the spatial array of processing elements with each node represented as a
dataflow operator in the spatial array of processing elements, and the spatial array of processing elements is to perform an operation by a respective, incoming operand
set arriving at each of the dataflow operators; a plurality of request address file circuits coupled to the spatial array of processing elements and a cache memory, each
request address file circuit of the plurality of request address file circuits to access data in the cache memory in response to a request for data access from the spatial
array of processing elements; a plurality of translation lookaside buffers comprising a translation lookaside buffer in each of the plurality of request address file circuits
to provide an output of a physical address for an input of a virtual address; and a translation lookaside buffer manager circuit comprising a higher level translation
lookaside buffer than the plurality of translation lookaside buffers, the translation lookaside buffer manager circuit to perform a first page walk in the cache memory for a
miss of an input of a virtual address into a first translation lookaside buffer and into the higher level translation lookaside buffer to determine a physical address mapped
to the virtual address, store a mapping of the virtual address to the physical address from the first page walk in the higher level translation lookaside buffer to cause the
higher level translation lookaside buffer to send the physical address to the first translation lookaside buffer in a first request address file circuit. The translation
lookaside buffer manager circuit may simultaneously, with the first page walk, perform a second page walk in the cache memory, wherein the second page walk is for a
miss of an input of a virtual address into a second translation lookaside buffer and into the higher level translation lookaside buffer to determine a physical address
mapped to the virtual address, store a mapping of the virtual address to the physical address from the second page walk in the higher level translation lookaside buffer to
cause the higher level translation lookaside buffer to send the physical address to the second translation lookaside buffer in a second request address file circuit. The
receipt of the physical address in the first translation lookaside buffer may cause the first request address file circuit to perform a data access for the request for data

https://patents.google.com/patent/EP3726389B1/en?q=(high+radix)&oq=high+radix&page=1

46/76

2/14/26, 2:00 PM

EP3726389B1 - Apparatuses, methods, and systems for memory interface circuit allocation in a configurable spatial accelera…

access from the spatial array of processing elements on the physical address in the cache memory. The translation lookaside buffer manager circuit may insert an
indicator in the higher level translation lookaside buffer for the miss of the input of the virtual address in the first translation lookaside buffer and the higher level
translation lookaside buffer to prevent an additional page walk for the input of the virtual address during the first page walk. The translation lookaside buffer manager
circuit may receive a shootdown message from a requesting entity for a mapping of a physical address to a virtual address, invalidate the mapping in the higher level
translation lookaside buffer, and send shootdown messages to only those of the plurality of request address file circuits that include a copy of the mapping in a
respective translation lookaside buffer, wherein each of those of the plurality of request address file circuits are to send an acknowledgement message to the translation
lookaside buffer manager circuit, and the translation lookaside buffer manager circuit is to send a shootdown completion acknowledgment message to the requesting
entity when all acknowledgement messages are received. The translation lookaside buffer manager circuit may receive a shootdown message from a requesting entity
for a mapping of a physical address to a virtual address, invalidate the mapping in the higher level translation lookaside buffer, and send shootdown messages to all of
the plurality of request address file circuits, wherein each of the plurality of request address file circuits are to send an acknowledgement message to the translation
lookaside buffer manager circuit, and the translation lookaside buffer manager circuit is to send a shootdown completion acknowledgment message to the requesting
entity when all acknowledgement messages are received.
6] In another embodiment, a method includes overlaying an input of a dataflow graph comprising a plurality of nodes into a spatial array of processing elements comprising
a communications network with each node represented as a dataflow operator in the spatial array of processing elements; coupling a plurality of request address file
circuits to the spatial array of processing elements and a cache memory with each request address file circuit of the plurality of request address file circuits accessing
data in the cache memory in response to a request for data access from the spatial array of processing elements; providing an output of a physical address for an input
of a virtual address into a translation lookaside buffer of a plurality of translation lookaside buffers comprising a translation lookaside buffer in each of the plurality of
request address file circuits; coupling a translation lookaside buffer manager circuit comprising a higher level translation lookaside buffer than the plurality of translation
lookaside buffers to the plurality of request address file circuits and the cache memory; and performing a first page walk in the cache memory for a miss of an input of a
virtual address into a first translation lookaside buffer and into the higher level translation lookaside buffer with the translation lookaside buffer manager circuit to
determine a physical address mapped to the virtual address, store a mapping of the virtual address to the physical address from the first page walk in the higher level
translation lookaside buffer to cause the higher level translation lookaside buffer to send the physical address to the first translation lookaside buffer in a first request
address file circuit. The method may include simultaneously, with the first page walk, performing a second page walk in the cache memory with the translation lookaside
buffer manager circuit, wherein the second page walk is for a miss of an input of a virtual address into a second translation lookaside buffer and into the higher level
translation lookaside buffer to determine a physical address mapped to the virtual address, and storing a mapping of the virtual address to the physical address from the
second page walk in the higher level translation lookaside buffer to cause the higher level translation lookaside buffer to send the physical address to the second
translation lookaside buffer in a second request address file circuit. The method may include causing the first request address file circuit to perform a data access for the
request for data access from the spatial array of processing elements on the physical address in the cache memory in response to receipt of the physical address in the
first translation lookaside buffer. The method may include inserting, with the translation lookaside buffer manager circuit, an indicator in the higher level translation
lookaside buffer for the miss of the input of the virtual address in the first translation lookaside buffer and the higher level translation lookaside buffer to prevent an
additional page walk for the input of the virtual address during the first page walk. The method may include receiving, with the translation lookaside buffer manager
circuit, a shootdown message from a requesting entity for a mapping of a physical address to a virtual address, invalidating the mapping in the higher level translation
lookaside buffer, and sending shootdown messages to only those of the plurality of request address file circuits that include a copy of the mapping in a respective
translation lookaside buffer, wherein each of those of the plurality of request address file circuits are to send an acknowledgement message to the translation lookaside
buffer manager circuit, and the translation lookaside buffer manager circuit is to send a shootdown completion acknowledgment message to the requesting entity when
all acknowledgement messages are received. The method may include receiving, with the translation lookaside buffer manager circuit, a shootdown message from a
requesting entity for a mapping of a physical address to a virtual address, invalidate the mapping in the higher level translation lookaside buffer, and sending shootdown
messages to all of the plurality of request address file circuits, wherein each of the plurality of request address file circuits are to send an acknowledgement message to
the translation lookaside buffer manager circuit, and the translation lookaside buffer manager circuit is to send a shootdown completion acknowledgment message to
the requesting entity when all acknowledgement messages are received.
7] In another embodiment, an apparatus includes a spatial array of processing elements comprising a communications network to receive an input of a dataflow graph
comprising a plurality of nodes, wherein the dataflow graph is to be overlaid into the spatial array of processing elements with each node represented as a dataflow
operator in the spatial array of processing elements, and the spatial array of processing elements is to perform an operation by a respective, incoming operand set
arriving at each of the dataflow operators; a plurality of request address file circuits coupled to the spatial array of processing elements and a plurality of cache memory
banks, each request address file circuit of the plurality of request address file circuits to access data in (e.g., each of) the plurality of cache memory banks in response to
a request for data access from the spatial array of processing elements; a plurality of translation lookaside buffers comprising a translation lookaside buffer in each of
the plurality of request address file circuits to provide an output of a physical address for an input of a virtual address; a plurality of higher level, than the plurality of
translation lookaside buffers, translation lookaside buffers comprising a higher level translation lookaside buffer in each of the plurality of cache memory banks to
provide an output of a physical address for an input of a virtual address; and a translation lookaside buffer manager circuit to perform a first page walk in the plurality of
cache memory banks for a miss of an input of a virtual address into a first translation lookaside buffer and into a first higher level translation lookaside buffer to
determine a physical address mapped to the virtual address, store a mapping of the virtual address to the physical address from the first page walk in the first higher
level translation lookaside buffer to cause the first higher level translation lookaside buffer to send the physical address to the first translation lookaside buffer in a first
request address file circuit. The translation lookaside buffer manager circuit may simultaneously, with the first page walk, perform a second page walk in the plurality of
cache memory banks, wherein the second page walk is for a miss of an input of a virtual address into a second translation lookaside buffer and into a second higher level
translation lookaside buffer to determine a physical address mapped to the virtual address, store a mapping of the virtual address to the physical address from the
second page walk in the second higher level translation lookaside buffer to cause the second higher level translation lookaside buffer to send the physical address to the
second translation lookaside buffer in a second request address file circuit. The receipt of the physical address in the first translation lookaside buffer may cause the first
request address file circuit to perform a data access for the request for data access from the spatial array of processing elements on the physical address in the plurality
of cache memory banks. The translation lookaside buffer manager circuit may insert an indicator in the first higher level translation lookaside buffer for the miss of the
input of the virtual address in the first translation lookaside buffer and the first higher level translation lookaside buffer to prevent an additional page walk for the input of
the virtual address during the first page walk. The translation lookaside buffer manager circuit may receive a shootdown message from a requesting entity for a mapping
of a physical address to a virtual address, invalidate the mapping in a higher level translation lookaside buffer storing the mapping, and send shootdown messages to
only those of the plurality of request address file circuits that include a copy of the mapping in a respective translation lookaside buffer, wherein each of those of the
plurality of request address file circuits are to send an acknowledgement message to the translation lookaside buffer manager circuit, and the translation lookaside
buffer manager circuit is to send a shootdown completion acknowledgment message to the requesting entity when all acknowledgement messages are received. The
translation lookaside buffer manager circuit may receive a shootdown message from a requesting entity for a mapping of a physical address to a virtual address,
invalidate the mapping in a higher level translation lookaside buffer storing the mapping, and send shootdown messages to all of the plurality of request address file
circuits, wherein each of the plurality of request address file circuits are to send an acknowledgement message to the translation lookaside buffer manager circuit, and
the translation lookaside buffer manager circuit is to send a shootdown completion acknowledgment message to the requesting entity when all acknowledgement
messages are received.
8] In yet another embodiment, a method includes: overlaying an input of a dataflow graph comprising a plurality of nodes into a spatial array of processing elements
comprising a communications network with each node represented as a dataflow operator in the spatial array of processing elements; coupling a plurality of request

https://patents.google.com/patent/EP3726389B1/en?q=(high+radix)&oq=high+radix&page=1

47/76

2/14/26, 2:00 PM

EP3726389B1 - Apparatuses, methods, and systems for memory interface circuit allocation in a configurable spatial accelera…

address file circuits to the spatial array of processing elements and a plurality of cache memory banks with each request address file circuit of the plurality of request
address file circuits accessing data in the plurality of cache memory banks in response to a request for data access from the spatial array of processing elements;
9] providing an output of a physical address for an input of a virtual address into a translation lookaside buffer of a plurality of translation lookaside buffers comprising a
translation lookaside buffer in each of the plurality of request address file circuits; providing an output of a physical address for an input of a virtual address into a higher
level, than the plurality of translation lookaside buffers, translation lookaside buffer of a plurality of higher level translation lookaside buffers comprising a higher level
translation lookaside buffer in each of the plurality of cache memory banks; coupling a translation lookaside buffer manager circuit to the plurality of request address file
circuits and the plurality of cache memory banks; and performing a first page walk in the plurality of cache memory banks for a miss of an input of a virtual address into a
first translation lookaside buffer and into a first higher level translation lookaside buffer with the translation lookaside buffer manager circuit to determine a physical
address mapped to the virtual address, store a mapping of the virtual address to the physical address from the first page walk in the first higher level translation
lookaside buffer to cause the first higher level translation lookaside buffer to send the physical address to the first translation lookaside buffer in a first request address
file circuit. The method may include simultaneously, with the first page walk, performing a second page walk in the plurality of cache memory banks with the translation
lookaside buffer manager circuit, wherein the second page walk is for a miss of an input of a virtual address into a second translation lookaside buffer and into a second
higher level translation lookaside buffer to determine a physical address mapped to the virtual address, and storing a mapping of the virtual address to the physical
address from the second page walk in the second higher level translation lookaside buffer to cause the second higher level translation lookaside buffer to send the
physical address to the second translation lookaside buffer in a second request address file circuit. The method may include causing the first request address file circuit
to perform a data access for the request for data access from the spatial array of processing elements on the physical address in the plurality of cache memory banks in
response to receipt of the physical address in the first translation lookaside buffer. The method may include inserting, with the translation lookaside buffer manager
circuit, an indicator in the first higher level translation lookaside buffer for the miss of the input of the virtual address in the first translation lookaside buffer and the first
higher level translation lookaside buffer to prevent an additional page walk for the input of the virtual address during the first page walk. The method may include
receiving, with the translation lookaside buffer manager circuit, a shootdown message from a requesting entity for a mapping of a physical address to a virtual address,
invalidating the mapping in a higher level translation lookaside buffer storing the mapping, and sending shootdown messages to only those of the plurality of request
address file circuits that include a copy of the mapping in a respective translation lookaside buffer, wherein each of those of the plurality of request address file circuits
are to send an acknowledgement message to the translation lookaside buffer manager circuit, and the translation lookaside buffer manager circuit is to send a
shootdown completion acknowledgment message to the requesting entity when all acknowledgement messages are received. The method may include receiving, with
the translation lookaside buffer manager circuit, a shootdown message from a requesting entity for a mapping of a physical address to a virtual address, invalidate the
mapping in a higher level translation lookaside buffer storing the mapping, and sending shootdown messages to all of the plurality of request address file circuits,
wherein each of the plurality of request address file circuits are to send an acknowledgement message to the translation lookaside buffer manager circuit, and the
translation lookaside buffer manager circuit is to send a shootdown completion acknowledgment message to the requesting entity when all acknowledgement
messages are received.
0] In another embodiment, a system includes a core with a decoder to decode an instruction into a decoded instruction and an execution unit to execute the decoded
instruction to perform a first operation; a spatial array of processing elements comprising a communications network to receive an input of a dataflow graph comprising
a plurality of nodes, wherein the dataflow graph is to be overlaid into the spatial array of processing elements with each node represented as a dataflow operator in the
spatial array of processing elements, and the spatial array of processing elements is to perform a second operation by a respective, incoming operand set arriving at
each of the dataflow operators; a plurality of request address file circuits coupled to the spatial array of processing elements and a cache memory, each request address
file circuit of the plurality of request address file circuits to access data in the cache memory in response to a request for data access from the spatial array of
processing elements; a plurality of translation lookaside buffers comprising a translation lookaside buffer in each of the plurality of request address file circuits to
provide an output of a physical address for an input of a virtual address; and a translation lookaside buffer manager circuit comprising a higher level translation lookaside
buffer than the plurality of translation lookaside buffers, the translation lookaside buffer manager circuit to perform a first page walk in the cache memory for a miss of
an input of a virtual address into a first translation lookaside buffer and into the higher level translation lookaside buffer to determine a physical address mapped to the
virtual address, store a mapping of the virtual address to the physical address from the first page walk in the higher level translation lookaside buffer to cause the higher
level translation lookaside buffer to send the physical address to the first translation lookaside buffer in a first request address file circuit. The translation lookaside
buffer manager circuit may simultaneously, with the first page walk, perform a second page walk in the cache memory, wherein the second page walk is for a miss of an
input of a virtual address into a second translation lookaside buffer and into the higher level translation lookaside buffer to determine a physical address mapped to the
virtual address, store a mapping of the virtual address to the physical address from the second page walk in the higher level translation lookaside buffer to cause the
higher level translation lookaside buffer to send the physical address to the second translation lookaside buffer in a second request address file circuit. The receipt of the
physical address in the first translation lookaside buffer may cause the first request address file circuit to perform a data access for the request for data access from the
spatial array of processing elements on the physical address in the cache memory. The translation lookaside buffer manager circuit may insert an indicator in the higher
level translation lookaside buffer for the miss of the input of the virtual address in the first translation lookaside buffer and the higher level translation lookaside buffer to
prevent an additional page walk for the input of the virtual address during the first page walk. The translation lookaside buffer manager circuit may receive a shootdown
message from a requesting entity for a mapping of a physical address to a virtual address, invalidate the mapping in the higher level translation lookaside buffer, and
send shootdown messages to only those of the plurality of request address file circuits that include a copy of the mapping in a respective translation lookaside buffer,
wherein each of those of the plurality of request address file circuits are to send an acknowledgement message to the translation lookaside buffer manager circuit, and
the translation lookaside buffer manager circuit is to send a shootdown completion acknowledgment message to the requesting entity when all acknowledgement
messages are received. The translation lookaside buffer manager circuit may receive a shootdown message from a requesting entity for a mapping of a physical address
to a virtual address, invalidate the mapping in the higher level translation lookaside buffer, and send shootdown messages to all of the plurality of request address file
circuits, wherein each of the plurality of request address file circuits are to send an acknowledgement message to the translation lookaside buffer manager circuit, and
the translation lookaside buffer manager circuit is to send a shootdown completion acknowledgment message to the requesting entity when all acknowledgement
messages are received.
1] In yet another embodiment, a system includes a core with a decoder to decode an instruction into a decoded instruction and an execution unit to execute the decoded
instruction to perform a first operation; a spatial array of processing elements comprising a communications network to receive an input of a dataflow graph comprising
a plurality of nodes, wherein the dataflow graph is to be overlaid into the spatial array of processing elements with each node represented as a dataflow operator in the
spatial array of processing elements, and the spatial array of processing elements is to perform a second operation by a respective, incoming operand set arriving at
each of the dataflow operators; a plurality of request address file circuits coupled to the spatial array of processing elements and a plurality of cache memory banks,
each request address file circuit of the plurality of request address file circuits to access data in (e.g., each of) the plurality of cache memory banks in response to a
request for data access from the spatial array of processing elements; a plurality of translation lookaside buffers comprising a translation lookaside buffer in each of the
plurality of request address file circuits to provide an output of a physical address for an input of a virtual address; a plurality of higher level, than the plurality of
translation lookaside buffers, translation lookaside buffers comprising a higher level translation lookaside buffer in each of the plurality of cache memory banks to
provide an output of a physical address for an input of a virtual address; and a translation lookaside buffer manager circuit to perform a first page walk in the plurality of
cache memory banks for a miss of an input of a virtual address into a first translation lookaside buffer and into a first higher level translation lookaside buffer to
determine a physical address mapped to the virtual address, store a mapping of the virtual address to the physical address from the first page walk in the first higher
level translation lookaside buffer to cause the first higher level translation lookaside buffer to send the physical address to the first translation lookaside buffer in a first
request address file circuit. The translation lookaside buffer manager circuit may simultaneously, with the first page walk, perform a second page walk in the plurality of
cache memory banks, wherein the second page walk is for a miss of an input of a virtual address into a second translation lookaside buffer and into a second higher level

https://patents.google.com/patent/EP3726389B1/en?q=(high+radix)&oq=high+radix&page=1

48/76

2/14/26, 2:00 PM

EP3726389B1 - Apparatuses, methods, and systems for memory interface circuit allocation in a configurable spatial accelera…

translation lookaside buffer to determine a physical address mapped to the virtual address, store a mapping of the virtual address to the physical address from the
second page walk in the second higher level translation lookaside buffer to cause the second higher level translation lookaside buffer to send the physical address to the
second translation lookaside buffer in a second request address file circuit. The receipt of the physical address in the first translation lookaside buffer may cause the first
request address file circuit to perform a data access for the request for data access from the spatial array of processing elements on the physical address in the plurality
of cache memory banks. The translation lookaside buffer manager circuit may insert an indicator in the first higher level translation lookaside buffer for the miss of the
input of the virtual address in the first translation lookaside buffer and the first higher level translation lookaside buffer to prevent an additional page walk for the input of
the virtual address during the first page walk. The translation lookaside buffer manager circuit may receive a shootdown message from a requesting entity for a mapping
of a physical address to a virtual address, invalidate the mapping in a higher level translation lookaside buffer storing the mapping, and send shootdown messages to
only those of the plurality of request address file circuits that include a copy of the mapping in a respective translation lookaside buffer, wherein each of those of the
plurality of request address file circuits are to send an acknowledgement message to the translation lookaside buffer manager circuit, and the translation lookaside
buffer manager circuit is to send a shootdown completion acknowledgment message to the requesting entity when all acknowledgement messages are received. The
translation lookaside buffer manager circuit may receive a shootdown message from a requesting entity for a mapping of a physical address to a virtual address,
invalidate the mapping in a higher level translation lookaside buffer storing the mapping, and send shootdown messages to all of the plurality of request address file
circuits, wherein each of the plurality of request address file circuits are to send an acknowledgement message to the translation lookaside buffer manager circuit, and
the translation lookaside buffer manager circuit is to send a shootdown completion acknowledgment message to the requesting entity when all acknowledgement
messages are received.
2] In another embodiment, an apparatus (e.g., a processor) includes: a spatial array of processing elements comprising a communications network to receive an input of a
dataflow graph comprising a plurality of nodes, wherein the dataflow graph is to be overlaid into the spatial array of processing elements with each node represented as a
dataflow operator in the spatial array of processing elements, and the spatial array of processing elements is to perform an operation by a respective, incoming operand
set arriving at each of the dataflow operators; a plurality of request address file circuits coupled to the spatial array of processing elements and a cache memory, each
request address file circuit of the plurality of request address file circuits to access data in the cache memory in response to a request for data access from the spatial
array of processing elements; a plurality of translation lookaside buffers comprising a translation lookaside buffer in each of the plurality of request address file circuits
to provide an output of a physical address for an input of a virtual address; and a means comprising a higher level translation lookaside buffer than the plurality of
translation lookaside buffers, the means to perform a first page walk in the cache memory for a miss of an input of a virtual address into a first translation lookaside
buffer and into the higher level translation lookaside buffer to determine a physical address mapped to the virtual address, store a mapping of the virtual address to the
physical address from the first page walk in the higher level translation lookaside buffer to cause the higher level translation lookaside buffer to send the physical
address to the first translation lookaside buffer in a first request address file circuit.
3] In yet another embodiment, an apparatus includes a spatial array of processing elements comprising a communications network to receive an input of a dataflow graph
comprising a plurality of nodes, wherein the dataflow graph is to be overlaid into the spatial array of processing elements with each node represented as a dataflow
operator in the spatial array of processing elements, and the spatial array of processing elements is to perform an operation by a respective, incoming operand set
arriving at each of the dataflow operators; a plurality of request address file circuits coupled to the spatial array of processing elements and a plurality of cache memory
banks, each request address file circuit of the plurality of request address file circuits to access data in (e.g., each of) the plurality of cache memory banks in response to
a request for data access from the spatial array of processing elements; a plurality of translation lookaside buffers comprising a translation lookaside buffer in each of
the plurality of request address file circuits to provide an output of a physical address for an input of a virtual address; a plurality of higher level, than the plurality of
translation lookaside buffers, translation lookaside buffers comprising a higher level translation lookaside buffer in each of the plurality of cache memory banks to
provide an output of a physical address for an input of a virtual address; and a means to perform a first page walk in the plurality of cache memory banks for a miss of an
input of a virtual address into a first translation lookaside buffer and into a first higher level translation lookaside buffer to determine a physical address mapped to the
virtual address, store a mapping of the virtual address to the physical address from the first page walk in the first higher level translation lookaside buffer to cause the
first higher level translation lookaside buffer to send the physical address to the first translation lookaside buffer in a first request address file circuit.
4] In another embodiment, an apparatus comprises a data storage device that stores code that when executed by a hardware processor causes the hardware processor to
perform any method disclosed herein. An apparatus may be as described in the detailed description. A method may be as described in the detailed description.
5] In yet another embodiment, a non-transitory machine readable medium that stores code that when executed by a machine causes the machine to perform a method
comprising any method disclosed herein.
6] An instruction set (e.g., for execution by a core) may include one or more instruction formats. A given instruction format may define various fields (e.g., number of bits,
location of bits) to specify, among other things, the operation to be performed (e.g., opcode) and the operand(s) on which that operation is to be performed and/or other
data field(s) (e.g., mask). Some instruction formats are further broken down though the definition of instruction templates (or subformats). For example, the instruction
templates of a given instruction format may be defined to have different subsets of the instruction format's fields (the included fields are typically in the same order, but
at least some have different bit positions because there are less fields included) and/or defined to have a given field interpreted differently. Thus, each instruction of an
ISA is expressed using a given instruction format (and, if defined, in a given one of the instruction templates of that instruction format) and includes fields for specifying
the operation and the operands. For example, an exemplary ADD instruction has a specific opcode and an instruction format that includes an opcode field to specify that
opcode and operand fields to select operands (source1/destination and source2); and an occurrence of this ADD instruction in an instruction stream will have specific
contents in the operand fields that select specific operands. A set of SIMD extensions referred to as the Advanced Vector Extensions (AVX) (AVX1 and AVX2) and using
the Vector Extensions (VEX) coding scheme has been released and/or published (e.g., see Intel® 64 and IA-32 Architectures Software Developer's Manual, June 2016;
and see Intel® Architecture Instruction Set Extensions Programming Reference, February 2016).
Exemplary Instruction Formats
7] Embodiments of the instruction(s) described herein may be embodied in different formats. Additionally, exemplary systems, architectures, and pipelines are detailed
below. Embodiments of the instruction(s) may be executed on such systems, architectures, and pipelines, but are not limited to those detailed.
Generic Vector Friendly Instruction Format
8] A vector friendly instruction format is an instruction format that is suited for vector instructions (e.g., there are certain fields specific to vector operations). While
embodiments are described in which both vector and scalar operations are supported through the vector friendly instruction format, alternative embodiments use only
vector operations the vector friendly instruction format.
9] Figures 74A-74B are block diagrams illustrating a generic vector friendly instruction format and instruction templates thereof according to embodiments of the
disclosure. Figure 74A is a block diagram illustrating a generic vector friendly instruction format and class A instruction templates thereof according to embodiments of
the disclosure; while Figure 74B is a block diagram illustrating the generic vector friendly instruction format and class B instruction templates thereof according to
embodiments of the disclosure. Specifically, a generic vector friendly instruction format 7400 for which are defined class A and class B instruction templates, both of
which include no memory access 7405 instruction templates and memory access 7420 instruction templates. The term generic in the context of the vector friendly
instruction format refers to the instruction format not being tied to any specific instruction set.
0] While embodiments of the disclosure will be described in which the vector friendly instruction format supports the following: a 64 byte vector operand length (or size)
with 32 bit (4 byte) or 64 bit (8 byte) data element widths (or sizes) (and thus, a 64 byte vector consists of either 16 doubleword-size elements or alternatively, 8
quadword-size elements); a 64 byte vector operand length (or size) with 16 bit (2 byte) or 8 bit (1 byte) data element widths (or sizes); a 32 byte vector operand length (or
size) with 32 bit (4 byte), 64 bit (8 byte), 16 bit (2 byte), or 8 bit (1 byte) data element widths (or sizes); and a 16 byte vector operand length (or size) with 32 bit (4 byte), 64
bit (8 byte), 16 bit (2 byte), or 8 bit (1 byte) data element widths (or sizes); alternative embodiments may support more, less and/or different vector operand sizes (e.g.,
256 byte vector operands) with more, less, or different data element widths (e.g., 128 bit (16 byte) data element widths).

https://patents.google.com/patent/EP3726389B1/en?q=(high+radix)&oq=high+radix&page=1

49/76

2/14/26, 2:00 PM

EP3726389B1 - Apparatuses, methods, and systems for memory interface circuit allocation in a configurable spatial accelera…

1] The class A instruction templates in Figure 74A include: 1) within the no memory access 7405 instruction templates there is shown a no memory access, full round
control type operation 7410 instruction template and a no memory access, data transform type operation 7415 instruction template; and 2) within the memory access
7420 instruction templates there is shown a memory access, temporal 7425 instruction template and a memory access, non-temporal 7430 instruction template. The
class B instruction templates in Figure 74B include: 1) within the no memory access 7405 instruction templates there is shown a no memory access, write mask control,
partial round control type operation 7412 instruction template and a no memory access, write mask control, vsize type operation 7417 instruction template; and 2) within
the memory access 7420 instruction templates there is shown a memory access, write mask control 7427 instruction template.
2] The generic vector friendly instruction format 7400 includes the following fields listed below in the order illustrated in Figures 74A-74B .
3] Format field 7440 - a specific value (an instruction format identifier value) in this field uniquely identifies the vector friendly instruction format, and thus occurrences of
instructions in the vector friendly instruction format in instruction streams. As such, this field is optional in the sense that it is not needed for an instruction set that has
only the generic vector friendly instruction format.
4] Base operation field 7442 - its content distinguishes different base operations.
5] Register index field 7444 - its content, directly or through address generation, specifies the locations of the source and destination operands, be they in registers or in
memory. These include a sufficient number of bits to select N registers from a PxQ (e.g. 32x512, 16x128, 32x1024, 64x1024) register file. While in one embodiment N
may be up to three sources and one destination register, alternative embodiments may support more or less sources and destination registers (e.g., may support up to
two sources where one of these sources also acts as the destination, may support up to three sources where one of these sources also acts as the destination, may
support up to two sources and one destination).
6] Modifier field 7446 - its content distinguishes occurrences of instructions in the generic vector instruction format that specify memory access from those that do not;
that is, between no memory access 7405 instruction templates and memory access 7420 instruction templates. Memory access operations read and/or write to the
memory hierarchy (in some cases specifying the source and/or destination addresses using values in registers), while non-memory access operations do not (e.g., the
source and destinations are registers). While in one embodiment this field also selects between three different ways to perform memory address calculations, alternative
embodiments may support more, less, or different ways to perform memory address calculations.
7] Augmentation operation field 7450 - its content distinguishes which one of a variety of different operations to be performed in addition to the base operation. This field is
context specific. In one embodiment of the disclosure, this field is divided into a class field 7468, an alpha field 7452, and a beta field 7454. The augmentation operation
field 7450 allows common groups of operations to be performed in a single instruction rather than 2, 3, or 4 instructions.
8] Scale field 7460 - its content allows for the scaling of the index field's content for memory address generation (e.g., for address generation that uses 2scale ∗ index +
base).
9] Displacement Field 7462A- its content is used as part of memory address generation (e.g., for address generation that uses 2scale ∗ index + base + displacement).
0] Displacement Factor Field 7462B (note that the juxtaposition of displacement field 7462A directly over displacement factor field 7462B indicates one or the other is
used) - its content is used as part of address generation; it specifies a displacement factor that is to be scaled by the size of a memory access (N) - where N is the
number of bytes in the memory access (e.g., for address generation that uses 2scale ∗ index + base + scaled displacement). Redundant low-order bits are ignored and
hence, the displacement factor field's content is multiplied by the memory operands total size (N) in order to generate the final displacement to be used in calculating an
effective address. The value of N is determined by the processor hardware at runtime based on the full opcode field 7474 (described later herein) and the data
manipulation field 7454C. The displacement field 7462A and the displacement factor field 7462B are optional in the sense that they are not used for the no memory
access 7405 instruction templates and/or different embodiments may implement only one or none of the two.
1] Data element width field 7464 - its content distinguishes which one of a number of data element widths is to be used (in some embodiments for all instructions; in other
embodiments for only some of the instructions). This field is optional in the sense that it is not needed if only one data element width is supported and/or data element
widths are supported using some aspect of the opcodes.
2] Write mask field 7470 - its content controls, on a per data element position basis, whether that data element position in the destination vector operand reflects the result
of the base operation and augmentation operation. Class A instruction templates support mergingwritemasking, while class B instruction templates support both
merging- and zeroingwritemasking. When merging, vector masks allow any set of elements in the destination to be protected from updates during the execution of any
operation (specified by the base operation and the augmentation operation); in other one embodiment, preserving the old value of each element of the destination where
the corresponding mask bit has a 0. In contrast, when zeroing vector masks allow any set of elements in the destination to be zeroed during the execution of any
operation (specified by the base operation and the augmentation operation); in one embodiment, an element of the destination is set to 0 when the corresponding mask
bit has a 0 value. A subset of this functionality is the ability to control the vector length of the operation being performed (that is, the span of elements being modified,
from the first to the last one); however, it is not necessary that the elements that are modified be consecutive. Thus, the write mask field 7470 allows for partial vector
operations, including loads, stores, arithmetic, logical, etc. While embodiments of the disclosure are described in which the write mask field's 7470 content selects one of
a number of write mask registers that contains the write mask to be used (and thus the write mask field's 7470 content indirectly identifies that masking to be
performed), alternative embodiments instead or additional allow the mask write field's 7470 content to directly specify the masking to be performed.
3] Immediate field 7472 - its content allows for the specification of an immediate. This field is optional in the sense that is it not present in an implementation of the generic
vector friendly format that does not support immediate and it is not present in instructions that do not use an immediate.
4] Class field 7468 - its content distinguishes between different classes of instructions. With reference to Figures 74A-B, the contents of this field select between class A
and class B instructions. In Figures 74A-B, rounded corner squares are used to indicate a specific value is present in a field (e.g., class A 7468A and class B 7468B for
the class field 7468 respectively in Figures 74A-B ).
Instruction Templates of Class A
5] In the case of the non-memory access 7405 instruction templates of class A, the alpha field 7452 is interpreted as an RS field 7452A, whose content distinguishes which
one of the different augmentation operation types are to be performed (e.g., round 7452A.1 and data transform 7452A.2 are respectively specified for the no memory
access, round type operation 7410 and the no memory access, data transform type operation 7415 instruction templates), while the beta field 7454 distinguishes which
of the operations of the specified type is to be performed. In the no memory access 7405 instruction templates, the scale field 7460, the displacement field 7462A, and
the displacement scale filed 7462B are not present.
No-Memory Access Instruction Templates - Full Round Control Type Operation
6] In the no memory access full round control type operation 7410 instruction template, the beta field 7454 is interpreted as a round control field 7454A, whose content(s)
provide static rounding. While in the described embodiments of the disclosure the round control field 7454A includes a suppress all floating point exceptions (SAE) field
7456 and a round operation control field 7458, alternative embodiments may support may encode both these concepts into the same field or only have one or the other
of these concepts/fields (e.g., may have only the round operation control field 7458).
7] SAE field 7456 - its content distinguishes whether or not to disable the exception event reporting; when the SAE field's 7456 content indicates suppression is enabled, a
given instruction does not report any kind of floating-point exception flag and does not raise any floating point exception handler.
8] Round operation control field 7458 - its content distinguishes which one of a group of rounding operations to perform (e.g., Round-up, Round-down, Round-towards-zero
and Round-to-nearest). Thus, the round operation control field 7458 allows for the changing of the rounding mode on a per instruction basis. In one embodiment of the
disclosure where a processor includes a control register for specifying rounding modes, the round operation control field's 7450 content overrides that register value.
No Memory Access Instruction Templates - Data Transform Type Operation
9] In the no memory access data transform type operation 7415 instruction template, the beta field 7454 is interpreted as a data transform field 7454B, whose content
distinguishes which one of a number of data transforms is to be performed (e.g., no data transform, swizzle, broadcast).

https://patents.google.com/patent/EP3726389B1/en?q=(high+radix)&oq=high+radix&page=1

50/76

2/14/26, 2:00 PM

EP3726389B1 - Apparatuses, methods, and systems for memory interface circuit allocation in a configurable spatial accelera…

40] In the case of a memory access 7420 instruction template of class A, the alpha field 7452 is interpreted as an eviction hint field 7452B, whose content distinguishes
which one of the eviction hints is to be used (in Figure 74A , temporal 7452B. 1 and non-temporal 7452B.2 are respectively specified for the memory access, temporal
7425 instruction template and the memory access, non-temporal 7430 instruction template), while the beta field 7454 is interpreted as a data manipulation field 7454C,
whose content distinguishes which one of a number of data manipulation operations (also known as primitives) is to be performed (e.g., no manipulation; broadcast; up
conversion of a source; and down conversion of a destination). The memory access 7420 instruction templates include the scale field 7460, and optionally the
displacement field 7462A or the displacement scale field 7462B.
41] Vector memory instructions perform vector loads from and vector stores to memory, with conversion support. As with regular vector instructions, vector memory
instructions transfer data from/to memory in a data element-wise fashion, with the elements that are actually transferred is dictated by the contents of the vector mask
that is selected as the write mask.
Memory Access Instruction Templates - Temporal
42] Temporal data is data likely to be reused soon enough to benefit from caching. This is, however, a hint, and different processors may implement it in different ways,
including ignoring the hint entirely.
Memory Access Instruction Templates - Non-Temporal
43] Non-temporal data is data unlikely to be reused soon enough to benefit from caching in the 1st-level cache and should be given priority for eviction. This is, however, a
hint, and different processors may implement it in different ways, including ignoring the hint entirely.
Instruction Templates of Class B
44] In the case of the instruction templates of class B, the alpha field 7452 is interpreted as a write mask control (Z) field 7452C, whose content distinguishes whether the
write masking controlled by the write mask field 7470 should be a merging or a zeroing.
45] In the case of the non-memory access 7405 instruction templates of class B, part of the beta field 7454 is interpreted as an RL field 7457A, whose content distinguishes
which one of the different augmentation operation types are to be performed (e.g., round 7457A. 1 and vector length (VSIZE) 7457A.2 are respectively specified for the no
memory access, write mask control, partial round control type operation 7412 instruction template and the no memory access, write mask control, VSIZE type operation
7417 instruction template), while the rest of the beta field 7454 distinguishes which of the operations of the specified type is to be performed. In the no memory access
7405 instruction templates, the scale field 7460, the displacement field 7462A, and the displacement scale filed 7462B are not present.
46] In the no memory access, write mask control, partial round control type operation 7410 instruction template, the rest of the beta field 7454 is interpreted as a round
operation field 7459A and exception event reporting is disabled (a given instruction does not report any kind of floating-point exception flag and does not raise any
floating point exception handler).
47] Round operation control field 7459A ― just as round operation control field 7458, its content distinguishes which one of a group of rounding operations to perform (e.g.,
Round-up, Round-down, Round-towards-zero and Round-to-nearest). Thus, the round operation control field 7459A allows for the changing of the rounding mode on a per
instruction basis. In one embodiment of the disclosure where a processor includes a control register for specifying rounding modes, the round operation control field's
7450 content overrides that register value.
48] In the no memory access, write mask control, VSIZE type operation 7417 instruction template, the rest of the beta field 7454 is interpreted as a vector length field 7459B,
whose content distinguishes which one of a number of data vector lengths is to be performed on (e.g., 128, 256, or 512 byte).
49] In the case of a memory access 7420 instruction template of class B, part of the beta field 7454 is interpreted as a broadcast field 7457B, whose content distinguishes
whether or not the broadcast type data manipulation operation is to be performed, while the rest of the beta field 7454 is interpreted the vector length field 7459B. The
memory access 7420 instruction templates include the scale field 7460, and optionally the displacement field 7462A or the displacement scale field 7462B.
0] With regard to the generic vector friendly instruction format 7400, a full opcode field 7474 is shown including the format field 7440, the base operation field 7442, and the
data element width field 7464. While one embodiment is shown where the full opcode field 7474 includes all of these fields, the full opcode field 7474 includes less than
all of these fields in embodiments that do not support all of them. The full opcode field 7474 provides the operation code (opcode).
1] The augmentation operation field 7450, the data element width field 7464, and the write mask field 7470 allow these features to be specified on a per instruction basis in
the generic vector friendly instruction format.
2] The combination of write mask field and data element width field create typed instructions in that they allow the mask to be applied based on different data element
widths.
3] The various instruction templates found within class A and class B are beneficial in different situations. In some embodiments of the disclosure, different processors or
different cores within a processor may support only class A, only class B, or both classes. For instance, a high performance general purpose out-of-order core intended
for general-purpose computing may support only class B, a core intended primarily for graphics and/or scientific (throughput) computing may support only class A, and a
core intended for both may support both (of course, a core that has some mix of templates and instructions from both classes but not all templates and instructions
from both classes is within the purview of the disclosure). Also, a single processor may include multiple cores, all of which support the same class or in which different
cores support different class. For instance, in a processor with separate graphics and general purpose cores, one of the graphics cores intended primarily for graphics
and/or scientific computing may support only class A, while one or more of the general purpose cores may be high performance general purpose cores with out of order
execution and register renaming intended for general-purpose computing that support only class B. Another processor that does not have a separate graphics core, may
include one more general purpose in-order or out-of-order cores that support both class A and class B. Of course, features from one class may also be implement in the
other class in different embodiments of the disclosure. Programs written in a high level language would be put (e.g., just in time compiled or statically compiled) into an
variety of different executable forms, including: 1) a form having only instructions of the class(es) supported by the target processor for execution; or 2) a form having
alternative routines written using different combinations of the instructions of all classes and having control flow code that selects the routines to execute based on the
instructions supported by the processor which is currently executing the code.
Exemplary Specific Vector Friendly Instruction Format
4] Figure 75 is a block diagram illustrating an exemplary specific vector friendly instruction format according to embodiments of the disclosure. Figure 75 shows a specific
vector friendly instruction format 7500 that is specific in the sense that it specifies the location, size, interpretation, and order of the fields, as well as values for some of
those fields. The specific vector friendly instruction format 7500 may be used to extend the x86 instruction set, and thus some of the fields are similar or the same as
those used in the existing x86 instruction set and extension thereof (e.g., AVX). This format remains consistent with the prefix encoding field, real opcode byte field, MOD
R/M field, SIB field, displacement field, and immediate fields of the existing x86 instruction set with extensions. The fields from Figure 74 into which the fields from
Figure 75 map are illustrated.
5] It should be understood that, although embodiments of the disclosure are described with reference to the specific vector friendly instruction format 7500 in the context
of the generic vector friendly instruction format 7400 for illustrative purposes, the disclosure is not limited to the specific vector friendly instruction format 7500 except
where claimed. For example, the generic vector friendly instruction format 7400 contemplates a variety of possible sizes for the various fields, while the specific vector
friendly instruction format 7500 is shown as having fields of specific sizes. By way of specific example, while the data element width field 7464 is illustrated as a one bit
field in the specific vector friendly instruction format 7500, the disclosure is not so limited (that is, the generic vector friendly instruction format 7400 contemplates other
sizes of the data element width field 7464).
6] The generic vector friendly instruction format 7400 includes the following fields listed below in the order illustrated in Figure 75A .
7] EVEX Prefix (Bytes 0-3) 7502 - is encoded in a four-byte form.
8] Format Field 7440 (EVEX Byte 0, bits [7:0]) - the first byte (EVEX Byte 0) is the format field 7440 and it contains Ox62 (the unique value used for distinguishing the vector
friendly instruction format in one embodiment of the disclosure).
9] The second-fourth bytes (EVEX Bytes 1-3) include a number of bit fields providing specific capability.

https://patents.google.com/patent/EP3726389B1/en?q=(high+radix)&oq=high+radix&page=1

51/76

2/14/26, 2:00 PM

EP3726389B1 - Apparatuses, methods, and systems for memory interface circuit allocation in a configurable spatial accelera…

0] REX field 7505 (EVEX Byte 1, bits [7-5]) - consists of a EVEX.R bit field (EVEX Byte 1, bit [7] - R), EVEX.X bit field (EVEX byte 1, bit [6] - X), and 7457BEX byte 1, bit[5] - B).
The EVEX.R, EVEX.X, and EVEX.B bit fields provide the same functionality as the corresponding VEX bit fields, and are encoded using Is complement form, i.e. ZMMO is
encoded as 1311B, ZMM15 is encoded as OOOOB. Other fields of the instructions encode the lower three bits of the register indexes as is known in the art (rrr, xxx, and
bbb), so that Rrrr, Xxxx, and Bbbb may be formed by adding EVEX.R, EVEX.X, and EVEX.B.
1] REX' field 7410 - this is the first part of the REX' field 7410 and is the EVEX.R' bit field (EVEX Byte 1, bit [4] - R') that is used to encode either the upper 16 or lower 16 of the
extended 32 register set. In one embodiment of the disclosure, this bit, along with others as indicated below, is stored in bit inverted format to distinguish (in the wellknown x86 32-bit mode) from the BOUND instruction, whose real opcode byte is 62, but does not accept in the MOD R/M field (described below) the value of 11 in the
MOD field; alternative embodiments of the disclosure do not store this and the other indicated bits below in the inverted format. A value of 1 is used to encode the lower
16 registers. In other words, R'Rrrr is formed by combining EVEX.R', EVEX.R, and the other RRR from other fields.
2] Opcode map field 7515 (EVEX byte 1, bits [3:0] - mmmm) - its content encodes an implied leading opcode byte (OF, OF 38, or OF 3).
3] Data element width field 7464 (EVEX byte 2, bit [7] - W) - is represented by the notation EVEX.W. EVEX.W is used to define the granularity (size) of the datatype (either 32bit data elements or 64-bit data elements).
4] EVEX.vvvv 7520 (EVEX Byte 2, bits [6:3]-vvvv)- the role of EVEX.ww may include the following: 1) EVEX.vvvv encodes the first source register operand, specified in
inverted (Is complement) form and is valid for instructions with 2 or more source operands; 2) EVEX.vvvv encodes the destination register operand, specified in Is
complement form for certain vector shifts; or 3) EVEX.vvvv does not encode any operand, the field is reserved and should contain 1311b. Thus, EVEX.vwv field 7520
encodes the 4 low-order bits of the first source register specifier stored in inverted (1s complement) form. Depending on the instruction, an extra different EVEX bit field
is used to extend the specifier size to 32 registers.
5] EVEX.U 7468 Class field (EVEX byte 2, bit [2]-U) - If EVEX.U = 0, it indicates class A or EVEX.U0; if EVEX.U = 1, it indicates class B or EVEX.U1.
6] Prefix encoding field 7525 (EVEX byte 2, bits [1:0]-pp) - provides additional bits for the base operation field. In addition to providing support for the legacy SSE instructions
in the EVEX prefix format, this also has the benefit of compacting the SIMD prefix (rather than requiring a byte to express the SIMD prefix, the EVEX prefix requires only 2
bits). In one embodiment, to support legacy SSE instructions that use a SIMD prefix (66H, F2H, F3H) in both the legacy format and in the EVEX prefix format, these legacy
SIMD prefixes are encoded into the SIMD prefix encoding field; and at runtime are expanded into the legacy SIMD prefix prior to being provided to the decoder's PLA (so
the PLA can execute both the legacy and EVEX format of these legacy instructions without modification). Although newer instructions could use the EVEX prefix
encoding field's content directly as an opcode extension, certain embodiments expand in a similar fashion for consistency but allow for different meanings to be
specified by these legacy SIMD prefixes. An alternative embodiment may redesign the PLA to support the 2 bit SIMD prefix encodings, and thus not require the
expansion.
7] Alpha field 7452 (EVEX byte 3, bit [7] - EH; also known as EVEX.EH, EVEX.rs, EVEX.RL, EVEX.write mask control, and EVEX.N; also illustrated with α) ― as previously
described, this field is context specific.
8] Beta field 7454 (EVEX byte 3, bits [6:4]-SSS, also known as EVEX.s2-0, EVEX.r2-0, EVEX.rr1, EVEX.LLO, EVEX.LLB; also illustrated with βββ) ― as previously described, this
field is context specific.
9] REX' field 7410 - this is the remainder of the REX' field and is the EVEX.V' bit field (EVEX Byte 3, bit [3] - V') that may be used to encode either the upper 16 or lower 16 of
the extended 32 register set. This bit is stored in bit inverted format. A value of 1 is used to encode the lower 16 registers. In other words, V'VVVV is formed by combining
EVEX.V', EVEX.vwv.
0] Write mask field 7470 (EVEX byte 3, bits [2:0]-kkk) ― its content specifies the index of a register in the write mask registers as previously described. In one embodiment
of the disclosure, the specific value EVEX.kkk=000 has a special behavior implying no write mask is used for the particular instruction (this may be implemented in a
variety of ways including the use of a write mask hardwired to all ones or hardware that bypasses the masking hardware).
1] Real Opcode Field 7530 (Byte 4) is also known as the opcode byte. Part of the opcode is specified in this field.
2] MOD R/M Field 7540 (Byte 5) includes MOD field 7542, Reg field 7544, and R/M field 7546. As previously described, the MOD field's 7542 content distinguishes between
memory access and non-memory access operations. The role of Reg field 7544 can be summarized to two situations: encoding either the destination register operand or
a source register operand, or be treated as an opcode extension and not used to encode any instruction operand. The role of R/M field 7546 may include the following:
encoding the instruction operand that references a memory address, or encoding either the destination register operand or a source register operand.
3] Scale, Index, Base (SIB) Byte (Byte 6) - As previously described, the scale field's 5450 content is used for memory address generation. SIB.xxx 7554 and SIB.bbb 7556 the contents of these fields have been previously referred to with regard to the register indexes Xxxx and Bbbb.
4] Displacement field 7462A (Bytes 7-10) - when MOD field 7542 contains 10, bytes 7-10 are the displacement field7462A, and it works the same as the legacy 32-bit
displacement (disp32) and works at byte granularity.
5] Displacement factor field7462B (Byte 7) - when MOD field 7542 contains 01, byte 7 is the displacement factor field7462B. The location of this field is that same as that of
the legacy x86 instruction set 8-bit displacement (disp8), which works at byte granularity. Since disp8 is sign extended, it can only address between -128 and 127 bytes
offsets; in terms of 64 byte cache lines, disp8 uses 8 bits that can be set to only four really useful values -128, -64, 0, and 64; since a greater range is often needed, disp32
is used; however, disp32 requires 4 bytes. In contrast to disp8 and disp32, the displacement factor field7462B is a reinterpretation of disp8; when using displacement
factor field7462B, the actual displacement is determined by the content of the displacement factor field multiplied by the size of the memory operand access (N). This
type of displacement is referred to as disp8∗N. This reduces the average instruction length (a single byte of used for the displacement but with a much greater range).
Such compressed displacement is based on the assumption that the effective displacement is multiple of the granularity of the memory access, and hence, the
redundant low-order bits of the address offset do not need to be encoded. In other words, the displacement factor field7462B substitutes the legacy x86 instruction set 8bit displacement. Thus, the displacement factor field7462B is encoded the same way as an x86 instruction set 8-bit displacement (so no changes in the ModRM/SIB
encoding rules) with the only exception that disp8 is overloaded to disp8∗N. In other words, there are no changes in the encoding rules or encoding lengths but only in
the interpretation of the displacement value by hardware (which needs to scale the displacement by the size of the memory operand to obtain a byte-wise address
offset). Immediate field7472 operates as previously described.
Full Opcode Field
6] Figure 75B is a block diagram illustrating the fields of the specific vector friendly instruction format 7500 that make up the full opcode field7474 according to one
embodiment of the disclosure. Specifically, the full opcode field7474 includes the format field 7440, the base operation field 7442, and the data element width (W) field
7464. The base operation field 7442 includes the prefix encoding field 7525, the opcode map field 7515, and the real opcode field 7530.
Register Index Field
7] Figure 75C is a block diagram illustrating the fields of the specific vector friendly instruction format 7500 that make up the register index field 7444 according to one
embodiment of the disclosure. Specifically, the register index field 7444 includes the REX field 7505, the REX' field 7510, the MODR/M.reg field 7544, the MODR/M.r/m
field 7546, the VWV field 7520, xxx field 7554, and the bbb field 7556.
Augmentation Operation Field
8] Figure 75D is a block diagram illustrating the fields of the specific vector friendly instruction format 7500 that make up the augmentation operation field 7450 according
to one embodiment of the disclosure. When the class (U) field 7468 contains 0, it signifies EVEX.U0 (class A 7468A); when it contains 1, it signifies EVEX.U1 (class B
7468B). When U=0 and the MOD field 7542 contains 11 (signifying a no memory access operation), the alpha field 7452 (EVEX byte 3, bit [7] - EH) is interpreted as the rs
field 7452A. When the rs field 7452A contains a 1 (round 7452A.1), the beta field 7454 (EVEX byte 3, bits [6:4]- SSS) is interpreted as the round control field 7454A. The
round control field 7454A includes a one bit SAE field 7456 and a two bit round operation field 7458. When the rs field 7452A contains a 0 (data transform 7452A.2), the
beta field 7454 (EVEX byte 3, bits [6:4]- SSS) is interpreted as a three bit data transform field 7454B. When U=0 and the MOD field 7542 contains 00, 01, or 10 (signifying a

https://patents.google.com/patent/EP3726389B1/en?q=(high+radix)&oq=high+radix&page=1

52/76

2/14/26, 2:00 PM

EP3726389B1 - Apparatuses, methods, and systems for memory interface circuit allocation in a configurable spatial accelera…

memory access operation), the alpha field 7452 (EVEX byte 3, bit [7] - EH) is interpreted as the eviction hint (EH) field 7452B and the beta field 7454 (EVEX byte 3, bits
[6:4]- SSS) is interpreted as a three bit data manipulation field 7454C.
9] When U=1, the alpha field 7452 (EVEX byte 3, bit [7] - EH) is interpreted as the write mask control (Z) field 7452C. When U=1 and the MOD field 7542 contains 11
(signifying a no memory access operation), part of the beta field 7454 (EVEX byte 3, bit [4]- So) is interpreted as the RL field 7457A; when it contains a 1 (round 7457A.1)
the rest of the beta field 7454 (EVEX byte 3, bit [6-5]- S2-1) is interpreted as the round operation field 7459A, while when the RL field 7457A contains a 0 (VSIZE 7457.A2)
the rest of the beta field 7454 (EVEX byte 3, bit [6-5]- S2-1) is interpreted as the vector length field 7459B (EVEX byte 3, bit [6-5]- L1-0). When U=1 and the MOD field 7542
contains 00, 01, or 10 (signifying a memory access operation), the beta field 7454 (EVEX byte 3, bits [6:4]- SSS) is interpreted as the vector length field 7459B (EVEX byte
3, bit [6-5]- L1-0) and the broadcast field 7457B (EVEX byte 3, bit [4]- B).
Exemplary Register Architecture
0] Figure 76 is a block diagram of a register architecture 7600 according to one embodiment of the disclosure. In the embodiment illustrated, there are 32 vector registers
7610 that are 512 bits wide; these registers are referenced as zmm0 through zmm31. The lower order 256 bits of the lower 16 zmm registers are overlaid on registers
ymmO-16. The lower order 128 bits of the lower 16 zmm registers (the lower order 128 bits of the ymm registers) are overlaid on registers xmm0-15. The specific vector
friendly instruction format 7500 operates on these overlaid register file as illustrated in the below tables.
Adjustable Vector Length

Class

Operations

A (Figure

5410, 7415,

Instruction Templates that do not include the

74A; U=0)

7425, 7430

vector length field 7459B

B (Figure 74B;
U=1)

Instruction templates that do include the vector

B (Figure 74B;

length field 7459B

U=1)

5412

5417, 7427

Registers
zmm registers (the vector length is 64 byte)

zmm registers (the vector length is 64 byte)
zmm, ymm, or xmm registers (the vector length is 64 byte, 32 byte, or 16 byte)
depending on the vector length field 7459B

1] In other words, the vector length field 7459B selects between a maximum length and one or more other shorter lengths, where each such shorter length is half the length
of the preceding length; and instructions templates without the vector length field 7459B operate on the maximum vector length. Further, in one embodiment, the class B
instruction templates of the specific vector friendly instruction format 7500 operate on packed or scalar single/double-precision floating point data and packed or scalar
integer data. Scalar operations are operations performed on the lowest order data element position in an zmm/ymm/xmm register; the higher order data element
positions are either left the same as they were prior to the instruction or zeroed depending on the embodiment.
2] Write mask registers 7615 - in the embodiment illustrated, there are 8 write mask registers (k0 through k7), each 64 bits in size. In an alternate embodiment, the write
mask registers 7615 are 16 bits in size. As previously described, in one embodiment of the disclosure, the vector mask register k0 cannot be used as a write mask; when
the encoding that would normally indicate k0 is used for a write mask, it selects a hardwired write mask of 0xFFFF, effectively disabling write masking for that instruction.
3] General-purpose registers 7625 - in the embodiment illustrated, there are sixteen 64-bit general-purpose registers that are used along with the existing x86 addressing
modes to address memory operands. These registers are referenced by the names RAX, RBX, RCX, RDX, RBP, RSI, RDI, RSP, and R8 through R15.
4] Scalar floating point stack register file (x87 stack) 7645, on which is aliased the MMX packed integer flat register file 7650 - in the embodiment illustrated, the x87 stack
is an eight-element stack used to perform scalar floating-point operations on 32/64/80-bit floating point data using the x87 instruction set extension; while the MMX
registers are used to perform operations on 64-bit packed integer data, as well as to hold operands for some operations performed between the MMX and XMM
registers.
5] Alternative embodiments of the disclosure may use wider or narrower registers. Additionally, alternative embodiments of the disclosure may use more, less, or different
register files and registers.
Exemplary Core Architectures, Processors, and Computer Architectures
6] Processor cores may be implemented in different ways, for different purposes, and in different processors. For instance, implementations of such cores may include: 1) a
general purpose in-order core intended for general-purpose computing; 2) a high performance general purpose out-of-order core intended for general-purpose computing;
3) a special purpose core intended primarily for graphics and/or scientific (throughput) computing. Implementations of different processors may include: 1) a CPU
including one or more general purpose in-order cores intended for general-purpose computing and/or one or more general purpose out-of-order cores intended for
general-purpose computing; and 2) a coprocessor including one or more special purpose cores intended primarily for graphics and/or scientific (throughput). Such
different processors lead to different computer system architectures, which may include: 1) the coprocessor on a separate chip from the CPU; 2) the coprocessor on a
separate die in the same package as a CPU; 3) the coprocessor on the same die as a CPU (in which case, such a coprocessor is sometimes referred to as special
purpose logic, such as integrated graphics and/or scientific (throughput) logic, or as special purpose cores); and 4) a system on a chip that may include on the same die
the described CPU (sometimes referred to as the application core(s) or application processor(s)), the above described coprocessor, and additional functionality.
Exemplary core architectures are described next, followed by descriptions of exemplary processors and computer architectures.
Exemplary Core Architectures
In-order and out-of-order core block diagram
7] Figure 77A is a block diagram illustrating both an exemplary in-order pipeline and an exemplary register renaming, out-of-order issue/execution pipeline according to
embodiments of the disclosure. Figure 77B is a block diagram illustrating both an exemplary embodiment of an in-order architecture core and an exemplary register
renaming, out-of-order issue/execution architecture core to be included in a processor according to embodiments of the disclosure. The solid lined boxes in Figures 77AB illustrate the in-order pipeline and in-order core, while the optional addition of the dashed lined boxes illustrates the register renaming, out-of-order issue/execution
pipeline and core. Given that the in-order aspect is a subset of the out-of-order aspect, the out-of-order aspect will be described.
8] In Figure 77A , a processor pipeline 7700 includes a fetch stage 7702, a length decode stage 7704, a decode stage 7706, an allocation stage 7708, a renaming stage
7710, a scheduling (also known as a dispatch or issue) stage 7712, a register read/memory read stage 7714, an execute stage 7716, a write back/memory write stage
7718, an exception handling stage 7722, and a commit stage 7724.
9] Figure 77B shows processor core 7790 including a front end unit 7730 coupled to an execution engine unit 7750, and both are coupled to a memory unit 7770. The core
7790 may be a reduced instruction set computing (RISC) core, a complex instruction set computing (CISC) core, a very long instruction word (VLIW) core, or a hybrid or
alternative core type. As yet another option, the core 7790 may be a special-purpose core, such as, for example, a network or communication core, compression engine,
coprocessor core, general purpose computing graphics processing unit (GPGPU) core, graphics core, or the like.
0] The front end unit 7730 includes a branch prediction unit 7732 coupled to an instruction cache unit 7734, which is coupled to an instruction translation lookaside buffer
(TLB) 7736, which is coupled to an instruction fetch unit 7738, which is coupled to a decode unit 7740. The decode unit 7740 (or decoder or decoder unit) may decode
instructions (e.g., macro-instructions), and generate as an output one or more micro-operations, micro-code entry points, micro-instructions, other instructions, or other
control signals, which are decoded from, or which otherwise reflect, or are derived from, the original instructions. The decode unit 7740 may be implemented using
various different mechanisms. Examples of suitable mechanisms include, but are not limited to, look-up tables, hardware implementations, programmable logic arrays
(PLAs), microcode read only memories (ROMs), etc. In one embodiment, the core 7790 includes a microcode ROM or other medium that stores microcode for certain
macro-instructions (e.g., in decode unit 7740 or otherwise within the front end unit 7730). The decode unit 7740 is coupled to a rename/allocator unit 7752 in the
execution engine unit 7750.
1] The execution engine unit 7750 includes the rename/allocator unit 7752 coupled to a retirement unit 7754 and a set of one or more scheduler unit(s) 7756. The scheduler
unit(s) 7756 represents any number of different schedulers, including reservations stations, central instruction window, etc. The scheduler unit(s) 7756 is coupled to the

https://patents.google.com/patent/EP3726389B1/en?q=(high+radix)&oq=high+radix&page=1

53/76

2/14/26, 2:00 PM

EP3726389B1 - Apparatuses, methods, and systems for memory interface circuit allocation in a configurable spatial accelera…

physical register file(s) unit(s) 7758. Each of the physical register file(s) units 7758 represents one or more physical register files, different ones of which store one or
more different data types, such as scalar integer, scalar floating point, packed integer, packed floating point, vector integer, vector floating point,, status (e.g., an
instruction pointer that is the address of the next instruction to be executed), etc. In one embodiment, the physical register file(s) unit 7758 comprises a vector registers
unit, a write mask registers unit, and a scalar registers unit. These register units may provide architectural vector registers, vector mask registers, and general purpose
registers. The physical register file(s) unit(s) 7758 is overlapped by the retirement unit 7754 to illustrate various ways in which register renaming and out-of-order
execution may be implemented (e.g., using a reorder buffer(s) and a retirement register file(s); using a future file(s), a history buffer(s), and a retirement register file(s);
using a register maps and a pool of registers; etc.). The retirement unit 7754 and the physical register file(s) unit(s) 7758 are coupled to the execution cluster(s) 7760.
The execution cluster(s) 7760 includes a set of one or more execution units 7762 and a set of one or more memory access units 7764. The execution units 7762 may
perform various operations (e.g., shifts, addition, subtraction, multiplication) and on various types of data (e.g., scalar floating point, packed integer, packed floating point,
vector integer, vector floating point). While some embodiments may include a number of execution units dedicated to specific functions or sets of functions, other
embodiments may include only one execution unit or multiple execution units that all perform all functions. The scheduler unit(s) 7756, physical register file(s) unit(s)
7758, and execution cluster(s) 7760 are shown as being possibly plural because certain embodiments create separate pipelines for certain types of data/operations
(e.g., a scalar integer pipeline, a scalar floating point/packed integer/packed floating point/vector integer/vector floating point pipeline, and/or a memory access pipeline
that each have their own scheduler unit, physical register file(s) unit, and/or execution cluster - and in the case of a separate memory access pipeline, certain
embodiments are implemented in which only the execution cluster of this pipeline has the memory access unit(s) 7764). It should also be understood that where
separate pipelines are used, one or more of these pipelines may be out-of-order issue/execution and the rest in-order.
2] The set of memory access units 7764 is coupled to the memory unit 7770, which includes a data TLB unit 7772 coupled to a data cache unit 7774 coupled to a level 2
(L2) cache unit 7776. In one exemplary embodiment, the memory access units 7764 may include a load unit, a store address unit, and a store data unit, each of which is
coupled to the data TLB unit 7772 in the memory unit 7770. The instruction cache unit 7734 is further coupled to a level 2 (L2) cache unit 7776 in the memory unit 7770.
The L2 cache unit 7776 is coupled to one or more other levels of cache and eventually to a main memory.
3] By way of example, the exemplary register renaming, out-of-order issue/execution core architecture may implement the pipeline 7700 as follows: 1) the instruction fetch
7738 performs the fetch and length decoding stages 7702 and 7704; 2) the decode unit 7740 performs the decode stage 7706; 3) the rename/allocator unit 7752
performs the allocation stage 7708 and renaming stage 7710; 4) the scheduler unit(s) 7756 performs the schedule stage 7712; 5) the physical register file(s) unit(s) 7758
and the memory unit 7770 perform the register read/memory read stage 7714; the execution cluster 7760 perform the execute stage 7716; 6) the memory unit 7770 and
the physical register file(s) unit(s) 7758 perform the write back/memory write stage 7718; 7) various units may be involved in the exception handling stage 7722; and 8)
the retirement unit 7754 and the physical register file(s) unit(s) 7758 perform the commit stage 7724.
4] The core 7790 may support one or more instructions sets (e.g., the x86 instruction set (with some extensions that have been added with newer versions); the MIPS
instruction set of MIPS Technologies of Sunnyvale, CA; the ARM instruction set (with optional additional extensions such as NEON) of ARM Holdings of Sunnyvale, CA),
including the instruction(s) described herein. In one embodiment, the core 7790 includes logic to support a packed data instruction set extension (e.g., AVX1, AVX2),
thereby allowing the operations used by many multimedia applications to be performed using packed data.
5] It should be understood that the core may support multithreading (executing two or more parallel sets of operations or threads), and may do so in a variety of ways
including time sliced multithreading, simultaneous multithreading (where a single physical core provides a logical core for each of the threads that physical core is
simultaneously multithreading), or a combination thereof (e.g., time sliced fetching and decoding and simultaneous multithreading thereafter such as in the Intel®
Hyperthreading technology).
6] While register renaming is described in the context of out-of-order execution, it should be understood that register renaming may be used in an in-order architecture.
While the illustrated embodiment of the processor also includes separate instruction and data cache units 7734/ 7774 and a shared L2 cache unit 7776, alternative
embodiments may have a single internal cache for both instructions and data, such as, for example, a Level 1 (L1) internal cache, or multiple levels of internal cache. In
some embodiments, the system may include a combination of an internal cache and an external cache that is external to the core and/or the processor. Alternatively, all
of the cache may be external to the core and/or the processor.
Specific Exemplary In-Order Core Architecture
7] Figures 78A-B illustrate a block diagram of a more specific exemplary in-order core architecture, which core would be one of several logic blocks (including other cores
of the same type and/or different types) in a chip. The logic blocks communicate through a high-bandwidth interconnect network (e.g., a ring network) with some fixed
function logic, memory I/O interfaces, and other necessary I/O logic, depending on the application.
8] Figure 78A is a block diagram of a single processor core, along with its connection to the on-die interconnect network 7802 and with its local subset of the Level 2 (L2)
cache 7804, according to embodiments of the disclosure. In one embodiment, an instruction decode unit 7800 supports the x86 instruction set with a packed data
instruction set extension. An L1 cache 7806 allows low-latency accesses to cache memory into the scalar and vector units. While in one embodiment (to simplify the
design), a scalar unit 7808 and a vector unit 7810 use separate register sets (respectively, scalar registers 7812 and vector registers 7814) and data transferred between
them is written to memory and then read back in from a level 1 (L1) cache 7806, alternative embodiments of the disclosure may use a different approach (e.g., use a
single register set or include a communication path that allow data to be transferred between the two register files without being written and read back).
9] The local subset of the L2 cache 7804 is part of a global L2 cache that is divided into separate local subsets, one per processor core. Each processor core has a direct
access path to its own local subset of the L2 cache 7804. Data read by a processor core is stored in its L2 cache subset 7804 and can be accessed quickly, in parallel
with other processor cores accessing their own local L2 cache subsets. Data written by a processor core is stored in its own L2 cache subset 7804 and is flushed from
other subsets, if necessary. The ring network ensures coherency for shared data. The ring network is bi-directional to allow agents such as processor cores, hf caches
and other logic blocks to communicate with each other within the chip. Each ring data-path is 1012-bits wide per direction.
0] Figure 78B is an expanded view of part of the processor core in Figure 78A according to embodiments of the disclosure. Figure 78B includes an L1 data cache 7806A
part of the L1 cache 7804, as well as more detail regarding the vector unit 7810 and the vector registers 7814. Specifically, the vector unit 7810 is a 16-wide vector
processing unit (VPU) (see the 16-wide ALU 7828), which executes one or more of integer, single-precision float, and double-precision float instructions. The VPU
supports swizzling the register inputs with swizzle unit 7820, numeric conversion with numeric convert units 7822A-B, and replication with replication unit 7824 on the
memory input. Write mask registers 7826 allow predicating resulting vector writes.
1] Figure 79 is a block diagram of a processor 7900 that may have more than one core, may have an integrated memory controller, and may have integrated graphics
according to embodiments of the disclosure. The solid lined boxes in Figure 79 illustrate a processor 7900 with a single core 7902A, a system agent 7910, a set of one or
more bus controller units 7916, while the optional addition of the dashed lined boxes illustrates an alternative processor 7900 with multiple cores 7902A-N, a set of one
or more integrated memory controller unit(s) 7914 in the system agent unit 7910, and special purpose logic 7908.
2] Thus, different implementations of the processor 7900 may include: 1) a CPU with the special purpose logic 7908 being integrated graphics and/or scientific (throughput)
logic (which may include one or more cores), and the cores 7902A-N being one or more general purpose cores (e.g., general purpose in-order cores, general purpose outof-order cores, a combination of the two); 2) a coprocessor with the cores 7902A-N being a large number of special purpose cores intended primarily for graphics and/or
scientific (throughput); and 3) a coprocessor with the cores 7902A-N being a large number of general purpose in-order cores. Thus, the processor 7900 may be a generalpurpose processor, coprocessor or special-purpose processor, such as, for example, a network or communication processor, compression engine, graphics processor,
GPGPU (general purpose graphics processing unit), a high-throughput many integrated core (MIC) coprocessor (including 30 or more cores), embedded processor, or the
like. The processor may be implemented on one or more chips. The processor 7900 may be a part of and/or may be implemented on one or more substrates using any of
a number of process technologies, such as, for example, BiCMOS, CMOS, or NMOS.
3] The memory hierarchy includes one or more levels of cache within the cores, a set or one or more shared cache units 7906, and external memory (not shown) coupled to
the set of integrated memory controller units 7914. The set of shared cache units 7906 may include one or more mid-level caches, such as level 2 (L2), level 3 (L3), level 4

https://patents.google.com/patent/EP3726389B1/en?q=(high+radix)&oq=high+radix&page=1

54/76

2/14/26, 2:00 PM

EP3726389B1 - Apparatuses, methods, and systems for memory interface circuit allocation in a configurable spatial accelera…

(L4), or other levels of cache, a last level cache (LLC), and/or combinations thereof. While in one embodiment a ring based interconnect unit 7912 interconnects the
integrated graphics logic 7908, the set of shared cache units 7906, and the system agent unit 7910/integrated memory controller unit(s) 7914, alternative embodiments
may use any number of well-known techniques for interconnecting such units. In one embodiment, coherency is maintained between one or more cache units 7906 and
cores 7902-A-N.
4] In some embodiments, one or more of the cores 7902A-N are capable of multithreading. The system agent 7910 includes those components coordinating and operating
cores 7902A-N. The system agent unit 7910 may include for example a power control unit (PCU) and a display unit. The PCU may be or include logic and components
needed for regulating the power state of the cores 7902A-N and the integrated graphics logic 7908. The display unit is for driving one or more externally connected
displays.
5] The cores 7902A-N may be homogenous or heterogeneous in terms of architecture instruction set; that is, two or more of the cores 7902A-N may be capable of
execution the same instruction set, while others may be capable of executing only a subset of that instruction set or a different instruction set.
Exemplary Computer Architectures
6] Figures 80- 83 are block diagrams of exemplary computer architectures. Other system designs and configurations known in the arts for laptops, desktops, handheld PCs,
personal digital assistants, engineering workstations, servers, network devices, network hubs, switches, embedded processors, digital signal processors (DSPs), graphics
devices, video game devices, set-top boxes, micro controllers, cell phones, portable media players, hand held devices, and various other electronic devices, are also
suitable. In general, a huge variety of systems or electronic devices capable of incorporating a processor and/or other execution logic as disclosed herein are generally
suitable.
7] Referring now to Figure 80 , shown is a block diagram of a system 8000 in accordance with one embodiment of the present disclosure. The system 8000 may include
one or more processors 8010, 8015, which are coupled to a controller hub 8020. In one embodiment the controller hub 8020 includes a graphics memory controller hub
(GMCH) 8090 and an Input/Output Hub (IOH) 8050 (which may be on separate chips); the GMCH 8090 includes memory and graphics controllers to which are coupled
memory 8040 and a coprocessor 8045; the IOH 8050 is couples input/output (I/O) devices 8060 to the GMCH 8090. Alternatively, one or both of the memory and
graphics controllers are integrated within the processor (as described herein), the memory 8040 and the coprocessor 8045 are coupled directly to the processor 8010,
and the controller hub 8020 in a single chip with the IOH 8050. Memory 8040 may include a CSA software module 8040A, for example, to store code that when executed
causes a processor to perform any method of this disclosure.
8] The optional nature of additional processors 8015 is denoted in Figure 80 with broken lines. Each processor 8010, 8015 may include one or more of the processing cores
described herein and may be some version of the processor 7900.
9] The memory 8040 may be, for example, dynamic random access memory (DRAM), phase change memory (PCM), or a combination of the two. For at least one
embodiment, the controller hub 8020 communicates with the processor(s) 8010, 8015 via a multi-drop bus, such as a frontside bus (FSB), point-to-point interface such as
QuickPath Interconnect (QPI), or similar connection 8095.
0] In one embodiment, the coprocessor 8045 is a special-purpose processor, such as, for example, a high-throughput MIC processor, a network or communication
processor, compression engine, graphics processor, GPGPU, embedded processor, or the like. In one embodiment, controller hub 8020 may include an integrated
graphics accelerator.
1] There can be a variety of differences between the physical resources 8010, 8015 in terms of a spectrum of metrics of merit including architectural, microarchitectural,
thermal, power consumption characteristics, and the like.
2] In one embodiment, the processor 8010 executes instructions that control data processing operations of a general type. Embedded within the instructions may be
coprocessor instructions. The processor 8010 recognizes these coprocessor instructions as being of a type that should be executed by the attached coprocessor 8045.
Accordingly, the processor 8010 issues these coprocessor instructions (or control signals representing coprocessor instructions) on a coprocessor bus or other
interconnect, to coprocessor 8045. Coprocessor(s) 8045 accept and execute the received coprocessor instructions.
3] Referring now to Figure 81 , shown is a block diagram of a first more specific exemplary system 8100 in accordance with an embodiment of the present disclosure. As
shown in Figure 81 , multiprocessor system 8100 is a point-to-point interconnect system, and includes a first processor 8170 and a second processor 8180 coupled via a
point-to-point interconnect 8150. Each of processors 8170 and 8180 may be some version of the processor 7900. In one embodiment of the disclosure, processors 8170
and 8180 are respectively processors 8010 and 8015, while coprocessor 8138 is coprocessor 8045. In another embodiment, processors 8170 and 8180 are respectively
processor 8010 coprocessor 8045.
4] Processors 8170 and 8180 are shown including integrated memory controller (IMC) units 8172 and 8182, respectively. Processor 8170 also includes as part of its bus
controller units point-to-point (P-P) interfaces 8176 and 8178; similarly, second processor 8180 includes P-P interfaces 8186 and 8188. Processors 8170, 8180 may
exchange information via a point-to-point (P-P) interface 8150 using P-P interface circuits 8178, 8188. As shown in Figure 81 , IMCs 8172 and 8182 couple the
processors to respective memories, namely a memory 8132 and a memory 8134, which may be portions of main memory locally attached to the respective processors.
5] Processors 8170, 8180 may each exchange information with a chipset 8190 via individual P-P interfaces 8152, 8154 using point to point interface circuits 8176, 8194,
8186, 8198. Chipset 8190 may optionally exchange information with the coprocessor 8138 via a high-performance interface 8139. In one embodiment, the coprocessor
8138 is a special-purpose processor, such as, for example, a high-throughput MIC processor, a network or communication processor, compression engine, graphics
processor, GPGPU, embedded processor, or the like.
6] A shared cache (not shown) may be included in either processor or outside of both processors, yet connected with the processors via P-P interconnect, such that either
or both processors' local cache information may be stored in the shared cache if a processor is placed into a low power mode.
7] Chipset 8190 may be coupled to a first bus 8116 via an interface 8196. In one embodiment, first bus 8116 may be a Peripheral Component Interconnect (PCI) bus, or a
bus such as a PCI Express bus or another third generation I/O interconnect bus, although the scope of the present disclosure is not so limited.
8] As shown in Figure 81 , various I/O devices 8114 may be coupled to first bus 8116, along with a bus bridge 8118 which couples first bus 8116 to a second bus 8120. In
one embodiment, one or more additional processor(s) 8115, such as coprocessors, high-throughput MIC processors, GPGPU's, accelerators (such as, e.g., graphics
accelerators or digital signal processing (DSP) units), field programmable gate arrays, or any other processor, are coupled to first bus 8116. In one embodiment, second
bus 8120 may be a low pin count (LPC) bus. Various devices may be coupled to a second bus 8120 including, for example, a keyboard and/or mouse 8122,
communication devices 8127 and a storage unit 8128 such as a disk drive or other mass storage device which may include instructions/code and data 8130, in one
embodiment. Further, an audio I/O 8124 may be coupled to the second bus 8120. Note that other architectures are possible. For example, instead of the point-to-point
architecture of Figure 81 , a system may implement a multi-drop bus or other such architecture.
9] Referring now to Figure 82 , shown is a block diagram of a second more specific exemplary system 8200 in accordance with an embodiment of the present disclosure.
Like elements in Figures 81 and 82 bear like reference numerals, and certain aspects of Figure 81 have been omitted from Figure 82 in order to avoid obscuring other
aspects of Figure 82 .
0] Figure 82 illustrates that the processors 8170, 8180 may include integrated memory and I/O control logic ("CL") 8172 and 8182, respectively. Thus, the CL 8172, 8182
include integrated memory controller units and include I/O control logic. Figure 82 illustrates that not only are the memories 8132, 8134 coupled to the CL 8172, 8182,
but also that I/O devices 8214 are also coupled to the control logic 8172, 8182. Legacy I/O devices 8215 are coupled to the chipset 8190.
1] Referring now to Figure 83 , shown is a block diagram of a SoC 8300 in accordance with an embodiment of the present disclosure. Similar elements in Figure 79 bear like
reference numerals. Also, dashed lined boxes are optional features on more advanced SoCs. In Figure 83 , an interconnect unit(s) 8302 is coupled to: an application
processor 8310 which includes a set of one or more cores 202A-N and shared cache unit(s) 7906; a system agent unit 7910; a bus controller unit(s) 7916; an integrated
memory controller unit(s) 7914; a set or one or more coprocessors 8320 which may include integrated graphics logic, an image processor, an audio processor, and a
video processor; an static random access memory (SRAM) unit 8330; a direct memory access (DMA) unit 8332; and a display unit 8340 for coupling to one or more

https://patents.google.com/patent/EP3726389B1/en?q=(high+radix)&oq=high+radix&page=1

55/76

2/14/26, 2:00 PM

EP3726389B1 - Apparatuses, methods, and systems for memory interface circuit allocation in a configurable spatial accelera…

external displays. In one embodiment, the coprocessor(s) 8320 include a special-purpose processor, such as, for example, a network or communication processor,
compression engine, GPGPU, a high-throughput MIC processor, embedded processor, or the like.
2] Embodiments (e.g., of the mechanisms) disclosed herein may be implemented in hardware, software, firmware, or a combination of such implementation approaches.
Embodiments of the disclosure may be implemented as computer programs or program code executing on programmable systems comprising at least one processor, a
storage system (including volatile and non-volatile memory and/or storage elements), at least one input device, and at least one output device.
3] Program code, such as code 8130 illustrated in Figure 81, may be applied to input instructions to perform the functions described herein and generate output
information. The output information may be applied to one or more output devices, in known fashion. For purposes of this application, a processing system includes any
system that has a processor, such as, for example; a digital signal processor (DSP), a microcontroller, an application specific integrated circuit (ASIC), or a
microprocessor.
4] The program code may be implemented in a high level procedural or object oriented programming language to communicate with a processing system. The program
code may also be implemented in assembly or machine language, if desired. In fact, the mechanisms described herein are not limited in scope to any particular
programming language. In any case, the language may be a compiled or interpreted language.
5] One or more aspects of at least one embodiment may be implemented by representative instructions stored on a machine-readable medium which represents various
logic within the processor, which when read by a machine causes the machine to fabricate logic to perform the techniques described herein. Such representations,
known as "IP cores" may be stored on a tangible, machine readable medium and supplied to various customers or manufacturing facilities to load into the fabrication
machines that actually make the logic or processor.
6] Such machine-readable storage media may include, without limitation, non-transitory, tangible arrangements of articles manufactured or formed by a machine or device,
including storage media such as hard disks, any other type of disk including floppy disks, optical disks, compact disk read-only memories (CD-ROMs), compact disk
rewritable's (CD-RWs), and magneto-optical disks, semiconductor devices such as read-only memories (ROMs), random access memories (RAMs) such as dynamic
random access memories (DRAMs), static random access memories (SRAMs), erasable programmable read-only memories (EPROMs), flash memories, electrically
erasable programmable read-only memories (EEPROMs), phase change memory (PCM), magnetic or optical cards, or any other type of media suitable for storing
electronic instructions.
7] Accordingly, embodiments of the disclosure also include non-transitory, tangible machine-readable media containing instructions or containing design data, such as
Hardware Description Language (HDL), which defines structures, circuits, apparatuses, processors and/or system features described herein. Such embodiments may
also be referred to as program products.
Emulation (including binary translation, code morphing, etc.)
8] In some cases, an instruction converter may be used to convert an instruction from a source instruction set to a target instruction set. For example, the instruction
converter may translate (e.g., using static binary translation, dynamic binary translation including dynamic compilation), morph, emulate, or otherwise convert an
instruction to one or more other instructions to be processed by the core. The instruction converter may be implemented in software, hardware, firmware, or a
combination thereof. The instruction converter may be on processor, off processor, or part on and part off processor.
9] Figure 84 is a block diagram contrasting the use of a software instruction converter to convert binary instructions in a source instruction set to binary instructions in a
target instruction set according to embodiments of the disclosure. In the illustrated embodiment, the instruction converter is a software instruction converter, although
alternatively the instruction converter may be implemented in software, firmware, hardware, or various combinations thereof. Figure 84 shows a program in a high level
language 8402 may be compiled using an x86 compiler 8404 to generate x86 binary code 8406 that may be natively executed by a processor with at least one x86
instruction set core 8416. The processor with at least one x86 instruction set core 8416 represents any processor that can perform substantially the same functions as
an Intel processor with at least one x86 instruction set core by compatibly executing or otherwise processing (1) a substantial portion of the instruction set of the Intel
x86 instruction set core or (2) object code versions of applications or other software targeted to run on an Intel processor with at least one x86 instruction set core, in
order to achieve substantially the same result as an Intel processor with at least one x86 instruction set core. The x86 compiler 8404 represents a compiler that is
operable to generate x86 binary code 8406 (e.g., object code) that can, with or without additional linkage processing, be executed on the processor with at least one x86
instruction set core 8416. Similarly, Figure 84 shows the program in the high level language 8402 may be compiled using an alternative instruction set compiler 8408 to
generate alternative instruction set binary code 8410 that may be natively executed by a processor without at least one x86 instruction set core 8414 (e.g., a processor
with cores that execute the MIPS instruction set of MIPS Technologies of Sunnyvale, CA and/or that execute the ARM instruction set of ARM Holdings of Sunnyvale, CA).
The instruction converter 8412 is used to convert the x86 binary code 8406 into code that may be natively executed by the processor without an x86 instruction set core
8414. This converted code is not likely to be the same as the alternative instruction set binary code 8410 because an instruction converter capable of this is difficult to
make; however, the converted code will accomplish the general operation and be made up of instructions from the alternative instruction set. Thus, the instruction
converter 8412 represents software, firmware, hardware, or a combination thereof that, through emulation, simulation or any other process, allows a processor or other
electronic device that does not have an x86 instruction set processor or core to execute the x86 binary code 8406.

Patent Citations (356)
Publication number

Priority date

Publication date

Assignee

Title

US672177A

1900-02-08

1901-04-16

William H Metcalf

Inhaler.

US5093920A

1987-06-25

1992-03-03

At&T Bell Laboratories

Programmable processing elements interconnected by a
communication network including field operation unit for
performing field operations

GB8717689D0

1987-07-25

1987-09-03

British Petroleum Co Plc

Computers

US4965716A

1988-03-11

1990-10-23

International Business Machines Corporation

Fast access priority queue for managing multiple
messages at a communications node or managing
multiple programs in a multiprogrammed data processor

JPH03500461A

1988-07-22

1991-01-31

アメリカ合衆国

Data flow device for data-driven calculations

US5241635A

1988-11-18

1993-08-31

Massachusetts Institute Of Technology

Tagged token data processing system with operand
matching in activation frames

US5141747A

1989-05-23

1992-08-25

Minnesota Mining And Manufacturing Company

Denatured collagen membrane

DE69029065T2

1989-07-28

1997-03-06

Texas Instruments Inc

Logical circuitry and method for reordering for a graphic
video display memory

Family To Family Citations

https://patents.google.com/patent/EP3726389B1/en?q=(high+radix)&oq=high+radix&page=1

56/76

2/14/26, 2:00 PM

EP3726389B1 - Apparatuses, methods, and systems for memory interface circuit allocation in a configurable spatial accelera…

US5239634A

1989-09-21

1993-08-24

Digital Equipment Corporation

Memory controller for enqueuing/dequeuing process

US5655096A

1990-10-12

1997-08-05

Branigin; Michael H.

Method and apparatus for dynamic scheduling of
instructions to ensure sequentially coherent data in a
processor employing out-of-order execution

US5689719A

1991-06-28

1997-11-18

Sanyo Electric O., Ltd.

Parallel computer system including processing elements

EP0547247B1

1991-07-08

2001-04-04

Seiko Epson Corporation

Extensible risc microprocessor architecture

EP0592715B1

1992-10-15

1997-06-11

Siemens Aktiengesellschaft

Checking design for testability rules with a VHDL
simulator

US5410722A

1993-01-21

1995-04-25

Conner Peripherals, Inc.

Queue system for dynamically allocating and moving
memory registers between a plurality of pseudo queues

US5314503A

1993-03-23

1994-05-24

Rasor Associates, Inc.

Automatic sheath protection of hypodermic needle

US5393536A

1993-04-05

1995-02-28

Crane Plastics Company

Coextrusion apparatus

JPH0713945A

1993-06-16

1995-01-17

Nippon Sheet Glass Co Ltd

Bus structure of multiprocessor system with separated
arithmetic processing part and control/storage part

US6460131B1

1993-08-03

2002-10-01

Xilinx Inc.

FPGA input output buffer with registered tristate enable

US5574944A

1993-12-15

1996-11-12

Convex Computer Corporation

System for accessing distributed memory by breaking
each accepted access request into series of instructions
by using sets of parameters defined as logical channel
context

US5393454A

1994-02-03

1995-02-28

Colgate Palmolive Co.

Thickened composition containing polymeric thickener
and aliphatic hydrocarbon

JP3610600B2

1994-08-22

2005-01-12

チッソ株式会社

Method for producing optically active endo-2-norborneols

US5787029A

1994-12-19

1998-07-28

Crystal Semiconductor Corp.

Ultra low power multiplier

US6247064B1

1994-12-22

2001-06-12

Unisys Corporation

Enqueue instruction in a system architecture for improved
message passing and process synchronization

US5734601A

1995-01-30

1998-03-31

Cirrus Logic, Inc.

Booth multiplier with low power, high performance input
circuitry

US5818743A

1995-04-21

1998-10-06

Texas Instruments Incorporated

Low power multiplier

US6020139A

1995-04-25

2000-02-01

Oridigm Corporation

S-adenosyl methionine regulation of metabolic pathways
and its use in diagnosis and therapy

US5925099A

1995-06-15

1999-07-20

Intel Corporation

Method and apparatus for transporting messages
between processors in a multiple processor system

US5850395A

1995-07-19

1998-12-15

Fujitsu Network Communications, Inc.

Asynchronous transfer mode based service consolidation
switch

US5725364A

1996-02-20

1998-03-10

Wagner Spray Tech Corporation

Pressure control module

US5805827A

1996-03-04

1998-09-08

3Com Corporation

Distributed signal processing for data channels
maintaining channel bandwidth

US5790821A

1996-03-08

1998-08-04

Advanced Micro Devices, Inc.

Control bit vector storage for storing control vectors
corresponding to instruction operations in a
microprocessor

US5625630A

1996-04-24

1997-04-29

Lucent Technologies Inc.

Increasing testability by clock transformation

US6088780A

1997-03-31

2000-07-11

Institute For The Development Of Emerging
Architecture, L.L.C.

Page table walker that uses at least one of a default page
size and a page size selected for a virtual address space
to position a sliding field in a virtual address

US5840598A

1997-08-14

1998-11-24

Micron Technology, Inc.

LOC semiconductor assembled with room temperature
adhesive

US6604120B1

1997-09-04

2003-08-05

Cirrus Logic, Inc.

Multiplier power saving design

US5930484A

1997-09-18

1999-07-27

International Business Machines Corporation

Method and system for input/output control in a
multiprocessor system utilizing simultaneous variablewidth bus access

US5948081A

1997-12-22

1999-09-07

Compaq Computer Corporation

System for flushing queued memory write request
corresponding to a queued read request and all prior write
requests with counter indicating requests to be flushed

JP3946873B2

1998-06-19

2007-07-18

株式会社日立製作所

Disk array controller

https://patents.google.com/patent/EP3726389B1/en?q=(high+radix)&oq=high+radix&page=1

57/76

2/14/26, 2:00 PM

EP3726389B1 - Apparatuses, methods, and systems for memory interface circuit allocation in a configurable spatial accelera…

US6212623B1

1998-08-24

2001-04-03

Advanced Micro Devices, Inc.

Universal dependency vector/queue entry

US6141747A

1998-09-22

2000-10-31

Advanced Micro Devices, Inc.

System for store to load forwarding of individual bytes
from separate store buffer entries to form a single load
word

US6611891B1

1998-11-23

2003-08-26

Advanced Micro Devices, Inc.

Computer resource configuration mechanism across a
multi-pipe communication link

US6314503B1

1998-12-30

2001-11-06

Emc Corporation

Method and apparatus for managing the placement of
data in a storage system to achieve increased system
performance

US6295571B1

1999-03-19

2001-09-25

Times N Systems, Inc.

Shared memory apparatus and method for
multiprocessor systems

AU3829500A

1999-04-09

2000-11-14

Clearspeed Technology Limited

Parallel data processing apparatus

JP2002544588A

1999-05-06

2002-12-24

コーニンクレッカ フィリップス エレクトロニ
クス エヌ ヴィ

Data processing apparatus, method of executing load or
store instruction, and method of compiling program

US6393536B1

1999-05-18

2002-05-21

Advanced Micro Devices, Inc.

Load/store unit employing last-in-buffer indication for
rapid load-hit-store

US6205533B1

1999-08-12

2001-03-20

Norman H. Margolus

Mechanism for efficient data access and communication
in parallel computations on an emulated spatial lattice

US7911960B1

1999-08-13

2011-03-22

International Business Machines Corporation

Delayed-start method for minimizing internal switch
congestion

US6507947B1

1999-08-20

2003-01-14

Hewlett-Packard Company

Programmatic synthesis of processor element arrays

US6640267B1

1999-09-27

2003-10-28

Cypress Semiconductor Corp.

Architecture for multi-queue storage element

JP2001109661A

1999-10-14

2001-04-20

Hitachi Ltd

Cache memory allocation method, operating system, and
computer system having the operating system

US6601126B1

2000-01-20

2003-07-29

Palmchip Corporation

Chip-core framework for systems-on-a-chip

US7139901B2

2000-02-08

2006-11-21

Mips Technologies, Inc.

Extended instruction set for packet processing
applications

US6877044B2

2000-02-10

2005-04-05

Vicom Systems, Inc.

Distributed storage management platform architecture

US6886085B1

2000-04-19

2005-04-26

International Business Machines Corporation

Method and apparatus for efficient virtual memory
management

US7210025B1

2000-04-19

2007-04-24

Uht Augustus K

Automatic and transparent hardware conversion of
traditional control flow to predicates

EP1342364A2

2000-11-28

2003-09-10

SeaChange International, Inc.

Content/service handling and delivery

US6947416B1

2000-12-13

2005-09-20

Cisco Technology, Inc.

Generalized asynchronous HDLC services

GB2370381B

2000-12-19

2003-12-24

Picochip Designs Ltd

Processor architecture

GB2377519B

2001-02-14

2005-06-15

Clearspeed Technology Ltd

Lookup engine

US6728945B1

2001-02-26

2004-04-27

Cadence Design Systems, Inc.

Behavioral level observability analysis and its applications

US20020161978A1

2001-02-28

2002-10-31

George Apostol

Multi-service system-on-chip including on-chip memory
with multiple access path

US6553448B1

2001-03-01

2003-04-22

3Com Corporation

Method for unit distance encoding of asynchronous
pointers for non-power-of-two sized buffers

US7844796B2

2001-03-05

2010-11-30

Martin Vorbach

Data processing device and method

US6725364B1

2001-03-08

2004-04-20

Xilinx, Inc.

Configurable processor system

GB2374242B

2001-04-07

2005-03-16

Univ Dundee

Integrated circuit and related improvements

US6515333B1

2001-04-27

2003-02-04

Advanced Micro Devices, Inc.

Removal of heat from SOI device

AU2002344288A1

2001-05-25

2002-12-09

Annapolis Micro Systems, Inc.

Method and apparatus for modeling dataflow systems
and realization to hardware

US20020184291A1

2001-05-31

2002-12-05

Hogenauer Eugene B.

Method and system for scheduling in an adaptable
computing engine

US7305492B2

2001-07-06

2007-12-04

Juniper Networks, Inc.

Content service aggregation system

US6874079B2

2001-07-25

2005-03-29

Quicksilver Technology

Adaptive computing engine with dataflow graph based
sequencing in reconfigurable mini-matrices of composite

https://patents.google.com/patent/EP3726389B1/en?q=(high+radix)&oq=high+radix&page=1

58/76

2/14/26, 2:00 PM

EP3726389B1 - Apparatuses, methods, and systems for memory interface circuit allocation in a configurable spatial accelera…
functional blocks

US20030023830A1

2001-07-25

2003-01-30

Hogenauer Eugene B.

Method and system for encoding instructions for a VLIW
that reduces instruction memory requirements

US6834383B2

2001-11-26

2004-12-21

Microsoft Corporation

Method for binary-level branch reversal on computer
architectures supporting predicated execution

US8412915B2

2001-11-30

2013-04-02

Altera Corporation

Apparatus, system and method for configuration of
adaptive integrated circuitry having heterogeneous
computational elements

US20030105799A1

2001-12-03

2003-06-05

Avaz Networks, Inc.

Distributed processing architecture with scalable
processing layers

JP3912091B2

2001-12-04

2007-05-09

ソニー株式会社

Data communication system, data transmission
apparatus, data reception apparatus and method, and
computer program

US7047374B2

2002-02-25

2006-05-16

Intel Corporation

Memory read/write reordering

US9170812B2

2002-03-21

2015-10-27

Pact Xpp Technologies Ag

Data processing system having integrated pipelined array
data processor

EP1495412B1

2002-03-22

2012-11-28

Alandro Consulting NY LLC

Scalable high performance 3d graphics

US7987479B1

2002-03-28

2011-07-26

Cisco Technology, Inc.

System and method for distribution of content over a
network

US7200735B2

2002-04-10

2007-04-03

Tensilica, Inc.

High-performance hybrid processor with configurable
execution units

US6922714B2

2002-05-09

2005-07-26

International Business Machines Corporation

Floating point unit power reduction scheme

AU2003228069A1

2002-05-24

2003-12-12

Koninklijke Philips Electronics N.V.

A scalar/vector processor

EP1367499A1

2002-05-28

2003-12-03

Fujitsu Siemens Computers, LLC

Compute node to mesh interface for highly scalable
parallel processing system

JP2004005249A

2002-05-31

2004-01-08

Fujitsu Ltd

Signal distribution device for load-distributed
multiprocessor

US6986131B2

2002-06-18

2006-01-10

Hewlett-Packard Development Company, L.P.

Method and apparatus for efficient code generation for
modulo scheduled uncounted loops

US7415594B2

2002-06-26

2008-08-19

Coherent Logix, Incorporated

Processing system with interspersed stall propagating
processors and communication elements

US20040001458A1

2002-06-27

2004-01-01

Motorola, Inc.

Method and apparatus for facilitating a fair access to a
channel by participating members of a group
communication system

US7486678B1

2002-07-03

2009-02-03

Greenfield Networks

Multi-slice network processor

US7200137B2

2002-07-29

2007-04-03

Freescale Semiconductor, Inc.

On chip network that maximizes interconnect utilization
between processing elements

US7277449B2

2002-07-29

2007-10-02

Freescale Semiconductor, Inc.

On chip network

WO2004021176A2

2002-08-07

2004-03-11

Pact Xpp Technologies Ag

Method and device for processing data

US6986023B2

2002-08-09

2006-01-10

Intel Corporation

Conditional execution of coprocessor instruction based
on main processor arithmetic flags

US7724740B1

2002-08-27

2010-05-25

3Com Corporation

Computer system and network interface supporting class
of service queues

US7181578B1

2002-09-12

2007-02-20

Copan Systems, Inc.

Method and apparatus for efficient scalable storage
management

GB2424503B

2002-09-17

2007-06-20

Micron Technology Inc

An active memory device

GB2395299B

2002-09-17

2006-06-21

Micron Technology Inc

Control of processing elements in parallel processors

US6983456B2

2002-10-31

2006-01-03

Src Computers, Inc.

Process for converting programs in high-level
programming languages to a unified executable for hybrid
computing platforms

US7099983B2

2002-11-25

2006-08-29

Lsi Logic Corporation

Multi-core communications module, data
communications system incorporating a multi-core
communications module, and data communications
process

US7415540B2

2002-12-31

2008-08-19

Intel Corporation

Scheduling processing threads

https://patents.google.com/patent/EP3726389B1/en?q=(high+radix)&oq=high+radix&page=1

59/76

2/14/26, 2:00 PM

EP3726389B1 - Apparatuses, methods, and systems for memory interface circuit allocation in a configurable spatial accelera…

US7047370B1

2003-01-14

2006-05-16

Cisco Technology, Inc.

Full access to memory interfaces via remote request

SE0300742D0

2003-03-17

2003-03-17

Flow Computing Ab

Data Flow Machine

US7137021B2

2003-05-15

2006-11-14

International Business Machines Corporation

Power saving in FPU with gated power based on opcodes
and data

US7058785B1

2003-05-23

2006-06-06

Xilinx, Inc.

Addressing objects in a large persistent storage address
space

WO2004114577A2

2003-06-18

2004-12-29

Centillium Communications, Inc.

Event scheduling for multi-port xdsl transceivers

US7714870B2

2003-06-23

2010-05-11

Intel Corporation

Apparatus and method for selectable hardware
accelerators in a data driven architecture

US7088371B2

2003-06-27

2006-08-08

Intel Corporation

Memory command handler for use in an image signal
processor having a data driven architecture

US20130111188A9

2003-07-24

2013-05-02

Martin Vorbach

Low latency massive parallel data processing device

JP4700611B2

2003-08-28

2011-06-15

ペーアーツェーテー イクスペーペー テクノロ
ジーズ アクチエンゲゼルシャフト

Data processing apparatus and data processing method

US7257665B2

2003-09-29

2007-08-14

Intel Corporation

Branch-aware FIFO for interprocessor data sharing

US20050138323A1

2003-12-18

2005-06-23

Intel Corporation, A Delaware Corporation

Accumulator shadow register systems and methods

JP4104538B2

2003-12-22

2008-06-18

三洋電機株式会社

Reconfigurable circuit, processing device provided with
reconfigurable circuit, function determination method of
logic circuit in reconfigurable circuit, circuit generation
method, and circuit

TWI323584B

2003-12-26

2010-04-11

Hon Hai Prec Ind Co Ltd

Method and system for burning mac address

US7490218B2

2004-01-22

2009-02-10

University Of Washington

Building a wavecache

JP4502650B2

2004-02-03

2010-07-14

日本電気株式会社

Array type processor

US20050223131A1

2004-04-02

2005-10-06

Goekjian Kenneth S

Context-based direct memory access engine for use with
a memory system shared by devices associated with
multiple input and output ports

JP4546775B2

2004-06-30

2010-09-15

富士通株式会社

Reconfigurable circuit capable of time-division multiplex
processing

US7509484B1

2004-06-30

2009-03-24

Sun Microsystems, Inc.

Handling cache misses by selectively flushing the
pipeline

US7281116B2

2004-07-30

2007-10-09

Hewlett-Packard Development Company, L.P.

Multiprocessor system having plural memory locations
for respectively storing TLB-shootdown data for plural
processor nodes

US7890735B2

2004-08-30

2011-02-15

Texas Instruments Incorporated

Multi-threading processors, integrated circuit devices,
systems, and processes of operation and manufacture

US7877748B2

2004-11-19

2011-01-25

The United States Of America As Represented
By The Secretary Of The Air Force

Method and apparatus for timing information flow in a
distributed system

US7594102B2

2004-12-15

2009-09-22

Stmicroelectronics, Inc.

Method and apparatus for vector execution on a scalar
machine

US7136954B2

2005-01-31

2006-11-14

International Business Machines Corporation

Data communication method and apparatus utilizing
credit-based data transfer protocol and credit loss
detection mechanism

US7613886B2

2005-02-08

2009-11-03

Sony Computer Entertainment Inc.

Methods and apparatus for synchronizing data access to
a local memory in a multi-processor system

US7676646B2

2005-03-02

2010-03-09

Cisco Technology, Inc.

Packet processor with wide register set architecture

US7546331B2

2005-03-17

2009-06-09

Qualcomm Incorporated

Low power array multiplier

US8694589B2

2005-03-31

2014-04-08

Google Inc.

Methods and systems for saving draft electronic
communications

US7373444B2

2005-04-15

2008-05-13

Kabushiki Kaisha Toshiba

Systems and methods for manipulating entries in a
command buffer using tag information

US7793040B2

2005-06-01

2010-09-07

Microsoft Corporation

Content addressable memory architecture

JP5117383B2

2005-06-30

2013-01-16

アイメック

Memory arrays for multiprocessor systems

JP4536618B2

2005-08-02

2010-09-01

富士通セミコンダクター株式会社

Reconfigurable integrated circuit device

https://patents.google.com/patent/EP3726389B1/en?q=(high+radix)&oq=high+radix&page=1

60/76

2/14/26, 2:00 PM

EP3726389B1 - Apparatuses, methods, and systems for memory interface circuit allocation in a configurable spatial accelera…

US8275976B2

2005-08-29

2012-09-25

The Invention Science Fund I, Llc

Hierarchical instruction scheduler facilitating instruction
replay

US20160098279A1

2005-08-29

2016-04-07

Searete Llc

Method and apparatus for segmented sequential storage

US8099556B2

2005-09-13

2012-01-17

Arm Limited

Cache miss detection in a data processing apparatus

JP2007079958A

2005-09-14

2007-03-29

Hitachi Ltd

Storage control device, data processing method, and
computer program

US7472299B2

2005-09-30

2008-12-30

Intel Corporation

Low power arbiters in interconnection routers

US8620623B2

2005-11-14

2013-12-31

Globaltrak, Llc

Hierarchical and distributed information processing
architecture for a container security system

US20070143546A1

2005-12-21

2007-06-21

Intel Corporation

Partitioned shared cache

EP1808774A1

2005-12-22

2007-07-18

St Microelectronics S.A.

A hierarchical reconfigurable computer architecture

EP1966681A4

2005-12-29

2009-01-07

Intel Corp

High performance queue implementations in
multiprocessor systems

JP4795025B2

2006-01-13

2011-10-19

キヤノン株式会社

Dynamic reconfigurable device, control method, and
program

US8595279B2

2006-02-27

2013-11-26

Qualcomm Incorporated

Floating-point processor with reduced power
requirements for selectable subprecision

US7533244B2

2006-05-09

2009-05-12

Le Nguyen Tran

Network-on-chip dataflow architecture

US7817652B1

2006-05-12

2010-10-19

Integrated Device Technology, Inc.

System and method of constructing data packets in a
packet switch

US20080133895A1

2006-05-16

2008-06-05

Alexey Yurievich Sivtsov

Floating Point Addition

US8065459B2

2006-05-17

2011-11-22

Nxp B.V.

Multi-processing system and a method of executing a
plurality of data processing tasks

US7594055B2

2006-05-24

2009-09-22

International Business Machines Corporation

Systems and methods for providing distributed
technology independent memory controllers

US7493406B2

2006-06-13

2009-02-17

International Business Machines Corporation

Maximal flow scheduling for a stream processing system

US7613848B2

2006-06-13

2009-11-03

International Business Machines Corporation

Dynamic stabilization for a stream processing system

US8390325B2

2006-06-21

2013-03-05

Element Cxi, Llc

Reconfigurable integrated circuit architecture with on-chip
configuration and reconfiguration

US8395414B2

2006-06-21

2013-03-12

Element Cxi, Llc

Hierarchically-scalable reconfigurable integrated circuit
architecture with unit delay modules

US8456191B2

2006-06-21

2013-06-04

Element Cxi, Llc

Data-driven integrated circuit architecture

US20080072113A1

2006-08-30

2008-03-20

Siukwin Tsang

Method of locating packet for resend from retry buffer

US9946547B2

2006-09-29

2018-04-17

Arm Finance Overseas Limited

Load/store unit for a processor, and applications thereof

US8095699B2

2006-09-29

2012-01-10

Mediatek Inc.

Methods and apparatus for interfacing between a host
processor and a coprocessor

US8010766B2

2006-10-12

2011-08-30

International Business Machines Corporation

Increasing buffer locality during multiple table access
operations

US7660911B2

2006-12-20

2010-02-09

Smart Modular Technologies, Inc.

Block-based data striping to flash memory

US20090300324A1

2007-01-19

2009-12-03

Nec Corporation

Array type processor and data processing system

JP4933284B2

2007-01-25

2012-05-16

株式会社日立製作所

Storage apparatus and load balancing method

US8543742B2

2007-02-22

2013-09-24

Super Talent Electronics, Inc.

Flash-memory device with RAID-type controller

US8321597B2

2007-02-22

2012-11-27

Super Talent Electronics, Inc.

Flash-memory device with RAID-type controller

US7479802B2

2007-03-09

2009-01-20

Quadric, Inc

Programmable logic integrated circuit for digital
algorithmic functions

US7613909B2

2007-04-17

2009-11-03

Xmos Limited

Resuming thread to service ready port transferring data
externally at different clock rate than internal circuitry of a
processor

US7779298B2

2007-06-11

2010-08-17

International Business Machines Corporation

Distributed job manager recovery

https://patents.google.com/patent/EP3726389B1/en?q=(high+radix)&oq=high+radix&page=1

61/76

2/14/26, 2:00 PM

EP3726389B1 - Apparatuses, methods, and systems for memory interface circuit allocation in a configurable spatial accelera…

US9648325B2

2007-06-30

2017-05-09

Microsoft Technology Licensing, Llc

Video decoding implementations for a graphics
processing unit

US8347312B2

2007-07-06

2013-01-01

Xmos Limited

Thread communications

US7822951B2

2007-08-01

2010-10-26

Advanced Micro Devices, Inc.

System and method of load-store forwarding

US20090064287A1

2007-08-28

2009-03-05

Rohati Systems, Inc.

Application protection architecture with triangulated
authorization

WO2009035185A1

2007-09-11

2009-03-19

Core Logic Inc.

Reconfigurable array processor for floating-point
operations

KR101312281B1

2007-11-06

2013-09-30

재단법인서울대학교산학협력재단

Processor and memory control method

US8032772B2

2007-11-15

2011-10-04

Intel Corporation

Method, apparatus, and system for optimizing frequency
and performance in a multi-die microprocessor

US7936753B1

2007-11-30

2011-05-03

Qlogic, Corporation

Method and system for reliable multicast

US8078839B2

2007-12-13

2011-12-13

Wave Semiconductor

Concurrent processing element system, and method

US9219603B2

2008-01-09

2015-12-22

International Business Machines Corporation

System and method for encryption key management in a
mixed infrastructure stream processing framework

US8160975B2

2008-01-25

2012-04-17

Mcafee, Inc.

Granular support vector machine with random granularity

US8365111B2

2008-02-29

2013-01-29

Et International, Inc.

Data driven logic simulation

US7904700B2

2008-03-10

2011-03-08

International Business Machines Corporation

Processing unit incorporating special purpose register for
use with instruction-based persistent vector multiplexer
control

US8356162B2

2008-03-18

2013-01-15

International Business Machines Corporation

Execution unit with data dependent conditional write
instructions

EP2278873B1

2008-03-19

2015-05-06

Cryo-Save AG

Improved cryopreservation of adipose tissue for the
isolation of mesenchymal stem cells

RU2374684C1

2008-05-04

2009-11-27

Государственное образовательное
учреждение высшего профессионального
образования Курский государственный
технический университет

Parallel-conveyor device for vectorisation of aerospace
images of earth surface

US8316252B2

2008-05-30

2012-11-20

Advanced Micro Devices, Inc.

Distributed clock gating with centralized state machine
control

US8115659B2

2008-06-10

2012-02-14

International Business Machines Corporation

Method and apparatus for efficient gathering of
information in a multicore system

US8843691B2

2008-06-25

2014-09-23

Stec, Inc.

Prioritized erasure of data blocks in a flash storage
device

JP5056644B2

2008-07-18

2012-10-24

富士通セミコンダクター株式会社

Data conversion apparatus, data conversion method and
program

US8001510B1

2008-09-05

2011-08-16

Xilinx, Inc.

Automated method of architecture mapping selection
from constrained high level language description via
element characterization

US20100191911A1

2008-12-23

2010-07-29

Marco Heddes

System-On-A-Chip Having an Array of Programmable
Processing Elements Linked By an On-Chip Network with
Distributed On-Chip Shared Memory and External Shared
Memory

US8078848B2

2009-01-09

2011-12-13

Micron Technology, Inc.

Memory controller having front end and back end
channels for modifying commands

US8086783B2

2009-02-23

2011-12-27

International Business Machines Corporation

High availability memory system

US8248936B2

2009-04-01

2012-08-21

Lockheed Martin Corporation

Tuning congestion control in IP multicast to mitigate the
impact of blockage

US8055816B2

2009-04-09

2011-11-08

Micron Technology, Inc.

Memory controllers, memory systems, solid state drives
and methods for processing a number of commands

US8910168B2

2009-04-27

2014-12-09

Lsi Corporation

Task backpressure and deletion in a multi-flow network
processor architecture

US8576714B2

2009-05-29

2013-11-05

Futurewei Technologies, Inc.

System and method for relay node flow control in a
wireless communications system

GB2471067B

2009-06-12

2011-11-30

Graeme Roy Smith

Shared resource multi-thread array processor

https://patents.google.com/patent/EP3726389B1/en?q=(high+radix)&oq=high+radix&page=1

62/76

2/14/26, 2:00 PM

EP3726389B1 - Apparatuses, methods, and systems for memory interface circuit allocation in a configurable spatial accelera…

US20110004742A1

2009-07-06

2011-01-06

Eonsil, Inc.

Variable-Cycle, Event-Driven Multi-Execution Flash
Processor

US9429983B1

2013-09-12

2016-08-30

Advanced Processor Architectures, Llc

System clock distribution in a distributed computing
environment

US8332597B1

2009-08-11

2012-12-11

Xilinx, Inc.

Synchronization of external memory accesses in a
dataflow machine

US8650240B2

2009-08-17

2014-02-11

International Business Machines Corporation

Complex matrix multiplication operations with data preconditioning in a high performance computing
architecture

US8301803B2

2009-10-23

2012-10-30

Samplify Systems, Inc.

Block floating point compression of signal data

EP2333673B1

2009-12-07

2014-04-16

STMicroelectronics (Research & Development)
Limited

Signal sampling and transfer

GB201001621D0

2010-02-01

2010-03-17

Univ Catholique Louvain

A tile-based processor architecture model for high
efficiency embedded homogenous multicore platforms

US8578117B2

2010-02-10

2013-11-05

Qualcomm Incorporated

Write-through-read (WTR) comparator circuits, systems,
and methods use of same with a multiple-port file

US8495341B2

2010-02-17

2013-07-23

International Business Machines Corporation

Instruction length based cracking for instruction of
variable length storage operands

WO2011123151A1

2010-04-02

2011-10-06

Tabula Inc.

System and method for reducing reconfiguration power
usage

WO2011133030A1

2010-04-23

2011-10-27

Vector Fabrics B.V.

Improved embedded system performance

US9285860B2

2010-05-03

2016-03-15

Qualcomm Incorporated

Apparatus and methods employing variable clock gating
hysteresis for a communications port

US8051227B1

2010-05-10

2011-11-01

Telefonaktiebolaget L M Ericsson (Publ)

Programmable queue structures for multiprocessors

KR101751045B1

2010-05-25

2017-06-27

삼성전자 주식회사

3D Semiconductor device

US8438341B2

2010-06-16

2013-05-07

International Business Machines Corporation

Common memory programming

US8719455B2

2010-06-28

2014-05-06

International Business Machines Corporation

DMA-based acceleration of command push buffer
between host and target devices

CN101950282B

2010-08-30

2012-05-23

中国科学院计算技术研究所

Multiprocessor system and synchronous engine thereof

US9201801B2

2010-09-15

2015-12-01

International Business Machines Corporation

Computing device with asynchronous auxiliary execution
unit

US9052890B2

2010-09-25

2015-06-09

Intel Corporation

Execute at commit state update instructions, apparatus,
methods, and systems

TWI425357B

2010-09-27

2014-02-01

Silicon Motion Inc

Method for performing block management, and
associated memory device and controller thereof

KR101735677B1

2010-11-17

2017-05-16

삼성전자주식회사

Apparatus for multiply add fused unit of floating point
number, and method thereof

US8548104B2

2010-11-23

2013-10-01

Siano Mobile Silicon Ltd.

Receiver with configurable clock frequencies

US9274962B2

2010-12-07

2016-03-01

Intel Corporation

Apparatus, method, and system for instantaneous cache
state recovery from speculative abort/commit

US9026769B1

2011-01-31

2015-05-05

Marvell International Ltd.

Detecting and reissuing of loop instructions in reorder
structure

TWI432987B

2011-03-15

2014-04-01

Phison Electronics Corp

Memory storage device, memory controller thereof, and
method for virus scanning

US9170846B2

2011-03-29

2015-10-27

Daniel Delling

Distributed data-parallel execution engines for userdefined serial problems using branch-and-bound
algorithm

US8799880B2

2011-04-08

2014-08-05

Siemens Aktiengesellschaft

Parallelization of PLC programs for operation in multiprocessor environments

WO2012144043A1

2011-04-21

2012-10-26

ルネサスエレクトロニクス株式会社

Semiconductor integrated circuit and method for
operating same

US9817700B2

2011-04-26

2017-11-14

International Business Machines Corporation

Dynamic data partitioning for optimal resource utilization
in a parallel data processing system

US10078620B2

2011-05-27

2018-09-18

New York University

Runtime reconfigurable dataflow processor with multiport memory access module

https://patents.google.com/patent/EP3726389B1/en?q=(high+radix)&oq=high+radix&page=1

63/76

2/14/26, 2:00 PM

EP3726389B1 - Apparatuses, methods, and systems for memory interface circuit allocation in a configurable spatial accelera…

US9116634B2

2011-06-10

2015-08-25

International Business Machines Corporation

Configure storage class memory command

US9811783B2

2011-06-24

2017-11-07

Jobvite, Inc.

Method and system for channel optimization

US20130024875A1

2011-07-22

2013-01-24

Yilin Wang

Event System And Methods For Using Same

US9148495B2

2011-07-26

2015-09-29

International Business Machines Corporation

Dynamic runtime choosing of processing communication
methods

US8990452B2

2011-07-26

2015-03-24

International Business Machines Corporation

Dynamic reduction of stream backpressure

US9201817B2

2011-08-03

2015-12-01

Montage Technology (Shanghai) Co., Ltd.

Method for allocating addresses to data buffers in
distributed buffer chipset

US8694754B2

2011-09-09

2014-04-08

Ocz Technology Group, Inc.

Non-volatile memory-based mass storage devices and
methods for writing data thereto

KR101918464B1

2011-09-14

2018-11-15

삼성전자 주식회사

A processor and a swizzle pattern providing apparatus
based on a swizzled virtual register

US8966457B2

2011-11-15

2015-02-24

Global Supercomputing Corporation

Method and system for converting a single-threaded
software program into an application-specific
supercomputer

US8898505B2

2011-12-01

2014-11-25

International Business Machines Corporation

Dynamically configureable placement engine

US8892914B2

2011-12-08

2014-11-18

Active-Semi, Inc.

Programmable fault protect for processor controlled highside and low-side drivers

US20130160028A1

2011-12-14

2013-06-20

John E. Black

Method and apparatus for low latency communication
and synchronization for multi-thread applications

WO2013100783A1

2011-12-29

2013-07-04

Intel Corporation

Method and system for control signalling in a data path
module

KR101968512B1

2012-02-21

2019-04-12

삼성전자주식회사

Device and method for transceiving multamedia data
using near field communication

US9135077B2

2012-03-16

2015-09-15

Advanced Micro Devices, Inc.

GPU compute optimization via wavefront reforming

US9146775B2

2012-04-26

2015-09-29

International Business Machines Corporation

Operator graph changes in response to dynamic
connections in stream computing applications

US9128725B2

2012-05-04

2015-09-08

Apple Inc.

Load-store dependency predictor content management

US8995410B2

2012-05-25

2015-03-31

University Of Southern California

Airsync: enabling distributed multiuser MIMO with full
multiplexing gain

US9213571B2

2012-06-06

2015-12-15

2236008 Ontario Inc.

System and method for changing abilities of a process

US9026705B2

2012-08-09

2015-05-05

Oracle International Corporation

Interrupt processing unit for preventing interrupt loss

WO2014031495A2

2012-08-18

2014-02-27

Arteris SAS

System translation look-aside buffer with request-based
allocation and prefetching

US9110713B2

2012-08-30

2015-08-18

Qualcomm Incorporated

Microarchitecture for floating point fused multiply-add
with exponent scaling

US9063974B2

2012-10-02

2015-06-23

Oracle International Corporation

Hardware for table scan acceleration

US9632787B2

2012-10-23

2017-04-25

Ca, Inc.

Data processing system with data characteristic based
identification of corresponding instructions

US9829956B2

2012-11-21

2017-11-28

Nvidia Corporation

Approach to power reduction in floating-point operations

US9274971B2

2012-11-27

2016-03-01

International Business Machines Corporation

Low latency data exchange

WO2014098845A1

2012-12-19

2014-06-26

Intel Corporation

Vector mask driven clock gating for power efficiency of a
processor

US8625422B1

2012-12-20

2014-01-07

Unbound Networks

Parallel processing using multi-core processor

US9104474B2

2012-12-28

2015-08-11

Intel Corporation

Variable precision floating point multiply-add circuit

US9424045B2

2013-01-29

2016-08-23

Arm Limited

Data processing apparatus and method for controlling
use of an issue queue to represent an instruction suitable
for execution by a wide operand execution unit

US10467010B2

2013-03-15

2019-11-05

Intel Corporation

Method and apparatus for nearest potential store tagging

US9268528B2

2013-05-23

2016-02-23

Nvidia Corporation

System and method for dynamically reducing power
consumption of floating-point logic

https://patents.google.com/patent/EP3726389B1/en?q=(high+radix)&oq=high+radix&page=1

64/76

2/14/26, 2:00 PM

EP3726389B1 - Apparatuses, methods, and systems for memory interface circuit allocation in a configurable spatial accelera…

US9792252B2

2013-05-31

2017-10-17

Microsoft Technology Licensing, Llc

Incorporating a spatial array into one or more
programmable processor cores

US9886072B1

2013-06-19

2018-02-06

Altera Corporation

Network processor FPGA (npFPGA): multi-die FPGA chip
for scalable multi-gigabit network processing

US9715389B2

2013-06-25

2017-07-25

Advanced Micro Devices, Inc.

Dependent instruction suppression

US9424079B2

2013-06-27

2016-08-23

Microsoft Technology Licensing, Llc

Iteration support in a heterogeneous dataflow engine

US9891927B2

2013-08-28

2018-02-13

Via Technologies, Inc.

Inter-core communication via uncore RAM

US9524164B2

2013-08-30

2016-12-20

Advanced Micro Devices, Inc.

Specialized memory disambiguation mechanisms for
different memory read access types

US9292076B2

2013-09-16

2016-03-22

Intel Corporation

Fast recalibration circuitry for input/output (IO)
compensation finite state machine power-down-exit

US9996490B2

2013-09-19

2018-06-12

Nvidia Corporation

Technique for scaling the bandwidth of a processing
element to match the bandwidth of an interconnect

US9244827B2

2013-09-25

2016-01-26

Intel Corporation

Store address prediction for memory disambiguation in a
processing device

US10331583B2

2013-09-26

2019-06-25

Intel Corporation

Executing distributed memory operations using
processing elements connected by distributed channels

HUP1300561A2

2013-09-27

2015-03-30

Pazmany Peter Katolikus Egyetem

Computer architecture and processing

US9594720B2

2013-10-21

2017-03-14

Xmos Limited

Interface between a bus and a inter-thread interconnect

JP6446995B2

2013-10-29

2019-01-09

株式会社リコー

Information processing system and information
processing method

US8924596B1

2013-12-06

2014-12-30

Concurrent Ventures, LLC

System and method for dividing and synchronizing a
processing task across multiple processing
elements/processors in hardware

US9699079B2

2013-12-30

2017-07-04

Netspeed Systems

Streaming bridge design with host interfaces and network
on chip (NoC) layers

US10591983B2

2014-03-14

2020-03-17

Wisconsin Alumni Research Foundation

Computer accelerator system using a trigger architecture
memory access processor

US20150268963A1

2014-03-23

2015-09-24

Technion Research & Development Foundation
Ltd.

Execution of data-parallel programs on coarse-grained
reconfigurable architecture hardware

US9870209B2

2014-03-28

2018-01-16

Intel Corporation

Instruction and logic for reducing data cache evictions in
an out-of-order processor

KR20150126484A

2014-05-02

2015-11-12

삼성전자주식회사

Apparatas and method for transforming source code into
machine code in an electronic device

US9696927B2

2014-06-19

2017-07-04

International Business Machines Corporation

Memory transaction having implicit ordering effects

JP6339240B2

2014-06-24

2018-06-06

インテル コーポレイション

Virtual machine power management

WO2016003646A1

2014-06-30

2016-01-07

Unisys Corporation

Enterprise management for secure network
communications over ipsec

US10409763B2

2014-06-30

2019-09-10

Intel Corporation

Apparatus and method for efficiently implementing a
processor pipeline

US9330433B2

2014-06-30

2016-05-03

Intel Corporation

Data distribution fabric in scalable GPUs

US10108241B2

2014-09-15

2018-10-23

Intel Corporation

Method and apparatus for saving power of a processor
socket in a multi-socket computer system

DE102014113430A1

2014-09-17

2016-03-17

Bundesdruckerei Gmbh

Distributed data storage using authorization tokens

US9836473B2

2014-10-03

2017-12-05

International Business Machines Corporation

Hardware acceleration for a compressed computation
database

US9473144B1

2014-11-25

2016-10-18

Cypress Semiconductor Corporation

Integrated circuit device with programmable analog
subsystem

US9851945B2

2015-02-16

2017-12-26

Advanced Micro Devices, Inc.

Bit remapping mechanism to enhance lossy compression
in floating-point applications

US9658676B1

2015-02-19

2017-05-23

Amazon Technologies, Inc.

Sending messages in a network-on-chip and providing a
low power state for processing cores

US9594521B2

2015-02-23

2017-03-14

Advanced Micro Devices, Inc.

Scheduling of data migration

https://patents.google.com/patent/EP3726389B1/en?q=(high+radix)&oq=high+radix&page=1

65/76

2/14/26, 2:00 PM

EP3726389B1 - Apparatuses, methods, and systems for memory interface circuit allocation in a configurable spatial accelera…

US9928190B2

2015-06-15

2018-03-27

International Business Machines Corporation

High bandwidth low latency data exchange between
processing elements

US9743151B2

2015-06-24

2017-08-22

Time Warner Cable Enterprises Llc

Multicast video program switching architecture

US10111024B2

2015-07-10

2018-10-23

Lg Electronics Inc.

Method and apparatus for an input data processing via a
local computing or offloading based on power harvesting
in a wireless communication system

US9990367B2

2015-07-27

2018-06-05

Sas Institute Inc.

Distributed data set encryption and decryption

US10216693B2

2015-07-30

2019-02-26

Wisconsin Alumni Research Foundation

Computer with hybrid Von-Neumann/dataflow execution
architecture

US10108417B2

2015-08-14

2018-10-23

Qualcomm Incorporated

Storing narrow produced values for instruction operands
directly in a register map in an out-of-order processor

US20170062075A1

2015-08-31

2017-03-02

Sandisk Technologies Inc.

Apparatus including core and clock gating circuit and
method of operating same

US20170083313A1

2015-09-22

2017-03-23

Qualcomm Incorporated

CONFIGURING COARSE-GRAINED RECONFIGURABLE
ARRAYS (CGRAs) FOR DATAFLOW INSTRUCTION BLOCK
EXECUTION IN BLOCK-BASED DATAFLOW INSTRUCTION
SET ARCHITECTURES (ISAs)

US10121553B2

2015-09-30

2018-11-06

Sunrise Memory Corporation

Capacitive-coupled non-volatile thin-film transistor NOR
strings in three-dimensional arrays

US9847783B1

2015-10-13

2017-12-19

Altera Corporation

Scalable architecture for IP block integration

US9762563B2

2015-10-14

2017-09-12

FullArmor Corporation

Resource access system and method

US20170116154A1

2015-10-23

2017-04-27

The Intellisis Corporation

Register communication in a network-on-a-chip
architecture

CN105512060B

2015-12-04

2018-09-14

上海兆芯集成电路有限公司

Input/output circuitry and data transfer control method

US9923905B2

2016-02-01

2018-03-20

General Electric Company

System and method for zone access control

US9520876B1

2016-02-17

2016-12-13

International Business Machines Corporation

Power gating and clock gating in wiring levels

US9959068B2

2016-03-04

2018-05-01

Western Digital Technologies, Inc.

Intelligent wide port phy usage

KR20170105353A

2016-03-09

2017-09-19

삼성전자주식회사

Electronic apparatus and control method thereof

US20180225403A1

2016-03-31

2018-08-09

Wave Computing, Inc.

Dynamic configuration of a reconfigurable hum fabric

US20170286169A1

2016-03-31

2017-10-05

National Instruments Corporation

Automatically Mapping Program Functions to Distributed
Heterogeneous Platforms Based on Hardware Attributes
and Specified Constraints

GB2565934B

2016-04-27

2022-08-10

Coda Project Inc

System, method, and apparatus for operating a unified
document surface workspace

US20170315812A1

2016-04-28

2017-11-02

Microsoft Technology Licensing, Llc

Parallel instruction scheduler for block isa processor

US10764781B2

2016-05-03

2020-09-01

Qualcomm Incorporated

Systems and methods for reordering data received from a
plurality of radio access technologies (RATs)

US10110233B2

2016-06-23

2018-10-23

Altera Corporation

Methods for specifying processor architectures for
programmable integrated circuits

US20180081834A1

2016-09-16

2018-03-22

Futurewei Technologies, Inc.

Apparatus and method for configuring hardware to
operate in multiple modes during runtime

US20180081806A1

2016-09-22

2018-03-22

Qualcomm Incorporated

Memory violation prediction

US10168758B2

2016-09-29

2019-01-01

Intel Corporation

Techniques to enable communication between a
processor and voltage regulator

US10402168B2

2016-10-01

2019-09-03

Intel Corporation

Low energy consumption mantissa multiplication for
floating point multiply-add operations

US10795853B2

2016-10-10

2020-10-06

Intel Corporation

Multiple dies hardware processors and methods

US10037267B2

2016-10-21

2018-07-31

Advanced Micro Devices, Inc.

Instruction set architecture and software support for
register state migration

US10474375B2

2016-12-30

2019-11-12

Intel Corporation

Runtime address disambiguation in acceleration
hardware

US10558575B2

2016-12-30

2020-02-11

Intel Corporation

Processors, methods, and systems with a configurable
spatial accelerator

https://patents.google.com/patent/EP3726389B1/en?q=(high+radix)&oq=high+radix&page=1

66/76

2/14/26, 2:00 PM

EP3726389B1 - Apparatuses, methods, and systems for memory interface circuit allocation in a configurable spatial accelera…

US10416999B2

2016-12-30

2019-09-17

Intel Corporation

Processors, methods, and systems with a configurable
spatial accelerator

US10572376B2

2016-12-30

2020-02-25

Intel Corporation

Memory ordering in acceleration hardware

US10180928B2

2016-12-31

2019-01-15

Intel Corporation

Heterogeneous hardware accelerator architecture for
processing sparse matrix data with skewed non-zero
distributions

US20180189675A1

2016-12-31

2018-07-05

Intel Corporation

Hardware accelerator architecture and template for webscale k-means clustering

US11853244B2

2017-01-26

2023-12-26

Wisconsin Alumni Research Foundation

Reconfigurable computer accelerator providing stream
processor and dataflow processor

US10490251B2

2017-01-30

2019-11-26

Micron Technology, Inc.

Apparatuses and methods for distributing row hammer
refresh events across a memory device

US10754829B2

2017-04-04

2020-08-25

Oracle International Corporation

Virtual configuration systems and methods

CN108694014A

2017-04-06

2018-10-23

群晖科技股份有限公司

Method and device for reserving and managing memory
space

WO2018193353A1

2017-04-17

2018-10-25

Cerebras Systems Inc.

Neuron smearing for accelerated deep learning

US10452452B2

2017-04-17

2019-10-22

Wave Computing, Inc.

Reconfigurable processor fabric implementation using
satisfiability analysis

US10778767B2

2017-04-28

2020-09-15

International Business Machines Corporation

Persistent memory replication in RDMA-capable networks

US10645448B2

2017-05-15

2020-05-05

Omnivision Technologies, Inc.

Buffer-aware transmission rate control for real-time video
streaming system

US10191871B2

2017-06-20

2019-01-29

Infineon Technologies Ag

Safe double buffering using DMA safe linked lists

US10346145B2

2017-06-23

2019-07-09

Intel Corporation

Loop execution with predicate computing for dataflow
machines

US10467183B2

2017-07-01

2019-11-05

Intel Corporation

Processors and methods for pipelined runtime services in
a spatial array

US10445451B2

2017-07-01

2019-10-15

Intel Corporation

Processors, methods, and systems for a configurable
spatial accelerator with performance, correctness, and
power reduction features

US10445234B2

2017-07-01

2019-10-15

Intel Corporation

Processors, methods, and systems for a configurable
spatial accelerator with transactional and replay features

US10387319B2

2017-07-01

2019-08-20

Intel Corporation

Processors, methods, and systems for a configurable
spatial accelerator with memory system performance,
power reduction, and atomics support features

US10469397B2

2017-07-01

2019-11-05

Intel Corporation

Processors and methods with configurable networkbased dataflow operator circuits

US20190004878A1

2017-07-01

2019-01-03

Intel Corporation

Processors, methods, and systems for a configurable
spatial accelerator with security, power reduction, and
performace features

US10515046B2

2017-07-01

2019-12-24

Intel Corporation

Processors, methods, and systems with a configurable
spatial accelerator

US11157287B2

2017-07-24

2021-10-26

Tesla, Inc.

Computational array microprocessor system with variable
latency memory access

US10461747B2

2017-09-20

2019-10-29

Apple Inc.

Low power clock gating circuit

US11086816B2

2017-09-28

2021-08-10

Intel Corporation

Processors, methods, and systems for debugging a
configurable spatial accelerator

US10496574B2

2017-09-28

2019-12-03

Intel Corporation

Processors, methods, and systems for a memory fence in
a configurable spatial accelerator

US20190101952A1

2017-09-30

2019-04-04

Intel Corporation

Processors and methods for configurable clock gating in
a spatial array

US10445098B2

2017-09-30

2019-10-15

Intel Corporation

Processors and methods for privileged configuration in a
spatial array

US10380063B2

2017-09-30

2019-08-13

Intel Corporation

Processors, methods, and systems with a configurable
spatial accelerator having a sequencer dataflow operator

US10402176B2

2017-12-27

2019-09-03

Intel Corporation

Methods and apparatus to compile code to generate data
flow code

https://patents.google.com/patent/EP3726389B1/en?q=(high+radix)&oq=high+radix&page=1

67/76

2/14/26, 2:00 PM

EP3726389B1 - Apparatuses, methods, and systems for memory interface circuit allocation in a configurable spatial accelera…

US10565134B2

2017-12-30

2020-02-18

Intel Corporation

Apparatus, methods, and systems for multicast in a
configurable spatial accelerator

US10445250B2

2017-12-30

2019-10-15

Intel Corporation

Apparatus, methods, and systems with a configurable
spatial accelerator

US10417175B2

2017-12-30

2019-09-17

Intel Corporation

Apparatus, methods, and systems for memory
consistency in a configurable spatial accelerator

US20190303263A1

2018-03-30

2019-10-03

Kermin E. Fleming, JR.

Apparatus, methods, and systems for integrated
performance monitoring in a configurable spatial
accelerator

US20190303297A1

2018-04-02

2019-10-03

Intel Corporation

Apparatus, methods, and systems for remote memory
access in a configurable spatial accelerator

US11307873B2

2018-04-03

2022-04-19

Intel Corporation

Apparatus, methods, and systems for unstructured data
flow in a configurable spatial accelerator with predicate
propagation and merging

US10564980B2

2018-04-03

2020-02-18

Intel Corporation

Apparatus, methods, and systems for conditional queues
in a configurable spatial accelerator

US10552339B2

2018-06-12

2020-02-04

Advanced Micro Devices, Inc.

Dynamically adapting mechanism for translation
lookaside buffer shootdowns

US10776087B2

2018-06-25

2020-09-15

Intel Corporation

Sequence optimizations in a high-performance
computing environment

US11200186B2

2018-06-30

2021-12-14

Intel Corporation

Apparatuses, methods, and systems for operations in a
configurable spatial accelerator

Priority date

Publication date

Assignee

Title

WO2020055921A1

2018-09-10

2020-03-19

GigaIO Networks, Inc.

Methods and apparatus for high-speed data bus connection and fabric
management

US12045644B2 *

2019-05-24

2024-07-23

Texas Instruments
Incorporated

Pseudo-random way selection

US11392498B2

2019-05-24

2022-07-19

Texas Instruments
Incorporated

Aliased mode for cache controller

US11176065B2 *

2019-08-12

2021-11-16

Micron Technology, Inc.

Extended memory interface

US11403247B2

2019-09-10

2022-08-02

GigaIO Networks, Inc.

Methods and apparatus for network interface fabric send/receive operations

US11593288B2 *

2019-10-02

2023-02-28

GigalO Networks, Inc.

Methods and apparatus for fabric interface polling

KR102786338B1 *

2019-10-04

2025-03-26

삼성전자 주식회사

Apparatus and method for boosting network data throughput in electronic
device

WO2021081409A1

2019-10-25

2021-04-29

GigaIO Networks, Inc.

Methods and apparatus for dma engine descriptors for high speed data
systems

US11907713B2

2019-12-28

2024-02-20

Intel Corporation

Apparatuses, methods, and systems for fused operations using sign
modification in a processing element of a configurable spatial accelerator

US11416399B2 *

2020-06-25

2022-08-16

Nokia Solutions And
Networks Oy

Dedicated memory buffers for supporting deterministic inter-FPGA
communication

US11809908B2 *

2020-07-07

2023-11-07

SambaNova Systems, Inc.

Runtime virtualization of reconfigurable data flow resources

CN111985626B *

2020-09-25

2022-06-07

苏州浪潮智能科技有限公司

System, method and storage medium for accelerating RNN (radio network
node)

US12159225B2

2020-10-14

2024-12-03

Google Llc

Queue allocation in machine learning accelerators

CN112100121B *

2020-11-17

2021-02-12

北京壁仞科技开发有限公司

Computing device, computing equipment and programmable scheduling
method

CN112965854B *

2021-04-16

2022-04-29

吉林大学

Method, system and equipment for improving reliability of convolutional
neural network

US11733902B2

2021-04-30

2023-08-22

International Business
Machines Corporation

Integrating and increasing performance of disaggregated memory in
operating systems

* Cited by examiner, † Cited by third party

Cited By (27)
Publication number
Family To Family Citations

https://patents.google.com/patent/EP3726389B1/en?q=(high+radix)&oq=high+radix&page=1

68/76

2/14/26, 2:00 PM

EP3726389B1 - Apparatuses, methods, and systems for memory interface circuit allocation in a configurable spatial accelera…

CN113761004B *

2021-05-07

2025-06-27

腾讯科技（深圳）有限公司

Network model data processing, data display method, device and storage
medium

CN113360565B *

2021-08-10

2021-11-02

蚂蚁金服（杭州）网络技术
有限公司

Method and apparatus for flow control

US12068745B2 *

2021-08-31

2024-08-20

Arm Limited

Multi-bit scan chain with error-bit generator

CN113891396B *

2021-09-01

2022-07-26

深圳金信诺高新技术股份有
限公司

Data packet processing method and device, computer equipment and storage
medium

US11709611B2

2021-10-26

2023-07-25

SambaNova Systems, Inc.

Determining and using memory unit partitioning solutions for reconfigurable
dataflow computing systems

KR102767977B1 *

2021-11-02

2025-02-14

리벨리온 주식회사

AI core, AI core system and load/store method of AI core system

US12242403B2

2022-03-18

2025-03-04

SambaNova Systems, Inc.

Direct access to reconfigurable processor memory

US12298932B2 *

2022-05-25

2025-05-13

SambaNova Systems, Inc.

Load balancing system for the execution of applications on reconfigurable
processors

WO2024092547A1 *

2022-11-02

2024-05-10

Paypal, Inc.

Graph computing for electronic communication risk detection

US12169459B2 *

2023-01-19

2024-12-17

SambaNova Systems, Inc.

Method and apparatus for data access in a heterogeneous processing
system with multiple processors using memory extension operation

US20250093937A1 *

2023-09-14

2025-03-20

Apple Inc.

Multi-Processor Power Management Circuit

* Cited by examiner, † Cited by third party, ‡ Family to family citation

Similar Documents
Publication

Publication Date

Title

EP3726389B1

2021-11-17

Apparatuses, methods, and systems for memory interface circuit allocation in a configurable spatial accelerator

US12086080B2

2024-09-10

Apparatuses, methods, and systems for a configurable accelerator having dataflow execution circuits

US10564980B2

2020-02-18

Apparatus, methods, and systems for conditional queues in a configurable spatial accelerator

US10380063B2

2019-08-13

Processors, methods, and systems with a configurable spatial accelerator having a sequencer dataflow operator

US11307873B2

2022-04-19

Apparatus, methods, and systems for unstructured data flow in a configurable spatial accelerator with predicate propagation and
merging

US10565134B2

2020-02-18

Apparatus, methods, and systems for multicast in a configurable spatial accelerator

US10417175B2

2019-09-17

Apparatus, methods, and systems for memory consistency in a configurable spatial accelerator

US10817291B2

2020-10-27

Apparatuses, methods, and systems for swizzle operations in a configurable spatial accelerator

US11029958B1

2021-06-08

Apparatuses, methods, and systems for configurable operand size operations in an operation configurable spatial accelerator

US10891240B2

2021-01-12

Apparatus, methods, and systems for low latency communication in a configurable spatial accelerator

US20190303297A1

2019-10-03

Apparatus, methods, and systems for remote memory access in a configurable spatial accelerator

US10678724B1

2020-06-09

Apparatuses, methods, and systems for in-network storage in a configurable spatial accelerator

US11037050B2

2021-06-15

Apparatuses, methods, and systems for memory interface circuit arbitration in a configurable spatial accelerator

US20190303263A1

2019-10-03

Apparatus, methods, and systems for integrated performance monitoring in a configurable spatial accelerator

EP3343388A1

2018-07-04

Processors, methods, and systems with a configurable spatial accelerator

US10459866B1

2019-10-29

Apparatuses, methods, and systems for integrated control and data processing in a configurable spatial accelerator

US20190018815A1

2019-01-17

Processors, methods, and systems with a configurable spatial accelerator

US10853073B2

2020-12-01

Apparatuses, methods, and systems for conditional operations in a configurable spatial accelerator

EP3757814A1

2020-12-30

Apparatuses, methods, and systems for time-multiplexing in a configurable spatial accelerator

US11907713B2

2024-02-20

Apparatuses, methods, and systems for fused operations using sign modification in a processing element of a configurable spatial
accelerator

Priority And Related Applications

https://patents.google.com/patent/EP3726389B1/en?q=(high+radix)&oq=high+radix&page=1

69/76

2/14/26, 2:00 PM

EP3726389B1 - Apparatuses, methods, and systems for memory interface circuit allocation in a configurable spatial accelera…

Applications Claiming Priority (1)
Application

Filing date

Title

US16/370,928

2019-03-30

Apparatuses, methods, and systems for memory interface circuit allocation in a configurable spatial accelerator

Legal Events
Date

Code

Title

Description

2020-09-18

PUAI

Public reference made under article 153(3) epc to a published international application that has entered the european phase

Free format text: ORIGINAL
CODE: 0009012

2020-09-18

STAA

Information on the status of an ep patent application or granted ep patent

Free format text: STATUS:
THE APPLICATION HAS
BEEN PUBLISHED

2020-10-21

AK

Designated contracting states

Kind code of ref document
A1
Designated state(s): AL AT
BE BG CH CY CZ DE DK EE
ES FI FR GB GR HR HU IE IS
IT LI LT LU LV MC MK MT N
NO PL PT RO RS SE SI SK S
TR

2020-10-21

AX

Request for extension of the european patent

Extension state: BA ME

2021-04-16

STAA

Information on the status of an ep patent application or granted ep patent

Free format text: STATUS:
REQUEST FOR
EXAMINATION WAS MADE

2021-05-19

17P

Request for examination filed

Effective date: 20210414

2021-05-19

RBV

Designated contracting states (corrected)

Designated state(s): AL AT
BE BG CH CY CZ DE DK EE
ES FI FR GB GR HR HU IE IS
IT LI LT LU LV MC MK MT N
NO PL PT RO RS SE SI SK S
TR

2021-05-28

REG

Reference to a national code

Ref country code: DE
Ref legal event code: R079
Ref document number:
602020000990
Country of ref document: D
Free format text: PREVIOU
MAIN CLASS:
G06F0012020000
Ipc: G06F0013160000

2021-06-20

GRAP

Despatch of communication of intention to grant a patent

Free format text: ORIGINAL
CODE: EPIDOSNIGR1

2021-06-20

STAA

Information on the status of an ep patent application or granted ep patent

Free format text: STATUS:
GRANT OF PATENT IS
INTENDED

https://patents.google.com/patent/EP3726389B1/en?q=(high+radix)&oq=high+radix&page=1

70/76

2/14/26, 2:00 PM
2021-07-07

EP3726389B1 - Apparatuses, methods, and systems for memory interface circuit allocation in a configurable spatial accelera…
RIC1

Information provided on ipc code assigned before grant

Ipc: G06F 13/16
20060101AFI20210528BH
Ipc: G06F 12/0846
20160101ALI20210528BH
Ipc: G06F 12/0853
20160101ALI20210528BH
Ipc: G06F 12/0866
20160101ALI20210528BH
Ipc: G06F 12/0855
20160101ALI20210528BH
Ipc: G06F 15/80
20060101ALI20210528BH

2021-07-21

INTG

Intention to grant announced

Effective date: 20210621

2021-10-12

GRAS

Grant fee paid

Free format text: ORIGINAL
CODE: EPIDOSNIGR3

2021-10-15

GRAA

(expected) grant

Free format text: ORIGINAL
CODE: 0009210

2021-10-15

STAA

Information on the status of an ep patent application or granted ep patent

Free format text: STATUS:
THE PATENT HAS BEEN
GRANTED

2021-11-17

AK

Designated contracting states

Kind code of ref document
B1
Designated state(s): AL AT
BE BG CH CY CZ DE DK EE
ES FI FR GB GR HR HU IE IS
IT LI LT LU LV MC MK MT N
NO PL PT RO RS SE SI SK S
TR

2021-11-17

REG

Reference to a national code

Ref country code: GB
Ref legal event code: FG4D

2021-12-02

REG

Reference to a national code

Ref country code: DE
Ref legal event code: R096
Ref document number:
602020000990
Country of ref document: D

2021-12-08

REG

Reference to a national code

Ref country code: IE
Ref legal event code: FG4D

2021-12-15

REG

Reference to a national code

Ref country code: AT
Ref legal event code: REF
Ref document number:
1448652
Country of ref document: A
Kind code of ref document
Effective date: 20211215

2022-02-02

REG

Reference to a national code

Ref country code: NL
Ref legal event code: FP

2022-03-10

REG

Reference to a national code

Ref country code: LT
Ref legal event code: MG9

2022-04-15

REG

Reference to a national code

Ref country code: AT
Ref legal event code: MK05
Ref document number:
1448652

https://patents.google.com/patent/EP3726389B1/en?q=(high+radix)&oq=high+radix&page=1

71/76

2/14/26, 2:00 PM

EP3726389B1 - Apparatuses, methods, and systems for memory interface circuit allocation in a configurable spatial accelera…
Country of ref document: A
Kind code of ref document
Effective date: 20211117

2022-04-29

PG25

Lapsed in a contracting state [announced via postgrant information from national office to epo]

Ref country code: RS
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20211117
Ref country code: LT
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20211117
Ref country code: FI
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20211117
Ref country code: BG
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20220217
Ref country code: AT
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20211117

2022-06-01

PG25

Lapsed in a contracting state [announced via postgrant information from national office to epo]

Ref country code: IS
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20220317
Ref country code: SE
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20211117
Ref country code: PT
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20220317
Ref country code: PL
Free format text: LAPSE
BECAUSE OF FAILURE TO

https://patents.google.com/patent/EP3726389B1/en?q=(high+radix)&oq=high+radix&page=1

72/76

2/14/26, 2:00 PM

EP3726389B1 - Apparatuses, methods, and systems for memory interface circuit allocation in a configurable spatial accelera…
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20211117
Ref country code: NO
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20220217
Ref country code: LV
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20211117
Ref country code: HR
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20211117
Ref country code: GR
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20220218

2022-07-29

PG25

Lapsed in a contracting state [announced via postgrant information from national office to epo]

Ref country code: SM
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20211117
Ref country code: SK
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20211117
Ref country code: RO
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20211117
Ref country code: ES
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20211117
Ref country code: EE
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O

https://patents.google.com/patent/EP3726389B1/en?q=(high+radix)&oq=high+radix&page=1

73/76

2/14/26, 2:00 PM

EP3726389B1 - Apparatuses, methods, and systems for memory interface circuit allocation in a configurable spatial accelera…
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20211117
Ref country code: DK
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20211117
Ref country code: CZ
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20211117

2022-08-18

REG

Reference to a national code

Ref country code: DE
Ref legal event code: R097
Ref document number:
602020000990
Country of ref document: D

2022-09-23

PLBE

No opposition filed within time limit

Free format text: ORIGINAL
CODE: 0009261

2022-09-23

STAA

Information on the status of an ep patent application or granted ep patent

Free format text: STATUS:
NO OPPOSITION FILED
WITHIN TIME LIMIT

2022-09-30

PG25

Lapsed in a contracting state [announced via postgrant information from national office to epo]

Ref country code: MC
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20211117

2022-10-24

REG

Reference to a national code

Ref country code: BE
Ref legal event code: MM
Effective date: 20220228

2022-10-26

26N

No opposition filed

Effective date: 20220818

2022-10-31

PG25

Lapsed in a contracting state [announced via postgrant information from national office to epo]

Ref country code: LU
Free format text: LAPSE
BECAUSE OF NON-PAYMEN
OF DUE FEES
Effective date: 20220218
Ref country code: AL
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20211117

2022-11-30

PG25

Lapsed in a contracting state [announced via postgrant information from national office to epo]

Ref country code: SI
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT

https://patents.google.com/patent/EP3726389B1/en?q=(high+radix)&oq=high+radix&page=1

74/76

2/14/26, 2:00 PM

EP3726389B1 - Apparatuses, methods, and systems for memory interface circuit allocation in a configurable spatial accelera…
Effective date: 20211117

2023-01-31

PG25

Lapsed in a contracting state [announced via postgrant information from national office to epo]

Ref country code: IE
Free format text: LAPSE
BECAUSE OF NON-PAYMEN
OF DUE FEES
Effective date: 20220218

2023-02-28

PG25

Lapsed in a contracting state [announced via postgrant information from national office to epo]

Ref country code: BE
Free format text: LAPSE
BECAUSE OF NON-PAYMEN
OF DUE FEES
Effective date: 20220228

2023-05-31

PG25

Lapsed in a contracting state [announced via postgrant information from national office to epo]

Ref country code: IT
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20211117

2023-06-21

P01

Opt-out of the competence of the unified patent court (upc) registered

Effective date: 20230518

2023-09-29

REG

Reference to a national code

Ref country code: CH
Ref legal event code: PL

2023-10-31

PG25

Lapsed in a contracting state [announced via postgrant information from national office to epo]

Ref country code: LI
Free format text: LAPSE
BECAUSE OF NON-PAYMEN
OF DUE FEES
Effective date: 20230228
Ref country code: CH
Free format text: LAPSE
BECAUSE OF NON-PAYMEN
OF DUE FEES
Effective date: 20230228

2024-04-30

PG25

Lapsed in a contracting state [announced via postgrant information from national office to epo]

Ref country code: MK
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20211117
Ref country code: CY
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20211117

2024-05-31

PG25

Lapsed in a contracting state [announced via postgrant information from national office to epo]

Ref country code: HU
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT;
INVALID AB INITIO
Effective date: 20200218

2024-09-30

PG25

Lapsed in a contracting state [announced via postgrant information from national office to epo]

Ref country code: MT
Free format text: LAPSE
BECAUSE OF FAILURE TO

https://patents.google.com/patent/EP3726389B1/en?q=(high+radix)&oq=high+radix&page=1

75/76

2/14/26, 2:00 PM

EP3726389B1 - Apparatuses, methods, and systems for memory interface circuit allocation in a configurable spatial accelera…
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20211117

2025-01-16

PGFP

Annual fee paid to national office [announced via postgrant information from national office to epo]

Ref country code: NL
Payment date: 20241203
Year of fee payment: 6

2025-01-20

PGFP

Annual fee paid to national office [announced via postgrant information from national office to epo]

Ref country code: FR
Payment date: 20241129
Year of fee payment: 6

2025-04-11

PGFP

Annual fee paid to national office [announced via postgrant information from national office to epo]

Ref country code: DE
Payment date: 20250121
Year of fee payment: 6

2025-04-28

PGFP

Annual fee paid to national office [announced via postgrant information from national office to epo]

Ref country code: GB
Payment date: 20250116
Year of fee payment: 6

2025-12-12

PG25

Lapsed in a contracting state [announced via postgrant information from national office to epo]

Ref country code: TR
Free format text: LAPSE
BECAUSE OF FAILURE TO
SUBMIT A TRANSLATION O
THE DESCRIPTION OR TO
PAY THE FEE WITHIN THE
PRESCRIBED TIME-LIMIT
Effective date: 20211117

Data provided by IFI CLAIMS Patent Services

About

Send Feedback

Public Datasets

Terms

Privacy Policy

https://patents.google.com/patent/EP3726389B1/en?q=(high+radix)&oq=high+radix&page=1

Help

76/76

2/14/26, 1:56 PM

EP4042270B1 - Hardware algorithm for complex-valued exponentiation and logarithm using simplified sub-steps - Google P…

Patents
Back to results

6 of 125,048

high radix

(high radix);

Hardware algorithm for complex-valued exponentiation and logarithm using simplified sub-steps
Images (4)

EP4042270B1
European Patent Office

Download PDF

Find Prior Art

Similar

Other languages: German, French
Inventor: Benjamin John Oliver LONG
Current Assignee : Ultraleap Ltd

Classifications
G06F7/5446 Methods or arrangements for performing computations using exclusively
denominational number representation, e.g. using binary, ternary, decimal representation using
non-contact-making devices, e.g. tube, solid state device; using unspecified devices for evaluating

Worldwide applications
2020 WO EP US 2024 US

functions by calculation using crossaddition algorithms, e.g. CORDIC
G06F7/57 Arithmetic logic units [ALU], i.e. arrangements or devices for performing two or
more of the operations covered by groups G06F7/483 – G06F7/556 or for performing logical

Application EP20793109.8A events
2020-10-13

Application filed by Ultraleap Ltd

2022-08-17

Publication of EP4042270A1

2025-03-19

Application granted

2025-03-19

Publication of EP4042270B1

2025-03-19

Publication of EP4042270C0

G06F7/523 Multiplying only

Status

Active

G06F7/556 Logarithmic or exponential functions

2040-10-13

Anticipated expiration

operations
G06F5/01 Methods or arrangements for data conversion without changing the order or
content of the data handled for shifting, e.g. justifying, scaling, normalising
G06F7/4818 Computations with complex numbers using coordinate rotation digital computer
[CORDIC]

Hide more classifications
Info: Cited by (22), Legal events, Similar documents, Priority and
Related Applications

Landscapes

External links: Espacenet, EPO GPI, EP Register, Global Dossier,

Engineering & Computer Science

Discuss

Physics & Mathematics
Show more

Hide Dependent

Claims (8)

1.

A system comprising:
a hardware component having at least one input and at least one output;
wherein the hardware component implements a switchable complex-valued unit having a to-logarithm functionality and a to-exponential functionality;
wherein the at least one input and the at least one output are complex valued;
wherein shift-and-add processes corresponding to multiplications by a value 1+2-ndn are applied to values in the hardware component that effect a separable
multiplication of: i) the at least one input; ii) (1+c); and (1+di);
wherein "c" is a real value and "di" is an imaginary value,
characterised in that
dn is a selected from the group: 0, -1, +1, -i, +i, -1-i+2-n i, -1+i+2-n i, +1+i+2-n i, +1-i+2-n i.

2.

A system comprising:
a hardware component having at least one input and at least one output;

https://patents.google.com/patent/EP4042270B1/en?q=(high+radix)&oq=high+radix

1/37

2/14/26, 1:56 PM

EP4042270B1 - Hardware algorithm for complex-valued exponentiation and logarithm using simplified sub-steps - Google P…

wherein the hardware component implements a switchable complex-valued unit having a to-logarithm functionality and a to-exponential functionality;
wherein the at least one input and the at least one output are complex valued;
wherein shift-and-add processes corresponding to multiplications by a value 1+2-ndn are applied to values in the hardware component that effect a separable
multiplication of: i) the at least one input; ii) (1+c); and (1+di);
wherein "c" is a real value and "di" is an imaginary value,
characterised in that dn is selected from the group -1-i+2-n i, -1+i-2 -n i, 1+i+2-n i, +1-i+2-n i.
3.

The system of claim 1 or 2, wherein a logarithm process implemented by the unit is an affine logarithm process; and wherein an exponential processes
implemented by the unit are an affine exponential process.

4.

The system of claim 3, wherein the relation:
2 tan

−1

−𝑝

2

≈

𝜋

1
2

−𝑝

log 1 + 2
2

,

is used to approximate a lookup value of an arctangent expression as an existing lookup value of a binary logarithm expression and a smaller delta table.
5.

The system of claim 3, wherein an imaginary part of the input in the affine logarithm process is tested and, if less than -½ or greater than or equal to +½, rotated by
45 degrees prior to an iteration.

6.

The system of claim 1 or 2, wherein the to-logarithm functionality conducts an iteration test on a value that is an existing value subtracted by +1.

7.

The system of claim 1 or 2, wherein the to- exponential functionality conducts an iteration test on a value that is an existing value subtracted by +1.

8.

The system of any preceding claim, wherein "c" is a non-zero value and "di" is a non-zero value.

Description

FIELD OF THE DISCLOSURE
[0001] The present disclosure relates generally to developing and applying hardware algorithms for complex-valued exponentiation and logarithm using simplified substeps.
BACKGROUND
[0002] The BKM algorithm is a shift-and-add algorithm for computing elementary functions, first published in 1994 by Jean-Claude Bajard, Sylvanus Kla, and JeanMichel Muller. BKM is based on computing complex logarithms (L-mode) and exponentials (E-mode) using a method similar to the algorithm Henry Briggs used
to compute logarithms. By using a precomputed table of logarithms of negative powers of two, the BKM algorithm computes elementary functions using only
integer add, shift, and compare operations.
[0003] BKM is similar to CORDIC but uses a table of logarithms rather than a table of arctangents. On each iteration, a choice of coefficient is made from a set of nine
complex numbers, 1, 0, -1, i, -i, 1+i, 1-i, -1+i, -1-i, rather than only -1 or +1 as used by CORDIC. BKM provides a simpler method of computing some elementary
functions, and unlike CORDIC, BKM needs no result scaling factor. The convergence rate of BKM is approximately one bit per iteration, like CORDIC, but BKM
requires more precomputed table elements for the same precision because the table stores logarithms of complex operands.
[0004] As with other algorithms in the shift-and-add class, BKM is particularly well-suited to hardware implementation. The relative performance of software BKM
implementation in comparison to other methods such as polynomial or rational approximations will depend on the availability of fast multi-bit shifts (i.e. a barrel
shifter) or hardware floating point arithmetic.
[0005] Previously disclosed was an approach to recast the complex exponentiation and logarithm problem from the classical manipulation r + iθ ↔ er+iθ using the BKM
algorithm, to the manipulation 𝑟 + iθ ↔ 2 𝑒
𝑟

𝜋 iθ
2

using a revised algorithm which shall be referred to as the BKML algorithm. This revised BKML algorithm takes the form of two algorithms, each the reverse of
the other, one to compute:
𝑟

𝜋 iθ

𝑓𝑟 + iθ = 2 𝑒 2

,

as well as its inverse:
𝑓

−1

𝑟 + iθ =

arg𝑟 + iθ
𝜋/2

𝑖 + log 𝑟 + iθ,
2

wherein the real part of the logarithm has a base of 2 and the imaginary part has a base of 𝑒

𝜋
2

.
[0006] With some modifications, this can apply to any power-of-two base for the real part, and any power-of-two multiplied by pi and exponentiated for the base of the
imaginary part. As the real and imaginary part of the process has a different base, the mathematical part of the process described that was implemented was
novel and was not named initially. In this document, it shall be described and claimed as an 'affine logarithm' or 'affine exponential'. The process shall be
described as an 'affine logarithm process' or 'exponential-to-logarithm process' and an 'affine exponential process' or 'logarithm-to-exponential process'
interchangeably. This is proceeded by a series of n steps choosing a value dn for each in turn, where:
𝑑𝑛 ∈ 0, + 1, + 𝑖, − 1, + 𝑖, − 1 − 𝑖, − 1 + 𝑖, + 1 − 𝑖, + 1 + 𝑖,

and on the first of the complex values, we multiply by:
−𝑛

1+2

𝑑𝑛 ,

https://patents.google.com/patent/EP4042270B1/en?q=(high+radix)&oq=high+radix

2/37

2/14/26, 1:56 PM

EP4042270B1 - Hardware algorithm for complex-valued exponentiation and logarithm using simplified sub-steps - Google P…

and use the logarithm of this value precomputed in a table to attenuate the second complex value.
[0007] Through repeated choices of dn over n iterations, the iteration causes the first value to converge to the exponential and the second value converge to zero. The
reverse operation is also possible using much the same process allowing for much the same hardware to be run in a logarithm or exponentiation 'mode'. Without
loss of generality, due to the structure of the set from which dn is chosen, the storage cost of table is the number of bits to compute, say N, multiplied by the
number of symmetries (usually the total number of non-zero choices of dn ) here eight (if the existing algorithm is expanded fully, this is five real lookups and
three imaginary ones). This is a lookup size of 8N over each of N stages, yielding 8N2 bits dedicated to lookup tables when an implementation of the BKML
algorithm is used.
[0008] A further reason that the use of the prior art BKM algorithm is not well known and in widespread use is because the tabulated values take up a large amount of
room in a silicon implementation that could be dedicated to other tasks. This is a weakness shared by the BKML implementation of the revised algorithm
computing 𝑟 + iθ ↔ 2 𝑒
𝑟

𝜋 iθ
2

disclosed previously.
[0009] While it is difficult to determine precisely, it is quite possible that if eight lookup tables are necessary, computing the real and imaginary parts of the logarithm
separately without using the combined iteration demonstrated by the BKM algorithm and the previously disclosed BKML algorithm will in many cases be more
efficient with respect to hardware logic complexity and area - a drawback shared by the original BKM algorithm.
[0010] The requirement for eight lookup tables is considered to be due to the difficulty in achieving convergence in its classical r + iθ ↔ er+iθ form when the BKM
algorithm was conceived by its authors. However, it is shown that with the change of base actioned when the BKML algorithm was invented previously, a new
algorithm may be created that can overcome the need for eight look-up tables by requiring less stringent convergence criteria and therefore may be defined to
need fewer resources.
[0011] In practice, in the process of looking for a form of the algorithm that requires fewer look-up tables, methods were found that may be applied even to the BKM
algorithm.
SUMMARY
[0012] A method of generating complex exponentiation and logarithms in hardware is described that uses half the number of bits of lookup tables as the state-of-theart. By splitting up each of the iterations into more simplified stages or using more iterations, the amount of precomputed information that must be held by the
circuitry is reduced. This allows synthesis tools to take this more succinct logical description of the algorithm and make it into efficient gate level logic for
fabrication into more compact integrated circuitry.
BRIEF DESCRIPTION OF THE DRAWINGS
[0013] The accompanying figures, where like reference numerals refer to identical or functionally similar elements throughout the separate views, together with the
detailed description below, are incorporated in and form part of the specification, serve to further illustrate embodiments of concepts that include the claimed
invention and explain various principles and advantages of those embodiments.
Figures 1A and 1B show illustration of errors in the complex plane in the logarithm-to-exponential process (floating-point implementation) for the
BKML4m iteration.
Figures 2A and 2B show illustrations of errors in the complex plane in the exponential-to-logarithm process (floating-point implementation) for the
BKML4m process.
Figures 3A and 3B show illustrations of errors in the complex plane in the logarithm-to-exponential process (floating-point implementation) for the
BKML3dm iteration prior to the inclusion of the reduced entropy tables.
Figures 4A and 4B show illustration of errors in the complex plane in the exponential-to-logarithm process (floating-point implementation) for the
BKML3dm prior to the inclusion of the reduced entropy tables.
[0014] Skilled artisans will appreciate that elements in the figures are illustrated for simplicity and clarity and have not necessarily been drawn to scale. For example,
the dimensions of some of the elements in the figures may be exaggerated relative to other elements to help to improve understanding of embodiments of the
present invention.
[0015] The apparatus and method components have been represented where appropriate by conventional symbols in the drawings, showing only those specific details
that are pertinent to understanding the embodiments of the present invention so as not to obscure the disclosure with details that will be readily apparent to
those of ordinary skill in the art having the benefit of the description herein.
DETAILED DESCRIPTION
[0016] This disclosure describes the orthogonalization of the sub-steps in real and imaginary parts to achieve a reduction in the number of lookup tables required for
the algorithm and simplifications in the iterative procedure. Applying the orthogonalization to the previously disclosed BKML algorithm results in two algorithms.
The first algorithm is more effective when low radix methods are considered, so when throughput and area are prioritized over latency (suitable for
implementation in FPGA technologies). The second algorithm is more effective when high radix methods are considered, so when throughput and latency are
prioritized over area (suitable for implementation into an application-specific integrated circuit (ASIC) or as an extended capability for a central processing unit
(CPU) design).
[0017] The first, denoted BKML4m, requires four lookup values (which with some rewiring may be reduced to effectively three-and-a-half) per bit of result and chooses

dn in a similar way to BKML from nine candidates but with notable changes in the candidate set of dn drawn from. The BKML4m algorithm requires no extra
iterations over the extant previously disclosed BKML algorithm, requiring N radix-2 iterations to converge.
[0018] The second, denoted BKML3dm, requires three lookup values per bit of result, has a simplified method for choosing dn from four candidates, essentially
eliminating zero and on-axis dn choices. BKML3dm requires some extra iterations to achieve convergence, taking approximately N + log N radix-2 iterations to
converge. These extra steps are a problem at low radices, but the simplified choice mechanisms and reduced candidate pool means that this technique may be
readily extended to very high radices, necessary for designing high-speed modern hardware. This is especially true since dealing with propagation delays makes
arithmetic that need only be synchronized and resolved at key points in the algorithm valuable. Due to this, decision making based on fully resolved result values
must be minimized, meaning that when this process generates multiple bits of result per decision step (has high radix) it is particularly effective at reducing
latencies.
[0019] The value N takes is for brevity both the number of fraction bits and the number iterations of the method, without loss of generality. While these two properties
can take different values, algorithms including such definitions are often of reduced effectiveness, involve trivial changes to the method and are thus are
effectively included in the scope of this disclosure.
I. Optional multiplication and division
[0020] The exponentiation mode iteration for the method described may also be modified to provide a complex multiplication with the exponential value. If this is to be
achieved, this must be pre-loaded before the range reduction steps if the output is to be correct. It should also be noted that this would replace the output
exponentiation value and so should be not used if this value is required. It is also feasible to store and wait to apply the solution from the integer parts of the

https://patents.google.com/patent/EP4042270B1/en?q=(high+radix)&oq=high+radix

3/37

2/14/26, 1:56 PM

EP4042270B1 - Hardware algorithm for complex-valued exponentiation and logarithm using simplified sub-steps - Google P…

logarithm (the value z integer, output) to the end of the process. This may reduce the storage required for the intermediate registers in which the processing
occurs, although this should be weighed against the extra requirements of storage needed for the integer parts of the solution.
[0021] Alternatively, multiplication with the final exponentiation value may be achieved in parallel by creating extra registers and ensuring that equivalent operations
occur in these extra registers. In this way, the exponentiation process may complex multiply the output exponential value with almost arbitrarily many other
complex values with parallel hardware.
[0022] The logarithm mode iteration described may also be modified to provide a complex division with the input value. If this is to be achieved, it can only occur in
parallel by creating extra registers and ensuring that equivalent operations occur in these extra registers. This contrasts with the auxiliary multiplication, where
the original register could be overloaded, which cannot be achieved here because the modification of the exponentiation register in this mode would prevent
convergence of the algorithm. However, using auxiliary registers can circumvent this, allowing the logarithm process to, if desired, produce complex-valued
division of almost arbitrarily many other numerator complex values with the value input to this process as denominator.
II. Range reduction
[0023] More efficient range reduction was one of the primary motivations for the previously disclosed BKML algorithm. This is preserved as an integral part of the
algorithm in the presented reduced resource version in this disclosure. In the logarithm-to-exponential iteration, the integer real part of the logarithm input
denotes the bit shift applied to either the output registers at the end of the process or initialization of the output registers to a power-of-two at the beginning of
the process. The integer imaginary part of the input logarithm, due to the base of 𝑒

𝜋
2

, denotes the quadrant (aligned to the axes) of the complex plane in which the resulting exponentiation result must lie. In the exponential-to-logarithm process
the reverse is mostly true. The quadrant (aligned to the diagonals) of the complex plane is determined through testing the sign bits and absolute value of the real
and imaginary components to give the integer part of the imaginary logarithm. By permuting signs of the real and imaginary parts and potentially swapping
them, this rotation can be removed to yield a real part that is guaranteed to be positive and larger than the imaginary part. Counting leading zeroes of this real
and larger part allows the integer part of the logarithm to be substantially determined. This substantial determination may be removed by bit shifting both
components, such that the remaining portion of the real logarithm may be obtained via the iteration.
[0024] It may also be desirable to keep the integer portion determined by the range reduction step separate from the calculation for as long as possible. This allows the
method to perform complex logarithm to floating-point complex exponential conversions that are highly useful in the context of wave physics applications. To
achieve a true conversion to a standard floating-point type, the fractional part of the exponentiation may be tested to determine whether the result is too large or
small for the mantissa to fit into a particular format, depending on the region of convergence decided upon by the reduced range algorithm. This is necessary
because only the larger real part is tested to determine whether the value lies within the convergent region of the complex plane and the size of imaginary part is
untested at this time but must be in a known range of values. A final test on the real part of the exponential mantissa and an increment or decrement on the
exponential integer exponent then finalizes the representation ready for storage into the floating-point format. In the implementation described here, the complex
value may be up to

√
2

in size (thus in the interval 0.5

√
2

), which would if greater or equal to 1 require a divide-by-two and exponent increment to place into the region [0.5, 1) in which the integer part of the exponent is
completely described by the exponent of the floating-point value.
III. Multiplicative iterations in the complex plane
[0025] Noticing that each iteration requires that we multiply the running product by:
−𝑛

1+2

𝑑𝑛 ,

if we choose the real part of dn separately from the imaginary part we can choose:
𝑑

𝑛, ℜ

∈ 0, − 1, + 1,

𝑑𝑛, ℑ ∈ 0, − 𝑖, + 𝑖 .

The iteration may be modified to perform the running product multiplied by the further product:
−𝑛

1+2

−𝑛

𝑑𝑛, ℜ 1 + 2

−𝑛

𝑑𝑛, ℑ = 1 + 2
−𝑛

= 1+2

−2𝑛

𝑑𝑛, ℜ + 𝑑𝑛, ℑ + 2
−𝑛

𝑑𝑛, ℜ + 𝑑𝑛, ℑ + 2

𝑑𝑛, ℜ 𝑑𝑛, ℑ ,

𝑑𝑛, ℜ 𝑑𝑛, ℑ ,

on each iteration. As a result, the diagonal dn which have both a real and an imaginary part have an extra factor of 2

−2𝑛

𝑑𝑛, ℜ 𝑑𝑛, ℑ

which results in an extra shift (by 2n bit places) and add requirement for these schemes in the exponential part and a potential extra subtraction in the logarithm
part. As using this scheme allows the number of lookup tables to be reduced from eight to four in the worst case, the extra shift and add requirement is more
than compensated for as the four extra tables can be dropped as will be demonstrated.
IV. The BKML4m variant
[0026] Using dn as highlighted in the previous section but keeping the structure of the algorithm mostly the same leads us to a similar algorithm to that disclosed
previously but with a slightly different choice of dn due to the cross terms. This can be written out as an effectively expanded table for a general dn:
0,
−1,
+1,
−𝑖,
𝑑

𝑛

∈

+𝑖,

,
−𝑛

−1 − 𝑖 + 2

−𝑛

−1 + 𝑖 − 2

−𝑛

+1 + 𝑖 + 2

−𝑛

+1 − 𝑖 − 2

𝑖,
𝑖,
𝑖,
𝑖,

https://patents.google.com/patent/EP4042270B1/en?q=(high+radix)&oq=high+radix

4/37

2/14/26, 1:56 PM

EP4042270B1 - Hardware algorithm for complex-valued exponentiation and logarithm using simplified sub-steps - Google P…

[0027] Since the extra 2-n terms in d are n bit places away from the bit currently under scrutiny at any given time, while these extra terms need to be accounted for, they
n
only negligibly affect the convergence of the method. For the most part, this then converges in almost the same way as the original revised method in previous
disclosures (although the previous method would necessarily have the disadvantage of requiring eight lookup tables). The changes to the choice of dn amount
to an extra shift-and-add in the product of the exponentials and an extra addition in the summation of the logarithms per iteration.
V. Table lookup construction
[0028] When considering the logarithm portion of each iterative method (both logarithm-to-exponential and exponential-to-logarithm), it can be shown that only four
lookup tables containing the bit patterns of the logarithms need be constructed. These are:
−𝑛

𝑙𝑛, ℜ + = log 1 + 2
2

−𝑛

𝑙𝑛, ℜ − = log 1 − 2
2

𝑙

−𝑛

𝑛, ℜd

= log 1 ± 2
2

𝑙

𝑛, ℑ

=

2

𝑖 = log √1
2

2
𝜋

−𝑛

arg1 + 2

,
,

−𝑛 2

+ ±2

𝑖 =

2
𝜋

tan

−1

=

1
2

−2𝑛

log 1 + 2

−𝑛

2

2

,

,

[0029] Of these four, it is also possible to reduce it to effectively three and a half via the observation:
𝑙𝑛, ℜd =

1
2

𝑙2𝑛, ℜ + ,

where the preceding factor of a half may be a bit shift. By reusing table entries for 𝑙𝑛, ℜ +

and extending to 𝑙2𝑛, ℜ +

only for even values (or producing a table of 𝑙𝑛, ℜd

only for even values after the other table has been exhausted) the remainder may be filled by using only half a table.
[0030] Then the logarithms to use for the addition/subtraction portion will be:

These are then added to the running total of the logarithm upon whose upper bits the decision as to the direction to take is chosen for the next iteration.
VI. BKML4m: Exponentiation mode iteration
[0031] With the mechanism using four look-up tables established, the method to achieve complex exponentiation using this approach can be described. The method
and region cut-offs for choosing each dn from the input are very similar to the revised algorithm which required eight look-up tables in the BKML algorithm
disclosed prior to this. This allows the method to not require extra iterations to be inserted, because the only difference in the convergence between the previous
revised algorithm and this is the extra 2

−2𝑛

𝑑𝑛, ℜ 𝑑𝑛, ℑ

term, which has much less effect than the other terms in the expansion of the multiplication step.
[0032] Alternatively, multiplication with the final exponentiation value may be achieved in parallel by creating extra registers and ensuring that equivalent operations
occur in these extra registers, as described in previous sections. In this way, the exponentiation process may complex multiply the output exponential value with
almost arbitrarily many other complex values.
[0033] Assuming the fractional part of the input logarithms to be the input, the algorithm for the domain of convergence z input ∈ R = [-0.5, +0.5) + i[-0.5, +0.5) is:
1. Assuming there are four basic registers, labelled ℜ𝑧log , ℑ𝑧log , ℜ𝑧exp

and ℑ𝑧exp

https://patents.google.com/patent/EP4042270B1/en?q=(high+radix)&oq=high+radix

5/37

2/14/26, 1:56 PM

EP4042270B1 - Hardware algorithm for complex-valued exponentiation and logarithm using simplified sub-steps - Google P…

. Alongside, there are two extra slave multiplication registers ℜ′𝑧exp

and ℑ′𝑧exp

to demonstrate how the method operates when used for auxiliary complex multiplication. The initial values of these registers are:
ℜ𝑧log : = ℜ𝑧input ,

ℑ𝑧

log

: = ℑ𝑧

input

,

ℜ𝑧exp : = ℜ𝑧integer input,

output

ℑ𝑧exp : = ℑ𝑧integer input,

output

× 𝑧premultiply ,

× 𝑧premultiply ,

where z premultiply := 1.0, if there are no requirements for pre-multiplication. The slave multiplication registers may also be similarly constructed with:
ℜ𝑧′exp : = ℜ𝑧integer input,

ℑ𝑧′exp : = ℑ𝑧integer input,

output

output

× 𝑧′premultiply ,

× 𝑧′premultiply .

2. Iterate through the values 1, ··· , N - 1 as the index n:
3. Shift right by N - n and then truncate ℜ𝑧log

to form ℜ𝑧log, test

such that it has three bits; one sign bit and two integer bits in two's complement such that the range is [-4.0, +4.0) with the smallest change being 1. The
multiplication of this value by 2-n is implied by the initial shift.
4. Shift right by N - (n + 1) and then truncate ℑ𝑧log

to form ℑ𝑧log, test

such that it has three bits; one sign bit, one integer bit and one fraction bit in two's complement such that the range is [-2.0, +2.0) with the smallest change
being 0.5. The multiplication of this value by 2-n is implied by the initial shift.
5. Test the 3-bit values to determine dn :

ℜ𝑑𝑛 : =

ℑ𝑑𝑛 : =

−1,

if ℜ𝑧log,

test

< 3′sb111 or < − 1.0,

+1,

if ℜ𝑧log,

test

> 3′sb000 or ≥ + 1.0,

0,

if neither,

−𝑖,

if ℑ𝑧log,

test

< 3′sb111 or < − 0.5,

+𝑖,

if ℑ𝑧log,

test

> 3′sb000 or ≥ + 0.5,

0,

if neither .

6. Apply the shift-and-add process effecting the multiplication of the 2-n terms to the exponential registers:
ℜ𝑧exp,
−𝑛

−2

−𝑛

+ +2

ℜ𝑧exp,
ℜ𝑧

exp,

n−1

n−1

n

: = ℜ𝑧exp,

= − sraℜ𝑧exp,
= + sraℜ𝑧
0,

−𝑛

+2

−𝑛

+ −2

exp,

n−1

n−1

n−1

+

, 𝑛,

if ℜ𝑑𝑛 = − 1,

, 𝑛,

if ℜ𝑑

𝑛

= + 1,

if neither,

ℑ𝑧exp,

n−1

= + sraℑ𝑧exp,

n−1

, 𝑛,

if ℑ𝑑𝑛 = − 𝑖,

ℑ𝑧exp,

n−1

= − sraℑ𝑧exp,

n−1

, 𝑛,

if ℑ𝑑𝑛 = + 𝑖,

0,

if neither .

https://patents.google.com/patent/EP4042270B1/en?q=(high+radix)&oq=high+radix

6/37

2/14/26, 1:56 PM

EP4042270B1 - Hardware algorithm for complex-valued exponentiation and logarithm using simplified sub-steps - Google P…

And:
ℑ𝑧exp,
−𝑛

−2

−𝑛

+ +2

ℑ𝑧exp,
ℑ𝑧

exp,

n−1

n−1

n

: = ℑ𝑧exp,

= − sraℑ𝑧exp,
= + sraℑ𝑧
0,

−𝑛

−2

−𝑛

+ +2

n−1

n−1

exp,

n−1

+

, 𝑛,

if ℜ𝑑𝑛 = − 1,

, 𝑛,

if ℜ𝑑

= + 1,

𝑛

if neither,

ℜ𝑧exp,

n−1

= − sraℜ𝑧exp,

n−1

, 𝑛,

if ℑ𝑑𝑛 = − 𝑖,

ℜ𝑧exp,

n−1

= + sraℜ𝑧exp,

n−1

, 𝑛,

if ℑ𝑑𝑛 = + 𝑖,

0,

if neither .

Do the same to any auxiliary registers such as ℜ𝑧′exp, n

and ℑ𝑧′exp, n

to apply the multiplication process to these also.
7. Apply the shift-and-add process effecting the multiplication of the 2-2n term to the exponential registers. As this is the cross-term of a real and
imaginary part, it is guaranteed imaginary, so it has a more limited set of possible manifestations.
If ℜ𝑑𝑛 = − 1

and ℜ𝑑𝑛 = + 1

and ℑ𝑑𝑛 = − 𝑖

then 2−2𝑛 𝑑𝑛, ℜ 𝑑𝑛, ℑ = − 2−2𝑛 𝑖

:
ℜ𝑧exp,

ℑ𝑧

n

: = ℜ𝑧exp,

: = ℑ𝑧

exp,

n

ℜ𝑧exp,

n

exp,

n

n

+ sraℑ𝑧exp,

− sraℜ𝑧

n−1

, 2𝑛,

, 2𝑛,

exp,

n−1

− sraℑ𝑧exp,

n−1

Whereas if ℜ𝑑𝑛 = + 1

and ℑ𝑑𝑛 = + 𝑖

or ℜ𝑑𝑛 = − 1

and ℑ𝑑𝑛 =

-i then = +2 -2ni:

ℑ𝑧

exp,

n

: = ℜ𝑧exp,

: = ℑ𝑧

exp,

n

n

+ sraℜ𝑧

exp,

n−1

, 2𝑛,

, 2𝑛,

wherein the signs are reversed in the latter case.
Do the same to any auxiliary registers such as ℜ𝑧′exp, n

and ℑ𝑧′exp, n

to apply the multiplication process to these also.
8. Subtract the corresponding entry in the logarithm tables from the registers:

https://patents.google.com/patent/EP4042270B1/en?q=(high+radix)&oq=high+radix

7/37

2/14/26, 1:56 PM

EP4042270B1 - Hardware algorithm for complex-valued exponentiation and logarithm using simplified sub-steps - Google P…

This is achieved using the look-up table constructions described in the previous section by
.
9. Return to step 2 for the next iteration, until N is reached, at which point the registers will contain their final values:
ℜ𝑧input

ℜ𝑧exp,

N

: = ℜ𝑧integer input,

output

ℑ𝑧exp,

N

: = ℑ𝑧integer input,

output

ℜ′𝑧exp,

N

: = ℜ𝑧integer input,

× 𝑧premultiply × 2

ℜ𝑧

× 𝑧premultiply × 2

input

𝜋 iℑ𝑧

𝑒2

𝜋 iℑ𝑧

𝑒2

input

input

,

,

And:

ℑ′𝑧

exp,

N

: = ℑ𝑧

integer input,

ℜ𝑧

output

output

× 𝑧premultiply′ × 2

×𝑧

input

ℜ𝑧input

premultiply′

×2

𝜋 iℑ𝑧

𝑒2

𝜋 iℑ𝑧

𝑒2

input

input

,

.

[0034] Having appreciated the form of the process, it is easy to find other testing procedures that are convergent, even sometimes in the required domain, by forming
ℜ𝑧log,

test

, ℑ𝑧log, test

) or both using different number of bits or different comparison values, although we have endeavored to reduce complexity by specifying the required value tests
in the simplest known form.
[0035] An illustration of the application of this procedure to values zinput ∈ R = [-2. 0, +2.0) + i[-2.0, +2.0) is shown in Figures 1A and 1B.
[0036] Figures 1A and 1B show illustration of errors in the complex plane in the logarithm-to-exponential process (floating-point implementation) for the BKML4m
iteration. These figures correspond to illustrative tests of the iterative scheme in floating-point and with no range reduction steps to demonstrate the
mathematical viability of the dynamical system rather than being a faithful reproduction of the iterative procedures outlined.
[0037] In Figure 1A, shown is a simulation 1100A where the x-axis 1120A is real, and the y-axis 1110A is imaginary in the input to the algorithm. Shading denotes the
error (where a black shading 1130A implies the error is linearly related to the bits and iterations of the algorithm).
[0038] In Figure 1B, shown is a simulation 1100B where the x-axis 1120B is real, and the y-axis 1110B is imaginary in the input to the algorithm. The white square 1130B
is constructed by inverting the color and denotes the portion of the domain (z_"input" " R"=[ -0.5┤, ├ +0.5)+i[-0.5┤, ├ +0.5)) which is required to be convergent.
Therefore, if the algorithm functions in this zone, it is expected for the region to be shaded solid white.
VII. BKML4m: Logarithm mode iteration
[0039] The logarithm mode described in this section may also be modified to provide a complex division with the input value. If this is to be achieved, it can only occur
in parallel by creating extra registers and ensuring that equivalent operations occur in these extra registers. This contrasts with the auxiliary multiplication, where
the original register could be overloaded, which cannot be achieved here because the modification of the exponentiation register in this mode would prevent
convergence of the algorithm. However, using auxiliary registers can circumvent this by mirroring operations, allowing the logarithm process to, if desired,
produce complex-valued division of almost arbitrarily many other complex values with the value input to this process as denominator.
[0040] Assuming the fractional part of the output logarithms to be the output, the algorithm for the domain of convergence z input ∈ R = [+0.5, +1.0) + 𝑖−ℜ𝑅, + ℜ𝑅

is:
1. Assuming there are four basic registers, labelled ℜ𝑧log , ℑ𝑧log , ℜ𝑧exp

and ℑ𝑧exp

. Alongside, there are two extra slave division registers ℜ𝑧′exp

and ℑ𝑧′exp

to demonstrate how the method operates when used for auxiliary complex division. The initial values of these registers are:
ℜ𝑧

log

: = ℜ𝑧

integer output,

output

ℑ𝑧log : = ℑ𝑧integer output,

output

https://patents.google.com/patent/EP4042270B1/en?q=(high+radix)&oq=high+radix

,

,

8/37

2/14/26, 1:56 PM

EP4042270B1 - Hardware algorithm for complex-valued exponentiation and logarithm using simplified sub-steps - Google P…
ℜ𝑧exp : = ℜ𝑧input − 1.0,

ℑ𝑧exp : = ℑ𝑧input ,

The slave division registers may also be similarly constructed with:
ℜ′𝑧exp : = ℜ𝑧numerator′ ÷ 𝑧integer output,

ℑ′𝑧exp : = ℑ𝑧numerator′ ÷ 𝑧integer output,

input

input

,

.

Noting that the -1.0 is not applied to the registers ℜ𝑧′exp

and ℑ𝑧′exp

.
2. Iterate through the values 0, ··· , N - 1 as the index n:
3. Shift right by N - (n + 3) and then truncate ℜ𝑧exp

to form ℜ𝑧exp, test

) such that it has six bits; one sign bit, two integer bits and three fraction bits in two's complement such that the range is [-4.0, +4.0) with the smallest
change being 0.125. The multiplication of this value by 2-n is implied by the initial shift.
4. Shift right by N - (n + 1) and then truncate ℑ𝑧exp

to form ℑ𝑧exp, test

such that it has four bits; one sign bit, two integer bits and one fraction bits in two's complement such that the range is [-4.0, +4.0) with the smallest
change being 0.5. The multiplication of this value by 2-n is implied by the initial shift.
5. Test the two values to determine dn :

ℜ𝑑𝑛 : =

+1,

if ℜ𝑧exp,

test

< 6′sb111011 or < − 0.625,

−1,

if ℜ𝑧exp,

test

< 6′sb000100 or ≥ + 0.625,

0,

ℑ𝑑𝑛 : =

if neither,

+𝑖,

if ℑ𝑧exp,

test

−𝑖,

if ℑ𝑧exp,

test

< 4′sb1111 or < − 0.5,
> 4′sb0000 or ≥ + 0.5,

0,

if neither .

6. Apply the shift-and-add process effecting the multiplication of the 2-n terms to the exponential registers:
ℜ𝑧exp,
−𝑛

−2

−𝑛

+ +2

ℜ𝑧exp,
ℜ𝑧

exp,

n−1

n−1

n

−𝑛

−𝑛

+ −2

n−1

+ 1 = + sll1, 𝐹 − 𝑛 + sraℜ𝑧
0,

+2

: = ℜ𝑧exp,

+

+ 1 = − sll1, 𝐹 − 𝑛 + sraℜ𝑧exp,

ℑ𝑧

exp,

n−1

ℑ𝑧exp,

n−1

n−1

n−1

, 𝑛,

if ℜ𝑑𝑛 = − 1,

, 𝑛,

if ℜ𝑑

𝑛

= + 1,

if 𝑧zero is set,

= + sraℑ𝑧

exp,

n−1

= − sraℑ𝑧exp,

n−1

0,

exp,

, 𝑛,

if ℑ𝑑

, 𝑛,

if ℑ𝑑𝑛 = + 𝑖,

𝑛

= − 𝑖,

if 𝑧zero is set,

https://patents.google.com/patent/EP4042270B1/en?q=(high+radix)&oq=high+radix

9/37

2/14/26, 1:56 PM

EP4042270B1 - Hardware algorithm for complex-valued exponentiation and logarithm using simplified sub-steps - Google P…

And:
ℑ𝑧exp,
−𝑛

−2

−𝑛

+ +2

n

−𝑛

−2

−𝑛

+

n−1

n−1

= − sraℑ𝑧exp,

n−1

, 𝑛,

if ℜ𝑑𝑛 = − 1,

ℑ𝑧exp,

n−1

= + sraℑ𝑧exp,

n−1

, 𝑛,

if ℜ𝑑𝑛 = + 1,

0,

+ +2

: = ℑ𝑧exp,

ℑ𝑧exp,

if 𝑧

zero

is set,

ℜ𝑧exp,

n−1

+ 1 = − sll1,

𝐹 − 𝑛 + sraℜ𝑧exp,

n−1

, 𝑛,

if ℑ𝑑𝑛 = − 𝑖,

ℜ𝑧exp,

n−1

+ 1 = + sll1,

𝐹 − 𝑛 + sraℜ𝑧exp,

n−1

, 𝑛,

if ℑ𝑑𝑛 = + 𝑖,

0,

if 𝑧zero is set

Do the same to any auxiliary registers such as ℜ𝑧′exp

and ℑ𝑧′exp

to apply the division process to these. However, the register will not require the correction for the 1 in the real part so instead the procedure would be:
ℜ′𝑧exp,
−𝑛

−2

−𝑛

+ +2

n−1

+

ℜ𝑧′exp,

n−1

= − sraℜ𝑧′exp,

n−1

,

𝑛,

if ℜ𝑑𝑛 = − 1,

ℜ𝑧′exp,

n−1

= + sraℜ𝑧′exp,

n−1

,

𝑛,

if ℜ𝑑𝑛 = + 1,

n

0,
−𝑛

+2

−𝑛

+ −2

n−1

= + sraℑ𝑧′exp,

n−1

, 𝑛,

if ℑ𝑑𝑛 = − 𝑖,

ℑ𝑧′exp,

n−1

= − sraℑ𝑧′exp,

n−1

, 𝑛,

if ℑ𝑑𝑛 = + 𝑖,

ℑ′𝑧exp,
−𝑛

−𝑛

+ +2

if 𝑧zero is set .

n−1

+

ℑ𝑧′exp,

n−1

= − sraℑ𝑧′exp,

n−1

,

𝑛,

if ℜ𝑑𝑛 = − 1,

ℑ𝑧′exp,

n−1

= + sraℑ𝑧′exp,

n−1

,

𝑛,

if ℜ𝑑𝑛 = + 1,

n

0,
−𝑛

−2

−𝑛

+ +2

if 𝑧zero is set,

ℑ𝑧′exp,

0,

−2

: = ℜ′𝑧exp,

: = ℑ′𝑧exp,

if 𝑧

zero

is set,

ℜ𝑧′exp,

n−1

= − sraℜ𝑧′exp,

n−1

, 𝑛,

if ℑ𝑑𝑛 = − 𝑖,

ℜ𝑧′exp,

n−1

= + sraℜ𝑧′exp,

n−1

, 𝑛,

if ℑ𝑑𝑛 = + 𝑖,

0,

if 𝑧

zero

is set

7. Apply the shift-and-add process effecting the multiplication of the 2-2n term to the exponential registers. As this is the cross-term of a real and
imaginary part, it is guaranteed imaginary, so it has a more limited set of possible manifestations.
If ℜ𝑑𝑛 = − 1

and ℑ𝑑𝑛 = + 𝑖

or ℜ𝑑𝑛 = + 1

and ℑ𝑑𝑛 = − 𝑖

then 2−2𝑛 𝑑𝑛, ℜ 𝑑𝑛, ℑ = − 2−2𝑛 𝑖

https://patents.google.com/patent/EP4042270B1/en?q=(high+radix)&oq=high+radix

10/37

2/14/26, 1:56 PM

EP4042270B1 - Hardware algorithm for complex-valued exponentiation and logarithm using simplified sub-steps - Google P…
:
ℜ𝑧exp,

ℑ𝑧exp,

n

n

: = ℜ𝑧exp,

: = ℑ𝑧exp,

n

+ sraℑ𝑧exp,

n

n−1

, 2𝑛,

− sll1, 𝐹 − 2𝑛 + sraℜ𝑧exp,

n−1

, 2𝑛,

Whereas if ℜ𝑑𝑛 = + 1

and ℑ𝑑𝑛 = + 𝑖

or ℜ𝑑𝑛 = − 1

and ℑ𝑑𝑛 =

-i then 2

−2𝑛

−2𝑛

𝑑𝑛, ℜ 𝑑𝑛, ℑ = + 2

𝑖

:
ℜ𝑧exp,

ℑ𝑧

exp,

n

n

: = ℑ𝑧

: = ℜ𝑧exp, n − sraℑ𝑧exp,

exp, n

n−1

+ sll1, 𝐹 − 2𝑛 + sraℜ𝑧

, 2𝑛,

exp,

n−1

, 2𝑛,

wherein the signs are reversed in the latter case.
Do the same to any auxiliary registers such as ℜ𝑧′exp, n

and ℑ𝑧′exp, n

to apply the division process to these also. Crucially, in these cases the correction for the +1 should be omitted.
If ℜ𝑑𝑛 = − 1

and ℑ𝑑𝑛 = + 𝑖

or ℜ𝑑𝑛 = + 1

and ℑ𝑑𝑛 = − 𝑖

then 2

−2𝑛

−2𝑛

𝑑𝑛, ℜ 𝑑𝑛, ℑ = − 2

𝑖

:
ℜ𝑧′

exp,

n

ℑ𝑧′exp,

n

ℜ𝑧′exp,

n

: = ℜ𝑧′

exp,

: = ℑ𝑧′exp,

n

n

+ sraℑ𝑧′

exp,

n−1

− sraℜ𝑧′exp,

n−1

− sraℑ𝑧′exp,

n−1

, 2𝑛,

, 2𝑛,

Whereas if ℜ𝑑𝑛 = + 1

and ℑ𝑑𝑛 = + 𝑖

or ℜ𝑑𝑛 = − 1

and ℑ𝑑𝑛 =

-i then = +2-2ni:

ℑ𝑧′

exp,

n

: = ℜ𝑧′exp,

: = ℑ𝑧′

exp,

n

n

+ sraℜ𝑧′

https://patents.google.com/patent/EP4042270B1/en?q=(high+radix)&oq=high+radix

exp,

n−1

, 2𝑛,

, 2𝑛,

11/37

2/14/26, 1:56 PM

EP4042270B1 - Hardware algorithm for complex-valued exponentiation and logarithm using simplified sub-steps - Google P…

8. Subtract the corresponding entry in the logarithm tables from the registers:

This is achieved using the look-up table constructions described in the previous section by
.
9. Return to step 2 for the next iteration, until N is reached, at which point the registers will contain their final values:
ℜ𝑧log,

ℑ𝑧log,

N

N

: = log ℜ𝑧input + iℑ𝑧input ,

: =

2

2
𝜋

argℜ𝑧input + iℑ𝑧input ,

And:
ℜ𝑧′exp,

N

ℑ𝑧′exp,

N

: = ℜ𝑧numerator′ ÷ 𝑧integer output,

: = ℑ𝑧numerator′ ÷ 𝑧integer output,

input

input

× 𝑧input ,

× 𝑧input .

[0041] Having appreciated the form of the process, it is possible to find other testing procedures that are convergent, often even in the required domain of the form of
range reduction used here, by forming ℜ𝑧exp, test , ℑ𝑧exp, test

or both using different number of bits or different comparison values, although we have endeavored to reduce complexity by specifying the required value tests
in the simplest known form.
[0042] An illustration of the application of this procedure to values z input ∈ R = [-2. 0, +2.0) + i[-2.0, +2.0) is shown in Figures 2A and 2B.
[0043] Figures 2A and 2B show illustrations of errors in the complex plane in the exponential-to-logarithm process (floating-point implementation) for the BKML4m
process. These figures correspond to illustrative tests of the iterative scheme in floating-point and with no range reduction steps to demonstrate the
mathematical viability of the dynamical system rather than being a faithful reproduction of the iterative procedures outlined.
[0044] In Figure 2A, shown is a simulation 1200A where the x-axis 1220A is real, and the y-axis 1210A is imaginary in the input to the algorithm. Shading denotes the
error (where a black shading 1230A implies the error is linear related to the bits and iterations of the algorithm).
[0045] In Figure 2B, shown is a simulation 1200B where the x-axis 1220B is real, and the y-axis 1210B is imaginary in the input to the algorithm. The white trapezoid
1230B is constructed by inverting the color and denotes the portion of the domain ( 𝑧input ∈ R = +0.5, + 1.0 + 𝑖−ℜ𝑅, + ℜ𝑅

) which is required to be convergent. Therefore, if the algorithm functions in this zone, it is expected for the region to be shaded solid white. Branch cut artifacts
have been compensated for on the real line.
VIII. BKML4m: Unified logarithm-to-exponential and exponential-to-logarithm algorithm
[0046] Both directions can be unified into a single algorithm that can flip direction based on a bit switch.
IX. Simplification of the convergence test
[0047] The first point to note when unifying the algorithms is that the 'correction' of the exponential in the exponential-to-logarithm, wherein the value is shifted so the
origin is moved to zero by subtracting one, is only required by the test step. This means that the correction can be temporarily applied to the value under test on
each iteration. This is further simplified by the fact that adding or subtracting high bits affects only the bits to the left of the other operand value, so a relatively
large change of 1 can be made to affect only a single bit which is flipped when the exponential-to-logarithm mode is engaged via the bit switch.
X. Reduced entropy table
[0048] It can be observed that:
2 tan

−1

𝜋

−𝑝

2

≈

1
2

log

−𝑝

2

1+2

,

therefore, at the expense of an extra operation to correct for the error, a smaller table of corrections to the value log2 1 + 2 -p may be stored instead of a lookup
−1

table for the value 2 tan𝜋 2

−𝑝

. As the extra operation is inexpensive in logic compared to the full storage of the table, this is a way to encode operations using the fourth table storing the
imaginary logarithm using reduced entropy.
XI. Reduced bi-directional BKML4m
[0049] As the logarithm BKML4m requires one extra iteration with n = 0, this means that the bidirectional method also requires a zero iteration. Pulling this extra
iteration out from the logarithm iteration and into the preprocessing stages generates further effects that allow for further savings in complexity and thus cost,
as the zeroth iteration is the most non-linear in terms of the tests required for the iteration, so the form of the later iterations may be simplified.
XII. Bi-directional BKML4m description
[0050] The full algorithm required, including the range reduction steps, convergence simplification, reduce entropy table and hoisted zeroth iteration is then described
by:

https://patents.google.com/patent/EP4042270B1/en?q=(high+radix)&oq=high+radix

12/37

2/14/26, 1:56 PM

EP4042270B1 - Hardware algorithm for complex-valued exponentiation and logarithm using simplified sub-steps - Google P…

1. Assuming there are four basic input registers, labelled ℜ𝑧log , ℑ𝑧log

, ℜ𝑧exp

and ℑ𝑧exp

, to being with these may contain data that is beyond the region of convergence of the algorithms described. Therefore, we range reduce values outside
the region of convergence to allow results for all real values to be found:
a. If the process is taking logarithmic input and producing exponential output, then take the rounded integer part away from the real logarithm,
leaving an ℜ𝑧log

value in the range [-0.5, +0.5). This integer real part is to be saved for later as ℜ𝑧integer, log

. Further, take the quadrant number out from the imaginary part, leaving only the fraction of the quadrant, ℑ𝑧log

again in the range [-0.5, +0.5). The quadrant number may be 0, 1, 2 or 3, but any other upper bits in the imaginary logarithm are unnecessary and
are ignored. The quadrant number is also saved for later as ℑ𝑧integer, log

. ℜ𝑧exp

is generally initialized to 1, although any value may be passed through from the input. Equally, the imaginary part ℑ𝑧exp

is generally zero. The initial value of z exp will be multiplied by the antilog (base
multiplication through by the input antilog (base

) of the logarithm registers. Auxiliary registers will also have the

) applied.

b. If the process is taking exponential input and producing logarithmic output, then the sign bits are first considered. The sign bits can be used to
conditionally negate the values to compute absolute values of both the real and imaginary parts. By determining which of the real or imaginary part
is larger in absolute value, the value may be moved via an effective complex multiplication to the quadrant wherein ℑ𝑧exp < ℜ𝑧exp

and ℜ𝑧exp > 0

, while encoding the quadrant move in ℑ𝑧integer, log

. Once completed, since the real part ℜ𝑧exp > 0

, the leading zeroes may be counted and the bits of ℜ𝑧exp

(and also ℑ𝑧exp

) shifted up into the range such that 0.5 ≤ ℜ𝑧exp < 1

, where the number of bit places moved is recorded in ℜ𝑧integer, log

. The logarithm registers are initialized with the values in ℜ𝑧integer, log

and ℑ𝑧integer, log

. Auxiliary registers will have a division through by the input applied. Preprocess the zeroth iteration of the logarithm-to-exponentiation process
with the following steps:
i. Initialize Boolean constants which describe whether the imaginary value is greater in magnitude than the smallest valid real part (
𝑏 : = ℑ𝑧
0

exp

≥ 0.5

), and from there whether it is positive ( 𝑏+ : = ℑ𝑧exp ≥ + 0.5

) or negative (b- := (z exp) < -0.5).
ii. If b 0 is set, shift ℜ𝑧exp

and ℑ𝑧exp

https://patents.google.com/patent/EP4042270B1/en?q=(high+radix)&oq=high+radix

13/37

2/14/26, 1:56 PM

EP4042270B1 - Hardware algorithm for complex-valued exponentiation and logarithm using simplified sub-steps - Google P…
right by one bit.
ℜ

tmp

: =

ℑtmp : =

+sraℜ𝑧exp , 1, if 𝑏0 is set,
ℜ𝑧exp ,

otherwise,

+sraℑ𝑧exp , 1,
ℑ𝑧

exp

,

if 𝑏0 is set,

otherwise .

This will effectively add one to the real part of the initial logarithm, making it one if b 0 is set.
iii. Compute a shift-and-add depending on the previously set Boolean constants:
ℜ𝑧

exp

: = ℜ

tmp

+

+ℜtmp ,

if 𝑏+ is set,

−ℜ

if 𝑏− is set,

tmp

0,

ℑ𝑧exp : = ℑtmp +

,

otherwise .

+ℑtmp ,

if 𝑏+ is set,

−ℑtmp ,

if 𝑏− is set,

0,

otherwise .

Which therefore rotates by 45° (π/4) while multiplying through by the square root of two if b 0 is set.
iv. The square root of two change in magnitude from the previous step would denote a subtraction of the value of a half from the real part of
the logarithm, making the total change a positive half. The imaginary part is also a positive or negative half from the 45° (π/4) rotation. This
yields changes to the logarithm registers which at this point are usually initialized to zero:
ℜ𝑧log : = ℜ𝑧log +

ℑ𝑧log : = ℑ𝑧log +

+1 / 2,
0,

if 𝑏0 is set,

otherwise .

+1 / 2,

if 𝑏+ is set,

−1 / 2,

if 𝑏− is set,

0,

otherwise .

2. Iterate through the values 1, ···, N - 1 as the index n:
3. Extract the reduced set of bits on which to conduct the tests for this iteration:
a. If the process is taking logarithmic input and producing exponential output, then:
i. Shift right by N - n and truncate ℜ𝑧log

to form ℜ𝑧test

such that it has three bits; one sign bit and two integer bits in two's complement such that the range is [-4.0, +4.0) with the smallest change
being 1. The multiplication of this value by 2-n is implied by the initial shift.
ii. Shift right by N - (n + 1) and truncate ℑ𝑧log

to form ℑ𝑧test

such that it has three bits; one sign bit, one integer bit and one fraction bit in two's complement such that the range is [-2.0, +2.0) with the
smallest change being 0.5. The multiplication of this value by 2 -n is implied by the initial shift.
b. If the process is taking exponential input and producing logarithmic output, then:
i. Apply a subtraction of 1 from the value while testing ℜ𝑧exp

. Due to the range reduction enabled by the removal of the zeroth iteration, this simply means any integer bit in ℜ𝑧exp

is set for the purposes of testing (and therefore always causes the representation of a negative value). For computation purposes therefore:
ℜ𝑧tmp, exp : = ℜ𝑧exp − 1.0,

This can be computed in line with the shift right by N - (n + 1) and truncate ℜ𝑧exp

(or ℜ𝑧tmp, exp

https://patents.google.com/patent/EP4042270B1/en?q=(high+radix)&oq=high+radix

14/37

2/14/26, 1:56 PM

EP4042270B1 - Hardware algorithm for complex-valued exponentiation and logarithm using simplified sub-steps - Google P…
) to form ℜ𝑧test

such that it has three bits; one sign bit, one integer bit and one fraction bit in two's complement such that the range is [-2.0, +2.0) with the
smallest change being 0.5. The multiplication of this value by 2-n is implied by the shift.
ii. Shift right by N - (n + 1) and then truncate ℑ𝑧exp

to form ℑ𝑧test

such that it has three bits; one sign bit, one integer bit and one fraction bits in two's complement such that the range is [-2.0, +2.0) with the
smallest change being 0.5. The multiplication of this value by 2-n is implied by the initial shift.
4. Conduct tests on the two 3-bit values ℜ𝑧test

and ℑ𝑧test

to determine dn. Eliminating any binary point metainformation - these values are signed integers from here on having a sign bit and two integer bits - the
further operations may be harmonized, yielding:
𝑏

𝑏

ℜ, >

ℜ, <

: = ℜ𝑧

: = ℜ𝑧

test

test

≥ + 1,

< − 1,

𝑏ℑ, > : = ℑ𝑧test ≥ + 1,

𝑏ℑ, < : = ℑ𝑧test < − 1,

where finally, taking isexp as the Boolean value that denotes a process that take logarithmic input and produces exponential output when set:
ℜ𝑑𝑛 : =

−1,

if 𝑏ℜ, <

and isexp or 𝑏ℜ, >

and ¬isexp,

+1,

if 𝑏ℜ, >

and isexp or 𝑏ℜ, <

and ¬isexp,

0,

ℑ𝑑𝑛 : =

otherwise,

−1,

if 𝑏ℑ, <

and isexp or 𝑏ℑ, >

and ¬isexp,

+1,

if 𝑏ℑ, >

and isexp or 𝑏ℑ, <

and ¬isexp,

0,

if neither .

5. Apply the shift-and-add process effecting the multiplication of the 2-n terms to the exponential registers:
ℜ𝑧exp,

−𝑛

−2

−𝑛

+ +2

ℜ𝑧

exp,

n−1

ℜ𝑧exp,

n−1

n

: = ℜ𝑧exp,

n−1

=

− sraℜ𝑧

exp,

n−1

=

+ sraℜ𝑧exp,

n−1

0,

−𝑛

+2

−𝑛

+ −2

ℑ𝑧

exp,

n−1

ℑ𝑧exp,

n−1

, 𝑛,

if ℜ𝑑

, 𝑛,

if ℜ𝑑𝑛 = + 1,

𝑛

= − 1,

if 𝑧zero is set,

=

+ sraℑ𝑧

exp,

n−1

=

− sraℑ𝑧exp,

n−1

0,

+

, 𝑛,

if ℑ𝑑

, 𝑛,

if ℑ𝑑𝑛 = + 𝑖,

𝑛

= − 𝑖,

if 𝑧zero is set,

And:
ℑ𝑧exp,
−𝑛

−2

−𝑛

+ +2

−𝑛

−2

−𝑛

+ +2

n-1

+

ℑ𝑧exp,

n−1

=

− sraℑ𝑧exp,

n−1

, 𝑛,

if ℜ𝑑𝑛 = − 1,

ℑ𝑧exp,

n−1

=

+ sraℑ𝑧exp,

n−1

, 𝑛,

if ℜ𝑑𝑛 = + 1,

ℜ𝑧exp,
ℜ𝑧

exp,

n−1

n−1

n

: = ℑ𝑧exp,

0,

if 𝑧

=

− sraℜ𝑧exp,

n−1

=

+ sraℜ𝑧

exp,

0,

n−1

zero

is set,

, 𝑛,

if ℑ𝑑𝑛 = − 𝑖,

, 𝑛,

if ℑ𝑑

𝑛

= + 𝑖,

if 𝑧zero is set

https://patents.google.com/patent/EP4042270B1/en?q=(high+radix)&oq=high+radix

15/37

2/14/26, 1:56 PM

EP4042270B1 - Hardware algorithm for complex-valued exponentiation and logarithm using simplified sub-steps - Google P…

Do the same to any auxiliary registers such as ℜ𝑧′exp, n − 1

and ℑ𝑧′exp, n − 1

to apply the multiplication or division process to these also.
6. Apply the shift-and-add process effecting the multiplication of the 2-2n term to the exponential registers (in some implementations, this may be
replaced by a second application of the previous step if the extra serialization can be amortized into the time cost for the step). As this is the cross-term
of a real and imaginary part, it is guaranteed imaginary, so it has a more limited set of possible manifestations.
If ℜ𝑑𝑛 = − 1

and ℑ𝑑𝑛 = + 𝑖

or ℜ𝑑𝑛 = + 1

and ℑ𝑑𝑛 = − 𝑖

then 2

−2𝑛

−2𝑛

𝑑𝑛, ℜ 𝑑𝑛, ℑ = − 2

𝑖

:
ℜ𝑧exp,

n

ℑ𝑧exp,

n

ℜ𝑧exp,

n

ℑ𝑧exp,

n

: = ℜ𝑧exp,

: = ℑ𝑧exp,

+ sraℑ𝑧exp,

n−1

− sraℜ𝑧exp,

n−1

− sraℑ𝑧exp,

n−1

+ sraℜ𝑧exp,

n−1

n

n

, 2𝑛,

, 2𝑛,

Whereas if ℜ𝑑𝑛 = + 1

and ℑ𝑑𝑛 = + 𝑖

or ℜ𝑑𝑛 = − 1

and ℑ𝑑𝑛 =

-i then 2

−2𝑛

−2𝑛

𝑑𝑛, ℜ 𝑑𝑛, ℑ = + 2

𝑖

:
: = ℜ𝑧exp,

: = ℑ𝑧exp,

n

n

, 2𝑛,

, 2𝑛,

wherein the signs are reversed in the latter case.
Do the same to any auxiliary registers such as ℜ𝑧′exp, n

and ℑ𝑧′exp, n

to apply the multiplication or division process to these also.
7. Subtract the corresponding entry in the logarithm tables from the registers:

This is achieved using the look-up table constructions described in the previous section by

and for the imaginary part may be approximated by the low entropy table method given in the previous section.

https://patents.google.com/patent/EP4042270B1/en?q=(high+radix)&oq=high+radix

16/37

2/14/26, 1:56 PM

EP4042270B1 - Hardware algorithm for complex-valued exponentiation and logarithm using simplified sub-steps - Google P…

8. Return to step 2 for the next iteration, until N is reached, at which point the registers will contain the final values for the fractional portion of the
calculation.
9. Compute range expansion on the values present in the registers, so:
a. If the process is taking logarithmic input and producing exponential output, then the quadrant number held in the integer ℑ𝑧integer, log

is expanded, rotating back via multiplication of the exponentiated value z exp, N by the appropriate value from {1, i, -1, -i}. If the integer part of the
logarithm was not applied, either this may be applied as a bit shift, or kept as an exponent, allowing the process to emit a floating-point value.
b. If the process is taking exponential input and producing logarithmic output, then if the integer part of the logarithm described by the leading
zeroes count of the first step has not yet been applied, add this value.
XIII. The BKML3dm variant
[0051] A new solution was derived by choosing dn from the set of four possible values:
−𝑛

−1 − 𝑖 + 2

−𝑛

−1 + 𝑖 − 2

𝑑𝑛 ∈

−𝑛

+1 + 𝑖 + 2

−𝑛

+1 − 𝑖 − 2

requiring only three logarithm lookup tables to obtain the logarithms (base

𝑖,
𝑖,
𝑖,

,

𝑖,

) for each of the four values. This results in not only fewer lookup tables but has a

further side effect of reducing further the complexity of the tests required and the dependency chains for each iteration. As each relies on fewer bits for the
result, they may be computed more efficiently, or multiple steps may be calculated within each clock cycle.
[0052] A drawback of this approach is that some iterations (with a seemingly functional heuristic wherein those numbered with Fibonacci numbers must be processed
twice) must be repeated to achieve convergence. As the repeated iterations share the same lookup tables, it is likely these may be computed in the same step
without expanding the dependencies significantly.
[0053] This approach leads to a binary choice of modifier for each real value and imaginary value at each step. Intuitively, this must be more closely approaching an
optimal solution to the overall problem.
[0054] With the proposed changes, the size of the lookup tables is reduced to N discrete groups of 3N bits, with 3N 2 bits overall.
XIV. Look-up table construction
[0055] When considering the logarithm register (zlog) portion of the exponentiation and logarithm iterations, it can be shown that only three lookup tables need be
constructed to contain the bit patterns of the logarithms required. These are:
−𝑛

𝑙𝑛, ℜ + = log 1 + 2
2

𝑙

2

−𝑛

𝑛, ℜ −

= log 1 − 2
2

𝑙𝑛, ℑ =

2
𝜋

−𝑛

+ log 1 ± 2

−𝑛

+ log 1 ± 2
2

tan

−1

−𝑛

2

𝑖,

𝑖,

,

Then the logarithms to use for the addition/subtraction portion will be for each possible dn ∈ {-1 -i + 2-ni, -1 + i - 2-ni, +1 + i + 2-ni, +1 - i - 2-ni}:

These are then subtracted from the running total of the logarithm. In each case, the decision of the dn to use is based on the sign bit of the logarithm or the
exponential with 1.0 subtracted to co-locate the origin of both logarithm-to-exponential and exponential-to-logarithm iterations. It is anticipated that using an
estimation scheme may allow high-radix iterations to slice the domain into parallelized operations allowing for lower latency implementations.
XV. BKML3dm: Exponentiation mode iteration
[0056] Assuming the fractional part of the input logarithms to be the input, the algorithm for this method for the domain of convergence z input ∈ R = [-0.5, +0.5) + i [-0.5,
+0.5) can be written for exponentiation as:
1. Assuming there are four basic registers, labelled ℜ𝑧log , ℑ𝑧log , ℜ𝑧exp

and ℑ𝑧exp

. Alongside, there are two extra slave multiplication registers ℜ′𝑧exp

and ℑ′𝑧exp

to demonstrate how the method operates when used for auxiliary complex multiplication. The initial values of these registers are:
ℜ𝑧

log

: = ℜ𝑧

input

,

ℑ𝑧log : = ℑ𝑧input ,

ℜ𝑧exp : = ℜ𝑧integer input,

output

ℑ𝑧exp : = ℑ𝑧integer input,

output

× 𝑧premultiply ,

× 𝑧premultiply ,

https://patents.google.com/patent/EP4042270B1/en?q=(high+radix)&oq=high+radix

17/37

2/14/26, 1:56 PM

EP4042270B1 - Hardware algorithm for complex-valued exponentiation and logarithm using simplified sub-steps - Google P…

where z premultiply := 1.0, if there are no requirements for pre-multiplication. The slave multiplication registers may also be similarly constructed with:
ℜ𝑧′exp : = ℜ𝑧integer input,

output

ℑ𝑧′exp : = ℑ𝑧integer input,

output

× 𝑧′premultiply ,

× 𝑧′premultiply ,

2. Iterate through the values 1, ··· , N as the index n, but repeating elements part of the Fibonacci sequence. These first few n would therefore be:
𝑛 = 1, 1, 2, 2, 3, 3, 4, 5, 5, 6, 7, 8, 8, 9, …

3. Test the sign bits of ℜ𝑧log, n − 1

and ℑ𝑧log, n − 1

to determine dn :
ℜ𝑑𝑛 : =

−1,

if ℜ𝑧log, n − 1 < 0,

+1,

if ℜ𝑧

−𝑖,

if ℑ𝑧log, n − 1 < 0,

+𝑖,

if ℑ𝑧log, n − 1 ≥ 0,

ℑ𝑑 : =
𝑛

log, n − 1

≥ 0,

4. Apply the shift-and-add process effecting the multiplication of the 2-n terms to the exponential registers:
ℜ𝑧
−𝑛

−2
+

−𝑛

+2

n

: = ℜ𝑧

exp,

n−1

+

n−1

= − sraℜ𝑧exp,

n−1

, 𝑛,

ℜ𝑧exp,

n−1

= + sraℜ𝑧exp,

n−1

, 𝑛,

if ℜ𝑑𝑛 = + 1,

, 𝑛,

if ℑ𝑑

, 𝑛,

if ℑ𝑑𝑛 = + 𝑖,

−𝑛

+2
+

exp,

ℜ𝑧exp,

−𝑛

−2

ℑ𝑧

exp,

n−1

ℑ𝑧exp,

n−1

= + sraℑ𝑧

exp,

n−1

= − sraℑ𝑧exp,

n−1

if ℜ𝑑𝑛 = − 1,

𝑛

= − 𝑖,

And:
ℑ𝑧
−𝑛

−2
+

−𝑛

+2

−𝑛

−2
+

−𝑛

+2

ℑ𝑧exp,
ℑ𝑧

exp,

exp,

n

: = ℑ𝑧

exp,

= − sraℑ𝑧exp,
n−1
n−1

= + sraℑ𝑧

exp,

n−1

+

, 𝑛,
n−1
n−1

ℜ𝑧exp,

= − sraℜ𝑧exp,
n−1

ℜ𝑧exp,

n−1

= + sraℜ𝑧exp,

, 𝑛,

if ℜ𝑑𝑛 = − 1,
if ℜ𝑑

𝑛

= + 1,

, 𝑛,
n−1

if ℑ𝑑𝑛 = − 𝑖,

, 𝑛,

if ℑ𝑑𝑛 = + 𝑖,

n−1

Do the same to any auxiliary registers such as ℜ𝑧′exp, n − 1

and ℑ𝑧′exp, n − 1

to apply the multiplication process to these also.
5. Apply the shift-and-add process effecting the multiplication of the 2-2n term to the exponential registers. As this is the cross-term of a real and
imaginary part, it is guaranteed imaginary, so it has a more limited set of possible manifestations.
If ℜ𝑑𝑛 = − 1

and ℑ𝑑𝑛 = + 𝑖

or ℜ𝑑𝑛 = + 1

and ℑ𝑑𝑛 = − 𝑖

https://patents.google.com/patent/EP4042270B1/en?q=(high+radix)&oq=high+radix

18/37

2/14/26, 1:56 PM

EP4042270B1 - Hardware algorithm for complex-valued exponentiation and logarithm using simplified sub-steps - Google P…
then 2

−2𝑛

−2𝑛

𝑑𝑛, ℜ 𝑑𝑛, ℑ = − 2

𝑖

:
ℜ𝑧exp,

ℑ𝑧

n

: = ℜ𝑧exp,

: = ℑ𝑧

exp,

n

ℜ𝑧exp,

n

ℑ𝑧exp,

n

exp,

n

n

+ sraℑ𝑧exp,

− sraℜ𝑧

n−1

, 2𝑛,

, 2𝑛,

exp,

n−1

− sraℑ𝑧exp,

n−1

+ sraℜ𝑧exp,

n−1

Whereas if ℜ𝑑𝑛 = + 1

and ℑ𝑑𝑛 = + 𝑖

or ℜ𝑑𝑛 = − 1

and ℑ𝑑𝑛 =

-i then 2

−2𝑛

−2𝑛

𝑑𝑛, ℜ 𝑑𝑛, ℑ = + 2

𝑖

:
: = ℜ𝑧exp,

: = ℑ𝑧exp,

n

n

, 2𝑛,

, 2𝑛,

wherein the signs are reversed in the latter case.
Do the same to any auxiliary registers such as ℜ𝑧′exp, n

and ℑ𝑧′exp, n

to apply the multiplication process to these also.
6. Subtract the corresponding entry in the logarithm tables from the registers:

This is achieved using the look-up table constructions described in the previous section by

.
7. Return to step 2 for the next iteration, until N is reached, at which point the registers will contain the final values for the fractional portion of the
calculation:
ℜ𝑧exp,

ℑ𝑧

: = ℜ𝑧integer input,

N

: = ℑ𝑧

exp,

N

ℜ′𝑧exp,

N

ℑ′𝑧exp,

N

integer input,

ℜ𝑧

output

output

× 𝑧premultiply × 2

×𝑧

input

ℜ𝑧input

premultiply

×2

𝜋 iℑ𝑧

𝑒2

𝜋 iℑ𝑧

𝑒2

input

input

,

,

And:
: = ℜ𝑧integer input,

: = ℑ𝑧integer input,

ℜ𝑧input

output

× 𝑧premultiply′ × 2

ℜ𝑧input

output

× 𝑧premultiply′ × 2

𝜋 iℑ𝑧

𝑒2

𝜋 iℑ𝑧

𝑒2

input

input

,

.

[0057] Having appreciated the form of the process, it is easy to find other testing procedures that are convergent, although we have endeavored to reduce complexity by
specifying the required domain region tests in the simplest known form.
[0058] An illustration of the application of this procedure to values zinput ∈ R = [-2. 0, +2.0) + i[-2.0, +2.0) is shown in Figures 3A and 3B.
[0059] Figures 3A and 3B show illustrations of errors in the complex plane in the logarithm-to-exponential process (floating-point implementation) for the BKML3dm
iteration prior to the inclusion of the reduced entropy tables. These figures correspond to illustrative tests of the iterative scheme in floating-point and with no
range reduction steps to demonstrate the mathematical viability of the dynamical system rather than being a faithful reproduction of the iterative procedures
outlined.

https://patents.google.com/patent/EP4042270B1/en?q=(high+radix)&oq=high+radix

19/37

2/14/26, 1:56 PM

EP4042270B1 - Hardware algorithm for complex-valued exponentiation and logarithm using simplified sub-steps - Google P…

[0060] In Figure 3A, shown is a simulation 900A where the x-axis 920A is real, and the y-axis 910A is imaginary in the input to the algorithm. Shading denotes the error
(where a black shading 930A implies the error is linearly related to the bits and iterations of the algorithm).
[0061] In Figure 3B, shown is a simulation 900B where the x-axis 920B is real, and the y-axis 910B is imaginary in the input to the algorithm. The white square 930B is
constructed by inverting the color and denotes the portion of the domain (z_"input" " R"=[-0.51┤, ├ +0.5)+i[-0.5┤, ├ +0.5)) which is required to be convergent.
Therefore, if the algorithm functions in this zone, it is expected for the region to be shaded solid white.
XVI. BKML3dm: Logarithm mode iteration
[0062] Assuming the fractional part of the output logarithms to be the output, the algorithm for the domain of convergence zinput ∈ R = [+0.5, +1.0) + 𝑖−ℜ𝑅, + ℜ𝑅

is:
1. Assuming there are four basic registers, labelled ℜ𝑧log , ℑ𝑧log , ℜ𝑧exp

and ℑ𝑧exp

. Alongside, there are two extra slave division registers ℜ′𝑧exp

and ℑ′𝑧exp

to demonstrate how the method operates when used for auxiliary complex division. The initial values of these registers are:
ℜ𝑧log : = ℜ𝑧integer output,

ℑ𝑧

log

: = ℑ𝑧

integer output,

output

output

,

,

ℜ𝑧exp : = ℜ𝑧input ,

ℑ𝑧exp : = ℑ𝑧input ,

The slave division registers may also be similarly constructed with:
ℜ𝑧′exp : = ℜ𝑧′numerator ÷ 𝑧integer output,

ℑ𝑧′exp : = ℑ𝑧′numerator ÷ 𝑧integer output,

input

input

,

.

2. Iterate through the values 1, ··· , N as the index n, but repeating elements part of the Fibonacci sequence. These first few n would therefore be:
𝑛 = 1, 1, 2, 2, 3, 3, 4, 5, 5, 6, 7, 8, 8, 9, …

3. Test the sign bits of ℜ𝑧exp, n − 1 − 1

(where the -1 is computed by permuting the top two bits of the register) and ℑ𝑧exp, n − 1

to determine dn :
ℜ𝑑𝑛 : =

+1,

if ℜ𝑧exp, n − 1 − 1 < 0,

−1,

if ℜ𝑧exp, n − 1 − 1 ≥ 0,

ℑ𝑑𝑛 : =

+𝑖,

if ℑ𝑧exp, n − 1 < 0,

−𝑖,

if ℑ𝑧

exp, n − 1

≥ 0,

4. Apply the shift-and-add process effecting the multiplication of the 2-n terms to the exponential registers:
ℜ𝑧exp,
−𝑛

−2
+

−𝑛

+2

n−1

ℜ𝑧exp,

n−1

= + sraℜ𝑧exp,

n−1

n−1

= + sraℑ𝑧exp,

n−1

−𝑛

−𝑛

−2

: = ℜ𝑧exp,

= − sraℜ𝑧exp,
n−1

+2
+

n

ℜ𝑧exp,

ℑ𝑧exp,
ℑ𝑧

exp,

n−1

= − sraℑ𝑧

exp,

+

, 𝑛,
n−1

n−1

if ℜ𝑑𝑛 = − 1,

, 𝑛,

if ℜ𝑑𝑛 = + 1,

, 𝑛,

if ℑ𝑑𝑛 = − 𝑖,

, 𝑛,

if ℑ𝑑

https://patents.google.com/patent/EP4042270B1/en?q=(high+radix)&oq=high+radix

𝑛

= + 𝑖,

20/37

2/14/26, 1:56 PM

EP4042270B1 - Hardware algorithm for complex-valued exponentiation and logarithm using simplified sub-steps - Google P…

And:
ℑ𝑧exp,
−𝑛

−2
+

−𝑛

−2
+

−𝑛

+2

: = ℑ𝑧exp,

n−1

+

n−1

= − sraℑ𝑧exp,

n−1

, 𝑛,

if ℜ𝑑𝑛 = − 1,

ℑ𝑧exp,

n−1

= + sraℑ𝑧exp,

n−1

, 𝑛,

if ℜ𝑑𝑛 = + 1,

ℜ𝑧exp,

= − sraℜ𝑧exp,
n−1

, 𝑛,
n−1

if ℑ𝑑𝑛 = − 𝑖,

−𝑛

+2

n

ℑ𝑧exp,

ℜ𝑧

exp,

n−1

= + sraℜ𝑧

exp,

n−1

, 𝑛,

if ℑ𝑑

𝑛

= + 𝑖,

Do the same to any auxiliary registers such as ℜ𝑧′exp, n − 1

and ℑ𝑧′exp, n − 1

to apply the multiplication process to these also.
5. Apply the shift-and-add process effecting the multiplication of the 2-2n term to the exponential registers. As this is the cross-term of a real and
imaginary part, it is guaranteed imaginary, so it has a more limited set of possible manifestations.
If ℜ𝑑𝑛 = − 1

and ℑ𝑑𝑛 = + 1

or ℜ𝑑𝑛 = + 1

and ℑ𝑑𝑛 = − 𝑖

then 2−2𝑛 𝑑𝑛, ℜ 𝑑𝑛, ℑ = − 2−2𝑛 𝑖

:
ℜ𝑧

exp,

n

ℑ𝑧exp,

n

ℜ𝑧exp,

n

ℑ𝑧exp,

n

: = ℜ𝑧

exp,

: = ℑ𝑧exp,

n

n

+ sraℑ𝑧

exp,

n−1

− sraℜ𝑧exp,

n−1

− sraℑ𝑧exp,

n−1

+ sraℜ𝑧exp,

n−1

, 2𝑛,

, 2𝑛,

Whereas if ℜ𝑑𝑛 = + 1

and ℑ𝑑𝑛 = + 𝑖

or ℜ𝑑𝑛 = − 1

and ℑ𝑑𝑛 =

-i then 2

−2𝑛

−2𝑛

𝑑𝑛, ℜ 𝑑𝑛, ℑ = + 2

𝑖

:
: = ℜ𝑧exp,

: = ℑ𝑧exp,

n

n

, 2𝑛,

, 2𝑛,

wherein the signs are reversed in the latter case.
Do the same to any auxiliary registers such as ℜ𝑧′exp, n

and ℑ𝑧′exp, n

to apply the multiplication process to these also.

https://patents.google.com/patent/EP4042270B1/en?q=(high+radix)&oq=high+radix

21/37

2/14/26, 1:56 PM

EP4042270B1 - Hardware algorithm for complex-valued exponentiation and logarithm using simplified sub-steps - Google P…

6. Subtract the corresponding entry in the logarithm tables from the registers:

This is achieved using the look-up table constructions described in the previous section by
.
7. Return to step 2 for the next iteration, until N is reached, at which point the registers will contain their final values:
ℜ𝑧log,

ℑ𝑧

log,

N

N

: = log ℜ𝑧input + iℑ𝑧input ,

: =

2

2
𝜋

argℜ𝑧

input

+ iℑ𝑧

input

,

And:
ℜ𝑧′exp,

N

ℑ𝑧′exp,

N

: = ℜ𝑧′numerator ÷ 𝑧integer output,

: = ℑ𝑧′numerator ÷ 𝑧integer output,

input

input

× 𝑧input ,

× 𝑧input .

[0063] Having appreciated the form of the process, it is possible to find other testing procedures that are convergent, even sometimes in the required domain, by
forming ℜ𝑧exp, test , ℑ𝑧exp, test

or both using different number of bits or different comparison values, although we have endeavored to reduce complexity by specifying the required value tests
in the simplest known form.
[0064] An illustration of the application of this procedure to values z input ∈ R = [-2.0, +2.0) + i[- 2.0, +2.0) is shown in Figures 4A and 4B.
[0065] Figures 4A and 4B show illustration of errors in the complex plane in the exponential-to-logarithm process (floating-point implementation) for the BKML3dm
prior to the inclusion of the reduced entropy tables. These figures correspond to illustrative tests of the iterative scheme in floating-point and with no range
reduction steps to demonstrate the mathematical viability of the dynamical system rather than being a faithful reproduction of the iterative procedures outlined.
[0066] In Figure 4A, shown is a simulation 1000A where the x-axis 1020A is real, and the y-axis 1010A is imaginary in the input to the algorithm. Shading denotes the
error (where a black shading 1030A implies the error is linear related to the bits and iterations of the algorithm).
[0067] In Figure 4B, shown is a simulation 1000B where the x-axis 1020B is real, and the y-axis 1010B is imaginary in the input to the algorithm. The white trapezoid
1030B is constructed by inverting the color and denotes the portion of the domain (z_"input" " R"= [+0.5┤, ├ +1.0)+i[-R(R)┤, ├ +R(R))) which is required to be
convergent. Therefore, if the algorithm functions in this zone, it is expected for the region to be shaded solid white. Branch cut artifacts have been compensated
for on the real line.
XVII. BKML3dm: Unified logarithm-to-exponential and exponential-to-logarithm algorithm
[0068] With only three lookup tables and four possible values of dn for both directions of the algorithm, merging these in a bi-directional algorithm can be achieved. The
steps are:
1. Assuming there are four basic input registers, labelled ℜ𝑧log , ℑ𝑧log

, ℜ𝑧exp

and ℑ𝑧exp

, to being with these may contain data that is beyond the region of convergence of the algorithms described. Therefore, we range reduce values outside
the region of convergence to allow results for all real values to be found:
a. If the process is taking logarithmic input and producing exponential output, then take the rounded integer part away from the real logarithm,
leaving an ℜ𝑧log

value in the range [-0.5, +0.5). This integer real part is to be saved for later as ℜ𝑧integer, log

. Further, take the quadrant number out from the imaginary part, leaving only the fraction of the quadrant, ℑ𝑧log

again in the range [-0.5, +0.5). The quadrant number may be 0, 1, 2 or 3, but any other upper bits in the imaginary logarithm are unnecessary and
are ignored. The quadrant number is also saved for later as ℑ𝑧integer, log

. ℜ𝑧exp

https://patents.google.com/patent/EP4042270B1/en?q=(high+radix)&oq=high+radix

22/37

2/14/26, 1:56 PM

EP4042270B1 - Hardware algorithm for complex-valued exponentiation and logarithm using simplified sub-steps - Google P…
is generally initialized to 1, although any value may be passed through from the input. Equally, the imaginary part ℑ𝑧exp

is generally zero. The initial value of z exp will be multiplied by the antilog (base
multiplication through by the input antilog (base

) of the logarithm registers. Auxiliary registers will also have the

) applied.

b. If the process is taking exponential input and producing logarithmic output, then the sign bits are first considered. The sign bits can be used to
conditionally negate the values to compute absolute values of both the real and imaginary parts. By determining which of the real or imaginary part
is larger in absolute value, the value may be moved via an effective complex multiplication to the quadrant wherein ℑ𝑧exp < ℜ𝑧exp

and ℜ𝑧exp > 0

, while encoding the quadrant move in ℑ𝑧integer, log

. Once completed, since the real part ℜ𝑧exp > 0

, the leading zeroes may be counted and the bits of ℜ𝑧exp

(and also ℑ𝑧exp

) shifted up into the range such that 0.5 ≤ ℜ𝑧exp < 1

, where the number of bit places moved is recorded in ℜ𝑧integer, log

. The logarithm registers are initialized with the values in ℜ𝑧integer, log

and ℑ𝑧integer, log

. Auxiliary registers will have a division through by the input applied.
2. Iterate through the values 1, ··· , N as the index n, but repeating elements part of the Fibonacci sequence. These first few n would therefore be:
𝑛 = 1,

1,

2,

2,

3,

3,

4,

5,

5,

6,

7,

8,

8,

9,

…

3. Test the sign bits of the appropriate registers to determine dn. As the sign bits are themselves the set of Boolean tests, this set of tests can almost be
elided by taking the exclusive OR of the sign bit with a Boolean digit true when logarithmic output is intended:
a. If the process is taking logarithmic input and producing exponential output, then test the sign bits of ℜ𝑧log, n − 1

and ℑ𝑧log, n − 1

:
ℜ𝑑𝑛 : =

ℑ𝑑𝑛 : =

−1,

if ℜ𝑧log, n − 1 < 0,

+1,

if ℜ𝑧log, n − 1 ≥ 0,

−𝑖,

if ℑ𝑧

+𝑖,

if ℑ𝑧log, n − 1 ≥ 0,

log, n − 1

< 0,

So, the computation is:
sign

ℜ𝑑𝑛

sign

ℑ𝑑𝑛

: = sign

: = sign

ℜ𝑧log, n − 1

ℑ𝑧log, n − 1

,

,

b. If the process is taking exponential input and producing logarithmic output, then test the sign bits of ℜ𝑧exp, n − 1 − 1

(where the -1 is computed by permuting the top two bits of the register) and ℑ𝑧exp, n − 1

to determine dn :
ℜ𝑑𝑛 : =

+1,

ifℜ𝑧exp, n − 1 − 1 < 0,

−1,

ifℜ𝑧exp, n − 1 − 1 ≥ 0,

https://patents.google.com/patent/EP4042270B1/en?q=(high+radix)&oq=high+radix

23/37

2/14/26, 1:56 PM

EP4042270B1 - Hardware algorithm for complex-valued exponentiation and logarithm using simplified sub-steps - Google P…
ℑ𝑑𝑛 : =

+𝑖,

if ℑ𝑧exp, n − 1 < 0,

−𝑖,

if ℑ𝑧exp, n − 1 ≥ 0,

So, the computation is:
sign

ℜ𝑑

sign

ℑ𝑑

: = ¬sign
𝑛

: = ¬sign
𝑛

ℜ𝑧

ℑ𝑧

−1

exp, n − 1

exp, n − 1

−1

,

,

4. Apply the shift-and-add process effecting the multiplication of the 2-n terms to the exponential registers:
ℜ𝑧exp,
−𝑛

−2
+

−𝑛

+2

ℜ𝑧exp,

ℜ𝑧exp,

−𝑛

+2
+

−𝑛

−2

ℑ𝑧exp,

: = ℜ𝑧exp,

= − sraℑ𝑧exp,

, 𝑛,

if sign

, 𝑛,

if ¬sign

n−1

n−1

= + sraℑ𝑧exp,

n−1

n−1

+

n−1

= + sraℜ𝑧exp,

n−1

ℑ𝑧exp,

n

= − sraℜ𝑧exp,

n−1

, 𝑛,

if sign

, 𝑛,

if ¬sign

n−1

n−1

,

ℜ𝑑𝑛

ℜ𝑑𝑛

ℑ𝑑𝑛

,

,

ℑ𝑑𝑛

,

And:
ℑ𝑧exp,
−𝑛

−2
+

−𝑛

+2

−2

−𝑛

+2

: = ℑ𝑧exp,

n−1

= − sraℑ𝑧exp,
n−1

ℑ𝑧exp,

= + sraℑ𝑧exp,
n−1

−𝑛

+

n

ℑ𝑧exp,

ℜ𝑧exp,

ℜ𝑧exp,

, 𝑛,

if sign

, 𝑛,
n−1

if ¬sign

n−1

= − sraℜ𝑧exp,

n−1

= + sraℜ𝑧exp,

n−1

+
,
𝑛

ℜ𝑑𝑛

, 𝑛,

if sign

, 𝑛,

if ¬sign

n−1

n−1

ℜ𝑑

ℑ𝑑

,

,
𝑛

ℑ𝑑𝑛

,

Do the same to any auxiliary registers such as ℜ𝑧'exp, n − 1

and ℑ𝑧'exp, n − 1

to apply the multiplication or division process to these also.
5. Apply the shift-and-add process effecting the multiplication of the 2-2n term to the exponential registers. As this is the cross-term of a real and
imaginary part, it is guaranteed imaginary, so it has a more limited set of possible manifestations.
Wherein XOR is the logical binary operator of exclusive-or, if sign

then 2

−2𝑛

−2𝑛

𝑑𝑛, ℜ 𝑑𝑛, ℑ = − 2

ℜ𝑑𝑛

XOR sign

ℑ𝑑𝑛

𝑖

:

Whereas if ¬sign

) then 2

−2𝑛

ℜ𝑑𝑛

XOR sign

−2𝑛

𝑑𝑛, ℜ 𝑑𝑛, ℑ = + 2

ℜ𝑧exp,

n

ℑ𝑧exp,

n

ℜ𝑧exp,

n

ℑ𝑧exp,

n

: = ℜ𝑧exp,

: = ℑ𝑧exp,

+ sraℑ𝑧exp,

n−1

− sraℜ𝑧exp,

n−1

− sraℑ𝑧exp,

n−1

+ sraℜ𝑧exp,

n−1

n

n

, 2𝑛,

, 2𝑛,

ℑ𝑑𝑛

𝑖

:
: = ℜ𝑧exp,

: = ℑ𝑧exp,

n

n

, 2𝑛,

, 2𝑛,

wherein the signs are reversed in the latter case.

https://patents.google.com/patent/EP4042270B1/en?q=(high+radix)&oq=high+radix

24/37

2/14/26, 1:56 PM

EP4042270B1 - Hardware algorithm for complex-valued exponentiation and logarithm using simplified sub-steps - Google P…
Do the same to any auxiliary registers such as ℜ𝑧′exp, n

and ℑ𝑧′exp, n

to apply the multiplication or division process to these also.
6. Subtract the corresponding entries in the logarithm tables from the registers:
ℜ𝑧log,

n

−𝑛

log 1 − 2
+

2

−𝑛

log 1 + 2
2

ℑ𝑧log,

: = ℜ𝑧log,

−2𝑛

√1 + 2

n

2

: = ℑ𝑧log,

− tan

−1

𝜋

+

2

+ tan
𝜋

−1

−𝑛

2

−𝑛

2

,

,

−

n−1

−2𝑛

√1 + 2

,

,

if sign

ℜ𝑑𝑛

if ¬sign

n−1

if sign

ℜ𝑑

,
,
𝑛

−

ℑ𝑑𝑛

if ¬sign

,

ℑ𝑑𝑛

,

7. Return to step 2 for the next iteration, until N is reached, at which point the registers will contain the final values for the fractional portion of the
calculation.
8. Compute range expansion on the values present in the registers, so:
a. If the process is taking logarithmic input and producing exponential output, then the quadrant number held in the integer ℑ𝑧integer, log

is expanded, rotating back via multiplication of the exponentiated value z exp, N by the appropriate value from {1, i, -1, -i}. If the integer part of the
logarithm was not applied, either this may be applied as a bit shift, or kept as an exponent, allowing the process to emit a floating-point value.
b. If the process is taking exponential input and producing logarithmic output, then if the integer part of the logarithm described by the leading
zeroes count of the first step has not yet been applied, add this value.
XVIII. BKML3dm: High radix implementations, with 8-radix logarithm-to-exponential example
[0069] The use of sign bits to drive the possible choices of dn allows the design to scale with radix, so iterations can be conceived which produce multiple bits of result
per iteration. This is because the radix-2 has few serial operations, as described in the table:
Radix

2

4

8

16

32

64

128

Real condition input (bits)

1

4*

6

7*

8*

9*

10*

Imaginary condition input (bits)

1

3*

7

8*

9*

10*

11*

Real condition output (bits)

1

2

3

4

5

6

7

Imaginary condition output (bits)

1

2

3

4

5

6

7

Parallelizable additions (logarithm terms)

1

2

3

4

5

6

7

Fully serial shift and add (serial terms)

1

2

3

4

5

6

7

Fully serial shift and add (parallel terms)

3

3

3

3

3

3

3

Fully parallel shift and add (serial terms)

1

1

1

1

1

1

1

Fully parallel shift and add (parallel terms)

3

15

63

255

1023

4095

16385

where * denote estimated values. This suggests that the conditional decision-making portion of an iteration of a radix-16 implementation may be implemented
as a 9-bit input, 4-bit output multiplexer or lookup table (LUT) for the real part and an 8-bit input, 4-bit output multiplexer or lookup table (LUT) for the imaginary
part. These conditional decision lookup tables are fixed for a given iteration in each direction (logarithm-to-exponential or exponential-to-logarithm) but may for
optimality differ between iterations.
[0070] Further, it should be noted that a radix-16 implementation of the multiply in the exponentiation part of the iteration may have 1 serial shift-and-add section, which
involves 255 parallel additions, or 2 serial shift-and-add sections which each involve the parallel addition of 15 partial terms or 4 serial shift-and-add sections
which each involve 3 parallel additions of shifted terms. Each can trade off calculation dependencies for quickly growing sets of terms.
[0071] A radix-8 implementation of logarithm-to-exponential (chosen because the conditional decision lookup tables required are linear, so can be written for the
general case if radix-4 behavior is acceptable due to the limitations of the extra iterations required which can be otherwise mostly overcome) for example may
be described by:
1. Assuming there are four basic registers, labelled ℜ𝑧log , ℑ𝑧log , ℜ𝑧exp

and ℑ𝑧exp

. Alongside, there are two extra slave multiplication registers ℜ′𝑧exp

https://patents.google.com/patent/EP4042270B1/en?q=(high+radix)&oq=high+radix

25/37

2/14/26, 1:56 PM

EP4042270B1 - Hardware algorithm for complex-valued exponentiation and logarithm using simplified sub-steps - Google P…

and ℑ′𝑧exp

to demonstrate how the method operates when used for auxiliary complex multiplication. The initial values of these registers are:
ℜ𝑧log : = ℜ𝑧input ,

ℑ𝑧

log

: = ℑ𝑧

input

,

ℜ𝑧exp : = ℜ𝑧integer input,

output

ℑ𝑧exp : = ℑ𝑧integer input,

output

× 𝑧premultiply ,

× 𝑧premultiply ,

where z premultiply := 1.0, if there are no requirements for pre-multiplication. The slave multiplication registers may also be similarly constructed with:
ℜ𝑧′exp : = ℜ𝑧integer input,

ℑ𝑧′exp : = ℑ𝑧integer input,

output

output

× 𝑧′premultiply ,

× 𝑧′premultiply .

2. Iterate through the values 1, ··· , N as the index n, but actually use triplets of consecutive bit shift numbers:
𝑆𝑛, 1 𝑆𝑛, 2 𝑆𝑛, 3
∈ 123, 345, 567, 789, 91011, 111213, … ,

3. Extract the reduced set of bits on which to conduct the tests for this logarithm-to-exponential iteration. This is the upper 8-bits of the real part and the
upper 7-bits of the imaginary part:
a. Shift right by N - (S n,1 + 3) and truncate ℜ𝑧log

to form ℜ𝑧test

such that it has six bits; one sign bit, two integer bits and three fraction bits in two's complement such that the range is [-4.0, +4.0) with the
smallest change being 0.125. The multiplication of this value by 2-S n,1 is implied by the initial shift.
b. Shift right by N - (S n,1 + 4) and truncate ℑ𝑧log

to form ℑ𝑧test

such that it has seven bits; one sign bit, two integer bit and five fraction bits in two's complement such that the range is [-4.0, +4.0) with the
smallest change being 0.0625. The multiplication of this value by 2-S n,1 is implied by the initial shift.
4. Conduct tests on the two values ℜ𝑧test

and ℑ𝑧test

to determine dn. In a production implementation of a uni- or bi-directional algorithm in either direction this may be brute forced to generated the least total
error at the end of the iteration. However, for the logarithm-to-exponential iteration, the process appears largely linear, so a consistent choice can be made
on that basis, yielding:
a. Real part test value:
7,
6,
5,
testℜ : =

if

if ℜ𝑧test < − 18,
− 18 ≤ ℜ𝑧test < − 12,

if

− 12 ≤ ℜ𝑧test < − 6,

4,

if

3,

if 0 ≤ ℜ𝑧test < + 6,

− 6 ≤ ℜ𝑧

test

< 0,

2,

if

+ 6 ≤ ℜ𝑧test < + 12,

1,

if

+ 12 ≤ ℜ𝑧test < + 18,

0,

if

+ 18 ≤ ℜ𝑧test ,

https://patents.google.com/patent/EP4042270B1/en?q=(high+radix)&oq=high+radix

26/37

2/14/26, 1:56 PM

EP4042270B1 - Hardware algorithm for complex-valued exponentiation and logarithm using simplified sub-steps - Google P…
b. Imaginary part test value:
7,
6,
5,
test : =
ℑ

if

if ℑ𝑧test < − 15,
− 15 ≤ ℑ𝑧test < − 10,

if

− 10 ≤ ℑ𝑧

test

< − 5,

4,

if

− 5 ≤ ℑ𝑧test < 0,

3,

if 0 ≤ ℑ𝑧test < + 5,

2,

if

+ 5 ≤ ℑ𝑧test < + 10,

1,

if

+ 10 ≤ ℑ𝑧test < + 15,

0,

if

+ 15 ≤ ℑ𝑧

test

,

5. Apply the shift-and-add process effecting the multiplication of the 2-S n,1 terms to the exponential registers:
ℜ𝑧exp,
−𝑆

−2
+

−𝑆𝑛, 1

+2

: = ℜ𝑧exp +
if testℜ ∧ 4,

ℜ𝑧exp = + sraℜ𝑧exp , 𝑆𝑛, 1 ,

if ¬testℜ ∧ 4,

−𝑆𝑛, 1

+2
+

𝑆𝑛, 1

ℜ𝑧exp = − sraℜ𝑧exp , 𝑆𝑛, 1 ,

𝑛, 1

−𝑆𝑛, 1

−2

ℑ𝑧exp = + sraℑ𝑧exp , 𝑆𝑛, 1 ,

if testℑ ∧ 4,

ℑ𝑧

if ¬test

exp

= − sraℑ𝑧

exp

,𝑆

𝑛, 1

,

ℑ

∧ 4,

And:
ℑ𝑧exp,
−𝑆𝑛, 1

−2
+

−𝑆

+2

𝑛, 1

−𝑆

−2
+

+2

: = ℑ𝑧exp +

ℑ𝑧exp = − sraℑ𝑧exp , 𝑆𝑛, 1 ,

if testℜ ∧ 4,

ℑ𝑧exp = + sraℑ𝑧exp , 𝑆𝑛, 1 ,

if ¬testℜ ∧ 4,

ℜ𝑧exp = − sraℜ𝑧exp , 𝑆𝑛, 1 ,

if testℑ ∧ 4,

ℜ𝑧exp = + sraℜ𝑧exp , 𝑆𝑛, 1 ,

if ¬testℑ ∧ 4,

𝑛, 1

−𝑆𝑛, 1

𝑆𝑛, 1

Do the same to any auxiliary registers such as ℜ𝑧′exp

and ℑ𝑧′exp

to apply the multiplication process to these also, producing ℜ𝑧′exp, 𝑆

and ℑ𝑧′exp, 𝑆

𝑛, 1

𝑛, 1

.
6. Apply the shift-and-add process effecting the multiplication of the 2-2S n,1 term to the exponential registers. As this is the cross-term of a real and
imaginary part, it is guaranteed imaginary, so it has a more limited set of possible manifestations.
Wherein XOR is the logical binary operator of exclusive-or, if testℜ ∧ 4 XOR testℑ ∧ 4

then 2

−2𝑆

𝑛, 1

𝑑𝑆

𝑛, 1

,ℜ

𝑑𝑆

−2𝑆

𝑛, 1

,ℑ

= −2

𝑛, 1

𝑖

:
ℜ𝑧exp : = ℜ𝑧exp,

ℑ𝑧exp : = ℑ𝑧exp,

𝑆𝑛, 1

𝑆

𝑛, 1

+ sraℑ𝑧exp,

− sraℜ𝑧exp,

https://patents.google.com/patent/EP4042270B1/en?q=(high+radix)&oq=high+radix

𝑆𝑛, 1

𝑆

𝑛, 1

, 2𝑆𝑛, 1 ,

, 2𝑆𝑛, 1 ,

27/37

2/14/26, 1:56 PM

EP4042270B1 - Hardware algorithm for complex-valued exponentiation and logarithm using simplified sub-steps - Google P…
Whereas if ¬testℜ ∧ 4 XOR testℑ ∧ 4

then 2

−2𝑆

−2𝑆𝑛, 1

+2

𝑛, 1

𝑑𝑆

𝑛, 1

,ℜ

𝑑𝑆

𝑛, 1

,ℑ

=

𝑖

:
ℜ𝑧exp : = ℜ𝑧exp, 𝑆

ℑ𝑧exp : = ℑ𝑧exp, 𝑆

𝑛, 1

𝑛, 1

− sraℑ𝑧exp,

+ sraℜ𝑧exp,

𝑆𝑛, 1

𝑆

𝑛, 1

, 2𝑆𝑛, 1 ,

, 2𝑆𝑛, 1 ,

wherein the signs are reversed in the latter case.
Do the same to any auxiliary registers such as ℜ𝑧′exp, 𝑆

and ℑ𝑧′exp, 𝑆

𝑛, 1

𝑛, 1

to apply the multiplication or division process to these also, producing ℜ𝑧′exp

and ℑ𝑧′exp

.
7. Apply the shift-and-add process effecting the multiplication of the 2-S n,2 terms to the exponential registers:
ℜ𝑧exp,
−𝑆𝑛, 2

−2
+

−𝑆𝑛, 2

+2

−𝑆

+2
+

ℜ𝑧

exp

𝑆𝑛, 2

: = ℜ𝑧exp +

= − sraℜ𝑧

exp

,𝑆

𝑛, 2

,

if test

ℜ𝑧exp = + sraℜ𝑧exp , 𝑆𝑛, 2 ,

∧ 2,

ℑ𝑧exp = + sraℑ𝑧exp , 𝑆𝑛, 2 ,

if testℑ ∧ 2,

ℑ𝑧exp = − sraℑ𝑧exp , 𝑆𝑛, 2 ,

if ¬testℑ ∧ 2,

𝑛, 2

−𝑆𝑛, 2

−2

ℜ

if ¬testℜ ∧ 2,

And:
ℑ𝑧exp,
−𝑆𝑛, 2

−2
+

−𝑆𝑛, 2

+2

+

−𝑆

+2

𝑛, 2

𝑛, 2

: = ℑ𝑧exp +

ℑ𝑧exp = − sraℑ𝑧exp , 𝑆𝑛, 2 ,

if testℜ ∧ 2,

ℑ𝑧

if ¬test

−𝑆𝑛, 2

−2

𝑆

exp

ℜ𝑧

= + sraℑ𝑧

exp

exp

= − sraℜ𝑧

,𝑆

exp

𝑛, 2

,𝑆

,

𝑛, 2

ℜ𝑧exp = + sraℜ𝑧exp , 𝑆𝑛, 2 ,

,

ℜ

if test

ℑ

∧ 2,

∧ 2,

if ¬testℑ ∧ 2,

Do the same to any auxiliary registers such as ℜ𝑧′exp

and ℑ𝑧′exp

to apply the multiplication process to these also, producing ℜ𝑧′exp, 𝑆

and ℑ𝑧′exp, 𝑆

𝑛, 2

𝑛, 2

.
8. Apply the shift-and-add process effecting the multiplication of the 2-2S n,2 term to the exponential registers. As this is the cross-term of a real and
imaginary part, it is guaranteed imaginary, so it has a more limited set of possible manifestations.
Wherein XOR is the logical binary operator of exclusive-or, if testℜ ∧ 2 XOR testℑ ∧ 2

https://patents.google.com/patent/EP4042270B1/en?q=(high+radix)&oq=high+radix

28/37

2/14/26, 1:56 PM

EP4042270B1 - Hardware algorithm for complex-valued exponentiation and logarithm using simplified sub-steps - Google P…
then 2

−2𝑆𝑛, 2

𝑑𝑆

𝑛, 2

,ℜ

𝑑𝑆

−2𝑆𝑛, 2

𝑛, 2

,ℑ

= −2

𝑖

:
ℜ𝑧

exp

: = ℜ𝑧

exp,

ℑ𝑧exp : = ℑ𝑧exp,

+ sraℑ𝑧

𝑆𝑛, 2

𝑆𝑛, 2

, 2𝑆

𝑛, 2

,

exp,

𝑆𝑛, 2

− sraℜ𝑧exp,

𝑆𝑛, 2

, 2𝑆𝑛, 2 ,

− sraℑ𝑧exp,

𝑆

, 2𝑆𝑛, 2 ,

+ sraℜ𝑧exp,

𝑆𝑛, 2

Whereas if ¬testℜ ∧ 2 XOR testℑ ∧ 2

then 2

−2𝑆𝑛, 2

𝑑𝑆

𝑛, 2

,ℜ

𝑑𝑆

𝑛, 2

,ℑ

=

+2-2S n,2 i:
ℜ𝑧exp : = ℜ𝑧exp, 𝑆

ℑ𝑧exp : = ℑ𝑧exp,

𝑛, 2

𝑆𝑛, 2

𝑛, 2

, 2𝑆𝑛, 2 ,

wherein the signs are reversed in the latter case.
Do the same to any auxiliary registers such as ℜ𝑧′exp, 𝑆

and ℑ𝑧′exp, 𝑆

𝑛, 2

𝑛, 2

to apply the multiplication or division process to these also, producing ℜ𝑧′exp

and ℑ𝑧′exp

.
9. Apply the shift-and-add process effecting the multiplication of the 2-S n,3 terms to the exponential registers:
ℜ𝑧exp,
−𝑆

−2
+

−𝑆𝑛, 3

+2

: = ℜ𝑧exp +

ℜ𝑧exp = − sraℜ𝑧exp , 𝑆𝑛, 3 ,

if testℜ ∧ 1,
if ¬testℜ ∧ 1,

−𝑆𝑛, 3

+2
+

S𝑛, 3

ℜ𝑧exp = + sraℜ𝑧exp , 𝑆𝑛, 3 ,

𝑛, 3

−𝑆𝑛, 3

−2

ℑ𝑧exp = + sraℑ𝑧exp , 𝑆𝑛, 3 ,

if testℑ ∧ 1,

ℑ𝑧

if ¬test

exp

= − sraℑ𝑧

exp

,𝑆

𝑛, 3

,

ℑ

∧ 1,

And:
ℑ𝑧exp,
−𝑆𝑛, 3

−2
+

−𝑆𝑛, 3

+2

−𝑆

−2
+

exp

: = ℑ𝑧exp +

= − sraℑ𝑧

exp

,𝑆

𝑛, 3

,

ℑ𝑧exp = + sraℑ𝑧exp , 𝑆𝑛, 3 ,

if test

ℜ

∧ 1,

if ¬testℜ ∧ 1,

ℜ𝑧exp = − sraℜ𝑧exp , 𝑆𝑛, 3 ,

if testℑ ∧ 1,

ℜ𝑧exp = + sraℜ𝑧exp , 𝑆𝑛, 3 ,

if ¬testℑ ∧ 1,

𝑛, 3

−𝑆𝑛, 3

+2

ℑ𝑧

S𝑛, 3

Do the same to any auxiliary registers such as ℜ𝑧′exp

and ℑ𝑧′exp

to apply the multiplication process to these also, producing ℜ𝑧′exp, 𝑆

𝑛, 3

https://patents.google.com/patent/EP4042270B1/en?q=(high+radix)&oq=high+radix

29/37

2/14/26, 1:56 PM

EP4042270B1 - Hardware algorithm for complex-valued exponentiation and logarithm using simplified sub-steps - Google P…

and ℑ𝑧′exp, 𝑆

𝑛, 3

.
10. Apply the shift-and-add process effecting the multiplication of the 2-2S n,3 term to the exponential registers. As this is the cross-term of a real and
imaginary part, it is guaranteed imaginary, so it has a more limited set of possible manifestations.
Wherein XOR is the logical binary operator of exclusive-or, if testℜ ∧ 1 XOR testℑ ∧ 1

then 2

−2𝑆𝑛, 3

𝑑𝑆

𝑛, 3

,ℜ

𝑑𝑆

−2𝑆𝑛, 3

𝑛, 3

,ℑ

= −2

𝑖

:
ℜ𝑧

exp

: = ℜ𝑧

exp,

ℑ𝑧exp : = ℑ𝑧exp,

𝑆𝑛, 3

𝑆𝑛, 3

+ sraℑ𝑧

, 2𝑆

𝑛, 3

,

exp,

𝑆𝑛, 3

− sraℜ𝑧exp,

𝑆𝑛, 3

, 2𝑆𝑛, 3 ,

− sraℑ𝑧exp,

𝑆

, 2𝑆𝑛, 3 ,

+ sraℜ𝑧exp,

𝑆𝑛, 3

Whereas if ¬testℜ ∧ 1 XOR testℑ ∧ 1

then 2−2𝑆

𝑛, 3

𝑑

𝑆𝑛, 3 , ℜ

𝑑

𝑆𝑛, 3 , ℑ

=

+2-2S n,3 i:
ℜ𝑧exp : = ℜ𝑧exp, 𝑆

ℑ𝑧exp : = ℑ𝑧exp,

𝑛, 3

𝑆𝑛, 3

𝑛, 3

, 2𝑆𝑛, 3 ,

wherein the signs are reversed in the latter case.
Do the same to any auxiliary registers such as (z' exp, Sn,3) and ℑ𝑧′exp, 𝑆

𝑛, 3

to apply the multiplication or division process to these also, producing ℜ𝑧′exp

and ℑ𝑧′exp

.
11. Subtract the corresponding entries in the logarithm tables from the registers:
−𝑆𝑛, 1

log 1 − 2
2

+

−𝑆𝑛, 1

log 1 + 2
2

−2𝑆𝑛, 1

−𝑆𝑛, 2

log 1 − 2
ℜ𝑧log : = ℜ𝑧log −

2

+

−𝑆

log 1 + 2

𝑛, 2

2

−𝑆𝑛, 3

2

2

−1

− tan

−2𝑆𝑛, 3

𝜋

2

+ tan

−1

2

ℑ𝑧log,

n

: = ℑ𝑧log −

+

−1

−𝑆𝑛, 2

𝜋

2

+ tan

−1

𝜋

2

− tan
+

+ tan
𝜋

−1

2

−𝑆

2

−1

𝜋

2

𝑛, 1

−𝑆𝑛, 1

,

𝜋

− tan

−𝑆

2

2

𝑛, 2

−𝑆𝑛, 3

2

,

,

ℜ

∧ 4,

if testℜ ∧ 2,
if ¬testℜ ∧ 2,

,

if test

ℜ

∧ 1,

if ¬testℜ ∧ 1,

if testℜ ∧ 4,
if ¬testℜ ∧ 4,

,

,

2

−𝑆𝑛, 3

,

,

if testℜ ∧ 4,
if ¬test

,

−2𝑆𝑛, 3

√1 + 2

2

𝑛, 2

√1 + 2

−𝑆𝑛, 3

log 1 + 2

+

−2𝑆𝑛, 2

−2𝑆

,

,

√1 + 2

√1 + 2

log 1 − 2
+

−2𝑆𝑛, 1

√1 + 2

√1 + 2

if test

ℜ

∧ 2,

if ¬testℜ ∧ 2,
,

if testℜ ∧ 1,
if ¬test

https://patents.google.com/patent/EP4042270B1/en?q=(high+radix)&oq=high+radix

ℜ

∧ 1,

30/37

2/14/26, 1:56 PM

EP4042270B1 - Hardware algorithm for complex-valued exponentiation and logarithm using simplified sub-steps - Google P…

12. Return to step 2 for the next iteration, until N is reached and the set of step-groups are exhausted up to the required precision, at which point the
registers will contain the final values for the fractional portion of the calculation.
13. Compute range expansion on the values present in the registers, so:
a. If the process is taking logarithmic input and producing exponential output, then the quadrant number held in the integer ℑ𝑧integer, log

is expanded, rotating back via multiplication of the exponentiated value z exp by the appropriate value from {1, i, -1, -i}. If the integer part of the
logarithm was not applied, either this may be applied as a bit shift, or kept as an exponent, allowing the process to emit a floating-point value.
b. If the process is taking exponential input and producing logarithmic output, then if the integer part of the logarithm described by the leading
zeroes count of the first step has not yet been applied to the result, add this value.
[0072] This process may be modified to accept different conditional testing tables and be extended to different radices. The multiplication approach to build the
conditional testing tables in the earlier steps will not be consistent across all 2 n - radix radices and iterations, but may be instead obtained through brute force,
finding a combination of subsets of the input bits which in a particular pattern of cutoff values generate an ascending or descending set of 2 n possible output
values which taken in concert generate a lookup for the n sub-iterations that exhibits the desired convergence behavior.
XIX. Closing Observations
[0073] This disclosure has demonstrated that a reduction in the number of logarithm lookup tables, from eight values per result bit down to three or four, is possible.
This is achieved by treating the real and imaginary parts as separate multiplies when looking up the logarithmic representation via the lookup tables of the
logarithm values. Further, the subtraction of one present in the exponential-to-logarithm process can be merged in the conditional decision-making structure with
negligible impact.
[0074] High radix functionality has been demonstrated by reducing the possible choices of shift-and-add multiplications, to the point where many simple bit switches
computable at the beginning of a single high radix iteration can trigger many parallelizable logarithm subtraction and exponential multiplications, yielding an
algorithm suitable for inclusion into modern high speed integrated circuitry.
XX. Example use cases for bi-directional BKML4m
[0075] It has been shown in BKML4m, which seems to be the most applicable variant of the algorithm taking in account the optimizations for reduced table size,
because of the simple implementation coupled with the ability to achieve both logarithm-to-exponential and exponential-to-logarithm modes within the same
design. This is useful in that it can be used to reversibly achieve micro-operations potentially dispatching per cycle without flushing and mode switching at a low
area cost to complete a greater scope of macro-scale operations than is usually possible. This algorithm is only slightly more expensive than the summed cost
of a real binary logarithm and CORDIC unit, but has greater flexibility than any possible implementation that involves these alone in that the operations computed
by the unit may be changed on a per-result basis without pipeline flushing.
[0076] In the following, it is shown through an example micro-architecture that operations may be completed by using the given invention to efficiently complete all of
the more involved arithmetic operations usually consigned to a plethora of sub-units in complex architectures.
[0077] For simplicity of illustration all registers in this example of a toy instruction set architecture are complex-valued and include an exponent (as real operations are
subsets of the complex-valued operations), $r0 contains a constant read-only zero and $r1 contains a constant read-only one (1.0 + 0.0i). In a real
implementation it is assumed that the details of the inputs and outputs are suitably multiplexed or stubbed to a more realistic register set. The mnemonic then
used to call the method block (which may be any bi-directional method from the above set) is:
METHOD <in_log> <in_exp> <out_log> <out_exp> <direction>

https://patents.google.com/patent/EP4042270B1/en?q=(high+radix)&oq=high+radix

31/37

2/14/26, 1:56 PM

EP4042270B1 - Hardware algorithm for complex-valued exponentiation and logarithm using simplified sub-steps - Google P…

where the auxiliary registers have been left out but may be optionally included for fast (and in the case of divide potentially faster) vector-scalar complex multiply
and divide. It should also be noted that a non-zero <direction> yields logarithm-to-exponential, whereas a zero-direction yield exponential-to-logarithm.
[0078] Then for example:
Logarithm of $r2 in $r3:
METHOD $r0 $r2 $r3 $r0 $r0
Exponential of $r2 in $r3:
METHOD $r2 $r0 $r0 $r3 $r1
Multiplication of $r2 by $r3 into $r4:
METHOD $r0 $r3 $r5 $r0 $r0
METHOD $r5 $r2 $r0 $r4 $r1
Division of $r2 by $r3 into $r4:
METHOD $r0 $r3 $r5 $r0 $r0
NEGATE $r5
METHOD $r5 $r2 $r0 $r4 $r1
Square of $r2 into $r3:
METHOD $r0 $r2 $r4 $r0 $r0
SRA $r5 $r4 1
METHOD $r5 $r0 $r0 $r3 $r1
Square root of $r2 into $r3:
METHOD $r0 $r2 $r4 $r0 $r0
SLA $r5 $r4 1
METHOD $r5 $r0 $r0 $r3 $r1
[0079] Multiplications and divisions may be accelerated further through the use of the vector auxiliary registers that are not included in the above. By subdividing the
real, imaginary and exponent parts of the registers using packing and unpacking instructions, it can be appreciated that sine, cosine, tangent, arcsine, arccosine,
arctangent, conversions between floating-point, fixed-point and integer, other base logarithms, exponentials and powers as well as conversions to degrees and
radians may be computed using this system in different configurations alongside simple bitwise operations. This allows for a succinct design when many
complex-valued operations are required in beam forming applications such as wireless routing, positioning systems, radar as well as applications involving
acoustics and ultrasonics, or a requirement for a single efficient block for computing a high density of mathematical operations.
[0080] A bizarre quirk of this design means that many arithmetic operations have complexities that differ significantly from traditional designs. For instance, complexvalued vector-by-scalar division is the only high-level operation achievable in one call of the method that is not a logarithm or exponential. In practically all
traditional systems this is the most expensive operation to perform, which should lead to a simple approach to detecting an infringing implementation within an
instruction set architecture.
XXII. Conclusion
[0081] In the foregoing specification, specific embodiments have been described. However, one of ordinary skill in the art appreciates that various modifications and
changes can be made without departing from the scope of the invention as set forth in the claims below. Accordingly, the specification and figures are to be
regarded in an illustrative rather than a restrictive sense, and all such modifications are intended to be included within the scope of present teachings.
[0082] Moreover, in this document, relational terms such as first and second, top and bottom, and the like may be used solely to distinguish one entity or action from
another entity or action without necessarily requiring or implying any actual such relationship or order between such entities or actions. The terms "comprises,"
"comprising," "has", "having," "includes", "including," "contains", "containing" or any other variation thereof, are intended to cover a non-exclusive inclusion, such
that a process, method, article, or apparatus that comprises, has, includes, contains a list of elements does not include only those elements but may include
other elements not expressly listed or inherent to such process, method, article, or apparatus. An element proceeded by "comprises ...a", "has ...a", "includes ...a",
"contains ...a" does not, without more constraints, preclude the existence of additional identical elements in the process, method, article, or apparatus that
comprises, has, includes, contains the element. The terms "a" and "an" are defined as one or more unless explicitly stated otherwise herein. The terms
"substantially", "essentially", "approximately", "about" or any other version thereof, are defined as being close to as understood by one of ordinary skill in the art.
The term "coupled" as used herein is defined as connected, although not necessarily directly and not necessarily mechanically. A device or structure that is
"configured" in a certain way is configured in at least that way but may also be configured in ways that are not listed.
[0083] The Abstract of the Disclosure is provided to allow the reader to quickly ascertain the nature of the technical disclosure. It is submitted with the understanding
that it will not be used to interpret or limit the scope or meaning of the claims. In addition, in the foregoing Detailed Description, various features are grouped
together in various embodiments for the purpose of streamlining the disclosure. This method of disclosure is not to be interpreted as reflecting an intention that
the claimed embodiments require more features than are expressly recited in each claim. Rather, as the following claims reflect, inventive subject matter lies in
less than all features of a single disclosed embodiment.

Cited By (22)
Publication number

Priority date

Publication date

Assignee

Title

GB2513884B

2013-05-08

2015-06-17

Univ Bristol

Method and apparatus for producing an acoustic field

GB2530036A

2014-09-09

2016-03-16

Ultrahaptics Ltd

Method and apparatus for modulating haptic feedback

WO2016132141A1

2015-02-20

2016-08-25

Ultrahaptics Ip
Limited

Algorithm improvements in a haptic system

EP3259653B1

2015-02-20

2019-04-24

Ultrahaptics Ip Ltd

Method for producing an acoustic field in a haptic system

US10818162B2

2015-07-16

2020-10-27

Ultrahaptics Ip Ltd

Calibration techniques in haptic systems

US10268275B2

2016-08-03

2019-04-23

Ultrahaptics Ip Ltd

Three-dimensional perceptions in haptic systems

US10943578B2

2016-12-13

2021-03-09

Ultrahaptics Ip Ltd

Driving techniques for phased-array systems

Family To Family Citations

https://patents.google.com/patent/EP4042270B1/en?q=(high+radix)&oq=high+radix

32/37

2/14/26, 1:56 PM

EP4042270B1 - Hardware algorithm for complex-valued exponentiation and logarithm using simplified sub-steps - Google P…

US11531395B2

2017-11-26

2022-12-20

Ultrahaptics Ip Ltd

Haptic effects from focused acoustic fields

EP3729417B1

2017-12-22

2025-09-10

Ultrahaptics Ip Ltd

Tracking in haptic systems

US11704983B2

2017-12-22

2023-07-18

Ultrahaptics Ip Ltd

Minimizing unwanted responses in haptic systems

IL321087A

2018-05-02

2025-07-01

Ultrahaptics Ip Ltd

Blocking element for acoustic transmission with improved efficiency

US11098951B2

2018-09-09

2021-08-24

Ultrahaptics Ip Ltd

Ultrasonic-assisted liquid manipulation

EP3906462B1

2019-01-04

2025-06-18

Ultrahaptics IP Ltd

Mid-air haptic textures

US12373033B2

2019-01-04

2025-07-29

Ultrahaptics Ip Ltd

Mid-air haptic textures

US11842517B2

2019-04-12

2023-12-12

Ultrahaptics Ip Ltd

Using iterative 3D-model fitting for domain adaptation of a hand-pose-estimation
neural network

WO2021074604A1

2019-10-13

2021-04-22

Ultraleap Limited

Dynamic capping with virtual microphones

US11374586B2

2019-10-13

2022-06-28

Ultraleap Limited

Reducing harmonic distortion by dithering

US11715453B2

2019-12-25

2023-08-01

Ultraleap Limited

Acoustic transducer structures

US20210303758A1 *

2020-03-31

2021-09-30

Ultraleap Limited

Accelerated Hardware Using Dual Quaternions

US11816267B2

2020-06-23

2023-11-14

Ultraleap Limited

Features of airborne ultrasonic fields

WO2022058738A1

2020-09-17

2022-03-24

Ultraleap Limited

Ultrahapticons

US12517585B2

2021-07-15

2026-01-06

Ultraleap Limited

Control point manipulation techniques in haptic systems

* Cited by examiner, † Cited by third party, ‡ Family to family citation

Similar Documents
Publication

Publication Date

Title

EP4042270B1

2025-03-19

Hardware algorithm for complex-valued exponentiation and logarithm using simplified sub-steps

US6904446B2

2005-06-07

Floating point multiplier/accumulator with reduced latency and method thereof

EP0421092B1

1999-01-13

Method and apparatus for performing mathematical functions using polynomial approximation and a rectangular aspect ratio
multiplier

US8725786B2

2014-05-13

Approximate SRT division method

JPH08185309A

1996-07-16

Execution method of quadruple-precision arithmetic

Kaivani et al.

2015

Floating-point butterfly architecture based on binary signed-digit representation

Bruguera

2019

Low latency floating-point division and square root unit

Hormigo et al.

2015

Measuring improvement when using HUB formats to implement floating-point systems under round-to-nearest

US20160034256A1

2016-02-04

Fast integer division

CN110235099A

2019-09-13

Device and method for handling input operand value

Savas et al.

2004

Multiplier architectures for GF (p) and GF (2 n)

Shieh et al.

2010

Word-based Montgomery modular multiplication algorithm for low-latency scalable architectures

Miteloudi et al.

2023

PQ. v. ALU. e: Post-quantum RISC-v custom ALU extensions on dilithium and kyber

Bruguera

2023

Radix-64 floating-point division and square root: Iterative and pipelined units

US7240204B1

2007-07-03

Scalable and unified multiplication methods and apparatus

JP7601776B2

2024-12-17

Converting Anchor Data Elements

Nannarelli

2018

Tunable floating-point for energy efficient accelerators

Forget et al.

2021

Comparing posit and IEEE-754 hardware cost

US6598065B1

2003-07-22

Method for achieving correctly rounded quotients in algorithms based on fused multiply-accumulate without requiring the
intermediate calculation of a correctly rounded reciprocal

US8892622B2

2014-11-18

Pipelined divide circuit for small operand sizes

https://patents.google.com/patent/EP4042270B1/en?q=(high+radix)&oq=high+radix

33/37

2/14/26, 1:56 PM

EP4042270B1 - Hardware algorithm for complex-valued exponentiation and logarithm using simplified sub-steps - Google P…

Stine et al.

2020

An efficient implementation of radix-4 integer division using scaling

Sutter et al.

2004

Comparative study of SRT-dividers in FPGA

US8015228B2

2011-09-06

Data processing apparatus and method for performing a reciprocal operation on an input value to produce a result value

US20050246406A9

2005-11-03

Emod a fast modulus calculation for computer systems

US8180822B2

2012-05-15

Method and system for processing the booth encoding 33RD term

Priority And Related Applications
Applications Claiming Priority (2)
Application

Filing date

US201962914498P

2019-10-13

PCT/GB2020/052544

2020-10-13

Title

Hardware algorithm for complex-valued exponentiation and logarithm using simplified sub-steps

Legal Events
Date

Code

Title

Description

2020-10-28

STAA

Information on the status of an ep patent application or granted ep patent

Free format text:
STATUS: UNKNOWN

2021-04-24

STAA

Information on the status of an ep patent application or granted ep patent

Free format text:
STATUS: THE
INTERNATIONAL
PUBLICATION HAS
BEEN MADE

2022-07-15

PUAI

Public reference made under article 153(3) epc to a published international application that has entered the european phase

Free format text:
ORIGINAL CODE:
0009012

2022-07-15

STAA

Information on the status of an ep patent application or granted ep patent

Free format text:
STATUS: REQUEST FOR
EXAMINATION WAS
MADE

2022-08-17

17P

Request for examination filed

Effective date:
20220309

2022-08-17

AK

Designated contracting states

Kind code of ref
document: A1
Designated state(s): AL
AT BE BG CH CY CZ DE
DK EE ES FI FR GB GR
HR HU IE IS IT LI LT LU
LV MC MK MT NL NO
PL PT RO RS SE SI SK
SM TR

2023-01-18

DAV

Request for validation of the european patent (deleted)

2023-01-18

DAX

Request for extension of the european patent (deleted)

2024-10-09

GRAP

Despatch of communication of intention to grant a patent

Free format text:
ORIGINAL CODE:
EPIDOSNIGR1

2024-10-09

STAA

Information on the status of an ep patent application or granted ep patent

Free format text:
STATUS: GRANT OF
PATENT IS INTENDED

2024-11-13

INTG

Intention to grant announced

Effective date:
20241010

2025-02-07

GRAS

Grant fee paid

Free format text:
ORIGINAL CODE:
EPIDOSNIGR3

https://patents.google.com/patent/EP4042270B1/en?q=(high+radix)&oq=high+radix

34/37

2/14/26, 1:56 PM

EP4042270B1 - Hardware algorithm for complex-valued exponentiation and logarithm using simplified sub-steps - Google P…

2025-02-14

GRAA

(expected) grant

Free format text:
ORIGINAL CODE:
0009210

2025-02-14

STAA

Information on the status of an ep patent application or granted ep patent

Free format text:
STATUS: THE PATENT
HAS BEEN GRANTED

2025-03-19

AK

Designated contracting states

Kind code of ref
document: B1
Designated state(s): AL
AT BE BG CH CY CZ DE
DK EE ES FI FR GB GR
HR HU IE IS IT LI LT LU
LV MC MK MT NL NO
PL PT RO RS SE SI SK
SM TR

2025-03-19

REG

Reference to a national code

Ref country code: GB
Ref legal event code:
FG4D

2025-03-31

REG

Reference to a national code

Ref country code: CH
Ref legal event code:
EP

2025-04-09

REG

Reference to a national code

Ref country code: IE
Ref legal event code:
FG4D

2025-04-10

REG

Reference to a national code

Ref country code: DE
Ref legal event code:
R096
Ref document number:
602020047990
Country of ref
document: DE

2025-04-30

U01

Request for unitary effect filed

Effective date:
20250321

2025-04-30

U07

Unitary effect registered

Designated state(s): AT
BE BG DE DK EE FI FR IT
LT LU LV MT NL PT RO
SE SI
Effective date:
20250327

2025-07-07

PG25

Lapsed in a contracting state [announced via postgrant information from national office to epo]

Ref country code: RS
Free format text: LAPSE
BECAUSE OF FAILURE
TO SUBMIT A
TRANSLATION OF THE
DESCRIPTION OR TO
PAY THE FEE WITHIN
THE PRESCRIBED TIMELIMIT
Effective date:
20250619

2025-07-14

PG25

Lapsed in a contracting state [announced via postgrant information from national office to epo]

Ref country code: NO
Free format text: LAPSE
BECAUSE OF FAILURE
TO SUBMIT A
TRANSLATION OF THE
DESCRIPTION OR TO
PAY THE FEE WITHIN
THE PRESCRIBED TIMELIMIT
Effective date:
20250619

https://patents.google.com/patent/EP4042270B1/en?q=(high+radix)&oq=high+radix

35/37

2/14/26, 1:56 PM
2025-07-16

EP4042270B1 - Hardware algorithm for complex-valued exponentiation and logarithm using simplified sub-steps - Google P…
PG25

Lapsed in a contracting state [announced via postgrant information from national office to epo]

Ref country code: HR
Free format text: LAPSE
BECAUSE OF FAILURE
TO SUBMIT A
TRANSLATION OF THE
DESCRIPTION OR TO
PAY THE FEE WITHIN
THE PRESCRIBED TIMELIMIT
Effective date:
20250319

2025-07-21

PG25

Lapsed in a contracting state [announced via postgrant information from national office to epo]

Ref country code: GR
Free format text: LAPSE
BECAUSE OF FAILURE
TO SUBMIT A
TRANSLATION OF THE
DESCRIPTION OR TO
PAY THE FEE WITHIN
THE PRESCRIBED TIMELIMIT
Effective date:
20250620

2025-10-06

PG25

Lapsed in a contracting state [announced via postgrant information from national office to epo]

Ref country code: SM
Free format text: LAPSE
BECAUSE OF FAILURE
TO SUBMIT A
TRANSLATION OF THE
DESCRIPTION OR TO
PAY THE FEE WITHIN
THE PRESCRIBED TIMELIMIT
Effective date:
20250319

2025-10-09

PG25

Lapsed in a contracting state [announced via postgrant information from national office to epo]

Ref country code: ES
Free format text: LAPSE
BECAUSE OF FAILURE
TO SUBMIT A
TRANSLATION OF THE
DESCRIPTION OR TO
PAY THE FEE WITHIN
THE PRESCRIBED TIMELIMIT
Effective date:
20250319

2025-10-15

PG25

Lapsed in a contracting state [announced via postgrant information from national office to epo]

Ref country code: PL
Free format text: LAPSE
BECAUSE OF FAILURE
TO SUBMIT A
TRANSLATION OF THE
DESCRIPTION OR TO
PAY THE FEE WITHIN
THE PRESCRIBED TIMELIMIT
Effective date:
20250319

2025-10-22

PG25

Lapsed in a contracting state [announced via postgrant information from national office to epo]

Ref country code: CZ
Free format text: LAPSE
BECAUSE OF FAILURE
TO SUBMIT A
TRANSLATION OF THE
DESCRIPTION OR TO
PAY THE FEE WITHIN
THE PRESCRIBED TIMELIMIT
Effective date:
20250319

2025-10-27

PG25

Lapsed in a contracting state [announced via postgrant information from national office to epo]

Ref country code: SK
Free format text: LAPSE
BECAUSE OF FAILURE
TO SUBMIT A
TRANSLATION OF THE
DESCRIPTION OR TO

https://patents.google.com/patent/EP4042270B1/en?q=(high+radix)&oq=high+radix

36/37

2/14/26, 1:56 PM

EP4042270B1 - Hardware algorithm for complex-valued exponentiation and logarithm using simplified sub-steps - Google P…
PAY THE FEE WITHIN
THE PRESCRIBED TIMELIMIT
Effective date:
20250319

2025-10-28

PG25

Lapsed in a contracting state [announced via postgrant information from national office to epo]

Ref country code: IS
Free format text: LAPSE
BECAUSE OF FAILURE
TO SUBMIT A
TRANSLATION OF THE
DESCRIPTION OR TO
PAY THE FEE WITHIN
THE PRESCRIBED TIMELIMIT
Effective date:
20250719

2025-11-26

U20

Renewal fee for the european patent with unitary effect paid

Year of fee payment: 6
Effective date:
20251017

2026-01-09

PGFP

Annual fee paid to national office [announced via postgrant information from national office to epo]

Ref country code: GB
Payment date:
20251016
Year of fee payment: 6

2026-01-23

PLBE

No opposition filed within time limit

Free format text:
ORIGINAL CODE:
0009261

2026-01-23

STAA

Information on the status of an ep patent application or granted ep patent

Free format text:
STATUS: NO
OPPOSITION FILED
WITHIN TIME LIMIT

Data provided by IFI CLAIMS Patent Services

About

Send Feedback

Public Datasets

Terms

https://patents.google.com/patent/EP4042270B1/en?q=(high+radix)&oq=high+radix

Privacy Policy

Help

37/37

2/14/26, 1:54 PM

US10067821B2 - Apparatus and method for cyclic redundancy check - Google Patents

Patents
Back to results

3 of 125,048

high radix

(high radix);

Apparatus and method for cyclic redundancy check
Abstract
An apparatus and method for cyclic redundancy check device is provided. The apparatus includes a
multiplicity of sub-block CRC parts configured to receive a bit sequence from each sub-block of a

US10067821B2
United States

transport block that is divided into a multiplicity of sub-blocks and to perform CRC, and a Galois field
adding part configured to add second codes, which are output from the multiplicity of sub-block CRC

Download PDF

Find Prior Art

Similar

parts, in a Galois field, wherein each sub-block CRC part includes a Galois field multiplying part
configured to generate a weight bit sequence by multiplying a first code, which is obtained from CRC
calculation of a sub-block weight code that represents a weight allocated to each sub-block, and the
bit sequence in the Galois field, and a linear feedback shift register including n-numbered registers
and configured to output the second code by adding the weight bit sequence to each register in the

Inventor: Hye Ji KIM, Ji Hoon Kim
Current Assignee : Center for Integrated Smart Sensors
Foundation

Galois field.
Worldwide applications

Images (10)

2015 KR 2016 US

Application US15/184,723 events

Classifications
G06F11/1004 Adding special bits or symbols to the coded information, e.g. parity check,
casting out 9's or 11's to protect a block of data words, e.g. CRC or checksum

2016-06-16

Application filed by Center for Integrated Smart
Sensors Foundation

2016-06-16

Assigned to CENTER FOR INTEGRATED SMART
SENSORS FOUNDATION

2016-12-22

Publication of US20160371142A1

2018-09-04

Application granted

2018-09-04

Publication of US10067821B2

Status

Expired - Fee Related

2036-06-16

Anticipated expiration

G06F7/584 Pseudo-random number generators using finite field arithmetic, e.g. using a linear
feedback shift register
Info: Patent citations (5), Cited by (5), Legal events, Similar

G06F7/724 Finite field arithmetic

documents, Priority and Related Applications

H03M13/09 Error detection only, e.g. using cyclic redundancy check [CRC] codes or single
parity bit

External links: USPTO, USPTO PatentCenter, USPTO
Assignment, Espacenet, Global Dossier, Discuss

H03M13/091 Parallel or block-wise CRC computation
H03M13/618 Shortening and extension of codes
H04L69/324 Intralayer communication protocols among peer entities or protocol data unit
[PDU] definitions in the data link layer [OSI layer 2], e.g. HDLC

G06F2207/581 Generating an LFSR sequence, e.g. an m-sequence; sequence may be
generated without LFSR, e.g. using Galois Field arithmetic

Hide more classifications

Landscapes

Engineering & Computer Science
Physics & Mathematics
Show more

Claims (7)

Hide Dependent

What is claimed is:
1. An apparatus for cyclic redundancy check (CRC) operating with a polynomial having n degrees, the apparatus comprising:

https://patents.google.com/patent/US10067821B2/en?q=(high+radix)&oq=high+radix

1/8

2/14/26, 1:54 PM

US10067821B2 - Apparatus and method for cyclic redundancy check - Google Patents

a multiplicity of sub-block CRC parts configured to receive in parallel a bit sequence from each sub-block of a transport block that is divided into a multiplicity of sub-blocks,
and configured to perform CRC in parallel; and
a Galois field adding part configured to add second codes, which are output from the multiplicity of sub-block CRC parts, in a Galois field,
wherein each sub-block CRC part comprises:
a Galois field multiplying part configured to generate a weight bit sequence by multiplying a first code, which is obtained from CRC calculation of a sub-block weight code
that represents a weight allocated to each sub-block, and the bit sequence in the Galois field; and
a linear feedback shift register including n-numbered registers and configured to output the second code by adding the weight bit sequence to each register in the Galois
field.
2. The apparatus of claim 1, wherein orders of sub-blocks of the transport block increase toward a most significant bit from a least significant bit,
wherein, if an order of a sub-block is P and a size of the sub-block is N, the number of bit of a sub-block weight code of the sub-block is (P−1)×N+1, a most significant
bit of the sub-block weight code is 1, and the rest bits of the sub-block weight code are 0.
3. The apparatus of claim 1, if a size of a sub-block is N and an order of a corresponding sub-block of the transport block is P, further comprising:
a storage unit configured to store a reference weight CRC code that is preliminarily obtained through CRC calculation of a code that has the number of bits of N+1, a
most significant bit of 1, and the rest bits of 0; and
a first code generating part configured to generate a first code from a corresponding sub-block by multiplying the reference weight CRC codes, which are counted by
subtracting 1 from P, in the Galois field.
4. The apparatus of claim 1, wherein, provided a size of the bit sequence is S, if S=1, the linear feedback shift register is a 1-bit serial linear feedback shift register that
calculates 1 bit per one cycle, and if S>1, the linear feedback shift register is an S-bit parallel linear feedback shift register that calculates S bits per one cycle.
5. A method for cyclic redundancy check (CRC) operating with a polynomial having n degrees, the method comprising:
receiving, in parallel, a bit sequence from each sub-block of a transport block that is divided into a multiplicity of sub-blocks;
generating a weight bit sequence by multiplying a first code, which is obtained from CRC calculation of a sub-block weight code that represents a weight allocated to each
sub-block, and the bit sequence in a Galois field;
outputting a second code by adding the weight bit sequence to n-numbered registers, which are included a linear feedback shift register, in the Galois field; and
adding the second codes to the multiplicity of sub-blocks in the Galois field.
6. The method of claim 5, wherein the generating of the weight bit sequence further comprises:
invoking a reference weight CRC code, which is obtained by CRC-calculating of a code that has the number of bits of N+1, a most significant bit of 1, and the rest bits
of 0, from a storage unit; and
generating a first code of a corresponding sub-block by multiplying the reference the weight CRC codes, which are counted by subtracting 1 from P, in the Galois field,
wherein the N is a size of a corresponding sub-block, the P is an order of a corresponding sub-block of the transport block, and orders of sub-blocks of the transport
block increase toward the most significant bit from a least significant bit.
7. A computer-readable storage medium configured to store a computer program that executes the method for CRC according to claim 5.

Description
BACKGROUND
Embodiments of the inventive concept described herein relate to an apparatus and method for cyclic redundancy check.
Cyclic redundancy check (CRC) is an operation mode using a cyclic binary code for detecting an error which is generated during data transmission. CRC is usually
performed by dividing data into units of blocks at a transmission station, by adding a cyclic sign, which is obtained through calculation of a polynomial after the blocks, in
surplus to the data, and by transmitting the sign-added data. For this, a reception station calculates the received data in the same polynomial with the transmission
station and then finds a transmission error from whether the same cyclic sign is obtained as done the transmission state.
A general CRC apparatus takes a very long time for completing CRC due to the serial characteristics of CRC mode, reducing an amount of processing data.
SUMMARY
Embodiments of the inventive concept provide an apparatus and method for CRC, shortening a time for CRC to increase energy efficiency and an amount of processing
data.
In accordance with an aspect of the inventive concept, an apparatus for cyclic redundancy check (CRC) operating with a polynomial having n degrees may include a
multiplicity of sub-block CRC parts configured to receive a bit sequence from each sub-block of a transport block that is divided into a multiplicity of sub-blocks and to
perform CRC, and a Galois field adding part configured to add second codes, which are output from the multiplicity of sub-block CRC parts, in a Galois field, wherein each
sub-block CRC part may include a Galois field multiplying part configured to generate a weight bit sequence by multiplying a first code, which is obtained from CRC
calculation of a sub-block weight code that represents a weight allocated to each sub-block, and the bit sequence in the Galois field, and a linear feedback shift register
including n-numbered registers and configured to output the second code by adding the weight bit sequence to each register in the Galois field.
If an order of a sub-block is P and a size of the sub-block is N, the number of bit of a sub-block weight code of the sub-block may be (P−1)×N+1, a most significant bit of
the sub-block weight code may be 1, and the rest bits of the sub-block weight code may be 0.
If a size of a sub-block is N and an order of a corresponding sub-block of the transport block is P, the apparatus may further include a storage unit configured to store a
reference weight CRC code that is preliminarily obtained through CRC calculation of a code that has the number of bits of N+1, a most significant bit of 1, and the rest

https://patents.google.com/patent/US10067821B2/en?q=(high+radix)&oq=high+radix

2/8

2/14/26, 1:54 PM

US10067821B2 - Apparatus and method for cyclic redundancy check - Google Patents

bits of 0, and a first code generating part configured to generate a first code from a corresponding sub-block by multiplying the reference weight CRC codes, which are
counted by subtracting 1 from P, in the Galois field.
Provided a size of the bit sequence is S, if S=1, the linear feedback shift register may be a 1-bit serial linear feedback shift register that calculates 1 bit per one cycle, and
if S>1, the linear feedback shift register may be an S-bit parallel linear feedback shift register that calculates S bits per one cycle.
In accordance with another aspect of the inventive concept, a method for CRC operating with a polynomial having n degrees may include receiving a bit sequence from
each sub-block of a transport block that is divided into a multiplicity of sub-blocks, generating a weight bit sequence by multiplying a first code, which is obtained from
CR calculation of a sub-block weight code that represents a weight allocated to each sub-block, and the bit sequence in a Galois field, outputting a second code by
adding the weight bit sequence to n-numbered registers, which are included a linear feedback shift register, in the Galois field, and adding the second codes to the
multiplicity of sub-blocks in the Galois field.
The generating of the weight bit sequence may further include invoking a reference weight CRC code, which is obtained by CRC-calculating of a code that has the number
of bits of N+1, a most significant bit of 1, and the rest bits of 0, from a storage unit, and generating a first code of a corresponding sub-block by multiplying the reference
the weight CRC codes, which are counted by subtracting 1 from P, in the Galois field.
In accordance with still another aspect of the inventive concept, a method for CRC may be implemented in a computer program stored in a medium for the purpose of
execution in connection with a computer.
BRIEF DESCRIPTION OF THE FIGURES
The above and other objects and features will become apparent from the following description with reference to the following figures, wherein like reference numerals
refer to like parts throughout the various figures unless otherwise specified, and wherein:
FIG. 1 is a block diagram exemplarily illustrating an apparatus for CRC according to an embodiment of the inventive concept;
FIG. 2 is a diagram illustrating a sub-block CRC part of an apparatus for CRC, in detail, according to an embodiment of the inventive concept;
FIG. 3 is an exemplary flow chart showing a method for CRC according to an embodiment of the inventive concept;
FIG. 4A is a block diagram exemplarily illustrating a 1-bit serial linear feedback shift register;
FIG. 4B is a diagram illustrating a scheme of inputting a weight bit sequence into a 1-bit serial linear feedback shift register according to an embodiment of the inventive
concept;
FIG. 4C is a table showing a calculation process of a 1-bit serial linear feedback shift register according to an embodiment of the inventive concept;
FIG. 5A is a block diagram exemplarily illustrating a 2-bit parallel linear feedback shift register;
FIG. 5B is a diagram a scheme of inputting a weight bit sequence into a 2-bit serial linear feedback shift register according to an embodiment of the inventive concept;
and
FIG. 5C is a table showing a calculation process of a 1-bit serial linear feedback shift register according to an embodiment of the inventive concept.
DETAILED DESCRIPTION
Other aspects, advantages, and salient features of the present disclosure will become apparent to those skilled in the art from the following detailed embodiments.
Various embodiments described herein, however, may not be intentionally confined in specific embodiments, but should be construed as including diverse modifications,
equivalents, and/or alternatives. Various embodiments are merely provided to help those skilled in the art to clearly understand the technical scope of the inventive
concept and the present disclosure may be only defined by the scope of the annexed claims.
Unless otherwise defined herein, all the terms used herein (including technical or scientific terms) may have the same meaning that is generally acceptable by universal
technology in the related art of the inventive concept. It will be further understood that terms, which are defined in a dictionary and commonly used, may also be
interpreted as is customary in the relevantly related art and/or as is same in the description of the present application. Even in the case of terminological expression with
insufficient clarification, such terms may not be conceptualized or overly interpreted in formality.
In the description, the terms of a singular form may also include plural forms unless otherwise specified. The terms ‘include’ and/or its diverse inflections or
conjugations, for example, ‘inclusion’, ‘including’, ‘includes’, or ‘included’, as used herein, may be construed such that any one of a constitution, a component, an element,
a step, an operation, and/or a device does not exclude presence or addition of one or more different constitutions, components, elements, steps, operations, and/or
devices. Additionally, the term ‘and/or’ may be understood as indicating respective ones of elements or any one of various combinations between the elements.
As used herein, the terms ‘˜part’, ‘˜or (er)’, ‘˜block’, or ‘˜module’ may mean a unit of processing at least one function or operation. For example, the terms may mean
software or a hardware element such as FPGA or ASIC, but may not be restrictive to such software or hardware. Such an element named as ‘˜part’, ‘˜or (er)’, ‘˜block’, or
‘˜module’ may be configured in an addressable storage medium, or even configured to actuate one or more processors.
As an example, ‘˜part’, ‘˜or (er)’, ‘˜block’, or ‘˜module’ may include software elements, object-oriented software elements, class elements, task elements, processes,
functions, properties, procedures, sub-routines, segments of a program code, drivers, firmware, micro-codes, circuits, data, databases, data structures, tables, arrays, and
parameters. Functions provided to elements, ‘˜part’, ‘˜or (er)’, ‘˜block’, or ‘˜module’ may be coupled with a smaller number of elements, ‘˜parts’, ‘˜ors (ers)’, ‘˜blocks’, or
‘˜modules’, or may be further divided into additional elements, ‘˜parts’, ‘˜ors (ers)’, ‘˜blocks’, or ‘˜modules’
Hereafter, embodiments of the inventive concept will be described in conjunction with the accompanied figures.
FIG. 1 is a block diagram exemplarily illustrating an apparatus 100 for CRC according to an embodiment of the inventive concept.
As illustrated in FIG. 1, the CRC apparatus 100 may include a multiplicity of sub-block CRC parts 110 and a Galois field adding part 130.
The sub-block CRC part 110 may CRC-check sub-blocks which are divided from a transport block. The transport block may be divided into a multiplicity of sub-blocks and
the multiplicity of sub-block CRC parts 110 receives bit sequences respectively from respective sub-blocks.
According to an embodiment, the transport block may be divided into a multiplicity of sub-blocks and may be decoded in parallel in a parallel decoder, e.g., a turbo
decoder. For example, the parallel decoder may coincidently decode the multiplicity of sub-blocks.
According to an embodiment, the sub-block CRC part 110 may receive bit sequences from a high-radix parallel decoder and may CRC-check the received bit sequences.

https://patents.google.com/patent/US10067821B2/en?q=(high+radix)&oq=high+radix

3/8

2/14/26, 1:54 PM

US10067821B2 - Apparatus and method for cyclic redundancy check - Google Patents

The high-radix parallel decoder may decode received data and then may coincidently output a multiplicity of bit sequences. A radix-M parallel decoder may coincidently
output log2 M bit sequences. For example, a radix-4 parallel decoder may coincidently output 2 bit sequences and a radix-16 parallel decoder may coincidently output 4
bit sequences. As a result, the sub-block CRC part 110 may receive log2 M bit sequences from the radix-M parallel decoder 100 and may CRC-check the received log2 M
bit sequences.
The Galois field adding part 130 may add codes, which are output from the multiplicity of sub-block CRC part 110, in a Galois field. According to an embodiment, the
Galois field adding part 130 may be implemented using an XOR gate.
FIG. 2 is a diagram illustrating a sub-block CRC part 110 of an apparatus for CRC, in detail, according to an embodiment of the inventive concept.
As illustrated in FIG. 2, each sub-block CRC part 110 may include a Galois field multiplying part 1101 and a linear feedback shift register 1102.
The Galois field multiplying part 1101 may generate a weight bit sequence by multiplying a bit sequence, which is received from each sub-block, and a first code in the
Galois field. The first code may be obtained through CRC calculation of a sub-block weight code which represents a weight allocated to each sub-block.
The sub-block weight code may have the number of bits of
(P−1)×N+1,
and may have the most significant bit of 1. The rest bits of the sub-block weight code may be 0. In the equation, N denotes a size of a corresponding sub-block and P
denotes an order of a corresponding sub-block of a transport block.
For example, when a transport block is given as A(x)=[101101110010111101], the transport block A(x) may be divided into three sub-blocks Sub-block 3 ([101101]), Subblock 2 ([110010]), and Sub-block 1 ([111101]). Orders of the sub-blocks may increase toward the most significant bit from the least significant bit in the transport block.
A sub-block weight code of Sub-block 2 may have the number of bits of (2−1)·6+1=7 and may be [1000000] in which the most significant bit is 1 and the rest bits are 0.
[1000000] may be processed through CRC calculation with a CRC generation code [1000011] to obtain a first code [000011] to the Sub-block 2. The CRC generation code
may be variable in accordance with embodiments.
The linear feedback shift register 1102 may receive the weight bit sequence and then may output a second code. A Galois field adding part 130 may output a CRC code
by adding second codes, which are output from a multiplicity of sub-block CRC part 110, in the Galois field. An operation of the feedback shift register 1102 will be
detailed later in conjunction with FIGS. 4A to 5C.
Referring to FIG. 2, each sub-block CRC part 110 may further include a storage unit 1103 and a first code generating part 1104.
When a size of the sub-block is N, the storage unit 1103 may preliminarily calculate and store a reference weight CRC code which is obtained through CRC calculation of
a code in which the number of bits is N+1, the most significant bit is 1, and the rest bits are 0.
For example, a reference weight code of Sub-blocks 1 to 3 may be [000011] which is obtained through CRC calculation of [1000000], which has the number of bits 6+1=7,
the most significant bit of 1, and the rest bits of 0, with a CRC generation code [100011]. As also, the CRC generation code may be variable in accordance with
embodiments.
According to an embodiment, the storage unit 1103 may include a register and the reference weight CRC code may be formed of a lookup table. However, the storage
unit may not be restrictive in a kind and may even include a storage element such as memory.
When an order of a corresponding sub-block of a transport block is P, the first code generating part 1104 may generate a first code to the corresponding sub-block by
multiplying the reference weight CRC codes, which are counted by subtracting 1 from P, in the Galois field.
As an example, to generate a first code to Sub-block 3, the first code generating part 1104 may invoke a reference weight CRC code ([000011]) from the storage unit and
then may multiply two reference weight codes, which are counted by subtracting 1 from 3 that is an order of Sub-block 3, in the Galois field (i.e., the reference weight CRC
code of Sub-block 3=[000011] [000011], where

denotes a Galois field multiplication operator).

FIG. 3 is an exemplary flow chart showing a method 200 for CRC according to an embodiment of the inventive concept.
The CRC method 200 may be performed through a CRC apparatus 100 described above according to an embodiment of the inventive concept.
As illustrated in FIG. 3, the CRC method 200 may include steps of receiving a bit sequence and multiplying a first code and the received bit sequence in a Galois field to
generate a weight bit sequence (S203), outputting a second code from a linear feedback shift register by adding the weight bit sequence in the Galois field (S204), and
adding second codes of a multiplicity of sub-blocks in the Galois field (S205). The step of outputting the second code (S204) will be detailed later in conjunction with
FIGS. 4A to 5C.
According to an embodiment, the step S203 may include a step of receiving the bit sequence from a high-radix parallel decoder which decodes sub-blocks which are
divided from a transport block.
The high-radix parallel decoder may be a radix-4 parallel decoder, but the parallel decoder may not be restrictive in degrees of radix.
According to an embodiment of the inventive concept, the CRC method 200 may include steps of invoking a reference weight CRC code from a storage unit (S201), and
generating a first code of a sub-block by repeatedly multiplying the reference weight CRC code in a Galois field (S202).
According to an embodiment, the reference weight CRC code has the number of bits equal to a value that is made by summing up 1 into a size of a sub-block, in which
the most significant bit may be 1 and the rest bits may be composed of 0.
According to an embodiment, the step S202 of generating a first code of the sub block may include a step of multiplying reference weight CRC codes, which are counted
by subtracting 1 from an order of a corresponding sub-block of a transport block, in the Galois field.
According to an embodiment of the inventive concept, the step S201 and the step S202 may be performed before the step 203.
Hereafter, FIGS. 4A to 5C will be now referred to describe a calculation process for outputting a second code from a linear feedback shift register 1102 which receives a
weight bit sequence from a Galois field multiplying part 1101.
In embodiments shown in FIGS. 4A to 5C, a CRC generation polynomial and a CRC generation code, which are used in the CRC part 100, may be given as g(x) as follows,
and a CRC bit width may be assumed as 6 bits. Additionally, a transport block may be assumed as being given like C(x) as follows.

https://patents.google.com/patent/US10067821B2/en?q=(high+radix)&oq=high+radix

4/8

2/14/26, 1:54 PM
g(x)=x

6

US10067821B2 - Apparatus and method for cyclic redundancy check - Google Patents

+x+1=[1000011]2

C(x)=[101101110010111101]2
The transport block C(x) may be divided into a multiplicity of sub-blocks. The multiplicity of sub-blocks may be coincidently processed in parallel through a CRC
apparatus 100 according to an embodiment of the inventive concept.
C(x) may be divided into 3 sub-blocks. Although the divided sub-blocks are exemplified as having the same size of 6, embodiments of the inventive concept may not be
restrictive to the same size for all of the divided sub-blocks. The divided sub-blocks may be all different in sizes.

A 2(x)=[101101], A 1(x)=[110010], A 0(x)=[111101]
Orders P of the sub-blocks A2(x), A1(x), and A0(x) may become 3, 2, and 1, respectively. As the size N of the sub-blocks is the same as 6, weight codes of the sub-blocks
are [1000000000000], [1000000], and [1] respectively. A result of calculating a first code respectively to the sub-blocks by CRC-calculating the sub-block weight codes
with CRC generation codes may be given as follows.
CRC[x 12]=[000101], CRC[x 6]=[000011]. CRC[x 0]=[000001]
Hereafter, a calculation process of a linear feedback shift register will be described in two cases of dividing the transport block C(x) into a bit sequence of 1-bit data to
each sub-block, and dividing the transport block C(x) into a bit sequence of 2-bit data to each sub-block.
Referring to FIGS. 4A to 4C, in the case that data processed in one cycle by a linear feedback shift register according to an embodiment of the inventive concept is 1 bit, a
calculation process of the linear feedback shift register will be described in detail. In this process, the linear feedback shift register may be a 1-bit serial linear feedback
shift register.
In the case that a characteristic polynomial is

g(x)=x 6 +x+1,
FIG. 4A illustrates a 1-bit serial linear feedback shift register. As registers having outputs affecting inputs are ordered in 0 and 5, the characteristic polynomial may be
given as the g(x).
FIG. 4B illustrates a 1-bit serial linear feedback shift register included in a CRC apparatus 100 according to an embodiment of the inventive concept, in which an input
value to each register may be added in a Galois field and then may be shifted by 1 bit every cycle after XOR calculation.
FIG. 4C is a table showing a calculation process of a 1-bit serial linear feedback shift register. As illustrated in FIG. 2, a value input into the linear feedback shift register
1102 may be an output of a Galois field multiplying part 1101. As illustrated in FIG. 4C, a transport block C(x) may be divided into a multiplicity of sub-blocks and the
multiplicity of sub-blocks are processed at the same time.
The divided sub-blocks may be divided into 6 bit sequences, respectively. For example, A2(x) may be divided into [1], [0], [1], [1], [0], and [1]. Each bit sequence may be
input into the Galois field multiplying part 1101. The Galois field multiplying part 1101 may multiply a bit sequence and a first code in a Galois field to generate a weight
bit sequence which is made by applying a weight of a corresponding sub-block to each bit sequence.
A weight bit sequence of A2(x), i.e., an output of the Galois field multiplying part 1101, may be input one by one every cycle (clock cycle, T) into the linear feedback shift
register. As a result, the 1-bit serial linear shift register may process 1-bit data every cycle. Input values may be represented as follows in consideration that the weight bit
sequence is input and calculated in the linear feedback shift register.

T=1)[00010100000]
T=2)[00000000000]
T=3)[00000101000]
T=4)[00000010100]
T=5)[00000000000]
T=6)[00000000101]
As an input value to each register of the 1-bit serial linear feedback shift register is shifted by 1 bit every cycle, the input value may be the same with the shadow section
in the weight bit sequence, i.e., the column of ‘GF mult’ in the table of FIG. 4C.
In implementing a practical hardware structure, because there is no input of data ‘0’, practical hardware expressions of the input values of the linear feedback shift
register may be represented as shown in the following Table 1.
TABLE 1
T

1
101

2

3

4

5

6

0

0

0

0

0

000

0

0

0

0

101

0

0

0

101

0

0

000

0
101

Calculation of a linear feedback shift register may continue until the last data to each block is input and processed (T=6), and a calculated value of the last data may be
output as a second code. Referring to FIG. 4C, a value of LFSR out at the last cycle may be a second code of a corresponding sub-block. As an example, a linear feedback
shift register may receive a weight bit sequence of the sub-block A2(x) and may output the second code [011111].
A multiplicity of sub-block CRC parts 110 may coincidently process sub-blocks A2(x), A1(x), and A0(x) to output second codes [011111], [010101], and [111101]. A Galoir
field adding part 130 may generate a CRC code to the total transport block C(x) by adding the second codes in a Galoir field. Accordingly, the CRC code to C(x) may be
[110111] that is made by adding all of [011111], [010101], and [111101] in the Galoir field.
Referring to FIGS. 5A to 5C, a calculation process of a linear feedback shift register will be described in detail in the case that the linear feedback shift register according
to an embodiment of the inventive concept processes 2-bit data per cycle. In this process, the linear feedback shift register may be a 2-bit parallel linear feedback shift
register.

https://patents.google.com/patent/US10067821B2/en?q=(high+radix)&oq=high+radix

5/8

2/14/26, 1:54 PM

US10067821B2 - Apparatus and method for cyclic redundancy check - Google Patents

In the case that a characteristic polynomial is

g(x)=x6+x+1,
FIG. 5A illustrates a 2-bit parallel linear feedback shift register. As registers having outputs affecting inputs are ordered in 0 and 5, the characteristic polynomial may be
given as the g(x).
FIG. 5B illustrates a 2-bit parallel linear feedback shift register included in a CRC apparatus 100 according to an embodiment of the inventive concept, in which an input
value to each register may be added in a Galois field and then may be shifted by 2 bits every cycle after XOR calculation.
FIG. 5C is a table showing a calculation process of a 2-bit parallel linear feedback shift register. As illustrated in FIG. 2, a value input into the linear feedback shift register
1102 may be an output of a Galois field multiplying part 1101. As illustrated in FIG. 5C, a transport block C(x) may be divided into a multiplicity of sub-blocks and the
multiplicity of sub-blocks are processed at the same time.
The divided sub-blocks may be divided into 3 bit sequences, respectively. For example, A2(x) may be divided into [10], [11], and [01]. Each bit sequence may be input into
the Galois field multiplying part 1101. The Galois field multiplying part 1101 may multiply a bit sequence and a first code in a Galois field to generate a weight bit
sequence which is made by applying a weight of a corresponding sub-block to each bit sequence.
A weight bit sequence of A2(x), i.e., an output of the Galois field multiplying part 1101, may be input one by one every cycle into the linear feedback shift register. As a
result, the 2-bit parallel linear shift register may process 2-bit data every cycle.
Calculation of a linear feedback shift register may continue until the last data to each block is input and processed (T=3), and a calculated value of the last data may be
output as a second code. Referring to FIG. 5C, a value of LFSR out at the last cycle may be a second code of a corresponding sub-block. As an example, a linear feedback
shift register may receive a weight bit sequence of the sub-block A2(x) and may output the second code [011111].
A multiplicity of sub-block CRC parts 110 may coincidently process sub-blocks A2(x), A1(x), and A0(x) to output second codes [011111], [010101], and [111101]. A Galoir
field adding part 130 may generate a CRC code to the total transport block C(x) by adding the second codes in a Galoir field. Accordingly, the CRC code to C(x) may be
[110111] that is made by adding all of [011111], [010101], and [111101] in the Galoir field. It can be seen that this CRC code is the same with a value which is calculated
through a 1-bit serial linear feedback register.
It may be allowable to shorten a time for CRC, increasing energy efficiency and an amount of processing data.
While the inventive concept has been described with reference to exemplary embodiments, it will be apparent to those skilled in the art that various changes and
modifications may be made without departing from the spirit and scope of the inventive concept. Therefore, it should be understood that the above embodiments are not
limiting, but illustrative.

Patent Citations (5)
Publication number

Priority date

Publication date

Assignee

Title

US5912881A *

1995-12-20

1999-06-15

International Business Machines
Corporation

Method and apparatus for fast checking the frame check sequence of a
segmented message

US6189124B1 *

1996-10-29

2001-02-13

International Business Machines
Corporation

Method and apparatus for a two-step calculation of CRC-32

US9009565B1 *

2013-03-15

2015-04-14

Pmc-Sierra, Inc.

Systems and methods for mapping for solid-state memory

US9026867B1 *

2013-03-15

2015-05-05

Pmc-Sierra, Inc.

Systems and methods for adapting to changing characteristics of multilevel cells in solid-state memory

2007-09-14

2010-12-14

Motorola Mobility, Inc.

Multi-layer cyclic redundancy check code in wireless communication
system

Family To Family Citations
US7853857B2

* Cited by examiner, † Cited by third party

Cited By (5)
Publication number

Priority date

Publication date

Assignee

Title

US20250390380A1 *

2024-06-21

2025-12-25

Xilinx, Inc.

Redundant data storage using incremental error detection with
non-zero seeds

JP6920307B2 *

2015-12-31

2021-08-18

アイディーエーシー ホールディングス
インコーポレイテッド

Waveform-based data integrity check and error correction

CN108540137B *

2018-03-02

2021-09-03

江西清华泰豪三波电机有限公司

Cyclic redundancy check code generation method and device

GB2585207B *

2019-07-01

2021-06-16

Accelercomm Ltd

Cyclic redundancy check computation circuit, communication
unit, and method therefor

CN119987720B *

2025-04-15

2025-08-15

上海壁仞科技股份有限公司

Pseudo-random sequence generating circuit and pseudo-random
sequence generating method for processor

Family To Family Citations

* Cited by examiner, † Cited by third party, ‡ Family to family citation

https://patents.google.com/patent/US10067821B2/en?q=(high+radix)&oq=high+radix

6/8

2/14/26, 1:54 PM

US10067821B2 - Apparatus and method for cyclic redundancy check - Google Patents

Similar Documents
Publication

Publication Date

Title

US9071275B2

2015-06-30

Method and device for implementing cyclic redundancy check codes

US10067821B2

2018-09-04

Apparatus and method for cyclic redundancy check

US10187085B2

2019-01-22

Decoding method, decoding apparatus and decoder

US20040078411A1

2004-04-22

Galois field arithmetic unit for use within a processor

US7343472B2

2008-03-11

Processor having a finite field arithmetic unit utilizing an array of multipliers and adders

CN107239362B

2020-06-05

Parallel CRC (Cyclic redundancy check) code calculation method and system

WO2013006388A2

2013-01-10

Efficient and scalable cyclic redundancy check circuit using galois-field arithmetic

US7403964B2

2008-07-22

Galois field multiplier array for use within a finite field arithmetic unit

US10333554B2

2019-06-25

Efficient generalized tensor product codes encoding schemes

CN101847999B

2012-10-10

Method for performing parallel check by using cyclic redundancy check codes

US10171109B2

2019-01-01

Fast encoding method and device for Reed-Solomon codes with a small number of redundancies

US20030159103A1

2003-08-21

Efficient method for fast decoding of BCH binary codes

US20150007000A1

2015-01-01

Additional Error Correction Apparatus and Method

CN1192486C

2005-03-09

Integrated circuit implementing method and circuit of shortened cyclic code correcting interpretation algorithm

US10009041B2

2018-06-26

BCH decorder in which folded multiplier is equipped

US11360850B2

2022-06-14

Error protection key generation method and system

Zheng et al.

2016

An efficient eligible error locator polynomial searching algorithm and hardware architecture for one-pass Chase decoding of BCH
codes

US12034454B2

2024-07-09

Verifying data integrity in a receiver

KR100731985B1

2007-06-25

Pipeline structure Parallel cyclic redundancy check apparatus and method

CN115632662A

2023-01-20

Syndrome calculation method, device, equipment and medium in RS decoding

US12255668B2

2025-03-18

Error correction with fast syndrome calculation

US12218688B2

2025-02-04

Error correction with fast syndrome calculation

US9501262B2

2016-11-22

Vectorized Galois field multiplication

TWI551059B

2016-09-21

MULTI-CODE CHIEN&#39;S SEARCH CIRCUIT FOR BCH CODES WITH VARIOUS VALUES OF M IN GF(2m)

US8381080B2

2013-02-19

Reducing a degree of a polynomial in a polynomial division calculation

Priority And Related Applications
Applications Claiming Priority (2)
Application

Filing date

Title

KR1020150084788A

2015-06-16

Cyclic redundancy checking apparatus and method

KR10-2015-0084788

2015-06-16

Legal Events
Date

Code

Title

Description

2016-06-16

AS

Assignment

Owner name: CENTER FOR INTEGRATED SMART SENSORS FOUNDATION, KO
Free format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:KIM, HYE JI;KIM, JI
HOON;REEL/FRAME:039604/0602
Effective date: 20160616

https://patents.google.com/patent/US10067821B2/en?q=(high+radix)&oq=high+radix

7/8

2/14/26, 1:54 PM

US10067821B2 - Apparatus and method for cyclic redundancy check - Google Patents

2018-08-21

STCF

Information on status: patent grant

Free format text: PATENTED CASE

2022-04-25

FEPP

Fee payment procedure

Free format text: MAINTENANCE FEE REMINDER MAILED (ORIGINAL EVENT CODE: REM.); ENTITY
STATUS OF PATENT OWNER: SMALL ENTITY

2022-10-10

LAPS

Lapse for failure to pay maintenance fees

Free format text: PATENT EXPIRED FOR FAILURE TO PAY MAINTENANCE FEES (ORIGINAL EVENT
CODE: EXP.); ENTITY STATUS OF PATENT OWNER: SMALL ENTITY

2022-10-10

STCH

Information on status: patent discontinuation

Free format text: PATENT EXPIRED DUE TO NONPAYMENT OF MAINTENANCE FEES UNDER 37 CFR
1.362

2022-11-01

FP

Lapsed due to failure to pay maintenance fee

Effective date: 20220904

Data provided by IFI CLAIMS Patent Services

About

Send Feedback

Public Datasets

Terms

https://patents.google.com/patent/US10067821B2/en?q=(high+radix)&oq=high+radix

Privacy Policy

Help

8/8

2/14/26, 1:57 PM

US10153985B2 - Dragonfly processor interconnect network - Google Patents

Patents
Back to results

8 of 125,048

high radix

(high radix);

Dragonfly processor interconnect network
Abstract
A multiprocessor computer system comprises a dragonfly processor interconnect network that
comprises a plurality of processor nodes, a plurality of routers, each router directly coupled to a

US10153985B2
United States

plurality of terminal nodes, the routers coupled to one another and arranged into a group, and a
plurality of groups of routers, such that each group is connected to each other group via at least one

Download PDF

Find Prior Art

Similar

direct connection.
Inventor: John Kim, Dennis C. Abts, Steven L. Scott, William J.
Dally

Images (7)

Current Assignee : Intel Corp , Leland Stanford Junior University

Worldwide applications
2008 US 2014 US 2017 US

Application US15/435,952 events
2017-02-17

Application filed by Intel Corp, Leland Stanford
Junior University

Classifications
2017-02-17

Priority to US15/435,952

H04L49/15 Interconnection of switching modules

2017-12-07

Publication of US20170353401A1

G06F13/4027 Coupling between buses using bus bridges

2018-12-11

Application granted

G06F13/4221 Bus transfer protocol, e.g. handshake; Synchronisation on a parallel bus being

2018-12-11

Publication of US10153985B2

Status

Active

2028-08-20

Anticipated expiration

an input/output bus, e.g. ISA bus, EISA bus, PCI bus, SCSI bus
G06F15/17375 One dimensional, e.g. linear array, ring
G06F9/45533 Hypervisors; Virtual machine monitors
H04L45/58 Association of routers

Info: Patent citations (42), Non-patent citations (55) , Cited by
(54), Legal events, Similar documents, Priority and Related

H04L49/70 Virtual switches

Applications

H04L45/28 Routing or path finding of packets in data switching networks using route fault
recovery

External links: USPTO, USPTO PatentCenter, USPTO
Assignment, Espacenet, Global Dossier, Discuss

H04L49/1515 Non-blocking multistage, e.g. Clos
Hide more classifications

Landscapes

Engineering & Computer Science
Theoretical Computer Science
Show more

Claims (18)

Hide Dependent

The invention claimed is:
1. A multiprocessor computer system comprising a dragonfly processor interconnect network, the dragonfly processor interconnect network comprising:
a plurality of processor nodes;
a first plurality of routers, each router in the first plurality of routers directly coupled to a respective subset of the plurality of processor nodes, the routers in the first plurality
of routers coupled to one another and arranged into a first group of routers in a plurality of groups of routers,
a second plurality of routers, each router in the second plurality of routers directly coupled to a respective subset of the plurality of processor nodes, the routers in the
second plurality of routers coupled to one another and arranged into a second group of routers in a plurality of groups of routers;

https://patents.google.com/patent/US10153985B2/en?q=(high+radix)&oq=high+radix

1/13

2/14/26, 1:57 PM

US10153985B2 - Dragonfly processor interconnect network - Google Patents

wherein each group in the plurality of groups of routers is connected to each other group via a single respective direct connection, each of the direct connections comprises
a respective global channel, and the routers route data using credit round-trip latency as an indicator of channel congestion.
2. The multiprocessor computer system of claim 1, wherein each group acts as a high radix virtual router.
3. The multiprocessor computer system of claim 1, wherein a virtual radix of each group is a product of a number of routers in each group multiplied by a sum of a
number of processor nodes connected to each router plus a number of global channels in the group.
4. The multiprocessor computer system of claim 1, wherein a number of routers per group is equal to twice a number of processor nodes per router, and wherein the
number of processor nodes per router is equal to a number of channels per router connected to other groups.
5. The multiprocessor computer system of claim 1, wherein a number of routers in a group is greater than twice a number of global channels per router.
6. The multiprocessor computer system of claim 1, wherein a number of processor nodes per router is greater than a number of global channels per router.
7. The multiprocessor computer system of claim 1, wherein the routers within a group are connected via a flattened butterfly network.
8. The multiprocessor computer system of claim 1, wherein the router is to route data using selective virtual channel discrimination.
9. The multiprocessor computer system of claim 1, wherein all minimal routes within the network traverse at most one global channel.
10. A method of operating a multiprocessor computer system, comprising:
communicating a message from a processor node to a router, the router coupled to a plurality of processor nodes;
communicating the message between two or more routers, the routers coupled to one another and arranged into a group, wherein the group is one of a plurality of groups of
routers and each group is connected to each other group in the plurality of groups of routers via a respective global channel; and
communicating data between two groups of routers using a corresponding one of the global channels, wherein each group is connected to each other group of a network via
a single respective direct connection, wherein the routers to route data using credit round-trip latency as an indicator of channel congestion.
11. The method of operating a multiprocessor computer system of claim 10, wherein each group acts as a high radix virtual router.
12. The method of operating a multiprocessor computer system of claim 10, wherein a virtual radix of each group is a product of a number of routers in each group
multiplied by a sum of a number of processor nodes connected to each router plus a number of global channels in the group.
13. The method of operating a multiprocessor computer system of claim 10, wherein a number of routers per group is equal to twice a number of processor nodes per
router, and wherein the number of processor nodes per router is equal to a number of channels per router connected to other groups.
14. The method of operating a multiprocessor computer system of claim 10, wherein a number of routers in a group is greater than twice a number of global channels
per router.
15. The method of operating a multiprocessor computer system of claim 10, wherein a number of processor nodes per router is greater than a number of global channels
per router.
16. The method of operating a multiprocessor computer system of claim 10, wherein the routers within a group are connected via a flattened butterfly network.
17. The method of operating a multiprocessor computer system of claim 10, wherein the routers route data using selective virtual channel discrimination.
18. The method of operating a multiprocessor computer system of claim 10, wherein all minimal routes within the network traverse at most one global channel.

Description
RELATED APPLICATIONS
This application is a continuation of and claims priority to U.S. patent application Ser. No. 14/583,588, filed on Dec. 27, 2014 and entitled DRAGONFLY PROCESSOR
INTERCONNECT NETWORK, which application is a continuation of and claims priority to U.S. patent application Ser. No. 12/195,198, filed on Aug. 20, 2008 and entitled
DRAGONFLY PROCESSOR INTERCONNECT NETWORK. The disclosures of both of these prior Applications are considered part of and are incorporated by reference in the
disclosure of this application.
FIELD OF THE INVENTION
The invention relates generally to computer interconnect networks, and more specifically in one embodiment to a dragonfly topology processor interconnect network.
LIMITED COPYRIGHT WAIVER
A portion of the disclosure of this patent document contains material to which the claim of copyright protection is made. The copyright owner has no objection to the
facsimile reproduction by any person of the patent document or the patent disclosure, as it appears in the U.S. Patent and Trademark Office file or records, but reserves
all other rights whatsoever.
BACKGROUND
Computer systems have long relied on network connections to transfer data, whether from one computer system to another computer system, one computer component
to another computer component, or from one processor to another processor in the same computer. Most computer networks link multiple computerized elements to
one another, and include various functions such as verification that a message sent over the network arrived at the intended recipient, confirmation of the integrity of the
message, and a method of routing a message to the intended recipient on the network.
Processor interconnect networks are used in multiprocessor computer systems to transfer data from one processor to another, or from one group of processors to
another group. The number of interconnection links can be very large with computer systems having hundreds or thousands of processors, and system performance can
vary significantly based on the efficiency of the processor interconnect network. The number of connections, number of intermediate nodes between a sending and
receiving processing node, and the speed or type of connection all play a factor in the interconnect network performance.

https://patents.google.com/patent/US10153985B2/en?q=(high+radix)&oq=high+radix

2/13

2/14/26, 1:57 PM

US10153985B2 - Dragonfly processor interconnect network - Google Patents

Similarly, the network topology, or pattern of connections used to tie processing nodes together affects performance, and remains an area of active research. It is
impractical to directly link each node to each other node in systems having many tens of processors, and all but impossible as the number of processors reaches the
thousands.
Further, the cost of communications interfaces, cables, and other factors can add significantly to the cost of poorly designed or inefficient processor interconnect
networks, especially where long connections or high-speed fiber optic links are required. A processor interconnect network designer is thereby challenged to provide fast
and efficient communication between the various processing nodes, while controlling the number of overall links, and the cost and complexity of the processor
interconnect network.
The topology of a network, or the method used to determine how to link a processing node to other nodes in a multiprocessor computer system, is therefore an area of
interest.
SUMMARY
The invention comprises in one example a dragonfly topology network, comprising a plurality of processor nodes, a plurality of routers, each router directly coupled to a
plurality of terminal nodes, the routers coupled to one another and arranged into a group, and a plurality of groups of routers, such that each group is connected to each
other group via at least one direct connection.
Network data is routed in some embodiments using at least one of credit round-trip latency as an indicator of channel congestion and selective virtual channel
discrimination.
BRIEF DESCRIPTION OF THE FIGURES
FIG. 1 is a block diagram of a dragonfly network topology, consistent with an example embodiment of the invention.
FIG. 2 is a graph illustrating scalability of a dragonfly network in nodes for various router radices, consistent with an example embodiment of the invention.
FIG. 3 is a block diagram illustrating a dragonfly network topology, consistent with an example embodiment of the invention.
FIG. 4 is block diagram of dragonfly network topology groups, consistent with some example embodiments of the invention.
FIG. 5 is a block diagram of a dragonfly network illustrating minimal and non-minimal routing using virtual channels, consistent with an example embodiment of the
invention.
FIG. 6 is a graph illustrating latency v. offered load for a variety of routing algorithms using various traffic patterns, consistent with an example embodiment of the
invention.
FIG. 7 is a node group diagram of a dragonfly topology network illustrating adaptive routing via global channels using backpressure from intermediate nodes, consistent
with an example embodiment of the invention.
FIGS. 8A-8B are node diagram illustrating credit round trip latency tracking, consistent with an example embodiment of the invention.
DETAILED DESCRIPTION
In the following detailed description of example embodiments of the invention, reference is made to specific examples by way of drawings and illustrations. These
examples are described in sufficient detail to enable those skilled in the art to practice the invention, and serve to illustrate how the invention may be applied to various
purposes or embodiments. Other embodiments of the invention exist and are within the scope of the invention, and logical, mechanical, electrical, and other changes may
be made without departing from the subject or scope of the present invention. Features or limitations of various embodiments of the invention described herein, however
essential to the example embodiments in which they are incorporated, do not limit the invention as a whole, and any reference to the invention, its elements, operation,
and application do not limit the invention as a whole but serve only to define these example embodiments. The following detailed description does not, therefore, limit the
scope of the invention, which is defined only by the appended claims.
Interconnection networks are widely used to connect processors and memories in multiprocessors, as switching fabrics for high-end routers and switches, and for
connecting I/O devices. As processor and memory performance continues to increase in a multiprocessor computer system, the performance of the interconnection
network plays a central role in determining the overall performance of the system. The latency and bandwidth of the network largely establish the remote memory access
latency and bandwidth.
A good interconnection network typically designed around the capabilities and constraints of available technology. Increasing router pin bandwidth, for example, has
motivated the use of high-radix routers in which increased bandwidth is used to increase the number of ports per router, rather than maintaining a small number of ports
and increasing the bandwidth per port. The Cray Black Widow system, one of the first systems to employ a high-radix network, uses a variant of the folded-Clos topology
and radix-64 routers—a significant departure from previous low-radix 3-D torus networks. Recently, the advent of economical optical signaling enables topologies with
long channels. However, these long optical channels remain significantly more expensive than short electrical channels. In this paper, we introduce a Dragonfly topology,
that exploits emerging optical signaling technology by grouping routers to further increase the effective radix of the network.
The topology of an interconnection network largely determines both the performance and the cost of the network. Network cost is dominated by the cost of channels,
and in particular the cost of the long, global, inter-cabinet channels. Thus, reducing the number of global channels can significantly reduce the cost of the network. To
reduce global channels without reducing performance, the number of global channels traversed by the average packet must be reduced. The dragonfly topology
introduced in this paper reduces the number of global channels traversed per packet using minimal routing to one.
To achieve this global diameter of one, very high-radix routers, with a radix of approximately 2√N (where N is the size of the network) are used. While radix 64 routers
have been introduced, and a radix of 128 is feasible, much higher radices in the hundreds or thousands are needed to build machines that scale to 8K-1M nodes if each
packet is limited to only one global hop using traditional very high radix router technology. To achieve the benefits of a very high radix with routers without requiring
hundreds or thousands of ports per node, the Dragonfly network topology proposes using a group of routers connected into a subnetwork as one very high radix virtual
router. This very high effective radix in turn allows us to build a network in which all minimal routes traverse at most one global channel. It also increases the physical
length of the global channels, exploiting the capabilities of emerging optical signaling technology.
Achieving good performance on a wide range of traffic patterns on a dragonfly topology involves selecting a routing algorithm that can effectively balance load across
the global channels. Global adaptive routing (UGAL) can perform such load balancing if the load of the global channels is available at the source router, where the routing
decision is made. With the Dragonfly topology, however, the source router is most often not connected to the global channel in question. Hence, the adaptive routing
decision is made based on remote or indirect information.

https://patents.google.com/patent/US10153985B2/en?q=(high+radix)&oq=high+radix

3/13

2/14/26, 1:57 PM

US10153985B2 - Dragonfly processor interconnect network - Google Patents

The indirect nature of this decision leads to degradation in both latency and throughput when conventional UGAL (which uses local queue occupancy to make routing
decisions) is used. We propose two modifications to the UGAL routing algorithm for the Dragonfly network topology that overcome this limitation with performance
results approaching an ideal implementation using global information. Adding selective virtual-channel discrimination to UGAL (UGAL-VC H) eliminates bandwidth
degradation due to local channel sharing between minimal and non-minimal paths. Using credit-round trip latency to both sense global channel congestion and to
propagate this congestion information upstream (UGAL-CR) eliminates latency degradation by providing much stiffer backpressure than is possible using only queue
occupancy for congestion sensing.
High-radix networks reduce the diameter of the network but require longer cables compared to low-radix networks. Advances in signaling technology and the recent
development of active optical cables facilitate implementation of high-radix topologies with longer cables.
An interconnection network is embedded in a packaging hierarchy. At the lowest level, the routers are connected via circuit boards, which are then connected via a
backplane or midplane. One or more backplanes are packaged in a cabinet, with multiple cabinets connected by electrical or optical cables to form a complete system.
The global (inter-cabinet) cables and their associated transceivers often dominate the cost of a network. To minimize the network cost, the topology should be matched
to the characteristics of the available interconnect technologies, such as cost and performance.
The maximum bandwidth of an electrical cable drops with increasing cable length because signal attenuation due to skin effect and dielectric absorption increases
linearly with distance. For typical high-performance signaling rates (10-20 Gb/s) and technology parameters, electrical signaling paths are limited to about 1 m in circuit
boards and 10 m in cables. At longer distances, either the signaling rate must be reduced or repeaters inserted to overcome attenuation.
Historically, the high cost of optical signaling limited its use to very long distances or applications that demanded performance regardless of cost. Recent advances in
silicon photonics and their application to active optical cables such as Intel Connects Cables and Luxtera Blazar have provided designers with economical optical
interconnects. These active optical cables have electrical connections at either end and electrooptical and optoelectrical modules integrated into the cable itself.
Although optical cables have a higher fixed cost, their ability to transmit data over long distances at several times the data rate of copper cables results in a lower cost
per unit distance than electrical cables. Based on the data available using current technologies, the break-even point is at 10 m. For distances shorter than 10 m,
electrical signaling is less expensive. Beyond 10 m, optical signaling is more economical. The Dragonfly topology proposed here exploits this relationship between cost
and distance. By reducing the number of global cables, it minimizes the effect of the higher fixed overhead of optical signaling, and by making the global cables longer, it
maximizes the advantage of the lower per-unit cost of optical fibers.
To show an example Dragonfly network topology, the following symbols are used in the description of the dragonfly topology and in example routing algorithms
presented later:
N Number of network terminals
p Number of terminals connected to each router
a Number of routers in each group
k Radix of the routers
k_Effective radix of the group (or the virtual router)
h Number of channels within each router used to connect to other groups
g Number of groups in the system
q Queue depth of an output port
qvc Queue depth of an individual output VC
H Hop count
Outi Router output port i
The Dragonfly topology is a hierarchical network with three levels, as shown in FIG. 1: routers (104, 105, and 106), groups (101, 102, and 103), and system. At the router
level, each router has connections to p nodes, a−1 local channels—to other routers in the same group—and h global channels—to routers in other groups. Therefore the
radix (or degree) of each router is defined as k=p+a+h−1. A group consists of a routers connected via an intra-group interconnection network formed from local channels,
as shown at 101 in FIG. 1. Each group has ap connections to terminals and ah connections to global channels, and all of the routers in a group collectively act as a virtual
router with radix k′=a(p+h). This very high radix, k′>>k enables the system level network to be realized with very low global diameter (the maximum number of expensive
global channels on the minimum path between any two nodes). Up to g=ah+1 groups (N=ap(ah+1) terminals) can be connected with a global diameter of one. In
contrast, a system-level network built directly with radix k routers would require a larger global diameter.
In a maximum-size (N=ap(ah+1)) dragonfly, there is exactly one connection between each pair of groups. In smaller dragonflies, there are more global connections out of
each group than there are other groups. These extra global connections are distributed over the groups with each pair of groups connected by at least _ah+1 g_channels.
The dragonfly parameters a, p, and h can have any values. However, to balance channel load, the network in this example has a=2p=2h. Because each packet traverses
two local channels along its route (one at each end of the global channel) for one global channel and one terminal channel, this ratio maintains balance. Because global
channels are expensive, deviations from this 2:1 ratio are done in some embodiments in a manner that overprovisions local and terminal channels, so that the expensive
global channels remain fully utilized. That is, the network is balanced in such examples so that a≥2h, 2p≥2h.
The scalability of a balanced dragonfly is shown in FIG. 2. By increasing the effective radix, the dragonfly topology is highly scalable—with radix-64 routers, the topology
scales to over 256 k nodes with a network diameter of only three hops. Arbitrary networks can be used for the intra-group and inter-group networks in FIG. 1. In the
example presented here, we use a 1-D flattened butterfly or a completely-connected topology for both networks. A simple example of the dragonfly is shown in FIG. 3
with p=h=2 (two processing nodes per router and two channels within each router coupled to other groups), a=4 (four routers in each group) that scales to N=72 (72
nodes in the network) with k=7 (radix 7) routers. By using virtual routers, the effective radix is increased from k=7 to k′=16, as group G0 of FIG. 3 has eight global
connections and eight node connections.
The global radix, k′, can be increased further by using a higher-dimensional topology for the intra-group network. Such a network may also exploit intra-group packaging
locality. For example, a 2-D flattened butterfly is shown in FIG. 4 at 401, which has the same k′ as the group shown in FIG. 5 but exploits packaging locality by providing
more bandwidth to local routers. A 3-dimension flattened butterfly is used in FIG. 4 at 402 to increase the effective radix from k′=16 to K′=32—allowing the topology to
scale up to N=1056 using the same k=7 router as in FIG. 1.
To increase the terminal bandwidth of a high-radix network such as a dragonfly, channel slicing can be employed. Rather than make the channels wider, which would
decrease the router radix, multiple network can be connected in parallel to add capacity. Similarly, the dragonfly topology in some embodiments can also utilize parallel
networks to add capacity to the network. In addition, the dragonfly networks described so far assumed uniform bandwidth to all nodes in the network. However, if such
uniform bandwidth is not needed, bandwidth tapering can be implemented by removing inter-group channels among some of the groups.

https://patents.google.com/patent/US10153985B2/en?q=(high+radix)&oq=high+radix

4/13

2/14/26, 1:57 PM

US10153985B2 - Dragonfly processor interconnect network - Google Patents

A variety of minimal and non-minimal routing algorithms can be implemented using the dragonfly topology. Some embodiments of global adaptive routing using local
information lead to limited throughput and very high latency at intermediate loads. To overcome these problems, we introduce new mechanisms to global adaptive
routing, which provide performance that approaches an ideal implementation of global adaptive routing.
Minimal routing in a dragonfly from source node s attached to router Rs in group Gs to destination node d attached to router Rd in group Gd traverses a single global
channel and is accomplished in three steps:
Step 1: If Gs_=Gd and Rs does not have a connection to Gd, route within Gs from Rs to Ra, a router that has a global channel to Gd.
Step 2: If Gs_=Gd, traverse the global channel from Ra to reach router Rb in Gd.
Step 3: If Rb_=Rd, route within Gd from Rb to Rd.
This minimal routing works well for load-balanced traffic, but results in poor performance on adversarial traffic patterns. To load-balance adversarial traffic patterns,
Valiant's algorithm can be applied at the system level—routing each packet first to a randomly-selected intermediate group Gi and then to its final destination d. Applying
Valiant's algorithm to groups suffices to balance load on both the global and local channels. This randomized non-minimal routing traverses at most two global channels
and requires five steps:
Step 1: If Gs_=Gi and Rs does not have a connection to Gi, route within Gs from Rs to Ra, a router that has a global channel to Gi.
Step 2: If Gs_=Gi traverse the global channel from Ra to reach router Rx in Gi.
Step 3: If Gi_=Gd and Rx does not have a connection to Gd, route within Gi from Rx to Ry, a router that has a global channel to Gd.
Step 4: If Gi_=Gd, traverse the global channel from Ry to router Rb in Gd.
Step 5: If Rb_=Rd, route within Gd from Rb to Rd.
To prevent routing deadlock, two virtual channels (VCs) are needed for minimal routing and three VCs are required for non-minimal routing, as shown in FIG. 5. These
virtual router assignments eliminate all channel dependencies due to routing. For some applications, additional virtual channels may be required to avoid protocol
deadlock—e.g., for shared memory systems, separate sets of virtual channels may be required for request and reply messages.
A variety of routing algorithms for the dragonfly topology have been evaluated, including:
Minimal (MIN): The minimal path is taken as described previously.
Valiant (VAL) [32]: Randomized non-minimal routing as described previously.
Universal Globally-Adaptive Load-balanced [29] (UGALG,UGAL-L) UGAL chooses between MIN and VAL on a packet-by-packet basis to load-balance the
network. The choice is made by using queue length and hop count to estimate network delay and choosing the path with minimum delay. We implement
two versions of UGAL.
UGAL-L—uses local queue information at the current router node.
UGAL-G—uses queue information for all the global channels in Gs—assuming knowledge of queue lengths on other routers. While difficult to implement,
this represents an ideal implementation of UGAL since the load-balancing is required of the global channels, not the local channels.
Cycle accurate simulations are used to evaluate the performance of the different routing algorithms. We simulate a single-cycle, input-queued router switch but provide
sufficient speedup in order to generalize the results and ensure that routers do not become the bottleneck of the network. Packets are injected using a Bernoulli process.
The simulator is warmed up under load without taking measurements until steady-state is reached. Then a sample of injected packets is labeled during a measurement
interval. The simulation is run until all labeled packets exit the system. Unless otherwise noted, the simulation results are shown for dragonfly of size 1K node using
p=h=4 and a=8 parameters. Simulations of other size networks follow the same trend and are not presented due to space constraints. Single flit (flow control unit)
packets are used to separate the routing algorithm from flow control issues such as the use of wormhole or virtual cut-through flow control. The input buffers are initially
assumed to be 16 flits deep. The impact of different buffer sizes is also evaluated.
The different routing algorithms are evaluated using both benign and adversarial synthetic traffic patterns, as shown in FIG. 6. Latency v. offered load is shown for the
four routing algorithms, using both uniform random traffic at 601 and adversarial traffic at 602. The use of a synthetic traffic pattern allows us to stress the topology and
routing algorithm to fully evaluate the network. For benign traffic such as uniform random (UR), MIN is sufficient to provide low latency and high throughput, as shown at
601 of FIG. 6. VAL achieves approximately half of the network capacity because its load-balancing doubles the load on the global channels. Both UGAL-G and UGAL-L
approach the throughput of MIN, but with slightly higher latency near saturation. The higher latency is caused by the use of parallel or greedy allocation where the routing
decision at each port is made in parallel. The use of sequential allocation will reduce the latency at the expense of a more complex allocator.
To test the load-balancing ability of a routing algorithm, we use a worst-case (WC) traffic pattern where each node in group Gi sends traffic to a randomly selected node in
group Gi+1. With minimal routing, this pattern will cause all nodes in each group Gi to send all of their traffic across the single global channel to group Gi+1. Non-minimal
routing is required to load balance this traffic pattern by spreading the bulk of the traffic across the other global channels.
The evaluation for this WC traffic is shown in FIG. 6 at 602. Because MIN forwards all of the traffic from each group across a single channel, its throughput is limited to
1/ah. VAL achieves slightly under 50% throughput which is the maximum possible throughput with this traffic. UGAL-G achieves similar throughput as VAL but UGAL-L
leads to both limited throughput as well as high average packet latency at intermediate load. In the following section, we show how the indirect nature of adaptive routing
on the dragonfly leads to performance degradation. We identify the issues with UGAL-L and present mechanisms that can overcome these problems.
Adaptive routing on the dragonfly is challenging because it is the global channels, the group outputs, that need to be balanced, not the router outputs. This leads to an
indirect routing problem. Each router must pick a global channel to use using only local information that depends only indirectly on the state of the global channels.
Previous global adaptive routing methods used local queue information, source queues and output queues, to generate accurate estimates of network congestion. In
these cases, the local queues were an accurate proxy of global congestion, because they directly indicated congestion on the routes they initiated. With the dragonfly
topology, however, local queues only sense congestion on a global channel via backpressure over the local channels. If the local channels are overprovisioned, significant
numbers of packets must be enqueued on the overloaded minimal route before the source router will sense the congestion. This results in a degradation in throughput
and latency as shown earlier in FIG. 6 at 602.
A throughput issue with UGAL-L arises due to a single local channel handling both minimal and non-minimal traffic. For example, in FIG. 7, a packet in R1 has a minimal
path which uses gc7 and a nonminimal path which uses gc6. Both paths share the same local channel from R1 to R2. Because both paths share the same local queue
(and hence have the same queue occupancy) and the minimal path is shorter (one global hop vs two), the minimal channel will always be selected, even when it is
saturated. This leads to the minimal global channel being overloaded and the non-minimal global channels that share the same router as the minimal channel being
under utilized. With UGAL-G, the minimal channel is preferred and the load is uniformly balanced across all other global channels. With UGAL-L, on the other hand, the
non-minimal channels on the router that contains the minimal global channel are under utilized—resulting in a degradation of network throughput.
To overcome this limitation, we modify the UGAL algorithm to separate the queue occupancy into minimal and nonminimal components by using individual VCs (UGALLVC).

https://patents.google.com/patent/US10153985B2/en?q=(high+radix)&oq=high+radix

5/13

2/14/26, 1:57 PM

US10153985B2 - Dragonfly processor interconnect network - Google Patents

if (qm vcHm ≤ qnm vcHnm )
route minimally;
else
route nonminimally;
where the subscript m and nm denote the minimal and nonminimal paths. If the VC assignment of FIG. 5 is used, qm vc=q(V C1) and qnm vc=q(V C0).
When compared, UGAL-LVC matches the throughput of UGAL-G on a WC traffic pattern but for UR traffic, the throughput is limited, with approximately 30% reduction in
throughput. For the WC traffic, where most of the traffic needs to be sent non-minimally, UGALLVC performs well since the minimal queue is heavily loaded. However, for
load-balanced traffic when most traffic should be sent minimally, individual VCs do not provide an accurate representation of the channel congestion—resulting in
throughput degradation.
To overcome this limitation, we further modify the UGAL algorithm to separate the queue occupancy into minimal and non-minimal components only when the minimal
and nonminimal paths start with the same output port. Our hybrid modified UGAL routing algorithm (UGAL-LVC H) is:
if (qmHm ≤ qnmHnm && Outm_= Outnm ) | | (qm vcHm ≤ qnm
vcHnm && Outm = Outnm)
route minimally;
else
route nonminimally;
Compared to UGAL-LVC, UGAL-LVC H provides the same throughput on WC traffic pattern but matches the throughput of UGAL-G on UR traffic but resulting in nearly 2×
higher latency at an offered load of 0.8, near saturation. For WC traffic, UGAL-LVC H also results in higher intermediate latency compared to UGAL-G.
The high intermediate latency of UGAL-L is due to minimally-routed packets having to fill the channel buffers between the source and the point of congestion before
congestion is sensed. Our research shows that non-minimally routed packets have a latency curve comparable to UGAL-G while minimally-routed packets see
significantly higher latency. As input buffers are increased, the latency of minimally-routed packets increases and is proportional to the depth of the buffers. A histogram
of latency distribution shows two clear distributions—one large distribution with low latency for the non-minimal packets and another distribution with a limited number
of packets but with much higher latency for the minimal packets.
To understand this problem with UGAL-L, in the example dragonfly group shown in FIG. 7, assume a packet in R1 is making its global adaptive routing decision of routing
either minimally through gc0 or non-minimally through gc7. The routing decision needs to load balance global channel utilization and ideally, the channel utilization can
be obtained from the queues associated with the global channels, q0 and q3. However, q0 and q3 queue informations are only available at R0 and R2 and not readily
available at R1—thus, the routing decision can only be made indirectly through the local queue information available at R1.
In this example, q1 reflects the state of q0 and q2 reflects the state of q3. When either q0 or q3 is full, the flow control provides backpressure to q1 and q2 as shown with
the arrows in FIG. 7. As a result, in steady-state measurement, these local queue information can be used to accurately measure the throughput. Since the throughput is
defined as the offered load when the latency goes to infinity (or the queue occupancy goes to infinity), this local queue information is sufficient. However, q0 needs to be
completely full in order for q1 to reflect the congestion of gc0 and allow R1 to route packets non-minimally. Thus, using local information requires sacrificing some
packets to properly determine the congestion—resulting in packets being sent minimally having much higher latency. As the load increases, although minimally routed
packets continue to increase in latency, more packets are sent non-minimally and results in a decrease in average latency until saturation.
In order for local queues to provide a good estimate of global congestion, the global queues need to be completely full and provide a stiff backpressure towards the local
queues. The stiffness of the backpressure is inversely proportional to the depth of the buffer—with deeper buffers, it takes longer for the backpressure to propagate while
with shallower buffers, a much stiffer backpressure is provided. As the buffer size decreases, the latency at intermediate load is decreased because of the stiffer
backpressure. However, using smaller buffers comes at the cost of reduced network throughput.
To overcome the high intermediate latency, we propose using credit round-trip latency to sense congestion faster and reduce latency. In credit-based flow control,
illustrated in FIGS. 8A-8B, credit counts are maintained for buffers downstream. As packets are sent downstream, the appropriate credit count is decremented and once
the packet leaves downstream router, credits are sent back upstream and the credit count is incremented. The latency for the credits to return is referred to as credit
round-trip latency (tcrt) and if there is congestion downstream, the packet cannot be immediately processed and results in an increase in tcrt.
Referring to FIG. 8A, conventional credit flow control is illustrated at 801. As packets are sent downstream (1), the output credit count is decremented (2) and credits are
sent back upstream (3). This scheme is modified as shown in FIG. 8B at 802 to use credit round trip latency to estimate congestion in the network. In addition to the
output credit count being decremented (2), the time stamp is pushed into the credit time queue, denoted CTQ. Before sending the credit back upstream (4), the credit is
delayed (3), and when downstream credits are received (5), the credit count is updated as well as the credit round trip latency tcrt.
The value of tcrt can be used to estimate the congestion of global channels. By using this information to delay upstream credits, we stiffen the backpressure and more
rapidly propagate congestion information up stream. For each output O, tcrt(O) is measured and the quantity td(O)=tcrt(O)−tcrt0 is stored in a register. Then, when a flit is
sent to output O, instead of immediately sending a credit back upstream, the credit is delayed by td(O)−min [td(o)]. The credits sent across the global channels are not
delayed. This ensures that there is no cyclic loop in this mechanism and allows the global channels to be fully utilized.
The delay of returning credits provides the appearance of shallower buffers to create a stiff backpressure. However, to ensure that the entire buffer gets utilized and there
is no reduced throughput at high load, the credits needs to delayed by the variance of td across all outputs. We estimate the variance by finding min [td(o)] value and
using the difference. By delaying credits, the upstream routers observes congestion at a faster rate (compared to waiting for the queues to fill up) and leads to better
global adaptive routing decisions.
The UGAL-L routing algorithm evaluation using credit latency (UGAL-LCR) is investigated for both WC and UR traffic using buffers of depth 16 and 256. UGAL-LCR leads
to significant reduction in latency compared to UGALL and approaches the latency of UGAL-G. For WC traffic, UGAL-LCR reduces latency by up to 35% with 16 buffers and
up to over 20× reduction in intermediate latency with 256 buffers compared to UGAL-L. Unlike UGAL-L, the intermediate latency with UGAL-LCR is independent of buffer
size. For UR traffic, UGAL-LCR provides up to 50% latency reduction near saturation compared to UGAL-LVC H. However, both UGAL-LCR and UGALLVC H fall short of the
throughput of UGAL-G with UR traffic because their imprecise local information results in some packets being routed non-minimally.
The implementation of this scheme results in minimal complexity overhead as the following three features are needed at each router:
tracking credits individually to measure tcrt
registers to store td values

https://patents.google.com/patent/US10153985B2/en?q=(high+radix)&oq=high+radix

6/13

2/14/26, 1:57 PM

US10153985B2 - Dragonfly processor interconnect network - Google Patents
a delay mechanism in returning credits
The amount of storage required for td is minimal as only O(k) registers are required. The credits are often returned by piggybacking on data flits and
delaying credits to wait for the transmission of the next data flit upstream is required. The proposed mechanism only requires adding additional delay.

As for tracking individual credits, credits are conventionally tracked as a pool of credits in credit flow control—i.e., a single credit counter is maintained for each output VC
and increments when a credit is received. The implementation of UGAL-LCR requires tracking each credit individually. This can be done by pushing a timestamp on the
tail of a queue each time a flit is sent, with the use of a credit timestamp queue (CTQ), and popping the timestamp off the head of the queue when the corresponding
credit arrives. Because flits and credits are 1:1 and maintain ordering, the simple queue suffices to measure round-trip credit latency. The depth of the queue needs to be
proportional to the depth of the data buffers but the queue size can be reduced to utilize imprecise information to measure congestion—e.g., by having a queue which is
only ¼ of the data buffer size, only one of four credits are tracked to measure the congestion.
The cost of a dragonfly topology also compares favorably to a flattened butterfly, as well as to other topologies. The flattened butterfly topology reduces network cost of
a butterfly by removing intermediate routers and channels. As a result, the flattened butterfly reduces cost by approximately 50% compared to a folded-Clos on balanced
traffic. The dragonfly topology extends the flattened butterfly by increasing the effective radix of the routers to further reduce the cost and increase the scalability of the
network.
A comparison of dragonfly and flattened butterfly networks of 64 k nodes shows that a flattened butterfly uses 50% of the router ports for global channels, while a
dragonfly uses 25% of the ports for global connections. The flattened butterfly requires two additional dimensions, while the dragonfly is a single dimension. In addition,
the dragonfly provides better scalability because the group size can be increased to scale the network whereas scaling the flattened butterfly requires adding additional
dimensions. With the hop count nearly identical, the dragonfly trades off longer global cables for smaller number of global cables required to provide a more cost-efficient
topology better matched to emerging signaling technologies.
The dollar cost of a dragonfly also compares favorably to a flattened butterfly for networks larger than 1 k nodes, showing approximately a 10% savings for up to 4 k
nodes, and approximately a 20% cost savings relative to flattened butterfly topologies for more than 4 k nodes as the dragonfly has fewer long, global cables. Folded Clos
and 3-d torus networks suffer in comparison, because of the larger number of cables needed to support high network diameters. For a network of only 1 k nodes, the
dragonfly is 62% the cost of a 3-d torus network and 50% that of a folded Clos network. This reduction in network cost is directly correlated to a reduction in network
power consumed, which is a significant advantage for large networks as well as for installations that are desirably environmentally friendly.
The example embodiments of a dragonfly network presented here show how use of a group of routers as a virtual router can increase the effective radix of a network,
and hence reduce network diameter, cost, and latency. Because the dragonfly topology reduces the number global cables in a network, while at the same time increasing
their length, the dragonfly topology is particularly well suited for implementations using emerging active optical cables—which have a high fixed cost but a low cost per
unit length compared to electrical cables. Using active optical cables for the global channels, a dragonfly network reduces cost by 20% compared to a flattened butterfly
and by 52% compared to a folded Clos network of the same bandwidth.
Various embodiments of dragonfly networks described here also comprise two new variants of global adaptive routing that overcome the challenge of indirect adaptive
routing presented by the dragonfly. A dragonfly router will typically make a routing decision based on the state of a global channel attached to a different router in the
same group. Conventional global adaptive routing algorithms that use local queue occupancies to infer the state of this remote channel give degraded throughput and
latency. We introduce the selective use of virtual channel discrimination to overcome the bandwidth degradation. We also introduce the use of credit round-trip latency to
both sense and signal channel congestion. The combination of these two techniques gives a global adaptive routing algorithm that approaches the performance of an
ideal algorithm with perfect knowledge of remote channel state.
Although specific embodiments have been illustrated and described herein, it will be appreciated by those of ordinary skill in the art that any arrangement which is
calculated to achieve the same purpose may be substituted for the specific embodiments shown. This application is intended to cover any adaptations or variations of
the example embodiments of the invention described herein. It is intended that this invention be limited only by the claims, and the full scope of equivalents thereof.

Patent Citations (42)
Publication number

Priority date

Publication date

Assignee

Title

US4970658A

1989-02-16

1990-11-13

Tesseract Corporation

Knowledge engineering tool

US5079738A

1989-09-29

1992-01-07

Rockwell International
Corporation

Processor interconnect network for printing press system forming a star
network

US5249283A

1990-12-24

1993-09-28

Ncr Corporation

Cache coherency method and apparatus for a multiple path
interconnection network

US5425029A

1993-09-20

1995-06-13

Motorola, Inc.

Fast packet adaptation method for ensuring packet portability across
diversified switching type networks

JPH10285214A

1997-04-09

1998-10-23

Nec Corp

Fault recovery system

US5864738A

1996-03-13

1999-01-26

Cray Research, Inc.

Massively parallel processing system using two data paths: one
connecting router circuit to the interconnect network and the other
connecting router circuit to I/O controller

US5881135A

1992-06-15

1999-03-09

British Telecommunications
Public Limited Company

Service platform

JP2000201182A

1999-01-08

2000-07-18

Nippon Telegr & Teleph Corp
<Ntt>

Routing device and routing method

US6212636B1

1997-05-01

2001-04-03

Itt Manufacturing Enterprises

Method for establishing trust in a computer network via association

US20020124134A1

2000-12-28

2002-09-05

Emc Corporation

Data storage system cluster architecture

US20020131362A1

2001-03-16

2002-09-19

Ross Callon

Network routing using link failure information

https://patents.google.com/patent/US10153985B2/en?q=(high+radix)&oq=high+radix

7/13

2/14/26, 1:57 PM

US10153985B2 - Dragonfly processor interconnect network - Google Patents

US20030088696A1

1999-01-11

2003-05-08

Fastforward Networks, Inc.

Performing multicast communication in computer networks by using
overlay routing

US6643764B1

2000-07-20

2003-11-04

Silicon Graphics, Inc.

Multiprocessor system utilizing multiple links to improve point to point
bandwidth

US6766424B1

1999-02-09

2004-07-20

Hewlett-Packard Development
Company, L.P.

Computer architecture with dynamic sub-page placement

US20050073958A1

2003-10-03

2005-04-07

Avici Systems, Inc.

Selecting alternate paths for network destinations

US20050177344A1

2004-02-09

2005-08-11

Newisys, Inc. A Delaware
Corporation

Histogram performance counters for use in transaction latency analysis

US20050289101A1

2004-06-25

2005-12-29

Doddaballapur Jayasimha

Methods and systems for dynamic partition management of sharedinterconnect partitions

US7139926B1

2002-08-30

2006-11-21

Lucent Technologies Inc.

Stateful failover protection among routers that provide load sharing using
network address translation (LSNAT)

US20070198675A1

2004-10-25

2007-08-23

International Business Machines
Corporation

Method, system and program product for deploying and allocating an
autonomic sensor network ecosystem

US20080025208A1

2006-07-28

2008-01-31

Michael Tin Yau Chan

Wide-area wireless network topology

US20080123679A1

2003-11-19

2008-05-29

Cray Inc.

Routing table architecture

US7409506B2

2004-12-28

2008-08-05

Fujitsu Limited

Multiprocessor system with high-speed exclusive control

US20080285562A1

2007-04-20

2008-11-20

Cray Inc.

Flexible routing tables for a high-radix router

US20090106529A1

2007-10-05

2009-04-23

Abts Dennis C

Flattened butterfly processor interconnect network

US20100049942A1

2008-08-20

2010-02-25

John Kim

Dragonfly processor interconnect network

US7675857B1

2006-05-03

2010-03-09

Google Inc.

Method and apparatus to avoid network congestion

US20100103813A1

2008-10-28

2010-04-29

Nortel Networks Limited

Provisioned provider link state bridging (plsb) with routed back-up

US20100228798A1

2009-02-24

2010-09-09

Hitachi, Ltd.

Geographical distributed storage system based on hierarchical peer to
peer architecture

US20110153942A1

2009-12-21

2011-06-23

Prashant Jain

Reducing implementation costs of communicating cache invalidation
information in a multicore processor

US20110191088A1

2010-02-01

2011-08-04

Yar-Sun Hsu

Object-oriented network-on-chip modeling

US8018860B1

2003-03-12

2011-09-13

Sprint Communications
Company L.P.

Network maintenance simulator with path re-route prediction

US20120020349A1

2010-07-21

2012-01-26

GraphStream Incorporated

Architecture for a robust computing system

US20120059938A1

2010-06-28

2012-03-08

Cray Inc.

Dimension-ordered application placement in a multiprocessor computer

US20120072614A1

2010-09-22

2012-03-22

Amazon Technologies, Inc.

Transpose boxes for network interconnection

US20120072602A1

2010-09-22

2012-03-22

Amazon Technologies, Inc.

Transpose box based network scaling

EP2451127A1

2010-11-05

2012-05-09

Cray Inc.

Progressive adaptive routing in a dragonfly processor interconnect
network

JP2012105265A

2010-11-05

2012-05-31

Cray Inc

Table-driven routing in dragonfly processor interconnect network

US8260922B1

2005-09-16

2012-09-04

Cisco Technology, Inc.

Technique for using OER with an ECT solution for multi-homed sites

US8427980B2

2010-07-21

2013-04-23

Hewlett-Packard Development
Company, L. P.

Methods and apparatus to determine and implement multidimensional
network topologies

US8489718B1

2010-05-19

2013-07-16

Amazon Technologies, Inc.

Torroidal backbone connections for network deployment

US8495194B1

2010-06-29

2013-07-23

Amazon Technologies, Inc.

Connecting network deployment units

US8576715B2

2009-10-26

2013-11-05

Mellanox Technologies Ltd.

High-performance adaptive routing

Family To Family Citations
* Cited by examiner, † Cited by third party

Non-Patent Citations (55)
Title

https://patents.google.com/patent/US10153985B2/en?q=(high+radix)&oq=high+radix

8/13

2/14/26, 1:57 PM

US10153985B2 - Dragonfly processor interconnect network - Google Patents

Advisory Action in U.S. Appl. No. 12/195,198 dated Apr. 19, 2011.
Arimilli et al., "The PERCS High-Performance Interconnect," 2010 18th IEEE Symposium on High Performance Interconnects (2010) (8 pages).
BPAI Decision in U.S. Appl. No. 12/195,198 dated Jun. 25, 2014.
European Patent Office Action in European Patent Application Serial No. 11187953.2 dated Jul. 12, 2017.
European Patent Office Communication Pursuant to Article 94(3) EPC in EP Application Serial No. 11187952.4 dated Jul. 23, 2013.
European Patent Office Extended Search Report and Opinion in EP Application Serial No. 11187952.4 dated Feb. 28, 2012.
European Patent Office Search Report and Opinion in EP Application Serial No. 11187953.2 dated May 7, 2012.
Examiner's Answer to Appeal Brief in U.S. Appl. No. 12/195,198 dated Oct. 12, 2011.
Extended European Patent Office Search Report and Opinion in EP Application Serial No. 16180770.6 dated Jan. 13, 2017, 8 pages.
Final Office Action in U.S. Appl. No. 12/195,198 dated Jan. 26, 2011.
Final Office Action in U.S. Appl. No. 12/195,198 dated Jun. 5, 2015.
Final Office Action in U.S. Appl. No. 13/290,507 dated Oct. 1, 2013.
Final Office Action in U.S. Appl. No. 13/290,567 dated Jul. 17, 2014.
Final Office Action in U.S. Appl. No. 14/583,588 dated Feb. 2, 2016.
Final Office Action in U.S. Appl. No. 14/672,125 dated Oct. 7, 2016.
FOLDOC Computing Dictionary: "router," Retrieved and printed on Oct. 3, 2015 from http://foldoc.org/router.
Gratz, Paul et al., "Regional congestion awareness for load balance in networks-on-chip", High Performance Computer Architecture, 2008. HPCA 2008. IEEE 14th International
Symposium on, IEEE, Piscataway, NJ, USA, Feb. 16, 2008 (pp. 203-214.).
Japan Patent Office Action in Japanese Patent Application Serial No. 2011-241240 dated Aug. 18, 2015.
Japan Patent Office Action in Japanese Patent Application Serial No. 2015-012380 dated Dec. 1, 2015.
Japan Patent Office Action in Japanese Patent Application Serial No. 2015-103217 dated Aug. 2, 2016.
Japan Patent Office Action in Japanese Patent Application Serial No. 2015-103217 dated Mar. 22, 2016.
Japan Patent Office Notice of Allowance dated Nov. 20, 2015 Patent Application Serial No. 2011-241239.
Japan Patent Office Notification of Reasons for Rejection in Japanese Patent Application Serial No. 2011-241239 dated Aug. 11, 2015.
Japense Patent Office Notice of Allowance in Japanese Patent Application Serial No. 2015-103217 dated May 10, 2017.
Kim et al., "Cost-Efficient Dragonfly Topology for Large-Scale Systems", IEEE Micro (Impact Factor: 1.81). Mar. 2009; 29:33-40. DOI: 10.1109/MM.2009.5 (3 pages).
Kim, John et al., "Flattened Butterfly: A Cost Efficient Topology for High-Radix Networks", ISCA '07, Jun. 9-13, 2007 (pp. 126-137).
Kim, John et al.: "Technology-Driven, Highly-Scalable Dragonfly Topology", International Symposium on Computer Architecture, 2008. ISCA '08. 35th International Symposium on
Architecture, IEEE Piscataway, NJ, Jun. 21, 2008 (pp. 77-88).
Kim, John, et al., "Flattened Butterfly : A Cost-Efficient Topology for High-Radix Networks", ISCA '07, Jun. 9-13, 2007, San Diego, California, ACM 978-1-59593-706-3/0730006, pp.
126-137.
Non Final Office Action in U.S. Appl. No. 14/672,125 dated Feb. 26, 2016.
Non-Final Office Action in U.S. Appl. No. 12/195,198 dated Aug. 13, 2010.
Non-Final Office Action in U.S. Appl. No. 12/195,198 dated Jan. 23, 2015.
Non-Final Office Action in U.S. Appl. No. 13/290,507 dated May 13, 2014.
Non-Final Office Action in U.S. Appl. No. 13/290,507 dated May 16, 2013.
Non-Final Office Action in U.S. Appl. No. 13/290,567 dated Dec. 29, 2014.
Non-Final Office Action in U.S. Appl. No. 13/290,567 dated Feb. 12, 2014.
Non-Final Office Action in U.S. Appl. No. 14/583,579 dated Jun. 26, 2015.
Non-Final Office Action in U.S. Appl. No. 14/583,579 dated Oct. 8, 2015.
Non-Final Office Action in U.S. Appl. No. 14/583,588 dated Sep. 10, 2015.
Notice of Allowance in U.S. Appl. No. 12/195,198 dated Mar. 17, 2016.

https://patents.google.com/patent/US10153985B2/en?q=(high+radix)&oq=high+radix

9/13

2/14/26, 1:57 PM

US10153985B2 - Dragonfly processor interconnect network - Google Patents

Notice of Allowance in U.S. Appl. No. 12/195,198 dated Oct. 5, 2015.
Notice of Allowance in U.S. Appl. No. 12/195,198 dated Sep. 21, 2015.
Notice of Allowance in U.S. Appl. No. 13/290,507 dated Jan. 26, 2015.
Notice of Allowance in U.S. Appl. No. 13/290,507 dated May 6, 2015.
Notice of Allowance in U.S. Appl. No. 13/290,507 dated Oct. 15, 2014.
Notice of Allowance in U.S. Appl. No. 13/290,567 dated Jul. 15, 2015.
Notice of Allowance in U.S. Appl. No. 13/290,567 dated Nov. 2, 2015.
Notice of Allowance in U.S. Appl. No. 14/583,588 dated Aug. 3, 2016.
Notice of Allowance in U.S. Appl. No. 14/583,588 dated Nov. 18, 2016.
Pre-Brief Conference Decision in U.S. Appl. No. 12/195,198 dated Jun. 17, 2011.
Scott, Steve et al., "The Black Widow of High-Radix Clos Network," Proceedings of the 33rd International Symposium on Computer Architecture (ISCA'06), Copyright 2006 (12
pages).
U.S. Appl. No. 14/583,579, field Dec. 26, 2015 entitled Table-Driven Routing in a Dragonfly Processor Interconnect Network, inventors Michael Parker et al.
U.S. Appl. No. 14/583,588, filed Dec. 27, 2014 and entitled Dragonfly Processor Interconnect Network, Inventors John Kim et al.
U.S. Appl. No. 14/672,125, filed Mar. 28, 2015 and entitled "Progressive Adaptive Routing in a Dragonfly Processor Interconnect Network," Inventors Mike Parker et al.
Valinataj, Motjaba et al., "A Fault-Tolerant and Congestion-Aware Routing Algorithm for Networks-on-Chip", Design and Diagnostics of Electronic Circuits and Systems (DDECS),
2010 IEEE 13th International Symposium on, IEEE, Piscataway, NJ, USA, Apr. 14, 2010, (pp. 139-144).
Varadarajan, Srinidhi et al, "Reinforcing reachable routes", Computer Networks 43 (2003), Elsevier Science Publishers, Amsterdam, NL, vol. 43, No. 3; Oct. 22, 2003 pp. 389-416 (28
pages).
* Cited by examiner, † Cited by third party

Cited By (54)
Publication number

Priority date

Publication date

Assignee

Title

US11782869B2

2019-12-18

2023-10-10

Huawei Technologies Co., Ltd.

Data transmission method and related device

US12418475B2

2022-12-08

2025-09-16

Google Llc

Fault-tolerant routing algorithm for toroidal network topologies

US20100049942A1

2008-08-20

2010-02-25

John Kim

Dragonfly processor interconnect network

US8549092B2 *

2009-02-19

2013-10-01

Micron Technology, Inc.

Memory network methods, apparatus, and systems

US8489718B1 *

2010-05-19

2013-07-16

Amazon Technologies, Inc.

Torroidal backbone connections for network deployment

US8880739B1 *

2010-05-19

2014-11-04

Amazon Technologies, Inc.

Point backbones for network deployment

JP5860670B2 *

2010-11-05

2016-02-16

インテル コーポレイション

Table-driven routing in a Dragonfly processor interconnect
network

JP5913912B2 *

2010-11-05

2016-04-27

インテル コーポレイション

Innovative Adaptive Routing in Dragonfly Processor
Interconnect Network

CN102141975B *

2011-04-01

2013-10-09

华为技术有限公司

Computer system

US9577918B2 *

2012-11-19

2017-02-21

Cray Inc.

Increasingly minimal bias routing

CN103973564B *

2013-01-31

2017-12-15

清华大学

The adaptive routing method of interconnected network system

GB2511089A

2013-02-22

2014-08-27

Ibm

All-to-all message exchange in parallel computing systems

US9548960B2

2013-10-06

2017-01-17

Mellanox Technologies Ltd.

Simplified packet routing

US9729473B2

2014-06-23

2017-08-08

Mellanox Technologies, Ltd.

Network high availability using temporary re-routing

US9806994B2

2014-06-24

2017-10-31

Mellanox Technologies, Ltd.

Routing via multiple paths with efficient traffic distribution

WO2015196461A1 *

2014-06-27

2015-12-30

Tsinghua University

Deadlock-free adaptive routing of interconnect network

CN104079490B *

2014-06-27

2017-09-22

清华大学

Multi-level dragonfly interference networks and adaptive
routing method

Family To Family Citations

https://patents.google.com/patent/US10153985B2/en?q=(high+radix)&oq=high+radix

10/13

2/14/26, 1:57 PM

US10153985B2 - Dragonfly processor interconnect network - Google Patents

US9519605B2

2014-07-08

2016-12-13

International Business Machines
Corporation

Interconnection network topology for large scale high
performance computing (HPC) systems

US9699067B2 *

2014-07-22

2017-07-04

Mellanox Technologies, Ltd.

Dragonfly plus: communication over bipartite node groups
connected by a mesh network

US10002099B2 *

2014-11-13

2018-06-19

Cavium, Inc.

Arbitrated access to resources among multiple devices

US10229230B2 *

2015-01-06

2019-03-12

International Business Machines
Corporation

Simulating a large network load

US20160284021A1

2015-03-27

2016-09-29

Andrew Herdrich

Systems, Apparatuses, and Methods for Resource Bandwidth
Enforcement

US9894005B2

2015-03-31

2018-02-13

Mellanox Technologies, Ltd.

Adaptive routing controlled by source node

US9973435B2

2015-12-16

2018-05-15

Mellanox Technologies Tlv Ltd.

Loopback-free adaptive routing

US10819621B2

2016-02-23

2020-10-27

Mellanox Technologies Tlv Ltd.

Unicast forwarding of adaptive-routing notifications

US10178029B2

2016-05-11

2019-01-08

Mellanox Technologies Tlv Ltd.

Forwarding of adaptive routing notifications

US10200294B2

2016-12-22

2019-02-05

Mellanox Technologies Tlv Ltd.

Adaptive routing based on flow-control credits

US10477288B2

2018-02-05

2019-11-12

David I-Keong Wong

Data center interconnect as a switch

US10742513B2

2018-02-05

2020-08-11

David I-Keong Wong

Network interconnect as a switch

US10644995B2

2018-02-14

2020-05-05

Mellanox Technologies Tlv Ltd.

Adaptive routing in a box

CN110324243A *

2018-03-28

2019-10-11

清华大学

The dragonfly network architecture and its broadcast routing
method

CN110324249B *

2018-03-28

2023-05-26

清华大学

A dragonfly network architecture and its multicast routing
method

US11165686B2

2018-08-07

2021-11-02

International Business Machines
Corporation

Switch-connected Dragonfly network

US11005724B1

2019-01-06

2021-05-11

Mellanox Technologies, Ltd.

Network topology having minimal number of long connections
among groups of network elements

US11792114B2

2019-05-23

2023-10-17

Hewlett Packard Enterprise Development
Lp

System and method for facilitating efficient management of
non-idempotent operations in a network interface controller
(NIC)

US11641326B2 *

2019-06-28

2023-05-02

Intel Corporation

Shared memory mesh for switching

CN110784406B *

2019-10-23

2021-07-13

上海理工大学

Power-aware dynamic adaptive on-chip network threshold
routing method

US12248429B2 *

2020-03-26

2025-03-11

Graphcore Limited

Network computer with two embedded rings

CN111597141B *

2020-05-13

2022-02-08

中国人民解放军国防科技大学

Hierarchical exchange structure and deadlock avoidance
method for ultrahigh-order interconnection chip

US11575594B2

2020-09-10

2023-02-07

Mellanox Technologies, Ltd.

Deadlock-free rerouting for resolving local link failures using
detour paths

US11411911B2

2020-10-26

2022-08-09

Mellanox Technologies, Ltd.

Routing across multiple subnetworks using address mapping

US11870682B2

2021-06-22

2024-01-09

Mellanox Technologies, Ltd.

Deadlock-free local rerouting for handling multiple local link
failures in hierarchical network topologies

US11765103B2

2021-12-01

2023-09-19

Mellanox Technologies, Ltd.

Large-scale network with high port utilization

WO2023163858A1 *

2022-02-28

2023-08-31

Arris Enterprises Llc

Tunable latency with minimum jitter

US12155563B2

2022-09-05

2024-11-26

Mellanox Technologies, Ltd.

Flexible per-flow multipath managed by sender-side network
adapter

US12328251B2

2022-09-08

2025-06-10

Mellano Technologies, Ltd.

Marking of RDMA-over-converged-ethernet (RoCE) traffic
eligible for adaptive routing

US11765041B1 *

2022-09-15

2023-09-19

Huawei Technologies Co., Ltd.

Methods and systems for implementing a high radix network
topology

KR20240080980A

2022-11-30

2024-06-07

삼성전자주식회사

Device for networt systems

EP4387196A1

2022-12-13

2024-06-19

Barcelona Supercomputing Center-Centro
Nacional de Supercomputación

System and method for connecting complete interconnection
networks for high-performance computers

https://patents.google.com/patent/US10153985B2/en?q=(high+radix)&oq=high+radix

11/13

2/14/26, 1:57 PM

US10153985B2 - Dragonfly processor interconnect network - Google Patents

US12457170B2 *

2023-10-10

2025-10-28

Hewlett Packard Enterprise Development
Lp

Multipath routing for switching fabric

US20250240237A1 *

2024-01-24

2025-07-24

Cornelis Networks, Inc.

Topology for deadlock prevention in a dragonfly using two
virtual lanes

US20250240238A1 *

2024-01-24

2025-07-24

Cornelis Networks, Inc.

Deadlock prevention in a dragonfly using two virtual lanes

US20250247324A1 *

2024-01-30

2025-07-31

Cornelis Networks Inc.

Global first non-minimal routing in dragonfly toplogies

EP4664849A1

2024-06-12

2025-12-17

Barcelona Supercomputing Center-Centro
Nacional de Supercomputación

Interconnection system and routing method for highperformance computer networks

* Cited by examiner, † Cited by third party, ‡ Family to family citation

Similar Documents
Publication

Publication Date

Title

US10153985B2

2018-12-11

Dragonfly processor interconnect network

US9137143B2

2015-09-15

Progressive adaptive routing in a dragonfly processor interconnect network

US10469380B2

2019-11-05

Table-driven routing in a dragonfly processor interconnect network

Kim et al.

2008

Technology-driven, highly-scalable dragonfly topology

JP6093867B2

2017-03-08

Non-uniform channel capacity in the interconnect

EP4109854B1

2025-08-27

Telemetry-based load-balanced fine-grained adaptive routing in high-performance system interconnect

Sancho et al.

2002

Effective methodology for deadlock-free minimal routing in InfiniBand networks

CN112825512A

2021-05-21

Load balancing method and device

Zhang et al.

2017

A stable matching based elephant flow scheduling algorithm in data center networks

Ferraz et al.

2014

A two-phase multipathing scheme based on genetic algorithm for data center networking

Guay et al.

2011

vFtree-A fat-tree routing algorithm using virtual lanes to alleviate congestion

EP4109852B1

2024-10-16

Load-balanced fine-grained adaptive routing in high-performance system interconnect

EP4109853A1

2022-12-28

Filter with engineered damping for load-balanced fine-grained adaptive routing in high-performance system interconnect

Gómez et al.

2003

VOQ/sub SW: a methodology to reduce HOL blocking in InfiniBand networks

Vasiliadis et al.

2012

Class‐Based Weighted Fair Queuing Scheduling on Dual‐Priority Delta Networks

Zhang et al.

2009

A partially adaptive routing algorithm for Benes network on chip

Pan et al.

2021

CQPPS: A scalable multi‐path switch fabric without back pressure

Ohta

2019

Techniques for enhancing the rebalancing algorithm for folded Clos networks

Riadi et al.

2013

An opportunistic burst cloning scheme for optical burst switching over star networks

Fourneau et al.

2006

Convergence Routing under Bursty Traffic: Instability and an AIMD Controller

Cao et al.

2014

Technical report: Efficient buffering and scheduling for a single-chip crosspoint-queued switch

Row

0

Large Valency Serial Wormhole Routing Networks as a Scalable Multimedia Switching Infrastructure

Hassen

2017

Multistage Packet-Switching Fabrics for Data Center Networks

Kunert et al.

2005

Fibre-Optic AWG-based Real-Time Networks

Priority And Related Applications
Parent Applications (1)
Application

Priority date

Filing date

Relation

Title

US14/583,588

2008-08-20

2014-12-27

Continuation

Dragonfly processor interconnect network

Priority Applications (1)

https://patents.google.com/patent/US10153985B2/en?q=(high+radix)&oq=high+radix

12/13

2/14/26, 1:57 PM

US10153985B2 - Dragonfly processor interconnect network - Google Patents

Application

Priority date

Filing date

Title

US15/435,952

2008-08-20

2017-02-17

Dragonfly processor interconnect network

Applications Claiming Priority (3)
Application

Filing date

Title

US12/195,198

2008-08-20

Dragonfly processor interconnect network

US14/583,588

2014-12-27

Dragonfly processor interconnect network

US15/435,952

2017-02-17

Dragonfly processor interconnect network

Legal Events
Date

Code

Title

Description

2018-11-20

STCF

Information on status: patent grant

Free format text: PATENTED CASE

2021-12-30

MAFP

Maintenance fee payment

Free format text: PAYMENT OF MAINTENANCE FEE, 4TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE:
M1551); ENTITY STATUS OF PATENT OWNER: LARGE ENTITY
Year of fee payment: 4

2025-12-31

MAFP

Maintenance fee payment

Free format text: PAYMENT OF MAINTENANCE FEE, 8TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE:
M1552); ENTITY STATUS OF PATENT OWNER: LARGE ENTITY
Year of fee payment: 8

Data provided by IFI CLAIMS Patent Services

About

Send Feedback

Public Datasets

Terms

https://patents.google.com/patent/US10153985B2/en?q=(high+radix)&oq=high+radix

Privacy Policy

Help

13/13

2/14/26, 2:05 PM

US10469380B2 - Table-driven routing in a dragonfly processor interconnect network - Google Patents

Patents
Back to results

3 of 125,048

radix-3

(radix-3);

Table-driven routing in a dragonfly processor interconnect network
Abstract
A multiprocessor computer system comprises a dragonfly processor interconnect network that
comprises a plurality of processor nodes and a plurality of routers. The routers are operable to route

US10469380B2
United States

data by selecting from among a plurality of network paths from a target node to a destination node
in the dragonfly network based on one or more routing tables.

Download PDF

Find Prior Art

Similar

Inventor: Mike Parker, Steve Scott, Albert Cheng, Robert

Images (10)

Alverson
Current Assignee : Intel Corp

Worldwide applications
2011 JP EP EP US 2014 US 2015 JP 2016 US

Application US15/063,191 events

Classifications
H04L45/745 Address table lookup; Address filtering

View 7 more classifications

Landscapes

Engineering & Computer Science
Computer Networks & Wireless Communication

2016-03-07

Application filed by Intel Corp

2016-03-07

Priority to US15/063,191

2016-10-06

Publication of US20160294694A1

2019-11-05

Application granted

2019-11-05

Publication of US10469380B2

Status

Active

2032-04-04

Adjusted expiration

Info: Patent citations (44), Non-patent citations (52) , Cited by
(81), Legal events, Similar documents, Priority and Related

Show more

Applications
External links: USPTO, USPTO PatentCenter, USPTO
Assignment, Espacenet, Global Dossier, Discuss

Claims (18)

Hide Dependent

What is claimed is:
1. A multiprocessor computer system including a dragonfly processor interconnect network, comprising:
at least one router operable to route data by adaptively selecting from among a plurality of network paths from a target node to a destination node in the dragonfly network
based on set of routing tables,
wherein the dragonfly network comprises a plurality of routers, each of the routers is included in a respective one of a plurality of router groups, each router group is
connected via a respective link to each other router group in the plurality of router groups, the set of routing tables comprises one or more local tables to be used to route
data within router groups in the plurality of router groups, and the set of router tables further comprises one or more global tables to be used to route data between router
groups in the plurality of router groups.
2. The multiprocessor computer system of claim 1, wherein routing within a group comprises using one or more minimal or non-minimal local routing tables.
3. The multiprocessor computer system of claim 1, wherein routing between groups comprises using one or more minimal or non-minimal global routing tables.
4. The multiprocessor computer system of claim 1, the set of routing tables comprising minimal and non-minimal tables.
5. The multiprocessor computer system of claim 1, wherein the set of routing tables are employed to provide adaptive routing between the target and destination nodes.
6. The multiprocessor computer system of claim 1, wherein adaptive routing comprises using one or more of network congestion information from neighboring routers
and failed network link information from neighboring routers in selecting a route.
7. A method of operating a multiprocessor computer system, comprising:
routing data by selecting from among a plurality of network paths from a target node to a destination node in a dragonfly network based on one or more routing tables,
wherein the dragonfly network comprises a plurality of routers, each of the routers is included in a respective one of a plurality of router groups, each router group is
connected via a respective link to each other router group in the plurality of router groups, the set of routing tables comprises one or more local tables to be used to route

https://patents.google.com/patent/US10469380B2/en?q=(radix-3)&oq=radix-3

1/17

2/14/26, 2:05 PM

US10469380B2 - Table-driven routing in a dragonfly processor interconnect network - Google Patents

data within router groups in the plurality of router groups, and the set of router tables further comprises one or more global tables to be used to route data between router
groups in the plurality of router groups.
8. The method of operating a multiprocessor computer system of claim 7, wherein routing within a group comprises using one or more minimal or non-minimal local
routing tables.
9. The method of operating a multiprocessor computer system of claim 7, wherein routing between groups comprises using one or more minimal or non-minimal global
routing tables.
10. The method of operating a multiprocessor computer system of claim 7, the set of routing tables comprising minimal and non-minimal tables.
11. The method of operating a multiprocessor computer system of claim 7, wherein the set of routing tables are employed to provide adaptive routing between the target
and destination nodes.
12. The method of operating a multiprocessor computer system of claim 7, wherein adaptive routing comprises using one or more of network congestion information
from neighboring routers and failed network link information from neighboring routers in selecting a route.
13. A multiprocessor computer system router, comprising:
memory to store one or more routing tables defining one or more network paths from a target node to a destination node in the dragonfly network; and
routing logic, implemented at least in part in hardware to route data within the dragonfly network, wherein the dragonfly network comprises a plurality of routers, each of the
routers is included in a respective one of a plurality of router groups, each router group is connected via a respective link to each other router group in the plurality of router
groups, the set of routing tables comprises one or more local tables to be used to route data within router groups in the plurality of router groups, and the set of router tables
further comprises one or more global tables to be used to route data between router groups in the plurality of router groups.
14. The multiprocessor computer system router of claim 13, wherein routing within a group comprises using one or more minimal or non-minimal local routing tables.
15. The multiprocessor computer system router of claim 13, wherein routing between groups comprises using one or more minimal or non-minimal global routing tables.
16. The multiprocessor computer system router of claim 13, the set of routing tables comprising minimal and non-minimal tables.
17. The multiprocessor computer system router of claim 13, wherein the set of routing tables are employed to provide adaptive routing between the target and
destination nodes.
18. The multiprocessor computer system router of claim 17, wherein adaptive routing comprises using one or more of network congestion information from neighboring
routers and failed network link information from neighboring routers in selecting a route.

Description
CROSS-REFERENCE TO RELATED APPLICATIONS
This Application is a continuation (and claims the benefit of priority under 35 U.S.C. § 120) of U.S. patent application Ser. No. 13/290,567, filed on Nov. 7, 2011 and
entitled TABLE-DRIVEN ROUTING IN A DRAGONFLY PROCESSOR INTERCONNECT NETWORK, which application claims the benefit of priority, under 35 U.S.C. § 119(e), to
U.S. Provisional Patent Application Ser. No. 61/410,641, filed on Nov. 5, 2010 and entitled “TABLE-DRIVEN ROUTING IN A DRAGONFLY PROCESSOR INTERCONNECT
NETWORK,” inventors Mike Parker et al. The disclosures of the prior Applications are considered part of and are incorporated by reference in the disclosure of this
Application.
FIELD OF THE INVENTION
The invention relates generally to computer interconnect networks, and more specifically in one embodiment to table-driven routing in a dragonfly topology processor
interconnect network.
LIMITED COPYRIGHT WAIVER
A portion of the disclosure of this patent document contains material to which the claim of copyright protection is made. The copyright owner has no objection to the
facsimile reproduction by any person of the patent document or the patent disclosure, as it appears in the U.S. Patent and Trademark Office file or records, but reserves
all other rights whatsoever.
BACKGROUND
Computer systems have long relied on network connections to transfer data, whether from one computer system to another computer system, one computer component
to another computer component, or from one processor to another processor in the same computer. Most computer networks link multiple computerized elements to
one another, and include various functions such as verification that a message sent over the network arrived at the intended recipient, confirmation of the integrity of the
message, and a method of routing a message to the intended recipient on the network.
Processor interconnect networks are used in multiprocessor computer systems to transfer data from one processor to another, or from one group of processors to
another group. The number of interconnection links can be very large with computer systems having hundreds or thousands of processors, and system performance can
vary significantly based on the efficiency of the processor interconnect network. The number of connections, number of intermediate nodes between a sending and
receiving processing node, and the speed or type of connection all play a factor in the interconnect network performance.
Similarly, the network topology, or pattern of connections used to tie processing nodes together affects performance, and remains an area of active research. It is
impractical to directly link each node to each other node in systems having many tens of processors, and all but impossible as the number of processors reaches the
thousands.
Further, the cost of communications interfaces, cables, and other factors can add significantly to the cost of poorly designed or inefficient processor interconnect
networks, especially where long connections or high-speed fiber optic links are required. A processor interconnect network designer is thereby challenged to provide fast
and efficient communication between the various processing nodes, while controlling the number of overall links, and the cost and complexity of the processor
interconnect network.

https://patents.google.com/patent/US10469380B2/en?q=(radix-3)&oq=radix-3

2/17

2/14/26, 2:05 PM

US10469380B2 - Table-driven routing in a dragonfly processor interconnect network - Google Patents

The topology of a network, or the method used to determine how to link a processing node to other nodes in a multiprocessor computer system, is therefore an area of
interest.
SUMMARY
The invention comprises in one example a multiprocessor computer system having a dragonfly processor interconnect network that comprises a plurality of processor
nodes and a plurality of routers. The routers are operable route data by selecting from among a plurality of network paths from a target node to a destination node in the
dragonfly network based on one or more routing table, such as local and global routing tables, and minimal and non-minimal routing tables.
BRIEF DESCRIPTION OF THE FIGURES
FIG. 1 is a block diagram of a dragonfly network topology, consistent with an example embodiment of the invention.
FIG. 2 is a graph illustrating scalability of a dragonfly network in nodes for various router radices, consistent with an example embodiment of the invention.
FIG. 3 is a block diagram illustrating a dragonfly network topology, consistent with an example embodiment of the invention.
FIG. 4 is block diagram of dragonfly network topology groups, consistent with some example embodiments of the invention.
FIG. 5 is a block diagram of a dragonfly network illustrating minimal and non-minimal routing using virtual channels, consistent with an example embodiment of the
invention.
FIG. 6 is a graph illustrating latency v. offered load for a variety of routing algorithms using various traffic patterns, consistent with an example embodiment of the
invention.
FIG. 7 is a node group diagram of a dragonfly topology network illustrating adaptive routing via global channels using backpressure from intermediate nodes, consistent
with an example embodiment of the invention.
FIGS. 8A-8B are node diagrams illustrating credit round trip latency tracking, consistent with an example embodiment of the invention.
FIG. 9 shows a router configuration, consistent with an example embodiment of the invention.
FIG. 10 shows a group of nodes in a dragonfly processor interconnect network, consistent with an example embodiment of the invention.
FIG. 11 shows connections between several node groups in a dragonfly processor interconnect network, consistent with an example embodiment of the invention.
FIG. 12 shows a router table configuration for a dragonfly processor interconnect network router, consistent with an example embodiment of the invention.
DETAILED DESCRIPTION
In the following detailed description of example embodiments of the invention, reference is made to specific examples by way of drawings and illustrations. These
examples are described in sufficient detail to enable those skilled in the art to practice the invention, and serve to illustrate how the invention may be applied to various
purposes or embodiments. Other embodiments of the invention exist and are within the scope of the invention, and logical, mechanical, electrical, and other changes may
be made without departing from the subject or scope of the present invention. Features or limitations of various embodiments of the invention described herein, however
essential to the example embodiments in which they are incorporated, do not limit the invention as a whole, and any reference to the invention, its elements, operation,
and application do not limit the invention as a whole but serve only to define these example embodiments. The following detailed description does not, therefore, limit the
scope of the invention, which is defined only by the appended claims.
Interconnection networks are widely used to connect processors and memories in multiprocessors, as switching fabrics for high-end routers and switches, and for
connecting I/O devices. As processor and memory performance continues to increase in a multiprocessor computer system, the performance of the interconnection
network plays a central role in determining the overall performance of the system. The latency and bandwidth of the network largely establish the remote memory access
latency and bandwidth.
A good interconnection network typically designed around the capabilities and constraints of available technology. Increasing router pin bandwidth, for example, has
motivated the use of high-radix routers in which increased bandwidth is used to increase the number of ports per router, rather than maintaining a small number of ports
and increasing the bandwidth per port. The Cray Black Widow system, one of the first systems to employ a high-radix network, uses a variant of the folded-Clos topology
and radix-64 routers—a significant departure from previous low-radix 3-D torus networks. Recently, the advent of economical optical signaling enables topologies with
long channels. However, these long optical channels remain significantly more expensive than short electrical channels. A Dragonfly topology was therefore introduced,
exploiting emerging optical signaling technology by grouping routers to further increase the effective radix of the network.
The topology of an interconnection network largely determines both the performance and the cost of the network. Network cost is dominated by the cost of channels,
and in particular the cost of the long, global, inter-cabinet channels. Thus, reducing the number of global channels can significantly reduce the cost of the network. To
reduce global channels without reducing performance, the number of global channels traversed by the average packet must be reduced. The dragonfly topology reduces
the number of global channels traversed per packet using minimal routing to one.
Dragonfly Topology Example
To achieve this global diameter of one, very high-radix routers, with a radix of approximately 2√N (where N is the size of the network) are used. While radix 64 routers
have been introduced, and a radix of 128 is feasible, much higher radices in the hundreds or thousands are needed to build machines that scale to 8K-1M nodes if each
packet is limited to only one global hop using traditional very high radix router technology. To achieve the benefits of a very high radix with routers without requiring
hundreds or thousands of ports per node, the Dragonfly network topology proposes using a group of routers connected into a subnetwork as one very high radix virtual
router. This very high effective radix in turn allows us to build a network in which all minimal routes traverse at most one global channel. It also increases the physical
length of the global channels, exploiting the capabilities of emerging optical signaling technology.
Achieving good performance on a wide range of traffic patterns on a dragonfly topology involves selecting a routing algorithm that can effectively balance load across
the global channels. Global adaptive routing (UGAL) can perform such load balancing if the load of the global channels is available at the source router, where the routing
decision is made. With the Dragonfly topology, however, the source router is most often not connected to the global channel in question. Hence, the adaptive routing
decision is made based on remote or indirect information.
The indirect nature of this decision leads to degradation in both latency and throughput when conventional UGAL (which uses local queue occupancy to make routing
decisions) is used. We propose two modifications to the UGAL routing algorithm for the Dragonfly network topology that overcome this limitation with performance

https://patents.google.com/patent/US10469380B2/en?q=(radix-3)&oq=radix-3

3/17

2/14/26, 2:05 PM

US10469380B2 - Table-driven routing in a dragonfly processor interconnect network - Google Patents

results approaching an ideal implementation using global information. Adding selective virtual-channel discrimination to UGAL (UGAL-VC H) eliminates bandwidth
degradation due to local channel sharing between minimal and non-minimal paths. Using credit-round trip latency to both sense global channel congestion and to
propagate this congestion information upstream (UGAL-CR) eliminates latency degradation by providing much stiffer backpressure than is possible using only queue
occupancy for congestion sensing.
High-radix networks reduce the diameter of the network but require longer cables compared to low-radix networks. Advances in signaling technology and the recent
development of active optical cables facilitate implementation of high-radix topologies with longer cables.
An interconnection network is embedded in a packaging hierarchy. At the lowest level, the routers are connected via circuit boards, which are then connected via a
backplane or midplane. One or more backplanes are packaged in a cabinet, with multiple cabinets connected by electrical or optical cables to form a complete system.
The global (inter-cabinet) cables and their associated transceivers often dominate the cost of a network. To minimize the network cost, the topology should be matched
to the characteristics of the available interconnect technologies, such as cost and performance.
The maximum bandwidth of an electrical cable drops with increasing cable length because signal attenuation due to skin effect and dielectric absorption increases
linearly with distance. For typical high-performance signaling rates (10-20 Gb/s) and technology parameters, electrical signaling paths are limited to about 1 m in circuit
boards and 10 m in cables. At longer distances, either the signaling rate must be reduced or repeaters inserted to overcome attenuation.
Historically, the high cost of optical signaling limited its use to very long distances or applications that demanded performance regardless of cost. Although optical
cables have a higher fixed cost, their ability to transmit data over long distances at several times the data rate of copper cables results in a lower cost per unit distance
than electrical cables. Based on the data available using current technologies, the break-even point is at 10 m. For distances shorter than 10 m, electrical signaling is less
expensive. Beyond 10 m, optical signaling is more economical. The Dragonfly topology exploits this relationship between cost and distance. By reducing the number of
global cables, it minimizes the effect of the higher fixed overhead of optical signaling, and by making the global cables longer, it maximizes the advantage of the lower
per-unit cost of optical fibers.
The dollar cost of a dragonfly also compares favorably to a flattened butterfly for networks larger than 1 k nodes, showing approximately a 10% savings for up to 4 k
nodes, and approximately a 20% cost savings relative to flattened butterfly topologies for more than 4 k nodes as the dragonfly has fewer long, global cables. Folded Clos
and 3-d torus networks suffer in comparison, because of the larger number of cables needed to support high network diameters. For a network of only 1 k nodes, the
dragonfly is 62% the cost of a 3-d torus network and 50% that of a folded Clos network. This reduction in network cost is directly correlated to a reduction in network
power consumed, which is a significant advantage for large networks as well as for installations that are desirably environmentally friendly.
The example embodiments of a dragonfly network presented here show how use of a group of routers as a virtual router can increase the effective radix of a network,
and hence reduce network diameter, cost, and latency. Because the dragonfly topology reduces the number global cables in a network, while at the same time increasing
their length, the dragonfly topology is particularly well suited for implementations using emerging active optical cables—which have a high fixed cost but a low cost per
unit length compared to electrical cables. Using active optical cables for the global channels, a dragonfly network reduces cost by 20% compared to a flattened butterfly
and by 52% compared to a folded Clos network of the same bandwidth.
To show an example Dragonfly network topology, the following symbols are used in the description of the dragonfly topology and in example routing algorithms
presented later:
N Number of network terminals
p Number of terminals connected to each router
a Number of routers in each group
k Radix of the routers
k_ Effective radix of the group (or the virtual router)
h Number of channels within each router used to connect to other groups
g Number of groups in the system
q Queue depth of an output port
qvc Queue depth of an individual output VC
H Hop count
Outi Router output port i
The Dragonfly topology is a hierarchical network with three levels, as shown in FIG. 1: routers (104, 105, and 106), groups (101, 102, and 103), and system. At the router
level, each router has connections top nodes, a—1 local channels—to other routers in the same group—and h global channels—to routers in other groups. Therefore the
radix (or degree) of each router is defined as k=p+a+h−1. A group consists of a routers connected via an intra-group interconnection network formed from local channels,
as shown at 101 in FIG. 1. Each group has ap connections to terminals and ah connections to global channels, and all of the routers in a group collectively act as a virtual
router with radix k′=a(p+h). This very high radix, k′>>k enables the system level network to be realized with very low global diameter (the maximum number of expensive
global channels on the minimum path between any two nodes). Up to g=ah+1 groups (N=ap(ah+1) terminals) can be connected with a global diameter of one. In
contrast, a system-level network built directly with radix k routers would require a larger global diameter.
In a maximum-size (N=ap(ah+1)) dragonfly, there is exactly one connection between each pair of groups. In smaller dragonflies, there are more global connections out of
each group than there are other groups. These extra global connections are distributed over the groups with each pair of groups connected by at least _ah+1 g_channels.
The dragonfly parameters a, p, and h can have any values. However, to balance channel load, the network in this example has a=2p=2h. Because each packet traverses
two local channels along its route (one at each end of the global channel) for one global channel and one terminal channel, this ratio maintains balance. Because global
channels are expensive, deviations from this 2:1 ratio are done in some embodiments in a manner that overprovisions local and terminal channels, so that the expensive
global channels remain fully utilized. That is, the network is balanced in such examples so that a≥2h, 2p≥2h.
The scalability of a balanced dragonfly is shown in FIG. 2. By increasing the effective radix, the dragonfly topology is highly scalable—with radix-64 routers, the topology
scales to over 256 k nodes with a network diameter of only three hops. Arbitrary networks can be used for the intra-group and inter-group networks in FIG. 1. In the
example presented here, we use a 1-D flattened butterfly or a completely-connected topology for both networks. A simple example of the dragonfly is shown in FIG. 3
with p=h=2 (two processing nodes per router and two channels within each router coupled to other groups), a=4 (four routers in each group) that scales to N=72 (72
nodes in the network) with k=7 (radix 7) routers. By using virtual routers, the effective radix is increased from k=7 to k′=16, as group G0 of FIG. 3 has eight global
connections and eight node connections.
The global radix, k′, can be increased further by using a higher-dimensional topology for the intra-group network. Such a network may also exploit intra-group packaging
locality. For example, a 2-D flattened butterfly is shown in FIG. 4 at 401, which has the same k′ as the group shown in FIG. 5 but exploits packaging locality by providing

https://patents.google.com/patent/US10469380B2/en?q=(radix-3)&oq=radix-3

4/17

2/14/26, 2:05 PM

US10469380B2 - Table-driven routing in a dragonfly processor interconnect network - Google Patents

more bandwidth to local routers. A 3-dimension flattened butterfly is used in FIG. 4 at 402 to increase the effective radix from k′=16 to K′=32—allowing the topology to
scale up to N=1056 using the same k=7 router as in FIG. 1.
To increase the terminal bandwidth of a high-radix network such as a dragonfly, channel slicing can be employed. Rather than make the channels wider, which would
decrease the router radix, multiple network can be connected in parallel to add capacity. Similarly, the dragonfly topology in some embodiments can also utilize parallel
networks to add capacity to the network. In addition, the dragonfly networks described so far assumed uniform bandwidth to all nodes in the network. However, if such
uniform bandwidth is not needed, bandwidth tapering can be implemented by removing inter-group channels among some of the groups.
Dragonfly Routing Examples
A variety of minimal and non-minimal routing algorithms can be implemented using the dragonfly topology. Some embodiments of global adaptive routing using local
information lead to limited throughput and very high latency at intermediate loads. To overcome these problems, we introduce new mechanisms to global adaptive
routing, which provide performance that approaches an ideal implementation of global adaptive routing.
Minimal routing in a dragonfly from source node s attached to router Rs in group Gs to destination node d attached to router Rd in group Gd traverses a single global
channel and is accomplished in three steps:
Step 1: If Gs_=Gd and Rs does not have a connection to Gd, route within Gs from Rs to Ra, a router that has a global channel to Gd.
Step 2: If Gs_=Gd, traverse the global channel from Ra to reach router Rb in Gd.
Step 3: If Rb_=Rd, route within Gd from Rb to Rd.
This minimal routing works well for load-balanced traffic, but results in poor performance on adversarial traffic patterns. To load-balance adversarial traffic patterns,
Valiant's algorithm can be applied at the system level—routing each packet first to a randomly-selected intermediate group Gi and then to its final destination d. Applying
Valiant's algorithm to groups suffices to balance load on both the global and local channels. This randomized non-minimal routing traverses at most two global channels
and requires five steps:
Step 1: If Gs_=Gi and Rs does not have a connection to Gi, route within Gs from Rs to Ra, a router that has a global channel to Gi.
Step 2: If Gs_=Gi traverse the global channel from Ra to reach router Rx in Gi.
Step 3: If Gi_=Gd and Rx does not have a connection to Gd, route within Gi from Rx to Ry, a router that has a global channel to Gd.
Step 4: If Gi_=Gd, traverse the global channel from Ry to router Rb in Gd.
Step 5: If Rb_=Rd, route within Gd from Rb to Rd.
To prevent routing deadlock, two virtual channels (VCs) are employed for minimal routing and three VCs are required for non-minimal routing, as shown in FIG. 5. These
virtual router assignments eliminate all channel dependencies due to routing. For some applications, additional virtual channels may be required to avoid protocol
deadlock—e.g., for shared memory systems, separate sets of virtual channels may be required for request and reply messages.
A variety of routing algorithms for the dragonfly topology have been evaluated, including:
Minimal (MIN): The minimal path is taken as described previously.
Valiant (VAL) [32]: Randomized non-minimal routing as described previously.
Universal Globally-Adaptive Load-balanced [29] (UGALG,UGAL-L) UGAL chooses between MIN and VAL on a packet-by-packet basis to load-balance the
network. The choice is made by using queue length and hop count to estimate network delay and choosing the path with minimum delay. We implement
two versions of UGAL.
UGAL-L—uses local queue information at the current router node.
UGAL-G—uses queue information for all the global channels in Gs—assuming knowledge of queue lengths on other routers. While difficult to implement,
this represents an ideal implementation of UGAL since the load-balancing is required of the global channels, not the local channels.
The different routing algorithms are evaluated using both benign and adversarial synthetic traffic patterns, as shown in FIG. 6. Latency v. offered load is shown for the
four routing algorithms, using both uniform random traffic at 601 and adversarial traffic at 602. The use of a synthetic traffic pattern allows us to stress the topology and
routing algorithm to fully evaluate the network. For benign traffic such as uniform random (UR), MIN is sufficient to provide low latency and high throughput, as shown at
601 of FIG. 6. VAL achieves approximately half of the network capacity because its load-balancing doubles the load on the global channels. Both UGAL-G and UGAL-L
approach the throughput of MIN, but with slightly higher latency near saturation. The higher latency is caused by the use of parallel or greedy allocation where the routing
decision at each port is made in parallel. The use of sequential allocation will reduce the latency at the expense of a more complex allocator.
Adaptive routing on the dragonfly is challenging because it is the global channels, the group outputs, that need to be balanced, not the router outputs. This leads to an
indirect routing problem. Each router picks a global channel to use using only local information that depends only indirectly on the state of the global channels. Previous
global adaptive routing methods used local queue information, source queues and output queues, to generate accurate estimates of network congestion. In these cases,
the local queues were an accurate proxy of global congestion, because they directly indicated congestion on the routes they initiated. With the dragonfly topology,
however, local queues only sense congestion on a global channel via backpressure over the local channels. If the local channels are overprovisioned, significant numbers
of packets must be enqueued on the overloaded minimal route before the source router will sense the congestion. This results in a degradation in throughput and latency
as shown earlier in FIG. 6 at 602.
A throughput issue with UGAL-L arises due to a single local channel handling both minimal and non-minimal traffic. For example, in FIG. 7, a packet in R1 has a minimal
path which uses gc7 and a nonminimal path which uses gc6. Both paths share the same local channel from R1 to R2. Because both paths share the same local queue
(and hence have the same queue occupancy) and the minimal path is shorter (one global hop vs two), the minimal channel will always be selected, even when it is
saturated. This leads to the minimal global channel being overloaded and the non-minimal global channels that share the same router as the minimal channel being
under utilized. With UGAL-G, the minimal channel is preferred and the load is uniformly balanced across all other global channels. With UGAL-L, on the other hand, the
non-minimal channels on the router that contains the minimal global channel are under utilized—resulting in a degradation of network throughput.
To overcome this limitation, we modify the UGAL algorithm to separate the queue occupancy into minimal and nonminimal components by using individual VCs (UGAL-L
VC).
if(qm vcHm≤qnm vcHnm)
route minimally;
else
route nonminimally;
where the subscript m and nm denote the minimal and nonminimal paths. If the VC assignment of FIG. 5 is used, qm vc=q(V C1) and qnm
vc=q(V C0).

https://patents.google.com/patent/US10469380B2/en?q=(radix-3)&oq=radix-3

5/17

2/14/26, 2:05 PM

US10469380B2 - Table-driven routing in a dragonfly processor interconnect network - Google Patents

When compared, UGAL-LVC matches the throughput of UGAL-G on a WC traffic pattern but for UR traffic, the throughput is limited, with approximately 30% reduction in
throughput. For the WC traffic, where most of the traffic needs to be sent non-minimally, UGALLVC performs well since the minimal queue is heavily loaded. However, for
load-balanced traffic when most traffic should be sent minimally, individual VCs do not provide an accurate representation of the channel congestion—resulting in
throughput degradation.
To overcome this limitation, we further modify the UGAL algorithm to separate the queue occupancy into minimal and non-minimal components only when the minimal
and nonminimal paths start with the same output port. Our hybrid modified UGAL routing algorithm (UGAL-L VC H) is:
if(qmHm≤qnmHnm && Outm _ =Outnm)∥(qm vcHm≤qnm vcHnm && Outm=Outnm)
route minimally;
else
route nonminimally;
Compared to UGAL-L VC, UGAL-L VC H provides the same throughput on WC traffic pattern but matches the throughput of UGAL-G on UR traffic but resulting in nearly 2×
higher latency at an offered load of 0.8, near saturation. For WC traffic, UGAL-L VC H also results in higher intermediate latency compared to UGAL-G.
The high intermediate latency of UGAL-L is due to minimally-routed packets having to fill the channel buffers between the source and the point of congestion before
congestion is sensed. Our research shows that non-minimally routed packets have a latency curve comparable to UGAL-G while minimally-routed packets see
significantly higher latency. As input buffers are increased, the latency of minimally-routed packets increases and is proportional to the depth of the buffers. A histogram
of latency distribution shows two clear distributions—one large distribution with low latency for the non-minimal packets and another distribution with a limited number
of packets but with much higher latency for the minimal packets.
To understand this problem with UGAL-L, in the example dragonfly group shown in FIG. 7, assume a packet in R1 is making its global adaptive routing decision of routing
either minimally through gc0 or non-minimally through gc7. The routing decision needs to load balance global channel utilization and ideally, the channel utilization can
be obtained from the queues associated with the global channels, q0 and q3. However, q0 and q3 queue informations are only available at R0 and R2 and not readily
available at R1—thus, the routing decision can only be made indirectly through the local queue information available at R1.
In this example, q1 reflects the state of q0 and q2 reflects the state of q3. When either q0 or q3 is full, the flow control provides backpressure to q1 and q2 as shown with
the arrows in FIG. 7. As a result, in steady-state measurement, these local queue information can be used to accurately measure the throughput. Since the throughput is
defined as the offered load when the latency goes to infinity (or the queue occupancy goes to infinity), this local queue information is sufficient. However, q0 needs to be
completely full in order for q1 to reflect the congestion of gc0 and allow R1 to route packets non-minimally. Thus, using local information requires sacrificing some
packets to properly determine the congestion—resulting in packets being sent minimally having much higher latency. As the load increases, although minimally routed
packets continue to increase in latency, more packets are sent non-minimally and results in a decrease in average latency until saturation.
In order for local queues to provide a good estimate of global congestion, the global queues need to be completely full and provide a stiff backpressure towards the local
queues. The stiffness of the backpressure is inversely proportional to the depth of the buffer—with deeper buffers, it takes longer for the backpressure to propagate while
with shallower buffers, a much stiffer backpressure is provided. As the buffer size decreases, the latency at intermediate load is decreased because of the stiffer
backpressure. However, using smaller buffers comes at the cost of reduced network throughput.
To overcome the high intermediate latency, we propose using credit round-trip latency to sense congestion faster and reduce latency. In credit-based flow control,
illustrated in FIGS. 8A-8B, credit counts are maintained for buffers downstream. As packets are sent downstream, the appropriate credit count is decremented and once
the packet leaves downstream router, credits are sent back upstream and the credit count is incremented. The latency for the credits to return is referred to as credit
round-trip latency (tcrt) and if there is congestion downstream, the packet cannot be immediately processed and results in an increase in tcrt.
Referring to FIG. 8A, conventional credit flow control is illustrated at 801. As packets are sent downstream (1), the output credit count is decremented (2) and credits are
sent back upstream (3). This scheme is modified as shown in FIG. 8B at 802 to use credit round trip latency to estimate congestion in the network. In addition to the
output credit count being decremented (2), the time stamp is pushed into the credit time queue, denoted CTQ. Before sending the credit back upstream (4), the credit is
delayed (3), and when downstream credits are received (5), the credit count is updated as well as the credit round trip latency tcrt.
The value of tcrt can be used to estimate the congestion of global channels. By using this information to delay upstream credits, we stiffen the backpressure and more
rapidly propagate congestion information up stream. For each output O, tcrt(O) is measured and the quantity td(O)=tcrt(O)−tcrt0 is stored in a register. Then, when a flit is
sent to output O, instead of immediately sending a credit back upstream, the credit is delayed by td(O)−min [td(o)]. The credits sent across the global channels are not
delayed. This ensures that there is no cyclic loop in this mechanism and allows the global channels to be fully utilized.
The delay of returning credits provides the appearance of shallower buffers to create a stiff backpressure. However, to ensure that the entire buffer gets utilized and there
is no reduced throughput at high load, the credits needs to delayed by the variance of td across all outputs. We estimate the variance by finding min [td(o)] value and
using the difference. By delaying credits, the upstream routers observes congestion at a faster rate (compared to waiting for the queues to fill up) and leads to better
global adaptive routing decisions.
The UGAL-L routing algorithm evaluation using credit latency (UGAL-LCR) is investigated for both WC and UR traffic using buffers of depth 16 and 256. UGAL-LCR leads
to significant reduction in latency compared to UGALL and approaches the latency of UGAL-G. For WC traffic, UGAL-LCR reduces latency by up to 35% with 16 buffers and
up to over 20× reduction in intermediate latency with 256 buffers compared to UGAL-L. Unlike UGAL-L, the intermediate latency with UGAL-LCR is independent of buffer
size. For UR traffic, UGAL-LCR provides up to 50% latency reduction near saturation compared to UGAL-LVC H. However, both UGAL-LCR and UGALL VC H fall short of the
throughput of UGAL-G with UR traffic because their imprecise local information results in some packets being routed non-minimally.
The implementation of this scheme results in minimal complexity overhead as the following three features are needed at each router:
tracking credits individually to measure tcrt
registers to store td values
a delay mechanism in returning credits
The amount of storage required for td is minimal as only O(k) registers are required. The credits are often returned by piggybacking on data flits and
delaying credits to wait for the transmission of the next data flit upstream is required. The proposed mechanism only requires adding additional delay.
As for tracking individual credits, credits are conventionally tracked as a pool of credits in credit flow control—i.e., a single credit counter is maintained for each output VC
and increments when a credit is received. The implementation of UGAL-LCR requires tracking each credit individually. This can be done by pushing a timestamp on the
tail of a queue each time a flit is sent, as shown in FIG. 17(b) with the use of a credit timestamp queue (CTQ), and popping the timestamp off the head of the queue when
the corresponding credit arrives. Because flits and credits are 1:1 and maintain ordering, the simple queue suffices to measure round-trip credit latency. The depth of the

https://patents.google.com/patent/US10469380B2/en?q=(radix-3)&oq=radix-3

6/17

2/14/26, 2:05 PM

US10469380B2 - Table-driven routing in a dragonfly processor interconnect network - Google Patents

queue needs to be proportional to the depth of the data buffers but the queue size can be reduced to utilize imprecise information to measure congestion—e.g., by having
a queue which is only ¼ of the data buffer size, only one of four credits are tracked to measure the congestion.
The cost of a dragonfly topology also compares favorably to a flattened butterfly, as well as to other topologies. The flattened butterfly topology reduces network cost of
a butterfly by removing intermediate routers and channels. As a result, the flattened butterfly reduces cost by approximately 50% compared to a folded-Clos on balanced
traffic. The dragonfly topology extends the flattened butterfly by increasing the effective radix of the routers to further reduce the cost and increase the scalability of the
network.
A comparison of dragonfly and flattened butterfly networks of 64 k nodes shows that a flattened butterfly uses 50% of the router ports for global channels, while a
dragonfly uses 25% of the ports for global connections. The flattened butterfly requires two additional dimensions, while the dragonfly is a single dimension. In addition,
the dragonfly provides better scalability because the group size can be increased to scale the network whereas scaling the flattened butterfly requires adding additional
dimensions. With the hop count nearly identical, the dragonfly trades off longer global cables for smaller number of global cables required to provide a more cost-efficient
topology better matched to emerging signaling technologies.
Various embodiments of dragonfly networks described here also comprise two new variants of global adaptive routing that overcome the challenge of indirect adaptive
routing presented by the dragonfly. A dragonfly router will typically make a routing decision based on the state of a global channel attached to a different router in the
same group. Conventional global adaptive routing algorithms that use local queue occupancies to infer the state of this remote channel give degraded throughput and
latency. We introduce the selective use of virtual channel discrimination to overcome the bandwidth degradation. We also introduce the use of credit round-trip latency to
both sense and signal channel congestion. The combination of these two techniques gives a global adaptive routing algorithm that attempts to approach the
performance of an ideal algorithm with perfect knowledge of remote channel state.
Progressive Adaptive Routing in a Dragonfly Network
An improved routing method for Dragonfly processor interconnect networks is proposed here, providing deadlock-safe adaptive routing that is operable to choose among
multiple legal routes based on congestion or down links. This adaptive routing method provides improved routing performance and tolerance for downed or busy links
than prior methods, and explicitly communicates congestion across channels as opposed to withholding credits, which may negatively impact bandwidth.
In some embodiments, a network route is selected from among multiple minimal routes, such as routing in different dimensions first, and optionally further selected from
one or more non-minimal routes, such as using randomly chosen hops to avoid congestion or downed links.
Routing choices are presented via tables in one example, and may be biased toward certain routes or toward minimal or non-minimal routes depending on the network
configuration and state. For example, route choice may be biased toward minimal routing by default for highest efficiency, with a bias switch toward non-minimal routing
to protect a certain network link from arbitrarily or unnecessarily receiving additional traffic.
Congestion information is utilized in some embodiments by deriving an anticipated next link congestion from elements such as counting the number of messages in an
output queue and establishing a receiving buffer congestion estimate based on factors such as credits or messages in-flight. A node can query a potential receiving node
for the average “next link” output congestion, enabling the node to make a routing decision based on avoiding congested or down links.
FIG. 9 shows a Dragonfly network router, consistent with an example embodiment of the invention. The router block shown here comprises 48 tiles, with each tile
corresponding to an input/output pair. The tiles are organized in an 8×6 matrix, such that incoming packet data at a particular tile is routed across the row to one of the 8
columns, then up or down the 8 columns to one of the 6 rows, arriving at the appropriate tile for output. The channels in further embodiments feature multiple virtual
channels, virtual channel switching in-flight, error correction such as SECDED, and input buffering including dynamic allocation to virtual channels as needed to improve
network performance.
Referring again to the example of FIG. 9, forty of the tiles connect to external network links, while eight of the tiles connect to processor cores local to the processor
node. Each tile comprises an input queue, a subswitch, and a column buffer. The input queue receives packets from a serializer/deserializer interface to the network, and
determines how to route the packet. The packet is sent across the row bus to the subswitch in the appropriate column. The subswitch receives the packets, switches
them to the appropriate virtual channel, and sends the packet out one of the six column buses to the column buffer in the appropriate row. The column buffer collects the
packet data from the six tiles within the column and sends the packet data across the network channel.
The dragonfly network topology in this example is a hierarchical network of two layers of a flattened butterfly topology. The first layer is a two-dimensional flattened
butterfly that connects all of the router chips within a local group, such as a computer cabinet or chassis. Each group is treated as a very high-radix router, and a single
dimension flattened butterfly (all-to-all) connects all of the groups to form the second layer of the dragonfly topology example presented here.
The first dimension within the group, referred to for convenience as the “green” dimension, connects the 16 routers within a chassis. The second dimension within a
group is similarly called the “black” dimension, and connects the six chassis within a two cabinet group. This is reflected in the network configuration shown in the
network “group” of FIG. 10, which illustrates six chassis (represented as the six rows), made up of 16 routers per chassis (represented as the 16 columns).
Groups such as are illustrated in FIG. 10 are further coupled to one another using links in the “blue” dimension, as shown in FIG. 11. These “blue” links between groups
connect each group to each other group, to a maximum of 240 blue links per group in this example, or 241 groups per system. Each link can comprise multiple ports,
such as four ports per link or optical cable, resulting in four ports connecting each pair of groups over a single cable. In systems having fewer groups, unused ports from
the 240 blue ports per group can be used to provide additional bandwidth between configured groups, such as two links per group pair in a network having 120 groups
providing eight ports connecting each pair of groups.
In the network, packets route from a source node to a target node, traversing at least one but possibly all three dimensions shown in FIGS. 9-11. A routing path traversing
all three dimensions will likely first be routed in the green dimension and then the black dimension to reach the appropriate node in a group to link to the target group,
then the blue dimension to reach the intended target group. The packet is then routed in the green and black dimensions within the group to reach the intended target
node in the target group, resulting in five routings within three dimensions to reach the target.
The network supports both adaptive and deterministic routing in one embodiment. Deterministic routing sends a given packet over a predetermined route over the
network irrespective of network congestion. When multiple deterministic paths are available, deterministic traffic can be hashed based on a destination node, address, or
other such characteristics to distribute traffic between the multiple paths. Packets traveling between the same source and target will in some embodiments arrive at the
target in order, as all packets between the source and target take the same deterministic path.
Adaptive routing permits packets to take different routes based on congestion levels within the network. In some embodiments, packets may arrive out of order when
using adaptive routing, and may take non-minimal paths when congestion dictates avoiding a minimal path.
Minimal routing in a dragonfly occurs when a packet traverses at most one link in a given dimension. Minimal routing within a group, such as shown in FIG. 10, will
therefore take at the most one hop in the “green” dimension and one hop in the “black” dimension. A minimal path between nodes in different groups will take at most

https://patents.google.com/patent/US10469380B2/en?q=(radix-3)&oq=radix-3

7/17

2/14/26, 2:05 PM

US10469380B2 - Table-driven routing in a dragonfly processor interconnect network - Google Patents

one hop in the green dimension and one hop in the black dimension in each group, and will take one additional hop to travel from the source group to the target group.
As either the black or green dimensions may be traversed first, there are multiple minimal paths, both in the source and destination groups. If multiple links between
groups exist, one path may not require a hop in the black or green dimension in either the source or destination groups, reducing the total number of hops needed to
complete a minimal path to less than five.
Non-minimal routing can take multiple hops in either the black or green dimension in the source or target groups, resulting in more than five hops. Additional hops may be
desirable in circumstances where congestion is present in the minimal path or paths available to the router, improving the speed of message delivery to the target while
avoiding further congesting an already congested network link. Further embodiments attempt to spread traffic over available links, such as by randomizing or hashing
path selection to avoid creating additional congested network regions as a result of repeatedly routing the same path around a previously congested link.
In one such embodiment, an intermediate node is chosen in the group such as that of FIG. 10, such that the message is first minimally routed to the intermediate node,
and then routed from the intermediate node to the final node in the group. This results in up to two hops in each of the green and black dimensions, or double the number
of hops in minimal routing within a group. Routing may be nonminimal within the source group, nonminimal within the target group, or nonminimal in both the source and
target groups.
Nonminimal routing can also occur between groups, such as where a message is routed minimally within the source and target groups but is routed through an
intermediate group between the source and target groups to avoid congestion in the link between the source and target groups. Routing within the source, intermediate,
and target groups may further be minimal or nonminimal, depending on congestion within each of the groups.
The type of routing used for a given packet or message is determined in one embodiment by a routing control field in the packet header. For example, the routing control
symbol may indicate that deterministic non-minimal hashed routing is to be used when preserving packet order is desired. Packets are distributed across available paths
using the target node as a hash. Traffic is routed nonminimally, but distributing the packets among various intermediate nodes in the group results in reduced hot spots
or congestion.
Deterministic minimal hashed routing provides hashing of packets over minimal paths, which reduces the number of hops in a given group by permitting routing over
alternate minimal paths, such as black dimension before green dimension or green dimension before black dimension. This can result in severe network congestion in
certain situations, and so may not be desirable unless global traffic is particularly uniformly distributed.
Deterministic minimal non-hashed routing uses a single deterministic minimal path for all traffic, which provides packet ordering but does not provide good bandwidth or
load distribution among available paths. Such routing may be used for infrequent or small messages, such as control messages or latency-critical messages.
Adaptive routing can be sued as a default routing type when ordering is not required. Packets will attempt to route minimally, but may take non-minimal paths in groups
or between groups to avoid network congestion. Adaptive routing is provided in some embodiments using routing tables that provide two or more minimal and two or
more non-minimal ports for consideration in making a routing choice. A congestion value is computed for each node or tile in a router is calculated and distributed to
other tiles in the router, such as the router tiles shown in FIG. 9. The adaptive routing algorithm considers in this example the two minimal and two nonminimal paths
available, and selects from them based on the congestion values and optionally on various configured biases.
Port congestion values are derived in a further embodiment from factors such as downstream port congestion, estimated far-end link congestion, and near-end link
congestion. In a specific example, two bits of downstream port congestion information are propagated across the external channel corresponding to each tile in a router
chip, and updated periodically. These bits will be generated at the transmitting router chip by combining a view of congestion of downstream ports on the chip. The
downstream ports that are combined into this 2-bit congestion value are selected via an MMR-configurable mask at each tile. The congestion values of these
downstream ports are summed and compared to three programmable thresholds. If the sum is greater than the highest threshold, the congestion is 2′b11. If the sum is
less than the highest threshold, but greater than the middle threshold, the congestion is 2′b10. If the sum is less than the middle threshold and greater than the lowest
threshold, the congestion is 2′b01. Otherwise, if the sum is less than the lowest threshold, the congestion is 2′b00.
On the receiving side of the channel, this 2-bit value is mapped to a 4-bit value by indexing into a 4-entry by 4-bit wide downstream congestion remapping table. The
estimated far-end link congestion is computed by tracking the number of flits sent longer than the channel round trip latency in the past that have not yet been
acknowledged, and adjusting by the relative rates of flit transmission and acknowledgement receipt. The mechanism used to do this is a 5-bit wide 32-entry deep delay
chain. For an MMR-configurable number of cycles (1 to 31), the router counts the number of flits transmitted into the tail position of this delay chain. After this delay, all of
the values are shifted. The total expected outstanding flits on the channel (transmitted and ones for this an ack is expected) is the sum of the values in this chain. This
value is compared to the outstanding credit count. The total number of outstanding credits minus the expected flits on the channel represents an estimate for the
number of flits stored in the remote Input Queue.
The estimated far-end congestion is calculated as a 10-bit number. This number is converted to a 4-bit index according to a mapping table, and this 4-bit number is then
remapped to another programmable 4-bit value by indexing into a 16-entry far-end congestion remapping table.
The near-end link congestion is computed by summing the flits queued in the column buffer waiting to be transmitted across the link. This sum is also a 10-bit value and
is converted to a 4-bit value according to a mapping table. This 4-bit number is then remapped to another programmable 4-bit value by indexing into a 16-entry near-end
congestion remapping table.
The remapped 4-bit downstream port congestion value, the remapped 4-bit far-end link congestion value, and the remapped 4-bit near-end link congestion value are
combined to produce a single 4-bit congestion value per tile. This combination is done as a 3-input 4-bit unsigned saturating addition. This 4-bit congestion value is
propagated to all other tiles on the chip to aid those tiles in making informed adaptive choices.
A “link alive” signal is broadcast from each ntile on the chip to all other tiles on the chip. This link alive signal for each ntile indicates whether the corresponding tile has
an established serial link with the router it is connected to. Ports for which the link is not alive will be considered invalid from a port selection perspective. This allows the
router to adaptively avoid recently failed links which software has not yet been able to remove from the routing tables.
The link alive signals are propagated around the router via a 2-wire serial chain that connects all of the network tiles. Each tile places its link status information on the
serial chain at the appropriate bit timing. If all of the ports presented to the congestion logic are invalid, the packet will be discarded. In this case, it will be up to end-point
hardware to timeout on the missing packet and up to higher-level software to retransmit or handle the error as appropriate.
At each Input Queue, the broadcast congestion values are used in making the adaptive choice between the two minimal and two non-minimal port candidates. Before
using these congestion values, bias values are applied to the selected two minimal and non-minimal port congestion values. First, the values are logically extended to a
6-bit value by prepending two zeros to the most significant part of the value. The adaptive routing control type (adaptive0, adaptive1, adaptive2, or adaptive3) is used to
select a set of biases from a four entry bias table. Each entry has a pair of 2-bit shift value that determines how far left to shift the minimal ports and non-minimal ports
congestion values respectively. The 6-bit expanded congestion value can be shifted by zero, one, or two bits. The encoding of this field is 2′b00=shift left by zero bits
(multiply by one), 2′b01=shift left by one bit (multiply by two), 2′b10=shift left by two bits (multiply by four), 2′b11=reserved.

https://patents.google.com/patent/US10469380B2/en?q=(radix-3)&oq=radix-3

8/17

2/14/26, 2:05 PM

US10469380B2 - Table-driven routing in a dragonfly processor interconnect network - Google Patents

Each bias MMR also contains a pair of 6-bit values that is added to the 6-bit expanded minimal and non-minimal congestion values. The addition is performed as a
saturating add, resulting in a 6-bit number. The port corresponding to the lowest congestion is picked. If there is a tie between a minimal and a non-minimal port, the
router favors the minimal port. If there is a tie between the two ports presented as non-minimal or between the two ports presented as minimal, the choice is arbitrary
and may be made in any suitable way.
Table-Driven Routing Mechanism in a Dragonfly Network
The routing example presented here uses a variety of tables to determine paths available in routing a packet or message, and provide routing flexibility in the dragonfly
network configuration. Different tables exist to provide routing within a group and between groups, and for minimal and non-minimal routing paths.
The routing structures in the example router architecture presented here are divided into four distinct table sets: a global non-minimal (GN) table set, a global minimal
(GM) table, a local non-minimal (LN) table set, and a local minimal (LM) table. The logical flow of this specific example is shown in FIG. 12.
The global tables are used to determine how to route to a remote group, when the current group is not the target group. They are used to route toward a particular optical
port on which to exit the local group. Local tables are used to route to a particular router chip within the current group. They are used for “up” or “down” routing within the
group for local routing or for “up” routing in the intermediate group. Minimal tables specify minimal local or global routes. They are used when routing down or, in the
case of adaptive routing, when attempting to take a minimal path on the way up. Non-minimal tables specify non-minimal paths, and are only used when routing “up”.
They also provide a “root-detect” mechanism for determining when to stop routing up.
The global non-minimal table set is used to route non-minimal traffic to an intermediate group. It contains a list of ports that lead to “safe” intermediate groups, where a
“safe” intermediate group is one that is connected to all other groups. (In a healthy network, all groups are safe. In a partially healthy network, the tables should be
programmed to avoid sending traffic to an intermediate group that may not connect to the target group.) This table set consists of three tables. The first table selects
which rank in the green dimension to traverse to leave the current (source) group. The second table selects the black dimension to traverse. The third table selects the
optical port to leave the current router chip on.
The tables are arranged hierarchically in a fixed priority order. The green dimension table has the highest priority, and the blue dimension table has the lowest. Each table
lists a set of port numbers to leave the Aries on, or a special value that indicates that the current table is deferring its priority and the next table in the priority hierarchy
should be consulted. A special value on the lowest priority (blue) table, if referenced, will result in an error condition. Each table consists of 128 entries, each of which is a
6-bit port number or the special value of 6′b11xxxx. Each table is organized as 16 by 8 entries, with an accompany 7-bit ECC per each block of 8 entries.
This table should only contain routes to other router chips or optical port numbers that ultimately lead to an intermediate group that can safely route to all other groups in
the system. The table also provides the mechanism that distributes non-minimal traffic roughly evenly over the groups in the system. There are 128 entries in each table
so that even with an effective radix-18 dimension, each port is listed 7 or 8 times, leading to at most a 14.3% imbalance between two ports in the dimension. This
imbalance can be minimized by having the imbalanced ports differ on the multiple copies of the table throughout the group.
For global deterministic routing, this table set is indexed into by a hash value including the target, the tgtID, (possibly the local port number), and the optional hash field
from the packet header (which comes from the packet address). Each table will get a different index. For global adaptive routing, one of the blocks of 8 entries is
selected from the table at random. A second entry is selected at random from that 8-entry block. The two ports are compared with each other and with two entries from
the global minimal table to determine which path to route the packet.
The green tables in the ptiles will generally have each of the 15 green ports listed 8 times and will have 8 special values. Further, at the ptiles, the black tables will have
each of the 15 black ports listed approximately 7 times, with approximately 21 entries containing special values. The blue tables will have each of the optical ports listed
about 13 times each.
The green ntile ports will generally have all of the entries in the green table as the special value. The black and blue tables will be configured in the same proportions as in
the ptile case. The black ntile ports will generally have all of the entries in the green and black tables as the special value. The blue tables will be configured in the same
proportions as the ptiles.
The global minimal table is used to determine a direct path from the current group to the target group. It consists of 256 entries, each of which is 81-bits wide. Each entry
is divided in to two parts, a full port set and a restricted port set. The full port set consist of 8 6-bit port entries and a 3-bit modulo specifier. The modulo field indicates
the total number of valid ports in the associated entry. The modulo specifier is encoded as the modulo minus one. That is, a value of 7 in the modulo field will result in a
modulo of 8 operation. The restricted port set consists of 4 6-bit ports and a 2-bit modulo specifier. Each 81-bit entry will also have an 8-bit ECC.
This table is organized by target group numbers. Each target group corresponds to a “block” of 1, 2, 4, 8, 16, 32, 64, or 128 entries in the table, according to the size of the
system. A system with 241 groups would have 1 entry per block in the table. (15 of the entries would be unused.) A system with 65-128 groups would use 2 entries per
block. A system with 33-64 groups would use a block of four entries, and so forth. The group number along with zero to seven additional random (adaptive routing) or
hash (deterministic routing) bits are used to index into the table. Each entry contains a list of ports leading to Aries reachable from the current point in routing that
connect minimally to the associated target group, or leading directly to the target group over a blue link.
The full port set is used when just beginning to route minimally within a group (either at a ptile or an optical ntile) toward another group, or at any tile when routing nonminimally within the intermediate group and the root is detected in the local non-minimal table (see below). This side of the table lists all possible paths to all possible
optical ports that are connected minimally to the group specified by the index. The restricted port set is used for routing within the group other than in the root detect and
injection cases mention for the full port set table. This half of the table only represents paths in the network that are legal from the current point in the group network,
assuming we are routing minimally.
The key purpose of the restricted port list is to prevent packets from flowing back in the direction from whence they came. At a green port, the restricted table entries
should normally only list black and blue ports. At a black port, the restricted table entries should normally only list blue ports.
When all of the ports listed in the restricted set are invalid, this indicates to the adaptive routing logic that a packet has diverged from a legal minimal path. In this case
the adaptive routing logic will pick one of the non-minimal choices. (This should never occur for deterministically or minimally routed traffic as the tables should be
written in a consistent manner such that a packet never arrives at a point where it cannot route to the destination. If this does occur, the router will flag an error and
discard the packet.
When there are no legal restricted routed in a tile, the mod value can be set to any value. The route table should contain the special value of 6′b11xxxx in all of the entries
associated with the group number. When there is only one legal route, the port list should contain the legal route listed at least twice and the mod value set to two or
higher to match.
For deterministic routing one of the valid entries in either the full or restricted set is selected by computing a modulo of a hash by the number of valid entries in the
associated index. Like in the cases above, adaptive routing will choose 2 entries from the table but computing the mod on a random number and a second modulo of

https://patents.google.com/patent/US10469380B2/en?q=(radix-3)&oq=radix-3

9/17

2/14/26, 2:05 PM

US10469380B2 - Table-driven routing in a dragonfly processor interconnect network - Google Patents

N−1 to add to the first number plus one to get the offset of a second random but unique entry in the table.
The local non-minimal table set is used to pick a router chip in the local group that is used as the root for non-minimal routing within the group. This table is used for nonminimal routing when the source and target group are the same. It is also used for non-minimal routing in the intermediate group. This table set is structured like the
global non-minimal table, except that there is no blue table.
The local non-minimal table is indexed randomly for adaptive routing or by a hash for non-minimal deterministic routing. Similar to the global non-minimal table, for
adaptive routing two entries are produced by this table and compared. To reduce the number of total RAM macros in the design, these tables will be physically combined
with the global non-minimal tables in RAM.
This table lists Aries that are reachable from this tile that are safe to use for local non-minimal routing. In a healthy network, the ptiles and blue (optical) tiles should list
all Aries in the group roughly evenly. Approximately 15/16 of the entries in the green table should list green ports, and ˜⅙ should contain the special value indicating that
the green dimension has already been satisfied and that the black table should be used. Similarly, ˜⅚ of the entries in the black table should list black ports, and ˜⅙
should contain the special value indicating that the black dimensions has been satisfied. A special value in both the green and black tables indicates that the root has
been reached (“root detect”) and that the packet should be downrouted from this point.
The green tiles should fill the green table with special values (indicating that the green dimension has been satisfied), and should list the 6 aries reachable (including self,
using the special value) in the black table evenly. The black tiles should fill both the green and black tables with the special root detect value. The ptiles and optical tiles
need the full table set. Ntiles could technically do without the green table, however, the router table example presented here implements them for flexibility.
The local minimal table is used for minimal routing (“downrouting”) within the target group, and also when adaptively “uprouting” in the target group. This table has 128
entries. Each entry is 52 bits wide, consisting of 8 6-bit port numbers, a “diverged” bit, and a mod value indicating how many entries are valid in this line of the table. The
diverged bit indicates that the path within the target group has diverged from a minimal path, and thus this path cannot be used as a minimal path when adaptively
uprouting, and can only be used for downrouting. It is similar to the case in the global minimal table where all the ports in the restricted set are invalid.
This table is organized by the “target” Aries number within the group. Each local Aries number corresponds to a block of 1, 2, 4, 8, or 16 entries in the table, according to
the size of the group. A group of 65-128 Aries would used a block size of 1 entry per local Aries number. A group size of 33-64 Aries would use a block size of 2, and so
forth. The local Aries number along with zero to four additional random (adaptive routing) or hash (deterministic routing) bits are used to index into the table. Each entry
contains a list of ports leading to the associated local Aries.
For deterministic routing one of the valid entries in the table is selected by computing a modulo of a hash by the number of valid entries in the associated index. Like in
the cases above, adaptive routing will choose 2 entries from the table but computing the mod on a random number and a second modulo of N−1 to add to the first
number plus one to get the offset of a second random but unique entry in the table.
The global non-minimal tables are only used in the source group for traffic headed to another group. The global non-minimal and local non-minimal tables are never used
concurrently. Therefore, to reduce the total number of RAMs needed, the global non-minimal green table is stored in the same RAM as the local non-minimal green table.
The global non-minimal black table is stored in the same RAM as the local non-minimal black table. The global table is stored in the lower index value portion of each of
those two RAMs.
CONCLUSION
The above examples illustrate how routing in a Dragonfly network can be improved by using adaptive routing that is able to select a network path based on factors such
as network congestion or traffic type, and routing tables for various routings including minimal and non-minimal, and local and global routing.
Adaptive routing provides deadlock-safe routing that chooses among multiple legal routes based on congestion or down links, providing improved routing performance
and tolerance by explicitly communicating congestion across channels. Routing is performed across multiple minimal routes, such as routing in different dimensions
first, and optionally further selected from one or more non-minimal routes, such as using randomly chosen hops to avoid congestion or downed links.
Congestion information is based on anticipated next link congestion from elements such as counting the number of messages in an output queue and establishing a
receiving buffer congestion estimate through factors such as credits or messages in-flight. A node can query a potential receiving node for the average “next link” output
congestion, enabling the node to make a routing decision based on avoiding congested or down links. Other features, such as using a deterministic hash or a random
number to spread traffic in choosing a routing path are also provided, and are useful in spreading traffic to prevent congestion.
Routing choices are presented via tables in one example, and may be biased toward certain routes or toward minimal or non-minimal routes depending on the network
configuration and state. For example, route choice may be biased toward minimal routing by default for highest efficiency, with a bias switch toward non-minimal routing
to protect a certain network link from arbitrarily or unnecessarily receiving additional traffic. In a further example, routing tables include tables having local and global
routing tables, and minimal and non-minimal paths.
Although specific embodiments have been illustrated and described herein, it will be appreciated by those of ordinary skill in the art that any arrangement which is
calculated to achieve the same purpose may be substituted for the specific embodiments shown. This application is intended to cover any adaptations or variations of
the example embodiments of the invention described herein. It is intended that this invention be limited only by the claims, and the full scope of equivalents thereof.

Patent Citations (44)
Publication number

Priority date

Publication date

Assignee

Title

US4970658A

1989-02-16

1990-11-13

Tesseract Corporation

Knowledge engineering tool

US5079738A

1989-09-29

1992-01-07

Rockwell International
Corporation

Processor interconnect network for printing press system forming a star
network

US5249283A

1990-12-24

1993-09-28

Ncr Corporation

Cache coherency method and apparatus for a multiple path
interconnection network

US5425029A

1993-09-20

1995-06-13

Motorola, Inc.

Fast packet adaptation method for ensuring packet portability across
diversified switching type networks

JPH10285214A

1997-04-09

1998-10-23

Nec Corp

Fault recovery system

https://patents.google.com/patent/US10469380B2/en?q=(radix-3)&oq=radix-3

10/17

2/14/26, 2:05 PM

US10469380B2 - Table-driven routing in a dragonfly processor interconnect network - Google Patents

US5864738A

1996-03-13

1999-01-26

Cray Research, Inc.

Massively parallel processing system using two data paths: one
connecting router circuit to the interconnect network and the other
connecting router circuit to I/O controller

US5881135A

1992-06-15

1999-03-09

British Telecommunications
Public Limited Company

Service platform

US5970232A

1997-11-17

1999-10-19

Cray Research, Inc.

Router table lookup mechanism

JP2000201182A

1999-01-08

2000-07-18

Nippon Telegr & Teleph Corp
<Ntt>

Routing device and routing method

US6212636B1

1997-05-01

2001-04-03

Itt Manufacturing Enterprises

Method for establishing trust in a computer network via association

US20020124134A1

2000-12-28

2002-09-05

Emc Corporation

Data storage system cluster architecture

US20020131362A1

2001-03-16

2002-09-19

Ross Callon

Network routing using link failure information

US20030088696A1

1999-01-11

2003-05-08

Fastforward Networks, Inc.

Performing multicast communication in computer networks by using
overlay routing

US6643764B1

2000-07-20

2003-11-04

Silicon Graphics, Inc.

Multiprocessor system utilizing multiple links to improve point to point
bandwidth

US6766424B1

1999-02-09

2004-07-20

Hewlett-Packard Development
Company, L.P.

Computer architecture with dynamic sub-page placement

US20050073958A1

2003-10-03

2005-04-07

Avici Systems, Inc.

Selecting alternate paths for network destinations

US20050177344A1

2004-02-09

2005-08-11

Newisys, Inc. A Delaware
Corporation

Histogram performance counters for use in transaction latency analysis

US20050289101A1

2004-06-25

2005-12-29

Doddaballapur Jayasimha

Methods and systems for dynamic partition management of sharedinterconnect partitions

US7139926B1

2002-08-30

2006-11-21

Lucent Technologies Inc.

Stateful failover protection among routers that provide load sharing using
network address translation (LSNAT)

US20070198675A1

2004-10-25

2007-08-23

International Business Machines
Corporation

Method, system and program product for deploying and allocating an
autonomic sensor network ecosystem

US20080025208A1

2006-07-28

2008-01-31

Michael Tin Yau Chan

Wide-area wireless network topology

US20080123679A1

2003-11-19

2008-05-29

Cray Inc.

Routing table architecture

US7409506B2

2004-12-28

2008-08-05

Fujitsu Limited

Multiprocessor system with high-speed exclusive control

US20080285562A1

2007-04-20

2008-11-20

Cray Inc.

Flexible routing tables for a high-radix router

US20090106529A1

2007-10-05

2009-04-23

Abts Dennis C

Flattened butterfly processor interconnect network

US20100049942A1

2008-08-20

2010-02-25

John Kim

Dragonfly processor interconnect network

US7675857B1

2006-05-03

2010-03-09

Google Inc.

Method and apparatus to avoid network congestion

US20100103813A1

2008-10-28

2010-04-29

Nortel Networks Limited

Provisioned provider link state bridging (plsb) with routed back-up

US20100228798A1

2009-02-24

2010-09-09

Hitachi, Ltd.

Geographical distributed storage system based on hierarchical peer to
peer architecture

US20100265956A1 *

2009-04-16

2010-10-21

Futurewei Technologies, Inc.

Border Gateway Protocol (BGP) Grouped Route Withdrawals

US20110153942A1

2009-12-21

2011-06-23

Prashant Jain

Reducing implementation costs of communicating cache invalidation
information in a multicore processor

US20110191088A1

2010-02-01

2011-08-04

Yar-Sun Hsu

Object-oriented network-on-chip modeling

US8018860B1

2003-03-12

2011-09-13

Sprint Communications
Company L.P.

Network maintenance simulator with path re-route prediction

US20120020349A1

2010-07-21

2012-01-26

GraphStream Incorporated

Architecture for a robust computing system

US20120059938A1

2010-06-28

2012-03-08

Cray Inc.

Dimension-ordered application placement in a multiprocessor computer

US20120072602A1

2010-09-22

2012-03-22

Amazon Technologies, Inc.

Transpose box based network scaling

US20120072614A1

2010-09-22

2012-03-22

Amazon Technologies, Inc.

Transpose boxes for network interconnection

EP2451127A1

2010-11-05

2012-05-09

Cray Inc.

Progressive adaptive routing in a dragonfly processor interconnect
network

JP2012105265A

2010-11-05

2012-05-31

Cray Inc

Table-driven routing in dragonfly processor interconnect network

https://patents.google.com/patent/US10469380B2/en?q=(radix-3)&oq=radix-3

11/17

2/14/26, 2:05 PM

US10469380B2 - Table-driven routing in a dragonfly processor interconnect network - Google Patents

US8260922B1

2005-09-16

2012-09-04

Cisco Technology, Inc.

Technique for using OER with an ECT solution for multi-homed sites

US8427980B2

2010-07-21

2013-04-23

Hewlett-Packard Development
Company, L. P.

Methods and apparatus to determine and implement multidimensional
network topologies

US8489718B1

2010-05-19

2013-07-16

Amazon Technologies, Inc.

Torroidal backbone connections for network deployment

US8495194B1

2010-06-29

2013-07-23

Amazon Technologies, Inc.

Connecting network deployment units

US8576715B2

2009-10-26

2013-11-05

Mellanox Technologies Ltd.

High-performance adaptive routing

Family To Family Citations
* Cited by examiner, † Cited by third party

Non-Patent Citations (52)
Title
Advisory Action in U.S. Appl. No. 12/195,198 dated Apr. 19, 2011.
Arimilli et al., "The PERCS High-Performance Interconnect," 2010 18th IEEE Symposium on High Performance Interconnects (2010) (8 pages).
BPAI Decision in U.S. Appl. No. 12/195,198 mailed on Jun. 25, 2014.
European Patent Office Action in European Patent Application Serial No. 11187953.2 dated Jul. 12, 2017.
European Patent Office Communication Pursuant to Article 94(3) EPC in EP Application Serial No. 11187952.4 dated Jul. 23, 2013.
European Patent Office Extended Search Report and Opinion in EP Application Serial No. 11187952.4 dated Feb. 28, 2012.
European Patent Office Search Report and Opinion in EP Application Serial No. 11187953.2 dated May 7, 2012.
Examiner's Answer to Appeal Brief in U.S. Appl. No. 12/195,198 dated Oct. 12, 2011.
Extended European Patent Office Search Report and Opinion in EP Application Serial No. 16180770.6 dated Jan. 13, 2017, 8 pages.
Extended European Patent Office Search Report in EP Application Serial No. 11187953.2 dated May 7, 2012.
Final Office Action in U.S. Appl. No. 12/195,198 dated Jan. 26, 2011.
Final Office Action in U.S. Appl. No. 12/195,198 dated Jun. 5, 2015.
Final Office Action in U.S. Appl. No. 13/290,507 dated Oct. 1, 2013.
Final Office Action in U.S. Appl. No. 13/290,567 dated Jul. 17, 2014.
Final Office Action in U.S. Appl. No. 14/583,588 dated Feb. 2, 2016.
FOLDOC Computing Dictionary: "router," Retrieved and printed on Oct. 3, 2015 from http://foldoc.org/router.
Gratz, Paul et al., "Regional congestion awareness for load balance in networks-on-chip", High Performance Computer Architecture, 2008. HPCA 2008. IEEE 14th International
Symposium on, IEEE, Piscataway, NJ, USA, Feb. 16, 2008 (pp. 203-214.).
Japan Patent Office Action in Japanese Patent Application Serial No. 2011-241240 dated Aug. 18, 2015.
Japan Patent Office Action in Japanese Patent Application Serial No. 2015-012380 dated Dec. 1, 2015.
Japan Patent Office Action in Japanese Patent Application Serial No. 2015-103217 dated Aug. 2, 2016.
Japan Patent Office Action in Japanese Patent Application Serial No. 2015-103217 dated Mar. 22, 2016.
Japan Patent Office Notice of Allowance dated Nov. 20, 2015 Patent Application Serial No. 2011-241239.
Japan Patent Office Notification of Reasons for Rejection in Japanese Patent Application Serial No. 2011-241239 dated Aug. 11, 2015.
Japense Patent Office Notice of Allowance in Japanese Patent Application Serial No. 2015-103217 dated May 10, 2017.
Kim et al., "Cost-Efficient Dragonfly Topology for Large-Scale Systems", IEEE Micro (Impact Factor: 1.81). Mar. 2009; 29:33-40. DOI: 10.1109/MM.2009.5 (3 pages).
Kim, John et al., "Flattened Butterfly: A Cost Efficient Topology for High-Radix Networks", ISCA '07, Jun. 9-13, 2007 (pp. 126-137).
Kim, John et al.: "Technology-Driven, Highly-Scalable Dragonfly Topology", International Symposium on Computer Architecture, 2008. ISCA '08. 35th International Symposium on
Architecturei, IEEE Piscataway, NJ, Jun. 21, 2008 ( pp. 77-88).
Non Final Office Action in U.S. Appl. No. 14/672,125 dated Feb. 26, 2016.
Non-Final Office Action in U.S. Appl. No. 12/195,198 dated Aug. 13, 2010.

https://patents.google.com/patent/US10469380B2/en?q=(radix-3)&oq=radix-3

12/17

2/14/26, 2:05 PM

US10469380B2 - Table-driven routing in a dragonfly processor interconnect network - Google Patents

Non-Final Office Action in U.S. Appl. No. 12/195,198 dated Jan. 23, 2015.
Non-Final Office Action in U.S. Appl. No. 13/290,507 dated May 13, 2014.
Non-Final Office Action in U.S. Appl. No. 13/290,507 dated May 16, 2013.
Non-Final Office Action in U.S. Appl. No. 13/290,567 dated Dec. 29, 2014.
Non-Final Office Action in U.S. Appl. No. 13/290,567 dated Feb. 12, 2014.
Non-Final Office Action in U.S. Appl. No. 14/583,579 dated Jun. 26, 2015.
Non-Final Office Action in U.S. Appl. No. 14/583,579 dated Oct. 8, 2015.
Non-Final Office Action in U.S. Appl. No. 14/583,588 dated Sep. 10, 2015.
Notice of Allowance for Japanese Patent Application No. JP2015012380, dated Jun. 21, 2016, 2 pages-English Translation Available.
Notice of Allowance for Japanese Patent Application No. JP2015012380, dated Jun. 21, 2016, 2 pages—English Translation Available.
Notice of Allowance in U.S. Appl. No. 12/195,198 dated Sep. 21, 2015.
Notice of Allowance in U.S. Appl. No. 13/290,507 dated Jan. 26, 2015.
Notice of Allowance in U.S. Appl. No. 13/290,507 dated May 6, 2015.
Notice of Allowance in U.S. Appl. No. 13/290,507 dated Oct. 15, 2014.
Notice of Allowance in U.S. Appl. No. 13/290,567 dated Jul. 15, 2015.
Pre-Brief Conference Decision in U.S. Appl. No. 12/195,198 dated Jun. 17, 2011.
Scott, Steve et al., "The Black Widow of High-Radix Clos Network," Proceedings of the 33rd International Symposium on Computer Architecture (ISCA'06), Copyright 2006 (12
pages).
Technology-Driven, Highly-scalable Dragonfly Topology, 2008 IEEE John Kim, p. 78-88. *
U.S. Appl. No. 14/583,579 field on Dec. 26, 2015 entitled Table-Driven Routing in a Dragonfly Processor Interconnect Network, inventors Michael Parker et al.
U.S. Appl. No. 14/583,588, filed Dec. 27, 2014 and entitled Dragonfly Processor Interconnect Network, Inventors John Kim et al.
U.S. Appl. No. 14/672,125, filed Mar. 28, 2015 and entitled "Progressive Adaptive Routing in a Dragonfly Processor Interconnect Network," Inventors Mike Parker et al.
Valinataj, Motjaba et al., "A Fault-Tolerant and Congestion-Aware Routing Algorithm for Networks-on-Chip", Design and Diagnostics of Electronic Circuits and Systems (DDECS),
2010 IEEE 13th International Symposium on, IEEE, Piscataway, NJ, USA, Apr. 14, 2010, (pp. 139-144).
Varadarajan, Srinidhi et al, "Reinforcing reachable routes", Computer Networks 43 (2003), Elsevier Science Publishers, Amsterdam, NL, vol. 43, No. 3; Oct. 22, 2003 pp. 389-416 (28
pages).
* Cited by examiner, † Cited by third party

Cited By (81)
Publication number

Priority date

Publication date

Assignee

Title

US12418475B2

2022-12-08

2025-09-16

Google Llc

Fault-tolerant routing algorithm for toroidal network topologies

US20100049942A1

2008-08-20

2010-02-25

John Kim

Dragonfly processor interconnect network

JP5860670B2

2010-11-05

2016-02-16

インテル コーポレイション

Table-driven routing in a Dragonfly processor interconnect network

JP5913912B2

2010-11-05

2016-04-27

インテル コーポレイション

Innovative Adaptive Routing in Dragonfly Processor Interconnect Network

US8730965B2 *

2011-01-05

2014-05-20

Google Inc.

Systems and methods for dynamic routing in a multiprocessor network
using local congestion sensing

US8776207B2 *

2011-02-16

2014-07-08

Fortinet, Inc.

Load balancing in a network with session information

US8488601B1

2011-07-12

2013-07-16

Qlogic, Corporation

Method and system for link aggregation

US8467395B1 *

2011-07-12

2013-06-18

Qlogic, Corporation

Method and system for link aggregation

US9094309B2 *

2012-03-13

2015-07-28

International Business
Machines Corporation

Detecting transparent network communication interception appliances

US9274299B2

2012-08-29

2016-03-01

International Business
Machines Corporation

Modular optical backplane and enclosure

Family To Family Citations

https://patents.google.com/patent/US10469380B2/en?q=(radix-3)&oq=radix-3

13/17

2/14/26, 2:05 PM

US10469380B2 - Table-driven routing in a dragonfly processor interconnect network - Google Patents

US9577918B2 *

2012-11-19

2017-02-21

Cray Inc.

Increasingly minimal bias routing

CN104919442B *

2012-12-13

2017-10-10

相干逻辑公司

Multicomputer system with improved auxiliary interference networks

US9774498B2 *

2012-12-21

2017-09-26

Netspeed Systems

Hierarchical asymmetric mesh with virtual routers

US9634940B2

2013-01-31

2017-04-25

Mellanox Technologies, Ltd.

Adaptive routing using inter-switch notifications

CN103973564B *

2013-01-31

2017-12-15

清华大学

The adaptive routing method of interconnected network system

GB2511089A

2013-02-22

2014-08-27

Ibm

All-to-all message exchange in parallel computing systems

US9471726B2

2013-07-25

2016-10-18

Netspeed Systems

System level simulation in network on chip architecture

US9548960B2

2013-10-06

2017-01-17

Mellanox Technologies Ltd.

Simplified packet routing

US9197536B2

2013-11-22

2015-11-24

Dell Products L.P.

Use of alternate paths in forwarding of network packets

US9699079B2

2013-12-30

2017-07-04

Netspeed Systems

Streaming bridge design with host interfaces and network on chip (NoC)
layers

US10320746B2 *

2014-05-12

2019-06-11

Michael C. Wood

Computer security system and method based on user-intended final
destination

US9729473B2

2014-06-23

2017-08-08

Mellanox Technologies, Ltd.

Network high availability using temporary re-routing

US9806994B2

2014-06-24

2017-10-31

Mellanox Technologies, Ltd.

Routing via multiple paths with efficient traffic distribution

CN104079490B *

2014-06-27

2017-09-22

清华大学

Multi-level dragonfly interference networks and adaptive routing method

WO2015196461A1 *

2014-06-27

2015-12-30

Tsinghua University

Deadlock-free adaptive routing of interconnect network

US9519605B2

2014-07-08

2016-12-13

International Business
Machines Corporation

Interconnection network topology for large scale high performance
computing (HPC) systems

US9699067B2

2014-07-22

2017-07-04

Mellanox Technologies, Ltd.

Dragonfly plus: communication over bipartite node groups connected by a
mesh network

US9571341B1

2014-10-01

2017-02-14

Netspeed Systems

Clock gating for system-on-chip elements

US9660942B2

2015-02-03

2017-05-23

Netspeed Systems

Automatic buffer sizing for optimal network-on-chip design

US10348563B2

2015-02-18

2019-07-09

Netspeed Systems, Inc.

System-on-chip (SoC) optimization through transformation and generation
of a network-on-chip (NoC) topology

US9894005B2

2015-03-31

2018-02-13

Mellanox Technologies, Ltd.

Adaptive routing controlled by source node

US9864728B2

2015-05-29

2018-01-09

Netspeed Systems, Inc.

Automatic generation of physically aware aggregation/distribution networks

US9825809B2

2015-05-29

2017-11-21

Netspeed Systems

Dynamically configuring store-and-forward channels and cut-through
channels in a network-on-chip

US10218580B2

2015-06-18

2019-02-26

Netspeed Systems

Generating physically aware network-on-chip design from a physical systemon-chip specification

US9973435B2

2015-12-16

2018-05-15

Mellanox Technologies Tlv
Ltd.

Loopback-free adaptive routing

US10819621B2

2016-02-23

2020-10-27

Mellanox Technologies Tlv
Ltd.

Unicast forwarding of adaptive-routing notifications

US10178029B2

2016-05-11

2019-01-08

Mellanox Technologies Tlv
Ltd.

Forwarding of adaptive routing notifications

US10389636B2 *

2016-07-01

2019-08-20

Intel Corporation

Technologies for adaptive routing using network traffic characterization

US10630590B2 *

2016-07-14

2020-04-21

Mellanox Technologies Tlv
Ltd.

Credit loop deadlock detection and recovery in arbitrary topology networks

US10452124B2

2016-09-12

2019-10-22

Netspeed Systems, Inc.

Systems and methods for facilitating low power on a network-on-chip

US10281659B2 *

2016-11-03

2019-05-07

Alcatel Lucent

Fiber-management solution for an optical-network node

US20180159786A1

2016-12-02

2018-06-07

Netspeed Systems, Inc.

Interface virtualization and fast path for network on chip

US10200294B2

2016-12-22

2019-02-05

Mellanox Technologies Tlv
Ltd.

Adaptive routing based on flow-control credits

US10313269B2

2016-12-26

2019-06-04

Netspeed Systems, Inc.

System and method for network on chip construction through machine
learning

US10063496B2

2017-01-10

2018-08-28

Netspeed Systems Inc.

Buffer sizing of a NoC through machine learning

https://patents.google.com/patent/US10469380B2/en?q=(radix-3)&oq=radix-3

14/17

2/14/26, 2:05 PM

US10469380B2 - Table-driven routing in a dragonfly processor interconnect network - Google Patents

US10084725B2

2017-01-11

2018-09-25

Netspeed Systems, Inc.

Extracting features from a NoC for machine learning construction

US10469337B2

2017-02-01

2019-11-05

Netspeed Systems, Inc.

Cost management against requirements for the generation of a NoC

US10298485B2

2017-02-06

2019-05-21

Netspeed Systems, Inc.

Systems and methods for NoC construction

US10476780B2

2017-09-29

2019-11-12

Hewlett Packard Enterprise
Development Lp

Routing packets based on congestion of minimal and non-minimal routes

US11321136B2 *

2017-12-28

2022-05-03

Intel Corporation

Techniques for collective operations in distributed systems

US10644995B2

2018-02-14

2020-05-05

Mellanox Technologies Tlv
Ltd.

Adaptive routing in a box

US10983910B2

2018-02-22

2021-04-20

Netspeed Systems, Inc.

Bandwidth weighting mechanism based network-on-chip (NoC)
configuration

US10547514B2

2018-02-22

2020-01-28

Netspeed Systems, Inc.

Automatic crossbar generation and router connections for network-on-chip
(NOC) topology generation

US11144457B2

2018-02-22

2021-10-12

Netspeed Systems, Inc.

Enhanced page locality in network-on-chip (NoC) architectures

US10896476B2

2018-02-22

2021-01-19

Netspeed Systems, Inc.

Repository of integration description of hardware intellectual property for
NoC construction and SoC integration

US11023377B2

2018-02-23

2021-06-01

Netspeed Systems, Inc.

Application mapping on hardened network-on-chip (NoC) of fieldprogrammable gate array (FPGA)

US11176302B2

2018-02-23

2021-11-16

Netspeed Systems, Inc.

System on chip (SoC) builder

CN110324249B *

2018-03-28

2023-05-26

清华大学

A dragonfly network architecture and its multicast routing method

US10887217B2

2018-06-29

2021-01-05

Hewlett Packard Enterprise
Development Lp

Routing packets based on congestion metric thresholds and weights

US10944843B2

2018-11-05

2021-03-09

International Business
Machines Corporation

Topology aware computing device to reduce network latency

US11005724B1

2019-01-06

2021-05-11

Mellanox Technologies, Ltd.

Network topology having minimal number of long connections among
groups of network elements

US11792114B2

2019-05-23

2023-10-17

Hewlett Packard Enterprise
Development Lp

System and method for facilitating efficient management of non-idempotent
operations in a network interface controller (NIC)

US11316713B2 *

2019-11-25

2022-04-26

International Business
Machines Corporation

Virtual drawers in a server

US11561840B2

2020-01-30

2023-01-24

Alibaba Group Holding
Limited

Efficient inter-chip interconnect topology for distributed parallel deep
learning

GB2592211A

2020-02-19

2021-08-25

Nchain Holdings Ltd

Adapting connections of a layered network

GB2592225A

2020-02-19

2021-08-25

Nchain Holdings Ltd

Attestation service for use with a blockchain network

GB2594684A *

2020-02-19

2021-11-10

Nchain Holdings Ltd

Layered network

US11575594B2

2020-09-10

2023-02-07

Mellanox Technologies, Ltd.

Deadlock-free rerouting for resolving local link failures using detour paths

US11411911B2

2020-10-26

2022-08-09

Mellanox Technologies, Ltd.

Routing across multiple subnetworks using address mapping

US11870682B2

2021-06-22

2024-01-09

Mellanox Technologies, Ltd.

Deadlock-free local rerouting for handling multiple local link failures in
hierarchical network topologies

US11765103B2

2021-12-01

2023-09-19

Mellanox Technologies, Ltd.

Large-scale network with high port utilization

WO2023161831A1 *

2022-02-22

2023-08-31

Marvell Israel (M.I.S.L) Ltd.

Notification-based load balancing in a network

WO2023163858A1 *

2022-02-28

2023-08-31

Arris Enterprises Llc

Tunable latency with minimum jitter

CN115987702B *

2022-05-10

2025-08-26

清华大学

Broadcasting method, device, electronic device and storage medium

US12155563B2

2022-09-05

2024-11-26

Mellanox Technologies, Ltd.

Flexible per-flow multipath managed by sender-side network adapter

US12328251B2

2022-09-08

2025-06-10

Mellano Technologies, Ltd.

Marking of RDMA-over-converged-ethernet (RoCE) traffic eligible for
adaptive routing

US11765041B1 *

2022-09-15

2023-09-19

Huawei Technologies Co.,
Ltd.

Methods and systems for implementing a high radix network topology

KR20240080980A *

2022-11-30

2024-06-07

삼성전자주식회사

Device for networt systems

CN117081984B *

2023-09-27

2024-03-26

新华三技术有限公司

Route adjustment method, device and electronic equipment

https://patents.google.com/patent/US10469380B2/en?q=(radix-3)&oq=radix-3

15/17

2/14/26, 2:05 PM

US10469380B2 - Table-driven routing in a dragonfly processor interconnect network - Google Patents

US20250240237A1 *

2024-01-24

2025-07-24

Cornelis Networks, Inc.

Topology for deadlock prevention in a dragonfly using two virtual lanes

US20250240238A1 *

2024-01-24

2025-07-24

Cornelis Networks, Inc.

Deadlock prevention in a dragonfly using two virtual lanes

* Cited by examiner, † Cited by third party, ‡ Family to family citation

Similar Documents
Publication

Publication Date

Title

US10469380B2

2019-11-05

Table-driven routing in a dragonfly processor interconnect network

US9137143B2

2015-09-15

Progressive adaptive routing in a dragonfly processor interconnect network

US10153985B2

2018-12-11

Dragonfly processor interconnect network

US12386759B2

2025-08-12

Algorithms for use of load information from neighboring nodes in adaptive routing

JP6093867B2

2017-03-08

Non-uniform channel capacity in the interconnect

US8531968B2

2013-09-10

Low cost implementation for a device utilizing look ahead congestion management

US7623455B2

2009-11-24

Method and apparatus for dynamic load balancing over a network link bundle

US20170163744A1

2017-06-08

Network for transporting Ethernet and time sensitive data

US8085659B2

2011-12-27

Method and switch for routing data packets in interconnection networks

CN102263697A

2011-11-30

Method and device for sharing aggregated link traffic

Escudero-Sahuquillo et al.

2008

FBICM: Efficient congestion management for high-performance networks using distributed deterministic routing

Gómez et al.

2003

VOQ/sub SW: a methodology to reduce HOL blocking in InfiniBand networks

Escudero-Sahuquillo et al.

2010

An efficient strategy for reducing head-of-line blocking in fat-trees

Priority And Related Applications
Parent Applications (1)
Application

Priority date

Filing date

Relation

Title

US13/290,567

2010-11-05

2011-11-07

Continuation

Table-driven routing in a dragonfly processor interconnect network

Priority Applications (1)
Application

Priority date

Filing date

Title

US15/063,191

2010-11-05

2016-03-07

Table-driven routing in a dragonfly processor interconnect network

Applications Claiming Priority (3)
Application

Filing date

Title

US41064110P

2010-11-05

US13/290,567

2011-11-07

Table-driven routing in a dragonfly processor interconnect network

US15/063,191

2016-03-07

Table-driven routing in a dragonfly processor interconnect network

Legal Events
Date

Code

Title

Description

2019-03-11

STPP

Information on status: patent application and granting procedure in general

Free format text: NOTICE OF ALLOWANCE MAILED -- APPLICATION
RECEIVED IN OFFICE OF PUBLICATIONS

2019-06-17

STPP

Information on status: patent application and granting procedure in general

Free format text: DOCKETED NEW CASE - READY FOR EXAMINATION

https://patents.google.com/patent/US10469380B2/en?q=(radix-3)&oq=radix-3

16/17

2/14/26, 2:05 PM

US10469380B2 - Table-driven routing in a dragonfly processor interconnect network - Google Patents

2019-06-24

STPP

Information on status: patent application and granting procedure in general

Free format text: NOTICE OF ALLOWANCE MAILED -- APPLICATION
RECEIVED IN OFFICE OF PUBLICATIONS

2019-09-24

STPP

Information on status: patent application and granting procedure in general

Free format text: PUBLICATIONS -- ISSUE FEE PAYMENT VERIFIED

2019-10-16

STCF

Information on status: patent grant

Free format text: PATENTED CASE

2023-04-19

MAFP

Maintenance fee payment

Free format text: PAYMENT OF MAINTENANCE FEE, 4TH YEAR, LARGE
ENTITY (ORIGINAL EVENT CODE: M1551); ENTITY STATUS OF PATENT
OWNER: LARGE ENTITY
Year of fee payment: 4

Data provided by IFI CLAIMS Patent Services

About

Send Feedback

Public Datasets

https://patents.google.com/patent/US10469380B2/en?q=(radix-3)&oq=radix-3

Terms

Privacy Policy

Help

17/17

2/14/26, 2:04 PM

US11070437B2 - Network interconnect as a switch - Google Patents

Patents
Back to results

21 of 125,048

high radix

(high radix);

Network interconnect as a switch
Abstract
An interconnect as a switch module (“ICAS” module) comprising n port groups, each port group
comprising n−1 interfaces, and an interconnecting network implementing a full mesh topology

US11070437B2
United States

where each port group comprising a plurality of interfaces each connects an interface of one of the
Download PDF

other port groups, respectively. The ICAS module may be optically or electrically implemented.

Find Prior Art

Similar

According to the embodiments, the ICAS module may be used to construct a stackable switching
device and a multi-unit switching device, to replace a data center fabric switch, and to build a new,
high-efficient, and cost-effective data center.

Inventor: David I-Keong Wong
Current Assignee : Individual

Images (20)
Worldwide applications
2019 US CN CN 2020 US 2021 US 2022 US

Application US16/921,264 events

Classifications
H04B10/25 Arrangements specific to fibre transmission

View 18 more classifications

2020-07-06

Application filed by Individual

2020-07-06

Priority to US16/921,264

2020-10-22

Publication of US20200336386A1

2021-07-20

Application granted

2021-07-20

Publication of US11070437B2

Status

Active

2038-02-05

Anticipated expiration

Landscapes

Show all events

Engineering & Computer Science

Info: Patent citations (21), Cited by (48), Legal events, Similar
documents, Priority and Related Applications

Computer Networks & Wireless Communication

External links: USPTO, USPTO PatentCenter, USPTO
Assignment, Espacenet, Global Dossier, Discuss

Show more

Claims (3)

Hide Dependent

What is claimed is:
1. A multi-unit switching device, comprising one or more of ICAS modules each comprising:
n port groups, each port group comprising n−1 interfaces, wherein n is an integer equal or larger than 3;
an interconnecting network implementing a full mesh topology, wherein each of the n port groups connects one of the n−1 interfaces to another of the n port groups
statically, respectively; and
a plurality of first layer switches each configured and grouped into one or more of the n port groups each connects to the n port groups of a different said ICAS module
respectively, wherein a number of the plurality of first layer switches is n, wherein the plurality of first layer switches is indexed with an integer from 0 to n−1;
wherein the n−1 interfaces of the n port groups are labeled with the same indexes as those of connected n port groups; wherein an interface with index j of one of then port
groups with index i is connected to an interface with index i of one of the n port groups with index j, where i is in the range of 0 to n−1, j is in the range 0 to n−1, wherein i
does not equal to j, and wherein the interconnecting network comprises all connections between the n port groups; and
wherein said ICAS modules are implemented on one or more PCBs in an optical media or an electric circuit manner; wherein the multi-unit switching device further
comprises a plurality of switching devices, MCU- or CPU-based control cards, power modules, and cooling fan modules, and a multi-unit rackmount chassis, wherein each of
the ICAS modules is connected to the plurality of switching devices, such that the ICAS module interconnects at least two interfaces of each n port group each of different
switching devices to form a full mesh non-blocking interconnection, wherein the rest of the interfaces of each n port groups for interconnecting different switching devices
are configured as interfaces for uplink of an external network; and wherein said ICAS modules and the switching devices are implemented on the one or more PCBs as fabric
cards and line cards respectively and housed in the multi-unit rackmount chassis.
2. The multi-unit switching device of claim 1, wherein said ICAS modules are implemented on said PCBs in an optical media, wherein the optical media is optical fiber or
optical devices such as 3D MEMS structured configured to be full mesh topology and housed on said PCBs.

https://patents.google.com/patent/US11070437B2/en?q=(high+radix)&oq=high+radix&page=2

1/17

2/14/26, 2:04 PM

US11070437B2 - Network interconnect as a switch - Google Patents

3. The multi-unit switching device of claim 1, wherein said ICAS modules are implemented on said PCBs in an electric circuit, wherein the electric circuit comprises:
connectors that support high-speed differential signals and impedance matching;
copper differential pairs traces interconnecting the connectors in full-mesh topology; and
an active chip for each pair of the copper differential traces is usually added at the back end of the connector to restore and enhance the signal to increase the signal
transmission distance on said PCBs.

Description
CROSS-REFERENCE TO RELATED APPLICATION
This application is a divisional application of U.S. application Ser. No. 16/257,653; Filed on Jan. 25, 2019, which is a Continuation-in-Part application of U.S. application
Ser. No. 15/888,516 filed on Feb. 5, 2018, which is incorporated herein by reference in their entireties, including any figures, tables, and drawings.
FIELD OF INVENTION
The present invention relates to computer network. In particular, the present invention relates to interconnecting structure of ICAS module, stackable switching device,
multi-unit chassis switching device, network pod, fanout cable transpose rack and datacenter network.
DISCUSSION OF THE RELATED ART
As a result of the recent rapid growth in application needs—in both size and complexity—today's network infrastructure is scaling and evolving at a high rate. The data
traffic that flows from a data center to the Internet—i.e., “machine-to-user” traffic—is large, and ever increasing, as more people get connected, and as new products and
services are created. However, machine-to-user traffic is merely “the tip of the iceberg,” when one considers the data traffic within the data center—i.e., “machine-tomachine” traffic—necessary to generate the machine-to-user data traffic. Generally, machine-to-machine data traffic is several orders of magnitude larger than machineto-user data traffic.
The back-end service tiers and applications are distributed and logically interconnected within a data center. To service each user who uses an application program
(“app”) or a website, these back-end service tiers and applications rely on extensive real-time “cooperation” with each other to deliver the user's expected customized fast
and seamless experience at the front end. To keep up with the demand, even though the internal applications are constantly being optimized to improve efficiency, the
corresponding machine-to-machine traffic grows at an even faster rate than their continual optimization (e.g., at the current time, machine-to-machine data traffic is
growing roughly faster than doubling every year).
To be able to move fast and to support rapid growth are goals that are at the core of data center infrastructure design philosophy. In addition, the network infrastructure
within a data center (“data center network”) must be simple enough as to be managed by a small, highly efficient team of engineers. It is desired that the data center
network evolves in the direction that makes deploying and operating the network easier and faster over time, despite scale and exponential growth.
Some of these applications needs relate to the increasing use of data analytic tools (“big data”) and artificial intelligence (“AI”), for example. As discussed above, big data
and AI have become very significant distributed applications. Servicing these applications require handling large amounts of data (e.g., petabytes), using great
computation power (e.g., petaflops), and achieving very low latency (e.g., responses that become available within 100 ns). Simultaneously providing more powerful
processors (“scaling-up”) and exploiting greater parallel processing (“scaling-out”) have been the preferred approach to achieve performance. Unlike scientific
computation, however, big-data and AI applications are delivered in the form of services to large numbers of users across the world. Thus, like web servers for web
services, clusters of servers dedicated to big data and AI applications have become significant parts of the data center network.
At the current time, data center networks have largely transitioned from layer-2 to all layer-3 (e.g., running Border Gateway Protocol (BGP) and Equal-cost Multi-Path
(ECMP) protocols). A large-scale data center today is typically operating at tens of petabits-per-second scale (petascale) and expects growth into the hundreds of
petabits-per-second scale in the near future. The cost of provisioning such data center ranges from US$300 million to US$1.5 billion.
Let us define several terms in Table 1, before proceeding with the description of this patent.
TABLE 1
Terminology

Description

media

The media is a concept of a physical entity.
It can be optical or electronic.

interface

An interface is a concept of a physical entity.
It contains a transmission media and a reception
media. The media can be optical or electrical. An
interface can associate with a MAC (Medium
Access Control) entity or several interfaces
can associate with a MAC entity.

port

A port is a concept of a container entity.
It includes a set of interfaces. The number of the
interfaces depends on the technology. For
example, a 40G QSFP Ethernet port consists
of 4 interfaces (a total of 8 fibers). The 40G
QSFP port associates with a MAC entity.
The 10/40G optical technology is implemented
by reconfiguring the 40G QSFP Ethernet to 4
independent 10G interfaces each associates
with a MAC entity. As such, each interface can
provide connectivity and operate like a port.

https://patents.google.com/patent/US11070437B2/en?q=(high+radix)&oq=high+radix&page=2

2/17

2/14/26, 2:04 PM

US11070437B2 - Network interconnect as a switch - Google Patents

TABLE 1
Terminology

Description

port group

A port group is a concept of a
container entity; it includes a set of ports; the
number of ports depends on application.
Each interface in a port group is configured to
associate with a MAC presumably. In order to
meet the needs of description, this patent
introduces and defines the term of “port group”.

connection

A connection consists of media and
two interfaces communicating through
the media.

link

A link is a concept of a container entity.
It includes a set of connections. The
number of the connections depends on the
technology. For example, two 40G QSFP
connectors docked together to form one link
which contains 4 connections; two 10/40G
QSFP connectors docked together to form 4 links
each contains 1 connection.

downlink

Refers to the link that connects toward the hosts.

uplink

Refers to the link that connects toward the core
of the network.

intralink

Refers to the link that provides connectivity
inside a pod.

interlink

Refers to the link that provides connectivity
between two pods.

A review of our current state-of-the-art data center infrastructure is instructive. In the following context, data or traffic aggregation refers to multiplexing of
communication frames or packets. Aggregation model and disaggregation model refer to topologies of communication networks. The concept of data aggregation is
orthogonal to the concept of an aggregation or disaggregation model. Therefore, a disaggregation model can support data aggregation, as discussed below. A basic
concept in data communication is that communication channels can be error prone. Transmission over such communication channels at higher data rates and over large
distances requires complex and costly transceivers. Consequently, channel encoding, error detection and correction, and communication protocols are many techniques
to ensure data is transmitted over long distances with accuracy. In the past, as data transmission was expensive, data aggregation (e.g., multiplexing from different data
streams and multiplexing data from multiple topological sources) and data compression ensure even higher utilization of the communication channels and efficient
management of the communication cost. This is the origin of the aggregation (i.e., in both data and topology) paradigm. This paradigm dominates the networking
industry for decades. Such aggregation is widely used in wide area networks (WANs), where transmission cost dominates over other network costs. Today's hardware
architecture for data switching is also based on aggregation, i.e., each port is connected and aggregated from all other port. In today's communication networks, data is
typically aggregated before transmitting on to “uplink” to connect to external network (e.g., the Internet), which tends to be the most expensive port of the data switching
equipment. Due to both advances in semiconductor, fiber-optical, and interconnect technologies and economy of scale, network costs have reduced significantly. The
aggregation model is not necessarily the only—or the most suitable—solution in a data center. In today's data center networks, where machine-to-machine traffic (“eastwest traffic”) dominates most of the bandwidth, being several orders of magnitude than the machine-to-user bandwidth, multipath topology and routing (ECMP) are
deployed so that the combined network bandwidth is large. However, traffic is still aggregated from all incoming port on to each outgoing port. Nonetheless, the
multipath topology signifies a disaggregation model. The detailed description below places a structure and quantification onto the multipath topology and discloses a
disaggregation model, referred to herein as “interconnect as a Switch” (“ICAS”), which is significantly different from the more traditional aggregation model for data
centers.
Typically, in an enterprise or intranet environment, communication patterns are relatively predictable with a modest number of data sources and data destinations. These
data sources and data destinations are typically connected by a relatively small number of designated paths (“primary paths”), with some number of back-up or
“secondary paths,” which are provided primarily for fault tolerance. In such an environment, the routing protocols of the enterprise network are optimized to select a
shortest single path between each source-destination pair in the absence of a failure.
Distributed computing frameworks (e.g., MapReduce, Hadoop and Dryad) and web services (e.g., web search, ecommerce, social networking, data analytics, artificial
intelligence and scientific computing) bring a new paradigm of computing that requires both interconnections between a diverse range of hosts and significant aggregate
bandwidths. Due to the scarcity of ports even in the high-end commercial switches, a common hierarchical network topology that has evolved is a fat tree with higherspeed ports and increasing aggregate bandwidths, as one moves up the hierarchy (i.e., towards the roots). The data center network, which requires substantial intracluster bandwidths, represents a departure from the earlier hierarchical network topology. In the multi-rooted tree, the shortest single-path routing protocol can
significantly underutilize the available bandwidths. The ECMP is an improvement that statically stripes flows across available paths using flow hashing techniques. ECMP
is standardized in the IEEE 802.1Q Standard. ECMP allows “next-hop packet forwarding” to a single destination to occur over multiple “best paths,” as symmetric insuring
flows on deterministic paths. Equal cost multi-path routing can be used in conjunction with most routing protocols, because it is a per-hop decision limited to a single
router. It can substantially increase bandwidth by load-balancing traffic over multiple paths. When a data packet of a data stream arrives at the switch, and multiple
candidate paths are available for forwarding the data packet to its destination, selected fields of the data packet's headers are hashed to select one of the paths. In this
manner, the flows are spread across multiple paths, with the data packets of each flow taking the same path, so that the arrival order of the data packets at the
destination is maintained.
Note that ECMP performance intrinsically depends on both flow size and the number of flows arriving at a host. A hash-based forwarding scheme performs well in
uniform traffic, with the hosts in the network communicating all-to-all with each other simultaneously, or in which individual flow last only a few round-trip delay times
(“RTTs”). Non-uniform communication patterns, especially those involving transfers of large blocks of data, do not perform well under ECMP without careful scheduling
of flows to avoid network bottlenecks.

https://patents.google.com/patent/US11070437B2/en?q=(high+radix)&oq=high+radix&page=2

3/17

2/14/26, 2:04 PM

US11070437B2 - Network interconnect as a switch - Google Patents

In the detailed description below, the terms “fabric switch” and “spine switch” are used interchangeably. When both terms appear in a network, a fabric switch refers to a
device in a network layer which is used for multipath networking among TOR devices, while a spine switch refers to a device in a higher network layer which is used for
multipath networking among pods.
A fat tree network suffers from three types of drawbacks—i.e., 1) congestion due to hash collision, 2) congestion due to an aggregation model, and 3) congestion due to a
blocking condition. These congestions are further examined in the following.
First, under ECMP, two or more large, long-lived flows can hash to the same path (“hash collision”), resulting in congestion, as illustrated in FIG. 1 a. FIG. 1a shows four
fabric switches 10-0 to 10-3 interconnecting five TOR switches 11-0 to 11-4. As shown in FIG. 1 a, each TOR switch has four ports each communicating with a port of one
of the fabric switches 10-0 to 10-3, respectively. Each fabric switch has five ports each communicating with a port of one of TOR switches 11-0 to 11-4, respectively. In
FIG. 1 a, two flows designating TOR switch 11-0 are sourced from TOR switches 11-1 and 11-2. However, by chance, each flow is hashed to a path that goes through
fabric switch 10-0, which causes congestion at designating port 101 of fabric switch 10-0. (Of course, the congestion problem could have been avoided if one of the
flows is hashed to a path that goes through fabric switch 10-1 for instance). Furthermore, the static mapping of the flows to paths by hashing does not consider either
current network utilization or the sizes of the flows, so that the resulting collision overwhelms switch buffers, degrade overall switch utilization, and increases
transmission latency.
Second, in a fat tree network, the total bandwidth of the aggregated traffic may exceed the bandwidth of all the downlinks of all the fabric switches facing the same TOR
switch, resulting in aggregation congestion, as shown in FIG. 1 b. Such aggregation congestion is a common problem in the aggregation model of today's switching
network, and requires detailed rate limiting to avoid congestion. In FIG. b, the traffic through the fabric switches 12-0 to 12-3 facing the TOR switch 13-0 is sourced from
the TOR switches 13-1 to 13-4, but the aggregate traffic from the source (one source each from TOR switches 13-1 to 13-3 and two sources from TOR switch 13-4)
exceeds the combined bandwidth of all the downlinks of all the fabric switches 12-0 to 12-3 facing the TOR switch 13-0. More specifically, traffic is spread out evenly over
fabric switch 12-1 to 12-3 without congestion; additional traffic from TOR switch 13-4 exceeds the downlink bandwidth of port 121 of fabric switch 12-0 and thus causes
congestion.
Third, there is a blocking condition called the “strict-sense blocking condition,” which is applicable to statistically multiplexed flow-based networks (e.g., a TCP/IP
network). The blocking condition results from insufficient path diversity (or an inability to explore path diversity in the network) when the number and the size of the flows
become sufficiently large. FIG. 1c illustrates the blocking condition in a fat tree network. As shown in FIG. 1 c, the blocking condition occurs, for example, when paths
from fabric switches 14-0 and 14-1 to TOR switch 15-0 are busy and paths from fabric switches 14-2 and 14-3 to TOR switch 15-3 are busy, and a flow which requires a
path through TOR switch 15-0 arrives at TOR switch 15-3. An extra flow between TOR switch 15-0 and 15-3 can take one of 4 possible paths. Say it takes the path from
TOR switch 15-3 to fabric switch 14-1 and then from fabric switch 14-1 to TOR switch 15-0. However, the path from fabric switch 14-1 to TOR switch 15-0 is busy already.
Overall, multiplexing the blocked flow on to the existing flows results in increased congestion, latency and/or packet loss.
At the same time as the demand on the data center network grows, the rate of growth in CMOS circuit density (“Moore's law”) and the I/O circuit data rate appear to have
slowed. The cost of lithographic and heat density will ultimately limit how many transistors can be packed into a single silicon package. That is to say, an ultra large
storage or computing system is bound to be achieved through multiple chips. It is unlikely that an ultra large system will be integrated on a single chip with ultra-high
integration density as in the past. The question that arises here is how to build an ultra large bandwidth interconnection between the chips. It is instructive to learn that a
switching chip soldered on printed circuit board (PCB) employs high-speed serial differential I/O circuit to transmit and receive data to/from transceiver module. A
transceiver module interconnects to a transceiver module on a different system to accomplish network communications. An optical transceiver performs the electricalto-optical and optical-to-electrical conversion. An electrical transceiver performs complex electrical modulation and demodulation conversion. The primary obstacle that
hinders high-speed operation on PCB is the frequency-dependent losses of the copper-based interconnection due to skin effects, dielectric losses, channel reflections,
and crosstalk. Copper-based interconnection faces the challenge of bandwidth limit as the data rate exceeds several tens of gigabit per second (Gb/s). To satisfy
demands for bigger data bandwidth high-radix switch silicon integrates hundreds of differential I/O circuits. For example, Broadcom Trident-II chip and Barefoot Network
Tofino chip integrate 2×128 and 2×260 differential I/O circuits for 10 Gb/s transmit and receive respectively. To optimize system level port density, heat dissipation and
bandwidth the I/O circuits and interfaces are gathered in groups and standardized in specifications on electrical and optical properties. For SFP+, each port has a pair of
TX and RX serial differential interfaces at 10 Gb/s data rate. For QSFP, each port has four pairs of TX and RX serial differential interfaces at 10 Gb/s data rate each for a
total of 40 Gb/s or 4×10 Gb/s data rate. For QSFP28, each port has four pairs of TX and RX serial differential interfaces at 25 Gb/s data rate each for a total of 100 Gb/s
or 4×25 Gb/s data rate. For QSFP-DD, each port has eight pairs of TX and RX serial differential interfaces with a data rate of 50 Gb/s data rate each for a total of 400 Gb/s
or 8×50 Gb/s data rate. State of the art data centers and switch silicon employ 4 or 8 interfaces (TX, RX) at 10 Gb/s or 25 Gb/s or 50 Gb/s per port as design
considerations. These groupings are not necessarily unique. MTP/MPO as an optical interconnect standard defines up to 48 interfaces per port where each interface
contains a pair of optical fibers one for transmit and one for receive. However the electrical and optical specifications of transceiver with up to 48 interfaces per module
are yet to come. The definition of “port group” in this patent disclosure is extended to include more interfaces crossing multiple ports (e.g., 8 interfaces from 2 QSFP's; 32
interfaces from 8 QSFP's, etc.). A person experienced in the art can understand that this invention is applicable to other interconnect standards where multiple various
number of interfaces other than 4 be grouped together in the future.
These limitations affect data center networks by, for example, increasing power consumption, slowing of performance increase, and increasing procurement cycle. These
developments exacerbate the power needs for the equipment, as well as their cooling, facility space, the cost of hardware, network performance (e.g., bandwidth,
congestion, and latency, management), and the required short time-to-build.
The impacts to network communication are several:
(a) The network industry may not have enough economy of scale to justify CMOS technology of a smaller footprint;
(b) Simpler solutions should be sought to advance network technology, rather than to create more complex ones and packing more transistors;
(c) Scale-out solutions (i.e., in complement to scale-up solution) should be sought to solve application problems (e.g., big-data, AI, HPC, and data center);
(d) The chip port density (i.e., the number of ports in the traditional sense) can become flat1; and 1Integration of optical technology to the CMOS device
may provide new opportunity. However, do not expect a very high-radix chip, which would allow network scalability, to emerge any time soon.
(e) Implementation of interfaces with signal rates in excess of 100G will become increasingly difficult2. 2 One must think beyond the aggregation model
(e.g., the disaggregation model) to meet new network challenges.
Historically, high-speed networks have two classes of design space. In the first class of design space, HPC and supercomputing networks typically adopt direct network
topologies. In a direct network topology, every switch is connected to servers, as well as other switches in the topology. Popular topologies include mesh, torus, and
hypercube. This type of network is highly resource efficient and offers high capacity through numerous paths of various lengths between a source and destination.
However, the choice of which path to forward traffic over is ultimately controlled by proprietary protocols (i.e., non-minimum routing) in switches, NICs, and by the endhost application logic. That is, an algorithm or manual configuration is required to achieve routing. Such routing protocols increase the burden on the developer and
create a tight coupling between applications and the network.

https://patents.google.com/patent/US11070437B2/en?q=(high+radix)&oq=high+radix&page=2

4/17

2/14/26, 2:04 PM

US11070437B2 - Network interconnect as a switch - Google Patents

In the second class of design space, data centers scaling-out have resulted in the development of indirect network topologies, such as folded-Clos and multi-rooted trees
(“fat trees”), in which servers are restricted to the edges of the network fabric. The interior of the network fabric consists of dedicated switches that are not connected to
any servers, but simply route traffic within the network fabric. Data center networks of this type thus have a much looser coupling between applications and network
topology, placing the burden of path selection on the network switches themselves. That is to say, based on Internet routing technology such as BGP (Border Gateway
Protocol) routing protocol. The BGP routing protocol has a complete set of loop prevention, shortest path and optimization mechanisms. However, there are strict
requirements and restrictions on the network topology. Data center technology based purely on Internet BGP routing cannot effectively support multipath with nonshortest path topologies. As a result data center networks have traditionally relied on fat tree topologies, simple routing and equal cost multipath selection mechanisms
(e.g., ECMP). It is precisely because data center routing technology has restrictions on the network topology. The benefits to datacenter from non-shortest multipath path
network topology other than the equal cost multipath topology have not been explored in the past years of developments of the datacenter technologies.
The BGP and ECMP protocols are not without flaws. ECMP relies on static hashing of flows across a fixed set of shortest equal cost paths to a destination. For
hierarchical topologies (e.g., fat tree), ECMP routing has been largely sufficient when there are no failures. However, even now direct network topologies (e.g., Dragonfly,
HyperX, Slim Fly, BCube, and Flattened Butterfly), which employ paths of different lengths, have not seen adoption in data centers because of the limitations imposed by
both commodity data center switches and the widespread adoption of ECMP routing in data center networks. ECMP is wasteful of network capacity when there is
localized congestion or hot-spots, as it ignores uncongested longer paths. Further, even in hierarchical networks, ECMP makes it hard to route efficiently in the presence
of failures, and when the network is no longer completely symmetric, and non-shortest paths are available for improving network utilization.
FIG. 2a shows an architecture of a typical state-of-the-art data center network, organized by three layers of switching devices—i.e., “top-of-rack” (TOR) switches and fabric
switches implemented in 96 server pods 21-0 to 21-95 and spine switches implemented in 4 spine planes 20-0 to 20-3—interconnected by interlinks in a fat tree topology.
Details of a spine plane is shown in FIG. 2b where a spine plane consists of 48 spine switches 22-0 to 22-47 each connecting to 96 server pods. The connections from all
48 spine switches are grouped into 96 interlinks each including a connection from one of spine switches 22-0 to 22-47, respectively, for a total of 48 connections per
interlink. Details of a server pod is shown in FIG. 2 c, in which a server pod is shown to consist of 48 TOR switches 24-0 to 24-47 and 4 fabric switches 23-0 to 23-3, with
each TOR switch connected to all 4 fabric switches. Combining the connection information from FIG. 2 b, server pod of FIG. 2c may comprise 4 fabric switches 23-0 to
23-3 each connects one of 4 spine planes by interlinks, respectively; each interlink comprising 48 connections each connects one of 48 spine switches in a spine plane,
respectively. Each TOR switch provides 48×10G connections in 12×QSFP interfaces as downlink to connect to servers. An edge pod is shown in FIG. 2 d, details will be
given in below.
As shown in FIGS. 2b and 2 c, and in conjunction with FIG. 2 a, the TOR, fabric and spine layers of switches include: (a) a TOR switch layer consisting of 96×48 TOR
switches which connect the servers in the data center and which are equally distributed over 96 “server pods”; (b) a spine switch layer consisting of 4×48 spine switches
equally distributed over the 4 “spine planes”; and (c) a fabric layer consisting of 96×4 fabric switches, also equally distributed over the 96 server pods. In addition, two of
the server pods can be converted to two edge pods. FIG. 2d shows an edge pod. As shown in FIG. 2 d, edge pod 250 may comprise 4 edge switches 25-0 to 25-3, each
connects one of 4 spine planes by interlinks, respectively; each interlink comprising 48 connections each connects one of 48 spine switches in a spine plane,
respectively. Each edge switch may include one or more uplinks that interconnect an external network.
Details of an implementation of the server and spine pods are further described below in FIG. 2 c, 2 b in relation to FIG. 2 a. This configuration facilitates modularity by
assembling each fabric switch and spine switch in an 8U multi-unit chassis with 96 QSFP ports. As shown in FIG. 2 c, each TOR switch is implemented by a switch with
16 QSFP ports, which allocates 12 QSFP ports to connect to the servers in 10G interfaces (i.e., downlinks) and 4 QSFP ports to connect to the four fabric switches in four
40G interfaces in the same server pod. (In this detailed description, a QSFP represents a 40 Gbits/second bandwidth, which can be provided in a single 40G interface or
four 10G interfaces, each 40G interface including four receive-transmit pairs of optical fibers and each 10G interface including a receive-transmit pair of optical fibers).
The 40G interface between a TOR switch and a fabric switch is used for both intra-pod and inter-pod data traffic.
Each fabric switch in a server pod is implemented by a 96 QSFP ports switch, which allocates (i) 48 QSFP ports in 48 40G interfaces with the 48 TOR switches in the
server pod in a fat tree topology, and (ii) 48 QSFP ports in 48 40G interfaces to the 48 spine switches in the single spine plane the fabric switch is connected.
Each spine switch in a spine plane is also implemented by a 96 QSFP ports switch, which provides all 96 QSFP ports in 96 40G interfaces with the 96 fabric switches
connected to the spine plane, one from each of the 96 server pods. The data traffic through the spine plane represents inter-pod communications mostly for the server
pods.
In the configuration of FIG. 2 a, each server pod includes (i) 384 QSFP transceivers, half of which are provided to the spine planes and half of which are provided to the
network side of the fabric switches, (ii) 192 QSFP transceivers provided to the network side of the TOR switches, (iii) 576 transceivers provided to the servers; (iv) 192
optical QSFP cables, (v) 36 application-specific integrated circuits (ASICs), which implements the fabric switches and (vi) 48 ASICs, which implements the TOR switches.
The ASIC suitable for this application may be, for example, the Trident-II Ethernet Switch (“Trident II ASIC”). Each spine plane includes 4608 QSFP transceivers, 4608
optical QSFP cables and 432 Trident II ASICs.
The implementation of FIG. 2a provides in practice improved congestion performance but does not eliminate congestion. The network organization is based on an
aggregation model, intended to improve cost and utilization of communication ports and transmission media under the aggregation model. While this aggregation model
may still be valuable for wide-area networks (e.g., the Internet), recent advances of semiconductor technology and economic of scale have called this aggregation model
into question, when applied to local area networks.
SUMMARY
According to one embodiment of the present invention, an interconnect as a switch module (“ICAS” module) comprises n port groups, each port groups comprising n−1
interfaces, and an interconnecting network implementing a full mesh topology where each port group comprising a plurality of interfaces each connects an interface of
one of the other port groups, respectively.
According to one embodiment of the present invention, a stackable switching device is provided, which includes one or more ICAS modules as depicted above, a plurality
of switching devices, and a stackable rackmount chassis, each ICAS module being connected to the plurality of switching devices, such that the ICAS module
interconnects at least some interfaces of at least some port groups of different switching devices to form a full mesh non-blocking interconnection, while the rest
interfaces of the at least some port groups for interconnecting different switching devices are configured as interfaces for uplink. The ICAS module and the switching
devices are housed in the stackable rackmount chassis.
One embodiment of the present invention provides a multi-unit switching device, which includes: one or more ICAS modules implemented on a PCB as a circuit, a plurality
of switching devices, and a multi-unit rackmount chassis, each ICAS module being connected to the plurality of switching devices, such that the ICAS module
interconnects at least some interfaces of at least some port groups of different switching devices to form a full mesh non-blocking interconnection, while the rest
interfaces of the at least some port groups for interconnecting different switching devices are configured as interfaces for uplink. The ICAS module and the switching
devices are packaged in the multi-unit rackmount chassis.

https://patents.google.com/patent/US11070437B2/en?q=(high+radix)&oq=high+radix&page=2

5/17

2/14/26, 2:04 PM

US11070437B2 - Network interconnect as a switch - Google Patents

According to one embodiment of the present invention, a network pod is disclosed, which includes: a plurality of first layer switching devices, each having a plurality of
interfaces for downlink interfaces configured to receive and transmit data signals from and to a plurality of servers, and each having a plurality of network side interfaces
divided into a plurality of interlinks and a plurality of intralinks, and the interlink interfaces being configured to connect to higher layer switching devices, and the intralink
interfaces of the first layer switching devices each being configured and grouped into one or more port groups; and one or more second layer devices of ICAS modules
whose interfaces are divided into intralink interfaces and uplink interfaces, and the intralink interfaces of an ICAS module being grouped into port groups to connect to
the corresponding port groups of the first layer switching devices, and each port groups of additional ICAS module being connected to the additional port group of each
of the first layer switches, and the uplink interfaces are configured to connect to the external network. The first layer switching devices and the second layer devices are
interconnected to implement a full mesh network of a predetermined number of nodes.
K spine planes each having p interlinks are used to connect p network pods each having k TOR switches. In a spine plane, k spine switches interconnect to a fanout cable
transpose rack.
According to one embodiment of the present invention, a fanout cable transpose rack may include: k first port groups connecting to corresponding port groups of k spine
switches through first plurality of fiber optic cables; p second port groups through connecting second plurality of fiber optic cables to form p interlinks. A plurality of
fanout cables are used to cross-connect the k first port groups and the p second port groups so that connections from all k spine switches are grouped into p interlinks,
each interlink including one connection from each spine switch, and each interlink having a total of k connections.
According to one embodiment of the present invention, a data center network may have a plurality of interfaces for downlink configured to receive and transmit data
signals from and to a plurality of servers, and a plurality of interfaces for uplink configured to connect the Internet or connect another data center network with a similar
configuration. The data center network may include: a group of network pods (server pods/ICAS pods), each network pod in the group including: (a) a group of first layer
switching devices, providing some interfaces as interfaces for downlink, and having the rest interfaces grouped into one or more network side port groups; and (b) one or
more second layer devices, configured to interconnect at least some interfaces between some port groups of the first layer switching devices, wherein the rest interfaces
of the some port groups for interconnecting the first layer switching devices are configured as interfaces for uplink. The first layer switching devices and the second layer
devices are interconnected to implement a full mesh network of a predetermined number of nodes. The network pod further comprises a group of switch clusters, each
including a group of third layer switching devices, each of which routes a plurality of data signals received from or transmitted to a corresponding first layer switching
device in each group of network pods.
By simplifying the data center network infrastructure and reducing hardware requirement, the present invention addresses the problems relating to the power needs for
the equipment and their cooling, facility space, the cost of hardware, network performance (e.g., bandwidth, congestion, and latency, management), and the required
short built time.
The present invention is better understood upon consideration of the detailed description below in conjunction with the accompanying drawings.
BRIEF DESCRIPTION OF THE DRAWINGS
FIG. 1a illustrates congestion due to hash collision in a fat tree network under ECMP.
FIG. 1b illustrates aggregation congestion in a fat tree network topology.
FIG. 1c illustrates congestion due to blocking condition in a fat tree network.
FIG. 2a shows the architecture of a state-of-the-art data center network.
FIG. 2b shows in detail an implementation of a spine plane of the data center network of FIG. 2 a.
FIG. 2c shows in detail an implementation of a server pod of FIG. 2a using four fabric switches to distribute machine-to-machine traffic across 48 top-of-rack switches.
FIG. 2d shows in detail an implementation of an edge pod of FIG. 2a using four edge switches to provide interfaces for uplink to connect to external network.
FIG. 3 illustrates a “full mesh” topology in a network of 9 nodes.
FIG. 4a shows ICAS module 400, which interconnects 9 nodes, according to the full mesh topology of FIG. 3.
FIG. 4b illustrates the connectivity between the internal interfaces and the external interfaces of port group 7 of the 9-node ICAS module 400, in accordance with one
embodiment of the present invention.
FIG. 5a shows ICAS module 500 connecting port group 2 of each of TOR switches 51-0 to 51-8 in a full mesh topology, in accordance with one embodiment of the
present invention.
FIG. 5b illustrates, in the full mesh topology network 500 of FIG. 5 a, port group 2 of TOR switch 51-1 routing a data packet to port group 2 of TOR switch 51-7 through
internal interface 52-1-7 of port group 50-1 and internal interface 52-7-1 of port group 50-7 of ICAS2 module 500, in accordance with one embodiment of the present
invention.
FIG. 6a shows network 600, which is a more compact representation of the network of FIG. 5 a.
FIG. 6b shows network 620, after additional ICAS modules are added to network 600 of FIG. 6 a, so as to provide greater bandwidth and path diversity.
FIG. 7a shows that, in the architecture of the data center of FIG. 2 a, the topology of a server pod may be reduced to a (4, 48) bipartite graph.
FIG. 7b shows, as an example, network 720 represented as a (5, 6) bipartite graph.
FIG. 7c shows the 6-node full mesh graph embedded in the (5, 6) bipartite graph of FIG. 7 b.
FIG. 8a shows an improved data center network 800, in accordance with one embodiment of the present invention; data center networks 800 includes 20 spine planes,
providing optional uplinks 801, and 188 server pods, providing optional uplinks 802, uplinks 801 and 802 connecting to one or more external networks.
FIG. 8b shows in detail an implementation of modified spine plane 820, having 20 spine switches, providing optional uplink 821 for connecting to an external network.
FIG. 8c shows in detail an implementation of modified server pod 830 in a (20, 21) fabric/TOR topology, having 20 fabric switches for distributing machine-to-machine
traffic across 20 top-of-rack switches, in accordance with one embodiment of the present invention; the 21st TOR switch is removed from the modified server pod 830 so
that the connections are provided as optional uplink 831 for connecting the fabric switches to an external network.

https://patents.google.com/patent/US11070437B2/en?q=(high+radix)&oq=high+radix&page=2

6/17

2/14/26, 2:04 PM

US11070437B2 - Network interconnect as a switch - Google Patents

FIG. 9a shows ICAS-based data center network 900, achieved by replacing the server pods of network 800 of FIG. 8a (e.g., server pod 830 of FIG. 8c ) with ICAS pods 910 to 91-197, each ICAS pod being shown in greater detail in FIG. 9 c, according to one embodiment of the present invention; in FIG. 9 a, optional uplinks 901, shared by 20
spine planes, and optional uplinks 902, shared by 188 ICAS pods are provided for connecting to an external network.
FIG. 9b shows in detail spine plane 920, which implements one of the spine planes in data center network 900 and which is achieved by integrating a fanout cable
transpose rack into spine plane 820 of FIG. 8 b, according to one embodiment of the present invention; the spine switches in spine plane 920 provide optional uplink 921
for connecting to an external network.
FIG. 9c shows in detail an implementation of ICAS pod 930, which is achieved by replacing fabric switches 83-0 to 83-19 in server pod 830 of FIG. 8 c, according to one
embodiment of the present invention; each ICAS pod provides 20×10G uplinks 932 for connecting to an external network.
FIG. 9d illustrates a spine switch implemented with a single chip high-radix (i.e., a high port count) switching integrated circuit; such a spine switch makes use of the
highest port count switching integrated circuit available at present time.
FIG. 9e shows a spine switch formed by stacking together 4 switch boxes implemented with a Trident-II ASICs (96×10G configuration each) and 1 ICAS box 953. ICAS
box 953 combines 4 ICAS modules 95-0 to 95-3 in one 1U chassis. Each ICAS module contains 3 copies of ICAS1X5 configuration. Together the ICAS box 953 provides
non-blocking 1:1 subscription ratio to each of the 4 switches 96-0 to 96-3.
FIG. 9f shows a spine switch of an ICAS-based multi-unit switching device where 4 ICAS-based fabric cards 97-0 to 97-3 get connected in a full mesh topology to
switching ASIC's 98-0 to 98-3. Switching ASIC 98-0 and 98-1 are housed in line card 973, and switching ASIC's 98-2 and 98-3 are housed in line card 974.
To facilitate cross-referencing among the figures and to simplify the detailed description, like elements are assigned like reference numerals.
DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENTS
The present invention simplifies the network architecture by eliminating the switches in the fabric layer based on a new fabric topology, referred herein as the
“interconnect-as-a-switch” (ICAS) topology. The ICAS topology of the present invention is based on the “full mesh” topology. In a full mesh topology, each node is
connected to all other nodes. The example of a 9-node full mesh topology is illustrated in FIG. 3. The inherent connectivity of a full mesh network can be exploited to
provide fabric layer switching.
As discussed in further detail below, the ICAS topology enables a data center network that is far superior to a network of the fat tree topology used in prior art data center
networks. Unlike other network topologies, the ICAS topology imposes a structure on the network which reduces congestion in a large extent. According to one
embodiment, the present invention provides an ICAS module as a component for interconnecting communicating devices. FIG. 4a shows ICAS module 400, which
interconnects 9 nodes according to the full mesh topology of FIG. 3.
FIG. 4a shows ICAS module 400 having port groups 40-0 to 40-8 and each port group providing 8 external interfaces and 8 internal interfaces. In ICAS module 400, each
of the internal interfaces of a port group connects an internal interface of one of the other port groups, respectively. In fact, each port group is connected to every one of
the other port groups through exactly one internal interface. In this context, each “interface ” includes a receive-transmit pair of optical fibers capable of, for example, a
10 Gbits per second data rate. In FIG. 4 a, the port groups are indexed as 0-8. Indexes can also be arbitrary unequal values (For example, these 9 port groups can also be
indexed as 5, 100, 77, 9, 11, 8, 13, 50, and 64). The 8 internal interfaces for these 9 port groups are indexed according to the indexes of the connected port groups (For
example, the internal interfaces for the 7-th port group are 0, 1, 2, 3, 4, 5, 6 and 8 in the first example; and are 5, 100, 77, 9, 11, 8, 13 and 64 in the second example).
Furthermore, internal interface j of port group i is connected to internal interface i of port group j. The external interfaces for each port group of ICAS module 400 are
indexed sequentially as 0-7.
FIG. 4b illustrates in detail the connectivity between the internal interfaces and the external interfaces of a port group 7 in ICAS module 400, in accordance with the
present invention. As shown in FIG. 4 b, in one embodiment, the external interfaces are connected one-to-one to the internal interfaces sequentially in the index order (For
example, for port group 7, external interfaces 42-0 to 42-7 are sequentially connected to internal interfaces 41-0 to 41-6 and 41-8). Therefore, for port group i, external
interfaces 0-7 are connected to internal interfaces 0, . . ., i−1, i+1, . . ., and 8 respectively. Therefore, it can be easily seen that any pair of port groups x and y are connected
through internal interface x of port group y and internal interface y of port group x. This indexing scheme allows an external switching device to assign routes for data
packets using the internal interface indices of the source port group and destination port group. No congestion condition (e.g., due to hash collision, aggregation model,
or strict-sense blocking) can occur between any pair of port groups.
The internal interconnection between the port groups of the ICAS module can be realized via an optical media to achieve a full mesh structure. The optical media may be
an optical fiber and/or 3D MEMS. The 3D MEMS uses a controllable micro-mirror to create an optical path to achieve a full mesh structure. In both of these
implementations MPO connectors are used. Alternatively, the ICAS module may also be electrically implemented using circuits. In this manner, the port groups of the
ICAS module are soldered or crimped onto a PCB using connectors that support high-speed differential signals and impedance matching. The interconnection between
the port groups is implemented using a copper differential pair on the PCB. Since signal losses significantly vary between different grades of high-speed differential
connectors and between copper differential pairs on different grades of PCBs, an active chip is usually added at the back end of the connector to restore and enhance the
signal to increase the signal transmission distance on the PCB. Housing the ICAS module in a 1U to multi-U rackmount chassis will form a 1U to multi-U interconnection
device. The ICAS-based interconnection devices are then interconnected with switching devices to form a full mesh non-blocking network. This novel network will be
explained in detail hereunder in a plurality of embodiments. When the ICAS module of the 1U to multi-U interconnection device is optically implemented (based on optical
fiber and 3D MEMS), MPO-MPO cables are used to connect the ICAS-based interconnection devices and the switching devices. When the ICAS module of the 1U to multiU interconnection device is electrically implemented as circuits (based on PCB+chip), DAC direct connection cables or AOC active optical cables are used to connect the
ICAS-based interconnection devices and the switching devices.
As switching in ICAS module 400 is achieved passively by its connectivity, no power is dissipated in performing the switching function. Typical port group-to-port group
delay through an ICAS passive switch is around 10 ns (e.g., 5 ns/meter, for an optical fiber), making it very desirable for a data center application, or for big data, AI and
HPC environments.
The indexing scheme of external-to-internal connectivity in ICAS module 400 of FIG. 4a is summarized in Table 2 below:
TABLE 2
ICAS

Index of External Interface

Port Group
0

1

2

3

4

5

6

7

0

1

2

3

4

5

6

7

8

1

0

2

3

4

5

6

7

8

https://patents.google.com/patent/US11070437B2/en?q=(high+radix)&oq=high+radix&page=2

7/17

2/14/26, 2:04 PM

US11070437B2 - Network interconnect as a switch - Google Patents

2

0

1

3

4

5

6

7

8

3

0

1

2

4

5

6

7

8

4

0

1

2

3

5

6

7

8

5

0

1

2

3

4

6

7

8

6

0

1

2

3

4

5

7

8

7

0

1

2

3

4

5

6

8

8

0

1

2

3

4

5

6

7

FIG. 5a shows network 500, in which ICAS module 510 and port group 2 of each of TOR switches 51-0 to 51-8 interconnects in a full mesh topology, in accordance with
one embodiment of the present invention.
As illustrated in FIG. 5 b, in an ICAS module 510 in the full mesh topology network 500 of FIG. 5 a, port group 51-1 of TOR switch 1 routes a data packet to port group 517 of TOR switch 7 through external interface 53-1-6 and internal interface 52-1-7 of port group 50-1 of ICAS module 510, and internal interface 52-7-1 and external
interface 53-7-1 of port group 50-7 of ICAS module 510, in accordance with one embodiment of the present invention. As shown in FIG. 5 b, TOR switch 51-1, which is
connected to port group 50-1 of ICAS module 510, receives a data packet with a destination reachable through internal port group 52-1-7 of ICAS module 510. TOR
switch 51-1 has a port group that includes 8 interfaces 54-1-0 to 54-1-7 (provided as two QSFP ports) mapping one-to-one to external interfaces 53-1-0 to 53-1-7 of port
group 50-1 of ICAS module 510, which in turn maps one-to-one to internal interfaces 52-1-0, 52-1-2 to 52-1-8 in sequential order of port group 50-1 of ICAS module 510.
TOR switch 51-7 has a port group that includes 8 interfaces 54-7-0 to 54-7-7 (provided as two QSFP ports) mapping one-to-one to external interfaces 53-7-0 to 53-7-7 of
port group 50-7 of ICAS module 510, which in turn maps one-to-one to internal interfaces 52-7-0 to 52-7-6 and 52-7-8 in sequential order of port group 50-7 of ICAS
module 510. Each interface in a TOR switch port may be a 10G interface, for example. As port groups 50-1 and 50-7 of ICAS module 510 are connected through the port
groups' corresponding internal interfaces 52-1-7 and 52-7-1, TOR switch 51-1 sends the data packet through its interface 54-1-6 to external interface 53-1-6 of ICAS
module 510. Since the connectivity in the ICAS module 510 adopts a full mesh topology, the data packet is routed to external interface 53-7-1 of ICAS module 510.
In full mesh topology network 500, the interfaces of each TOR switch is regrouped into port groups, such that each port group contains 8 interfaces. To illustrate this
arrangement, port group 2 from each TOR switch connects to ICAS module 510. As each TOR switch has a dedicated path through ICAS module 510 to each of the other
TOR switches, no congestion can result from two or more flows from different source switches being routed to the same port of destination switch (the “SingleDestination-Multiple-Source Traffic Aggregation” case). In that case, for example, when TOR switches 51-0 to 51-8 each have a 10-G data flow that has TOR switch 51-0
as destination, all the flows would be routed on paths through respective interfaces. Table 3 summarizes the separate designated paths:
TABLE 3
ICAS
ICAS Source
Source

Destination

Internal

Internal

Destination

T1.p2.c0

↔

ICAS2.p1.c0

↔

ICAS2.p0.c1

↔

T0.p2.c0

T2.p2.c0

↔

ICAS2.p2.c0

↔

ICAS2.p0.c2

↔

T0.p2.c1

T3.p2.c0

↔

ICAS2.p3.c0

↔

ICAS2.p0.c3

↔

T0.p2.c2

T4.p2.c0

↔

ICAS2.p4.c0

↔

ICAS2.p0.c4

↔

T0.p2.c3

T5.p2.c0

↔

ICAS2.p5.c0

↔

ICAS2.p0.c5

↔

T0.p2.c4

T6.p2.c0

↔

ICAS2.p6.c0

↔

ICAS2.p0.c6

↔

T0.p2.c5

T7.p2.c0

↔

ICAS2.p7.c0

↔

ICAS2.p0.c7

↔

T0.p2.c6

T8.p2.c0

↔

ICAS2.p8.c0

↔

ICAS2.p0.c8

↔

T0.p2.c7

In other words, in Table 3, the single-connection data between first layer switch i connected to the port group with index i and first layer switch j connected to the port
group with index j is directly transmitted through the interface with index j of the port group with index i and the interface with index i of the port group with index j.
In Table 3 (as well as in all Tables herein), the switch source and the switch destination are each specified by 3 values: Ti.pj.ck, where Ti is the TOR switch with index i, pj is
the port group with index j and ck is the interface with index k. Likewise, the source interface and destination interface in ICAS module 500 are also each specified by 3
values: ICASj.pi.ck, where ICASj is the ICAS module with index j, pi is the port group with index i and ck is the internal or external interface with index k.
An ICAS-based network is customarily allocated so that when its port groups are connected to port group i from all TOR switches the ICAS will be labeled as ICASi with
index i.
Congestion can also be avoided in full mesh topology network 500 with a suitable routing method, even when a source switch receives a large burst of aggregated data
(e.g., 80 Gbits per second) from all its connected servers to be routed to the same destination switch (the “Port-to-Port Traffic Aggregation” case). In this case, it is
helpful to imagine the TOR switches as consisting of two groups: the source switch i and the rest of the switches 0 to i−1, i+1 to 8. The rest of the switches are herein
collectively referred to as the “fabric group”. Suppose TOR switch 51-1 receives 80 Gbits per second (e.g., 8 10G flows) from all its connected servers all designating to
destination TOR switch 51-0. The routing method for the Port-to-Port Traffic Aggregation case allocates the aggregated traffic to its 8 10G interfaces with port group 51-1
as in FIG. 5 a, such that the data packets in each 10G interface is routed to a separate TOR switch in the fabric group (Table 4A):
TABLE 4A
ICAS

Source

ICAS Source

Destination

Internal

Internal

Destination

T1.p2.c0

↔

ICAS2.p1.c0

↔

ICAS2.p0.c1

↔

T0.p2.c0

T1.p2.c1

↔

ICAS2.p1.c2

↔

ICAS2.p2.c1

↔

T2.p2.c1

T1.p2.c2

↔

ICAS2.p1.c3

↔

ICAS2.p3.c1

↔

T3.p2.c1

T1.p2.c3

↔

ICAS2.p1.c4

↔

ICAS2.p4.c1

↔

T4.p2.c1

T1.p2.c4

↔

ICAS2.p1.c5

↔

ICAS2.p5.c1

↔

T5.p2.c1

T1.p2.c5

↔

ICAS2.p1.c6

↔

ICAS2.p6.c1

↔

T6.p2.c1

T1.p2.c6

↔

ICAS2.p1.c7

↔

ICAS2.p7.c1

↔

T7.p2.c1

https://patents.google.com/patent/US11070437B2/en?q=(high+radix)&oq=high+radix&page=2

8/17

2/14/26, 2:04 PM

US11070437B2 - Network interconnect as a switch - Google Patents

TABLE 4A
ICAS

Source
T1.p2.c7

↔

ICAS Source

Destination

Internal

Internal

ICAS2.p1.c8

↔

ICAS2.p8.c1

Destination

↔

T8.p2.c1

Note that the data routed to TOR switch 51-0 has arrived at its designation and therefore would not be routed further. Each TOR switch in the fabric group, other than TOR
switch 51-0, then allocates its interface 0 for forwarding its received data to TOR switch 51-0 (Table 4B):
TABLE 4B

Source

ICAS

ICAS

Source

Destination

Internal

Internal

Destination

—

↔

—

↔

—

↔

—

T2.p2.c0

↔

ICAS2.p2.c0

↔

ICAS2.p0.c2

↔

T0.p2.c1

T3.p2.c0

↔

ICAS2.p3.c0

↔

ICAS2.p0.c3

↔

T0.p2.c2

T4.p2.c0

↔

ICAS2.p4.c0

↔

ICAS2.p0.c4

↔

T0.p2.c3

T5.p2.c0

↔

ICAS2.p5.c0

↔

ICAS2.p0.c5

↔

T0.p2.c4

T6.p2.c0

↔

ICAS2.p6.c0

↔

ICAS2.p0.c6

↔

T0.p2.c5

T7.p2.c0

↔

ICAS2.p7.c0

↔

ICAS2.p0.c7

↔

T0.p2.c6

T8.p2.c0

↔

ICAS2.p8.c0

↔

ICAS2.p0.c8

↔

T0.p2.c7

In other words, at least one multi-connection data between the first layer switch i connected to the port group indexed i and the first layer switch j connected to the port
group indexed j is transmitted through the first layer switches connected to at least one of the port groups other than the port group with source index. The multiconnection data arriving at the destination switch will cease to be further routed and transmitted.
To put it more precisely, the multi-connection data transmission occurring between first layer switch i connected to the port group with index i and first layer switch j
connected to the port group with index j includes the transmissions includes: as in Table 4A, the first layer switch i is connected, via a plurality of interfaces of the port
group with a plurality of index i, to a plurality of first layer switches with a plurality of corresponding indexes for transmission; as in Table 4B, a plurality of the first layer
switches with the indexes as shown are connected, via interfaces with index j of the port groups, to the interfaces with the indexes as shown of the port groups with
index j of the first layer switches for transmission; those transmissions that arrive at a destination switch will stop routing.
Thus, the full mesh topology network of the present invention provides performance that is in stark contrast to prior art network topologies (e.g., fat tree), in which
congestions in the fabric switch cannot be avoided under Single-Destination-Multiple-Source Traffic Aggregation and Port-to-Port Traffic Aggregation cases.
Also, as discussed above, when TOR switches 51-0 to 51-8 abide by the rule m≥2n−2, where m is the number of network-side interfaces (e.g., the interfaces with a port
group in ICAS module 500) and n is the number of the TOR switch's input interfaces (e.g., interfaces to the servers within the data center), a strict blocking condition is
avoided. In other words, a static path is available between any pair of input interfaces under any traffic condition. Avoiding such a blocking condition is essential in a
circuit-switched network, but is not necessarily significant in a flow-based switched network.
In the full mesh topology network 500 of FIG. 5 a, each port group with 8 interfaces of ICAS module 500 connects to a port group with 8 interfaces (e.g., 8 10-G
interfaces) of a corresponding TOR switch. Full mesh topology network 500 of FIG. 5a may be redrawn in a more compact form in FIG. 6 a, with a slight modification. FIG.
6a illustrates ICAS2 module 60-2 interconnecting to port group 2 of each of TOR switches 61-0 to 61-8. In FIG. 6 a, the interfaces between port group 2 of TOR switch 610 and port group 0 of ICAS module 60-2 (now labeled ‘ICAS2’) are represented as a single line (e.g., the single line between port group 2 of TOR switch 61-0 and port
group 0 of ICAS module 60-2). Such a line, of course, represents all 8 eight interfaces between the TOR switch and a corresponding port group in ICAS module 60-2. This
is exactly the case in FIG. 6b where each TOR switch 63-0 to 63-8 is shown also to have 4 port groups, to allow configuring network 620 of FIG. 6 b, where three additional
ICAS modules 62-0, 62-1 and 62-3 in addition to 62-2 and corresponding interfaces are added to network 600 of FIG. 6 a.
In full mesh topology network 500, uniform traffic may be spread out to the fabric group and then forwarded to its destination. In network 620 of FIG. 6 b, the additional
ICAS modules may be used to provide greater bandwidth. So long as the additional port groups are available in the TOR switches, additional ICAS modules may be added
to the network to increase path diversity and bandwidth.
The inventor of the present invention investigated in detail the similarities and the differences between the full mesh topology of the present invention and other network
topologies, such as the fat tree topology in the data center network of FIGS. 2 a. The inventor first observes that, in the architecture of the data center network of FIG. 2 a,
the fat tree network represented in a server pod (the “fabric/TOR topology”) can be reduced to a (4, 48) bipartite graph, so long as the fabric switches merely perform an
interconnect function for traffic originated among the TOR switches. This (4, 48) bipartite graph is shown in FIG. 7 a. In FIG. 7 a, the upper set of nodes, nodes 0-3 (“fabric
nodes”) 70-0 to 70-3, represent the four fabric switches in the server pod of FIG. 2a and the lower set of 48 nodes (i.e., leaf 0-47), labeled 71-0 to 71-47, represent the 48
TOR switches in a server pod of FIG. 2 a.
The inventor discovered that an n-node full mesh graph is embedded in a fabric-leaf network represented by a bipartite graph with (n−1, n) nodes (i.e., a network with n−1
fabric nodes and n TOR switch leaves). FIG. 7b shows, as an example, a (5, 6) bipartite graph with 5 nodes 72-0 to 72-4 and 6 leaves 73-0 to 73-5. FIG. 7c shows the 6node full mesh graph 740 with 6 nodes 74-0 to 74-5 embedded in the (5, 6) bipartite graph of FIG. 7 b.
This discovery leads to the following rather profound results:
(a) An n-node full mesh graph is embedded in an (n−1, n)-bipartite graph; and the (n−1, n) bipartite graph and the data center Fabric/TOR topology have
similar connectivity characteristics;
(b) A network in the (n−1, n) Fabric/TOR topology (i.e., with n−1 fabric switches and n TOR switches) can operate in same connectivity characteristics as
a network with full mesh topology (e.g., network 500 of FIG. 5a );
(c) Fabric switches are unnecessary in an (n−1, n) Fabric/TOR topology network, as the fabric switches merely performs interconnecting function
among the TOR switches (i.e., these fabric switches can be replaced by direct connectivity among TOR switches);
(d) A data center network based on a fat tree topology (e.g., the Fabric/TOR topology) can be improved significantly using ICAS modules.

https://patents.google.com/patent/US11070437B2/en?q=(high+radix)&oq=high+radix&page=2

9/17

2/14/26, 2:04 PM

US11070437B2 - Network interconnect as a switch - Google Patents

In the following, a data center network that incorporates ICAS modules in place of fabric switches may be referred to as an “ICAS-based” data center network. An ICASbased data center network has the following advantages:
(a) less costly, as fabric switches are not used;
(b) lower power consumption, as ICAS modules are passive;
(c) less congestion;
(d) lower latency;
(e) effectively less network layers (2 hops less for inter-pod traffic; 1 hop less for intra-pod traffic);
(f) greater scalability as a data center network.
These results may be advantageously used to improve typical state-of-the-art data center networks. FIG. 8a shows an improved data center network 800, in accordance
with one embodiment of the present invention. Data center network 800 uses the same types of components as the data center network of FIG. 2a (i.e., spine switches,
fabric switches and TOR switches), except that the number of fabric switches are increased to one less than the number of TOR switches (FIG. 8c shows equal number
of fabric switches and TOR switches because one of the TOR switch, the 21st TOR switch, is removed so that the 20 interfaces connected to it from the 20 fabric
switches are provided as uplink to connect to external network).
FIG. 8a shows the architecture of an improved data center network, organized by three layers of switching devices—i.e., “top-of-rack” (TOR) switches and fabric switches
implemented in 188 server pods 81-0 to 81-187 and spine switches implemented in 20 spine planes 80-0 to 80-19—interconnected by interlinks in a fat tree topology. An
interlink refers to the network connections between a server pod and a spine plane. For example, interlink k of each of the 188 server pods is connected to spine plane k;
interlink p of each of the 20 spine planes is connected to server pod p. The 20 spine planes each provide an optional uplink (e.g. uplink 801) and the 188 server pods
each provide an optional uplink (e.g., uplink 802) for connection to one or more external networks. In this example, to allow comparison, the numbers of server pods and
spine plane are chosen so that the improved data center network 800 and the state-of-the-art data center network 200 have the same network characteristics (2.2 Pbps
total server-side bandwidth; 3:1 oversubscription ratio—server-side to network-side bandwidth ratio; Trident-II ASIC). Other configurations of the improved data center
network are also possible, for instance, 32-TOR server pod or 48-TOR server pod but with higher radix switching silicon than the Trident-II ASIC.
Details of a spine plane of FIG. 8a are shown in FIG. 8 b. In FIG. 8 b, spine plane 820 consists of 20 spine switches 82-0 to 82-19 each connecting to 188 server pods. The
connections from all 20 spine switches are grouped into 188 interlinks, with each interlink including a connection from each spine switch 82-0 to 82-19, for a total of 20
connections per interlink.
Details of a server pod of FIG. 8a are shown in FIG. 8 c. In FIG. 8 c, the network-side connection (as opposed to the server-side connection) of the server pod is separated
into intra-pod links and inter-pod links (i.e., the interlinks). The two types of links are made independent from each other. The intra-pod region 832 consists of the intrapod links, the 20 TOR switches 84-0 to 84-19 and the 20 fabric switches 83-0 to 83-19 interconnected by the intra-pod links in a fat tree topology. For example,
connection kin each of the 20 TOR switches is connected to fabric switch k; connection p of each of the 20 fabric switches is connected to TOR switch p. 20 fabric
switches each provide an optional uplink (e.g., uplink 831) to connect to an external network. The inter-pod region consists of the inter-pod links (i.e., the interlinks) and
20 TOR switches 84-0 to 84-19 on the interlink side. Each interlink provides 20 10G connections to connect to all 20 spine switches on the same spine plane. Each server
pod includes a total of 20 links. For example, interlink k of each of the 188 TOR switches across the 188 server pods are connected to spine plane k; interlink p of each of
the 20 spine planes are connected to server pod p. Each TOR switch provides 48×10G connections in 12×QSFP ports as downlink to connect to servers.
The data traffic through the fabric switches is primarily limited to intra-pod. The TOR switches now route both the intra-pod traffic as well as inter-pod traffic and are more
complex. The independent link types achieve massive scalability in data center network implementations. (Additional independent links provided from higher radix
switching ASIC may be created to achieve larger scale of connectivity objectives). Additionally, data center network 800 incorporates the full mesh topology concept
(without physically incorporating an ICAS module) to remove redundant network devices and allow the use of innovative switching methods, in order to achieve a “lean
and mean” data center fabric with improved data traffic characteristics.
As shown in FIG. 8 c, FIG. 8b and FIG. 8 a, data center network 800 includes 20×188 TOR switches and 20×188 fabric switches equally distributed over 188 server pods,
and 20×20 spine switches equally distributed over 20 spine planes. In FIG. 8 a, each TOR switch has 100 10G-connections (i.e., 25 QSFPs of bandwidth in 10G mode), of
which 60 10G-connections are provided server-side and 40 10G-connections are provided network-side. (Among the network-side connections 20 10G-connections are
used for intra-pod traffic and 20 10G-connections are used for inter-pod traffic). In each server pod, fabric switches 83-0 to 83-19 each include 21 10G-connections, of
which 20 10G-connections are allocated to connect with a 10G-connection in each of TOR switches 84-0 to 84-19, and the rest being converted to provide as uplink to
connect to external network. In this manner, fabric switches 83-0 to 83-19 support the intra-pod region data traffic and the uplinks in the server pod by a 21-node full
mesh topology (with the uplinks of fabrics switches 0-19 collectively seen as one node). Using a suitable routing algorithm, such as any of those described above in
conjunction with Single-Source-Multiple-Destination Traffic Aggregation and Port-to-Port Traffic Aggregation, network congestion can be eliminated from all fabric
switches.
As the network in the intra-pod region of each server pod can operate in the same connectivity characteristics as a full mesh topology network, all the 20 fabric switches
of the server pod may be replaced by an ICAS module. ICAS-based data center network 900 resulting from substituting fabric switches 83-0 to 83-19 of data center
network 800, is shown in FIG. 9 a. To distinguish from the server pod of data center network 800, a server pod with its fabric switches replaced by an ICAS module is
referred to as an “ICAS pod.”
FIG. 9a shows the architecture of an ICAS-based data center network, organized by three layers of devices—i.e., “top-of-rack” (TOR) switches, ICAS module implemented
in 188 server pods 91-0 to 91-187 and spine switches implemented in 20 spine planes for 90-0 to 90-19—interconnected by interlinks in a fat tree topology. 20 spine
planes provide optional uplinks 901 and 188 ICAS pods provide optional 188×20×10G uplinks 902 for connecting to an external network. The number of network devices
in the data center network should be interpreted as illustrative only.
Details of a spine plane of FIG. 9a are shown in FIG. 9b according to one embodiment. In FIG. 9 b, spine plane 920 includes 20 spine switches 92-0 to 92-19 and a fanout
cable transpose rack 921. The fanout cable transpose rack contains: k first port groups 923 are connected to corresponding port groups of k spine switches through a
plurality of first WO-MPO fiber cables, where each first port group including ┌p/m┐ first MPO adapters, and each first MPO adapter including m interfaces (where each
interface includes one transmit fiber channel and one receive fiber channel), and a plurality of first MPO fiber adapters from the k port groups 923 are connect to LC
optical fiber adapter mounting panel 922 through a plurality of first WO-LC fanout fiber cables , where k=20, p=188, m=4, and ┌ ┐ is a ceiling function; the fanout cable
transpose rack 921 includes p second port groups 924 that are connected to a plurality of second MPO-MPO fiber cables to form interlinks 99-0 to 99-187, each second
port group contains ┌k/m┐ second MPO fiber adapters, each of which includes m interfaces (where each interface includes one transmit fiber channel and one receive
fiber channel), and a plurality of second MPO fiber adapters from the p port groups 924 are connected to LC optical fiber adapter mounting panel 922 through a plurality
of the second MPO-LC fanout cables; a plurality of first MPO-LC fanout fiber cables cross-connect a plurality of second MPO-LC fanout fiber cables on the LC fiber
adapter mounting panel 922, through cross-connection, all connections from k spine switches 92-0 to 92-19 are reorganized into p interlinks 99-0 to 99-187, each
interlink includes one connection from each of the spine switches 92-0 to 92-19, each interlink contains k connections in total.

https://patents.google.com/patent/US11070437B2/en?q=(high+radix)&oq=high+radix&page=2

10/17

2/14/26, 2:04 PM

US11070437B2 - Network interconnect as a switch - Google Patents

That is, on one side of the fanout cable transpose rack 921 is k first port groups 923, each first port group has ┌p/m┐ of first MPO adapters, where ┌ ┐ is a ceiling
function, each port groups connects to a corresponding port group of a spine switch through the ┌p/m┐ first MPO-MPO cables. On the other side of the fanout cable
transpose rack 921 is p second port groups 924, each second port group has ┌k/m┐ of second MPO adapters, where ┌ ┐ is an ceiling function, each port group
connects to 5 second MPO-MPO cables to form an interlink to the ICAS pod.
As pointed out earlier in this detailed description, the state-of-the-art data centers and switch silicon are designed with 4 interfaces (TX, RX) at 10 Gb/s or 25 Gb/s each
per port in mind. Switching devices are interconnected at the connection level in ICAS-based data center. In such a configuration, a QSFP cable coming out from a QSFP
transceiver is separated into 4 interfaces, and 4 interfaces from different QSFP transceivers are combined in a QSFP cable for connecting to another QSFP transceiver.
Also, a spine plane may interconnect a large and varying number of ICAS pods (e.g., in the hundreds) because of the scalability of an ICAS-based data center network.
Such a cabling scheme is more suitable to be organized in a fanout cable transpose rack (e.g., fanout cable transpose rack 921), which may be one or multiple racks and
be integrated into the spine planes. Specifically, the spine switches and the TOR switches may each connect to the fanout cable transpose rack with QSFP straight
cables. Such an arrangement simplifies the cabling in a data center. FIG. 9b illustrates such an arrangement for data center network 900 of FIG. 9 a.
In the embodiment shown in FIG. 9 b, the first and the second optical fiber adapters are MPO adapters, the first and the second cables are MPO-MPO cables, the first and
the second fanout cables are MPO-LC fanout cables, the mounting panel is LC optical fiber adapter mounting panel. One skilled in the art would understand that different
types of optical fiber adapters/cable/optical fiber adapter mounting panel may also be used, such as FC, SC, LC, and MU.
Details of an ICAS pod of FIG. 9a are shown in FIG. 9 c. In FIG. 9 c, the network side interface (as opposed to the server-side interface) of an ICAS pod is divided into intrapod links (i.e. intralinks) and inter-pod links (i.e., interlinks) and the two types of links are made independent from each other. The intra-pod region consists of intralinks
between the 20 TOR switches 93-0 to 93-10 19 and ICAS module 931, interconnected by 10G connections in a full mesh topology. Each ICAS module may provide 20 10G
uplinks 932 to connect to one or more external networks. The inter-pod region consists of interlinks. ICAS pod may comprise 20 TOR switches 93-0 to 93-19 each
connects one of 20 spine planes by interlinks, respectively; each interline comprising 20 connections each connects one of 20 spine switches in a spine plane,
respectively. For example, interlink k of each of 188 TOR switches across the 188 ICAS pods is connected to spine plane k; interlink p of each of the 20 spine planes is
connected to server pod p. Each TOR switch provides 60×10G interfaces in 15×QSFP ports as a downlink for connecting to servers.
The data traffic through the ICAS module is primarily limited to intra-pod. The TOR switches now perform routing for the intra-pod traffic as well as inter-pod traffic and
are more complex. The independent link types achieve massive scalability in data center network implementations. (Additional independent link provided from higher
radix switching ASIC may be created to achieve a larger scale of connectivity objectives).
As shown in FIG. 9 c, FIG. 9b and FIG. 9 a, each TOR switch allocates 20×10G-interfaces (5×QSFPs in 10G mode) to connect to its associated ICAS module (e.g., ICAS
module 931) to support intra-pod traffic, and 5 QSFPs in 10G mode (20 10G-interfaces) to connect to the fiber transpose rack to support inter-pod traffic. As shown in
FIG. 9 c, each ICAS pod includes 20×5 QSFP transceivers for intra-pod traffic, connected by 100 QSFP straight cables, and 20×15 QSFP (10G mode) transceivers for
server traffic, for a total 500 QSFP transceivers. The 20 TOR switches in an ICAS pod may be implemented by 20 Trident II ASICs. Although 20 TOR switches are shown in
each ICAS pod in FIG. 9 c, the ICAS module is scalable to connect up to 48 TOR switches in an ICAS pod (based on 32×QSFP Trident-II+ switch ASIC).
Together, the ICAS pods and the spine planes form a modular network topology capable of accommodating hundreds of thousands of 10G-connected servers, scaling to
multi-petabit bisection bandwidth, and covering a data center with congestion improved and non-oversubscribed rack-to-rack performance.
According to one embodiment of the present invention, a spine switch can be implemented using a high-radix (e.g., 240×10G) single chip switching device, as shown in
FIG. 9 d. Single-chip implementation saves the cost of extra transceivers, cables, rack space, latency and power consumption than multi-unit (rack unit) chassis-based
switching device and stackable switching device implementations. The disadvantage of the single-chip spine switch approach is its network scalability, which limits the
system to 240 ICAS pods at this time. As mentioned above, the semiconductor implementation limits the scale of a high-radix switching integrated circuit.
To overcome the limitation on the port count of the silicon chip, one or more 1U to multi-U rackmount chassis each packaged with one or more ICAS modules, and a
plurality of 1U rackmount chassis each packaged with one or more switching devices, can be stacked up in one or more racks, interconnected, to form a higher-radix (i.e.
high network port count) stackable spine switching device (e.g., ICAS-based stackable switching device). Each ICAS module is connected to the plurality of switching
devices, such that the ICAS module interconnects at least some interfaces of at least some port groups of different switching devices to form a full mesh non-blocking
interconnection. The interfaces of the rest of the at least some port groups for interconnecting different switching devices are configured as an uplink. When the ICASmodule-based 1U to multi-U rackmount chassis are optically implemented (based on optical fiber and 3D MEMS), MPO-MPO cables may be used to connect the ICASbased interconnection devices and the switching devices. When the ICAS-module-based 1U to multi-U rackmount chassis are electrically implemented as circuits (based
on PCB+chip), DAC direct connection cables or AOC active optical cables may be used to connect the ICAS-based interconnection devices and the switching devices.
Details of an ICAS-based stackable switching device 950 are shown in FIG. 9 e. FIG. 9e shows ICAS modules 95-0 to 95-3 each connected in a full mesh topology to
switches 96-0 to 96-3. In one embodiment, 4 Trident-II ASIC-based switches 96-0 to 96-3, each having a switching bandwidth of 24 QSFPs in 10G mode provided in 1:1
subscription ratio, and an ICAS box 953 integrating 4 ICAS modules 95-0 to 95-3 in one 1U chassis and each ICAS module containing 3 duplicate copies of ICAS1X5 submodules and each sub-module providing 4×10G of uplink 951 may be used to builds a stackable spine switch, as shown in FIG. 9 e. The 4 switches 96-0 to 96-3 provide
ports 952 of 1.92 Tbps of bandwidth to connect to servers. The ICAS-based stackable switching device 950 provides total uplink bandwidth of 480 Gb/s (4×3×40 Gb/s)
to connect to external network, facilitates non-blocking 1:1 subscription ratio and provides full mesh non-blocking interconnect with a total of 1.92 Tbps of switching
bandwidth.
ICAS-based stackable switching device has the benefits of improved network congestion, saving the costs, power consumption and space savings than the switching
devices implemented in the state of the art data center. As shown in the “ICAS+Stackable Chassis” column of Table 5, data center with ICAS and ICAS-based stackable
switching device performs remarkably on data center network with total switching ASIC saving by 53.5%, total power consumption saving by 26.0%, total space saving by
25.6% and much improved network congestion. However total QSFP transceiver usage is increased by 2.3%.
The above stackable switching device is for illustrative purpose. A person experienced in the art can easily expand the scalability of the stackable switching device and
should not be limited as in the illustration.
The stackable switching device addresses the insufficiency in the number of ports of network switching chip, thus making possible a flexible network configuration.
However, a considerable number of connecting cables and conversion modules have to be used to interconnect the ICAS-based interconnection devices and the
switching devices. To further reduce the use of cables and conversion modules, ICAS modules and switch chips can be electronically interconnected using a PCB and
connectors, which is exactly how the multi-unit switching device is structured. Specifically, the ICAS module of the ICAS-based multi-unit switching device is electrically
implemented as circuits, and the port groups of the ICAS module are soldered or crimped onto a PCB using connectors that support high-speed differential signals and
impedance matching. The interconnection between the internal port groups is realized using a copper differential pair on the PCB. Since signal losses vary significantly
between different grades of high-speed differential connectors and between copper differential pairs on different grades of PCBs, an active chip can be added at the back
end of the connector to restore and enhance the signal to increase the signal transmission distance on the PCB. The ICAS module of the ICAS-based multi-unit switching
device may be implemented on a PCB called a fabric card, or on a PCB called a backplane. The copper differential pair on the PCB interconnects the high-speed

https://patents.google.com/patent/US11070437B2/en?q=(high+radix)&oq=high+radix&page=2

11/17

2/14/26, 2:04 PM

US11070437B2 - Network interconnect as a switch - Google Patents

differential connector on the PCB to form a full mesh connectivity in the ICAS architecture. The switch chips and related circuits are soldered onto a PCB called a line
card, which is equipped with a high-speed differential connector docking to the adapter on the fabric card. A multi-U chassis of the ICAS-based multi-unit switching
device includes a plurality of ICAS fabric cards, a plurality of line cards, and one or two MCU- or CPU-based control cards, one or more power modules and cooling fan
modules. “Rack unit” (“RU” or “U” for short) measures the height of a data center chassis, equal to 1.75 inches. A complete rack is 48U (48 rack units) in height.
One embodiment of the present invention also provides a chassis-based multi-unit (rack unit) switching device. A multi-unit chassis switching device groups multiple
switch ICs onto multiple line cards. Chassis-based multi-unit switching equipment interconnects with line cards, control cards, and CPU cards via PCB-based network
cards or backplanes, which saves the cost of transceivers, fiber optic cable and rack space required for interconnection.
Details of an ICAS-based multi-unit chassis switching device 970 are shown in FIG. 9 f. FIG. 9f shows 4 ICAS-based fabric cards 97-0 to 97-3 interconnected in a full
mesh topology to switching ASIC's 98-0 to 98-3. Switching ASIC 98-0 and 98-1 are housed in line card 973, and switching ASIC's 98-2 and 98-3 are housed in line card
974. Line cards 973 and 974 are connected through high speed PCB (printed circuit board) connectors to fabric cards 97-0 to 97-3. In one embodiment, 4 Trident-II ASICbased switches 98-0 to 98-3, each having a switching bandwidth of 24 QSFPs in 10G mode provided in 1:1 subscription ratio, and 4 ICAS-based fabric cards 97-0 to 97-3
containing 3 duplicate copies of ICAS1X5 sub-modules and each sub-module providing 4×10G of uplink 971 may be used to builds a multi-unit chassis switch, as shown
in FIG. 9f Two line cards provide data ports 972 of total 1.92 Tbps of bandwidth to connect to servers. ICAS-based multi-unit chassis switching device 970 provides total
uplink bandwidth of 480 Gb/s (4×3×40 Gb/s) to connect to external network, facilitates full mesh non-blocking 1:1 subscription ratio interconnect with a total of 1.92
Tbps of switching bandwidth.
Multi-unit chassis-based switching device with fabric cards that are ICAS-based full mesh topology has the benefits of improved network congestion, saving the costs
and power consumption than that of ASIC-based fabric cards implementation with fat tree topology. As shown in the “ICAS +Multi-unit Chassis” column of Table 5, data
center with ICAS and ICAS-based multi-unit chassis-based switching device performs remarkably on data center network with total QSFP transceiver saving by 12.6%,
total switching ASIC saving by 53.5%, total power consumption saving by 32.7%, total space saving by 29.95% and much improved network congestion.
The above multi-unit chassis switching device is for illustrative purpose. A person experienced in the art can easily expand the scalability of the multi-unit chassis
switching device and should not be limited as in the illustration.
The multi-unit chassis-based switching device has the disadvantage of a much longer development time and a higher cost to manufacture due to its system complexity,
and is also limited overall by the form factor of the multi-unit chassis. The multi-unit chassis-based switching device, though provides a much larger port count than the
single-chip switching device. Although the stackable switching device requires additional transceivers and cables than that of the multi-unit chassis-based approach, the
stackable switching device approach has the advantage of greater manageability in the internal network interconnection, virtually unlimited scalability, and requires
significantly less time for assembling a much larger switching device.
The material required for (i) the data center networks of FIG. 2a , using state of the art multi-unit switching device (“Fat tree+Multi-unit Chassis”), (ii) an implementation of
data center network 900 of FIG. 9a , using ICAS-based multi-unit switching device “ICAS+Multi-unit Chassis”, and (iii) an implementation of data center network 900 of
FIG. 9a , using ICAS-based stackable switching device “ICAS+Stackable Chassis” are summarized and compared in Table 5.
TABLE 5
Fat tree +

ICAS +

ICAS +

Multi-unit

Multi-unit

Stackable

Chassis

Chassis

Chassis

Intralink (within Pod)

N/A

5

5

Interlink (Across Pod)

4

5

5

Downlink (to Server)

12

15

15

Total

16

25

25

D:U ratio

3

3

3

D:I ratio

N/A

3

3

Number of 10G Interface

96

184.3

184.3

QSFP XCVR Module (Watt)

4

4

4

TOR Switch (Watt)

150

200

200

Multi-unit Chassis (Watt)

1660

0

0

Spine-side Interlink QSFP XCVR

18432

18800

38000

TOR-side Interlink QSFP XCVR

18432

18800

18800

Fabric/TOR-side Intralink

36864

18800

18800

Server-side QSFP XCVR

55296

56400

56400

Total QSFP XCVR

129024

112800

132000

(12.6%)

(−2.3%)

(for comparison)

QSFP XCVR

ASIC in Spine Switch

2304

1600

1600

ASIC in Fabric Switch

4608

0

0

ASIC in TOR Switch

4608

3760

3760

Total Switching ASIC

11520

5360

5360

(53.5%)

(53.5%)

Spine Switch (KW)

392.448

327.2

472.0

Fabric Switch (KW)

784.896

0

0

TOR Switch (KW)

986.112

1128.0

1128.0

Total Power Consumption (KW)

2163.456

1455.2

1600

(32.7%)

(26.0%)

https://patents.google.com/patent/US11070437B2/en?q=(high+radix)&oq=high+radix&page=2

12/17

2/14/26, 2:04 PM

US11070437B2 - Network interconnect as a switch - Google Patents

96 x QSFP Spine Switch (8U)

1536

0

0

96 x QSFP Fabric Switch (8U)

3072

0

0

48 x QSFP Spine Switch (4U)

0

1600

1600

TOR Switch (1U)

4608

3760

3760

ICAS1X5TRIPLE (1U)

0

0

400

ICAS5X21 (2U)

0

376

376

Transpose Rack (36U)

0

720

720

ICAS2X9 (1U)

0

0

0

ICAS8X33 (4U)

0

0

0

ICAS10X41 (6U)

0

0

0

ICAS16X65 (16U)

0

0

0

Total Rack Unit (U)

9216

6456

6856

(29.95%)

(25.6%)

Pod Interlink Bandwidth (Tbps)

7.7

4.0

4.0

Pod Intralink Bandwidth (Tbps)

7.7

4.0

4.0

Total Data Link Bandwidth (Pbps)

2.2

2.2

2.2

Per Plane Uplink Bandwidth (Tbps)

7.7/plane

0

0

Total Spine Uplink Bandwidth (Tbps)

0

150.4

601.6

Total ICAS Uplink Bandwidth (Tbps)

0

37.6

37.6

Spine-side Interlink QSFP Cable

18432

18800

18800

QSFP Fanout Cable (Transpose Rack)

0

37600

37600

QSFP Fanout Cable (ICAS5X21)

0

19740

19740

TOR-side Interlink QSFP Cable

0

18800

18800

TOR-side Intralink QSFP Cable

18432

18800

18800

Spine Switch QSFP Cable

0

0

19200

QSFP Fanout Cable

0

0

19200

Total QSFP Cable

36864

56400

75600

Total QSFP Fanout Cable

0

57340

76540

(ICAS1X5TRIPLE)

As shown in Table 5, the ICAS-based systems require significantly less power dissipation, ASICs and space, resulting in reduced material costs and energy.
The above detailed description is provided to illustrate specific embodiments of the present invention and is not intended to be limiting. Numerous modifications and
variations within the scope of the present invention are possible. The present invention is set forth in the accompanying claims.

Patent Citations (21)
Publication number

Priority date

Publication date

Assignee

Title

US20010053149A1 *

2000-05-05

2001-12-20

Li Mo

Method and system for quality of service (QoS) support in a packetswitched network

US20140098823A1 *

2012-10-10

2014-04-10

Cisco Technology, Inc.

Ensuring Any-To-Any Reachability with Opportunistic Layer 3
Forwarding in Massive Scale Data Center Environments

US20170063631A1 *

2015-08-28

2017-03-02

Tigera, Inc.

Data center networks

US20170310594A1 *

2016-04-25

2017-10-26

Linkedin Corporation

Expedited fabric paths in switch fabrics

US20180167307A1 *

2016-12-08

2018-06-14

Plexxi Inc.

Framework for universally specified affinity topologies with partial path
invalidation and generalized network flows

US20180287818A1 *

2017-03-29

2018-10-04

Fungible, Inc.

Non-blocking any-to-any data center network having multiplexed packet
spraying within access node groups

US20190109783A1 *

2017-10-10

2019-04-11

Vmware, Inc.

Methods and apparatus to perform network fabric migration in
virtualized server systems

US6567429B1 *

1998-06-02

2003-05-20

Dynamics Research Corporation

Wide area multi-service broadband network

CN100474822C *

2002-12-31

2009-04-01

浪潮电子信息产业股份有限公司

Multi branch fat tree network topological structure

US20100049942A1

2008-08-20

2010-02-25

John Kim

Dragonfly processor interconnect network

WO2011050185A2 *

2009-10-21

2011-04-28

Adc Telecommunications, Inc.

Fiber distribution hub and cable for use therewith

US8172468B2 *

2010-05-06

2012-05-08

Corning Incorporated

Radio frequency identification (RFID) in communication connections,
including fiber optic components

US9363208B1

2011-03-29

2016-06-07

Amazon Technologies, Inc.

Logical switches

Family To Family Citations

https://patents.google.com/patent/US11070437B2/en?q=(high+radix)&oq=high+radix&page=2

13/17

2/14/26, 2:04 PM

US11070437B2 - Network interconnect as a switch - Google Patents

US9197541B2 *

2012-11-15

2015-11-24

Compass Electro Optical Systems
Ltd.

Router with passive interconnect and distributed switchless switching

US20150162982A1 *

2013-12-06

2015-06-11

Corning Cable Systems Llc

Fiber optic assemblies for tapping live optical fibers in fiber optic
networks employing parallel optics

CN104883224B *

2014-02-27

2017-05-10

北京邮电大学

Method and node device for constructing data center switching
network

WO2016164769A1 *

2015-04-09

2016-10-13

Fiber Mountain, Inc.

Data center endpoint network device with built in switch

US9893950B2

2016-01-27

2018-02-13

International Business Machines
Corporation

Switch-connected HyperX network

US10797797B2 *

2017-03-31

2020-10-06

Nexans

Fiber optic extender

CN107800483B *

2017-11-01

2020-01-03

上海交通大学

Multi-core multi-wave short-distance interconnection network based on
array waveguide grating

US10477288B2

2018-02-05

2019-11-12

David I-Keong Wong

Data center interconnect as a switch

* Cited by examiner, † Cited by third party

Cited By (48)
Publication number

Priority date

Publication date

Assignee

Title

US20240171887A1 *

2022-11-17

2024-05-23

Panduit Corp.

Fabric modules for server to switch connections

US10430894B2

2013-03-21

2019-10-01

Khoros, Llc

Gamification for online social communities

US10902462B2

2017-04-28

2021-01-26

Khoros, Llc

System and method of providing a platform for managing data content campaign
on social networks

US10999278B2

2018-10-11

2021-05-04

Spredfast, Inc.

Proxied multi-factor authentication using credential and authentication
management in scalable data networks

US11050704B2

2017-10-12

2021-06-29

Spredfast, Inc.

Computerized tools to enhance speed and propagation of content in electronic
messages among a system of networked computing devices

US11570128B2

2017-10-12

2023-01-31

Spredfast, Inc.

Optimizing effectiveness of content in electronic messages among a system of
networked computing device

US11470161B2

2018-10-11

2022-10-11

Spredfast, Inc.

Native activity tracking using credential and authentication management in
scalable data networks

US10346449B2

2017-10-12

2019-07-09

Spredfast, Inc.

Predicting performance of content and electronic messages among a system of
networked computing devices

US10785222B2

2018-10-11

2020-09-22

Spredfast, Inc.

Credential and authentication management in scalable data networks

US10601937B2

2017-11-22

2020-03-24

Spredfast, Inc.

Responsive action prediction based on electronic messages among a system of
networked computing devices

US11061900B2

2018-01-22

2021-07-13

Spredfast, Inc.

Temporal optimization of data operations using distributed search and server
management

US10594773B2

2018-01-22

2020-03-17

Spredfast, Inc.

Temporal optimization of data operations using distributed search and server
management

JP7082282B2 *

2018-06-06

2022-06-08

富士通株式会社

Packet analysis program, packet analysis method and packet analysis device

US10855657B2

2018-10-11

2020-12-01

Spredfast, Inc.

Multiplexed data exchange portal interface in scalable data networks

US10931540B2

2019-05-15

2021-02-23

Khoros, Llc

Continuous data sensing of functional states of networked computing devices to
determine efficiency metrics for servicing electronic messages asynchronously

US10924435B2 *

2019-05-15

2021-02-16

Dell Products, L.P.

System and method for port reduction using multiple chassis link aggregation
group for stacked devices

CN113014611B *

2019-12-19

2024-05-14

华为技术有限公司

Load balancing method and related equipment

US11561916B2 *

2020-01-13

2023-01-24

Hewlett Packard
Enterprise Development
Lp

Processing task deployment in adapter devices and accelerators

US11275705B2 *

2020-01-28

2022-03-15

Dell Products L.P.

Rack switch coupling system

Family To Family Citations

https://patents.google.com/patent/US11070437B2/en?q=(high+radix)&oq=high+radix&page=2

14/17

2/14/26, 2:04 PM

US11070437B2 - Network interconnect as a switch - Google Patents

CN111404829B *

2020-04-17

2024-02-27

杭州迪普科技股份有限公
司

Port aggregation method, device, equipment and storage medium

US11381891B2 *

2020-04-30

2022-07-05

Hewlett Packard
Enterprise Development
Lp

Virtual fiber adapter for wavelength-as-a-service communications

CN113300870B *

2020-08-10

2024-09-10

阿里巴巴集团控股有限公
司

Access layer switch, upper layer switch and data center network system

US11438289B2

2020-09-18

2022-09-06

Khoros, Llc

Gesture-based community moderation

US11128589B1

2020-09-18

2021-09-21

Khoros, Llc

Gesture-based community moderation

US12120078B2

2020-09-18

2024-10-15

Khoros, Llc

Automated disposition of a community of electronic messages under moderation
using a gesture-based computerized tool

US11924375B2

2021-10-27

2024-03-05

Khoros, Llc

Automated response engine and flow configured to exchange responsive
communication data via an omnichannel electronic communication channel
independent of data source

US12197875B2

2021-07-31

2025-01-14

Khoros, Llc

Automated predictive response computing platform implementing adaptive data
flow sets to exchange data via an omnichannel electronic communication
channel independent of data source

US12158903B2

2020-11-06

2024-12-03

Khoros, Llc

Automated response engine to implement internal communication interaction
data via a secured omnichannel electronic data channel and external
communication interaction data

US11438282B2

2020-11-06

2022-09-06

Khoros, Llc

Synchronicity of electronic messages via a transferred secure messaging
channel among a system of various networked computing devices

US11627100B1

2021-10-27

2023-04-11

Khoros, Llc

Automated response engine implementing a universal data space based on
communication interactions via an omnichannel electronic data channel

US11714629B2

2020-11-19

2023-08-01

Khoros, Llc

Software dependency management

TWI888713B *

2021-04-01

2025-07-01

智邦科技股份有限公司

Scaling network fabric size system and adapter

US11716224B2 *

2021-05-11

2023-08-01

Airmar Technology
Corporation

Networking module for instrumentation and control devices

CN113810286B *

2021-09-07

2023-05-02

曙光信息产业(北京)有限
公司

Computer network system and routing method

US20220012206A1 *

2021-09-24

2022-01-13

Intel Corporation

Versatile adaptor for high communication link packing density

CN113840453A *

2021-09-28

2021-12-24

杭州云合智网技术有限公
司

Mainboard structure of switch

US12332934B2

2023-04-11

2025-06-17

Khoros, Llc

Automated response engine implementing a universal data space based on
communication interactions via an omnichannel electronic data channel

US11677654B1 *

2021-12-29

2023-06-13

Ziqiang He

Network TAP capable of tapping a 10Gbps network link

US12261844B2

2023-03-06

2025-03-25

Spredfast, Inc.

Multiplexed data exchange portal interface in scalable data networks

US20240314089A1 *

2023-03-14

2024-09-19

Samsung Electronics
Co., Ltd.

Multi-node computing system

US20240340242A1 *

2023-04-10

2024-10-10

Mellanox Technologies,
Ltd.

Systems, methods, and devices for load balancing in multiplane networks

CN116566488B *

2023-04-19

2025-11-21

长飞光纤光缆股份有限公
司

Multi-core optical fiber wiring system of data center

US20240353628A1 *

2023-04-20

2024-10-24

Panduit Corp.

Optical Interconnection Modules for High Radix Spine-Leaf Network Scale-Out

US20240405878A1 *

2023-06-02

2024-12-05

Nokia Solutions And
Networks Oy

Pluggable optical transceiver module

US12425355B1 *

2023-11-06

2025-09-23

Amazon Technologies,
Inc.

Routing network traffic in Clos fabrics

US20250240205A1 *

2023-12-20

2025-07-24

Mellanox Technologies,
Ltd.

System for allocation of network resources for executing deep learning
recommendation model (dlrm) tasks

WO2025158632A1 *

2024-01-25

2025-07-31

Ｎｔｔ株式会社

Optical circuit switch and communication network system

CN117807017B *

2024-03-01

2024-05-14

中国人民解放军国防科技
大学

High-performance computer with cube supernode multi-plane interconnection
and communication method thereof

https://patents.google.com/patent/US11070437B2/en?q=(high+radix)&oq=high+radix&page=2

15/17

2/14/26, 2:04 PM

US11070437B2 - Network interconnect as a switch - Google Patents

* Cited by examiner, † Cited by third party, ‡ Family to family citation

Similar Documents
Publication

Publication Date

Title

US11671330B2

2023-06-06

Network interconnect as a switch

US10477288B2

2019-11-12

Data center interconnect as a switch

US12143296B2

2024-11-12

Data center network with packet spraying

US11632606B2

2023-04-18

Data center network having optical permutors

US11368768B2

2022-06-21

Optical network system

Kachris et al.

2013

Optical interconnection networks in data centers: recent trends and future challenges

US10637685B2

2020-04-28

Non-blocking any-to-any data center network having multiplexed packet spraying within access node groups

US9960878B2

2018-05-01

Scalable ultra dense hypergraph network for data centers

US8358932B2

2013-01-22

All-optical data center network

Sankaran et al.

2017

A survey of hybrid optical data center network architectures

US8284771B1

2012-10-09

Run-time scalable switch fabric

US8942232B1

2015-01-27

Multi-stage switching topology

US11800266B2

2023-10-24

Hybrid optoelectrical switches

Farrington

2012

Optics in data center network architecture

US20150215208A1

2015-07-30

Packet switch using physical layer fiber pathways

Gumaste et al.

2016

DOSE: Double optics single electronics data-center using a switchless optical frontplane and backplane

Chen

2015

Literature Survey on Optical Data Centre Networks

Priority And Related Applications
Parent Applications (1)
Application

Priority date

Filing date

Relation

Title

US16/257,653

2018-02-05

2019-01-25

Division

Network interconnect as a switch

Application

Priority date

Filing date

Relation

Title

US17/349,628

2018-02-05

2021-06-16

Division

Network interconnect as a switch

Child Applications (1)

Priority Applications (3)
Application

Priority date

Filing date

Title

US16/921,264

2018-02-05

2020-07-06

Network interconnect as a switch

US17/349,628

2018-02-05

2021-06-16

Network interconnect as a switch

US17/966,735

2018-02-05

2022-10-14

Network interconnect as a switch

Applications Claiming Priority (3)
Application

Filing date

Title

US15/888,516

2018-02-05

Data center interconnect as a switch

US16/257,653

2019-01-25

Network interconnect as a switch

https://patents.google.com/patent/US11070437B2/en?q=(high+radix)&oq=high+radix&page=2

16/17

2/14/26, 2:04 PM

US11070437B2 - Network interconnect as a switch - Google Patents

US16/921,264

2020-07-06

Network interconnect as a switch

Legal Events
Date

Code

Title

Description

2020-07-06

FEPP

Fee payment procedure

Free format text: ENTITY STATUS SET TO UNDISCOUNTED (ORIGINAL
EVENT CODE: BIG.); ENTITY STATUS OF PATENT OWNER: SMALL
ENTITY

2020-07-15

FEPP

Fee payment procedure

Free format text: ENTITY STATUS SET TO SMALL (ORIGINAL EVENT
CODE: SMAL); ENTITY STATUS OF PATENT OWNER: SMALL ENTITY

2021-02-19

STPP

Information on status: patent application and granting procedure in general

Free format text: NOTICE OF ALLOWANCE MAILED -- APPLICATION
RECEIVED IN OFFICE OF PUBLICATIONS

2021-05-21

STPP

Information on status: patent application and granting procedure in general

Free format text: PUBLICATIONS -- ISSUE FEE PAYMENT RECEIVED

2021-05-24

STPP

Information on status: patent application and granting procedure in general

Free format text: PUBLICATIONS -- ISSUE FEE PAYMENT VERIFIED

2021-06-30

STCF

Information on status: patent grant

Free format text: PATENTED CASE

2024-08-05

MAFP

Maintenance fee payment

Free format text: PAYMENT OF MAINTENANCE FEE, 4TH YR, SMALL
ENTITY (ORIGINAL EVENT CODE: M2551); ENTITY STATUS OF PATENT
OWNER: SMALL ENTITY
Year of fee payment: 4

Concepts
machine-extracted

Download

Name

Image

fabric

claims,abstract,description

98

0.000

claims,abstract

11

0.000

blocking effect

claims,description

19

0.000

optical effect

claims,description

19

0.000

optical fiber

claims,description

15

0.000

Copper

claims,description

9

0.000

copper

claims,description

9

0.000

copper

claims,description

9

0.000

polychlorinated biphenyls

claims,description

8

0.000

cooling

claims,description

4

0.000

signal transmission

claims,description

3

0.000

ionisation spectroscopy

Sections

Count

Filter table

Query match

Show all concepts from the description section

Data provided by IFI CLAIMS Patent Services

About

Send Feedback

Public Datasets

Terms

Privacy Policy

https://patents.google.com/patent/US11070437B2/en?q=(high+radix)&oq=high+radix&page=2

Help

17/17

2/14/26, 1:52 PM

US20080307024A1 - Mixed Radix Number Generator with Chosen Statistical Artifacts - Google Patents

Patents
Mixed Radix Number Generator with Chosen Statistical Artifacts
Abstract
A method is provided for masking a process used in generating a number sequence. The method
includes generating a first sequence of numbers contained within a Galois field GF[M]. The method

US20080307024A1
United States

also includes performing a first modification to a first number in the first sequence of numbers. The
first modification includes summing the first number with a result of a modulo P operation

Download PDF

Find Prior Art

Similar

performed on a second number of the first sequence that proceeds the first number. M is relatively
prime with respect to P. The method further includes performing a second modification to the first
random number. The second modification is comprised of a modulo P operation. This second
modification is performed subsequent to the first modification. The method includes repeating the

Inventor: Alan J. Michaels, David B. Chester
Current Assignee : Harris Corp

first and second modification for a plurality of numbers comprising the first sequence of numbers to
generate a second sequence of numbers.

Worldwide applications
2007 US 2008 JP DE EP CA

Images (10)

Application US11/759,276 events

Classifications
G06F7/58 Random or pseudo-random number generators

G06F7/584 Pseudo-random number generators using finite field arithmetic, e.g. using a linear

2007-06-07

Application filed by Harris Corp

2007-06-07

Priority to US11/759,276

2008-12-11

Publication of US20080307024A1

2011-06-14

Application granted

2011-06-14

Publication of US7962540B2

Status

Active

2029-12-21

Adjusted expiration

Show all events

feedback shift register
Info: Patent citations (114), Cited by (58), Legal events, Similar

G06F7/724 Finite field arithmetic

documents, Priority and Related Applications

Hide more classifications

External links: USPTO, USPTO PatentCenter, USPTO
Assignment, Espacenet, Global Dossier, Discuss

Landscapes

Physics & Mathematics
General Physics & Mathematics
Show more

Claims (27)

Hide Dependent

1. A method for masking a process used in generating a number sequence, comprising:
generating a first sequence of numbers contained within a Galois field GF[M];
performing a first modification to a first number in said first sequence of numbers comprising summing said first number with a result of a modulo P operation performed on
a second number of said first sequence that proceeds said first number, where M is relatively prime with respect to P;
subsequent to said first modification, performing a second modification to said first random number comprising a modulo P operation;
repeating said first and second modification for a plurality of numbers comprising said first sequence of numbers to generate a second sequence of numbers.
2. The method according to claim 1, further comprising modifying a digital data stream with said second sequence of numbers.
3. The method according to claim 1, wherein said generating step further comprises generating a pseudo-random sequence of numbers containing chosen statistical
artifacts relating to said generating step, and wherein said statistical artifact are chosen to create a uniformly distributed sequence of random numbers on GF[P] by said
first and second modification step.
4. The method according to claim 1, wherein said generating step further comprises exhaustively producing said first sequence of numbers by using a mapping which is
periodically repeated.
5. The method according to claim 4, further comprising selecting said mapping to include a combination of an additive mapping and a multiplicative mapping.

https://patents.google.com/patent/US20080307024A1/en

1/15

2/14/26, 1:52 PM

US20080307024A1 - Mixed Radix Number Generator with Chosen Statistical Artifacts - Google Patents

6. The method according to claim 5, further comprising selecting said additive mapping and said multiplicative mapping in combination to include repeated
computations of an irreducible polynomial over a Galois field GF[M].
7. The method according to claim 1, further comprising selecting a value of M that is larger than P.
8. The method according to claim 1, further comprising selecting a value of M that is mutually prime with P and all of a plurality of prime factors of P including p1, p2, p3, .
. . pk .
9. The method according to claim 8, further comprising performing a third modification step on said second sequence of numbers, said third modification step
comprising generating a plurality of output number sequences from said second sequence of numbers.
10. The method according to claim 9, wherein said third modification step comprises a modulo p operation performed upon each number in said second sequence of
numbers to generate said plurality of output number sequences, where p includes a plurality of values selected from the group comprising p1, p2, p3, . . . pk.
11. The method according to claim 1, wherein said second number is immediately preceding said first number.
12. The method according to claim 1, wherein said second number precedes said first number by H positions, where N is greater than 1.
13. The method according to claim 1 wherein said second sequence of numbers has statistical artifacts that are evenly distributed over a plurality of equivalence classes
of said Galois field GF[P], and said plurality of equivalence classes include an equivalence class for each integer 0, 1, . . . , P−1.
14. A mixed radix number generator, comprising:
a number generator configured for generating a first sequence of numbers contained within a Galois field GF[M];
a mixed radix accumulator configured for (1) performing a first modification to a first number in said first sequence of numbers comprising summing said first number with a
result of a modulo P operation performed on a second number of said first sequence that proceeds said first number, where M is relatively prime with respect to P, (2)
subsequent to said first modification, performing a second modification to said first random number comprising a modulo P operation; and (3) repeating said first and
second modification for a plurality of numbers comprising said first sequence of numbers to generate a second sequence of numbers.
15. The system according to claim 14, further comprising means configured for using said second sequence of numbers to modify a digital data stream.
16. The system according to claim 14, wherein said number generator comprises a pseudo-random number generator for generating a pseudo-random sequence of
numbers containing statistical artifacts relating to said generation of said first sequence of numbers, and wherein said statistical artifact is eliminated by said mixed
radix accumulator.
17. The system according to claim 14, wherein said number generator is configured for exhaustively producing said first sequence of numbers by using a mapping which
is periodically repeated.
18. The system according to claim 17, wherein said mapping includes a combination of an additive mapping and a multiplicative mapping.
19. The system according to claim 18, wherein said additive mapping and said multiplicative mapping in combination include repeated computations of an irreducible
polynomial over a Galois field GF[M].
20. The system according to claim 14, wherein M is larger than P.
21. The system according to claim 14, further wherein a value of U is mutually prime with respect to a value of P and all of a plurality of prime factors of P including p1,
p 2, p 3, . . . pk .
22. The system according to claim 21, further comprising a plurality of arithmetic operator units each configured for performing a third modification on said second
sequence of numbers said plurality of arithmetic operator units generating a plurality of output number sequences from said second sequence of numbers.
23. The system according to claim 22, wherein said third modification comprises a modulo p operation performed upon each number in said second sequence of
numbers to generate said plurality of output number sequences, where p includes a plurality of values selected from the group comprising p1, p2, p3, . . . , pk.
24. The system according to claim 14, wherein said second number precedes said first number by one position.
25. The system according to claim 14, wherein said second number precedes said first number by H positions, where N is greater than 1.
26. The system according to claim 14, wherein said first sequence of numbers is limited to a finite number of elements M defined by a Galois field GF[M], said second
sequence of numbers has statistical artifacts that, are evenly distributed over a plurality of equivalence classes of said Galois field GF[P], and wherein said plurality of
equivalence classes include an equivalence class for each integer 0, 1, . . . , P−1.
27. The system according to claim 14, wherein said first sequence is operated upon by a filter structure configured to perform calculations inside a Galois field of equal
size to the second sequence.

Description

BACKGROUND OF THE INVENTION
[0001] 1. Statement of the Technical Field
[0002] The inventive arrangements relate to communications systems employing mixed radix conversion. More particularly, the inventive arrangements relate to a
method and system for performing a mixed radix ring generation and conversion to produce a random number sequence with chosen statistical characteristics
over all equivalence classes of a chosen Galois field GF[P].
[0003] 2. Description of the Related Art
[0004] Communication systems can include ring generators in numerous applications. A ring generator is a simple structure over a finite field that exhaustively
produces possible outputs through repeated mapping. The mapping is some combination of an additive and a multiplicative mapping; with irreducible
polynomials being ideal. For example, a ring generator includes repeated computations of an irreducible polynomial f(x) 3x3+3x2+x on a finite Galois field GF[11]

https://patents.google.com/patent/US20080307024A1/en

2/15

2/14/26, 1:52 PM

US20080307024A1 - Mixed Radix Number Generator with Chosen Statistical Artifacts - Google Patents

containing eleven (11) elements. A finite or Galois field GF[P] is a field that contains only a finite number of elements {0, 1, 2 . . . P−1}. The finite or Galois field
GF[P] has a finite field size defined by the Galois characteristic P, which is often chosen to be a prime number based on number theoretical consequences. The
computations ere typically implemented in digital hardware as lookup table operations, feedback loops, or multiplier structures.
[0005] Despite the advantages of such a ring generator, it suffers from certain drawbacks. For example, if the ring generator's Galois characteristic P is chosen to be a
prime number (not equal to two), then computation is typically inefficient in a digital (binary) domain. Also, lookup table operations performed in the finite or
Galois field GF[P] are memory intensive if the Galois characteristic P is large. Moreover, the ring generator's output values are highly deterministic. As such,
knowledge of a mapping and current finite field conditions gives complete knowledge of an output sequence.
[0006] One method to mask the output sequence of a ring generator from unintended re-construction is to combine two or more ring generators via algorithms that
perform bijective mappings into a larger effective domain. An example of this combination is through the Chinese Remainder Theorem (CRT) when the Galois
characteristics of the individual ring generators are mutually prime. Another method is to simply truncate the ring generator output value by performing a mixedradix conversion from a domain GF[P] to a binary domain GF[2k]. Both of these masking methods partially mask the original sequence, yet they still present
statistical artifacts that may be used to re-engineer the sequence values. In cryptology, such an attempt is often called a frequency attack, whereby an individual
can obtain partial information of the pseudo-random sequence's mapping and state characteristics through statistical analysis. A common layman's example of
this process is the word puzzles that exchange one letter for another. Knowledge of the English language gives partial knowledge that E's are more prevalent
than Z's. In effect, the search is reduced from brute force to a more logical one.
[0007] In view of the forgoing, there remains a need for a mixed-radix conversion method that is computationally efficient in a digital (binary) domain and does not have
any gross statistical artifacts. There is also a need for a ring generator having an implementation that is less hardware intensive than conventional ring generator
implementations and yield a pseudo-random number sequence that has chosen statistical characteristics. There is further a need for a ring generator having
orbits that appear non-deterministic.
SUMMARY OF THE INVENTION
[0008] The invention concerns a method for masking a process used in generating a number sequence. The method includes a generating step, a first modification
step, and a second modification step. The generating step involves generating a first sequence of numbers contained within a Galois field, GF[M]. The first
modification step involves performing a first modification to a first number in the first sequence of numbers. The first modification is achieved by summing the
first number with a result of a modulo P operation performed on a second number of the first sequence that proceeds the first number. M is relatively prime with
respect to P. The second modification step involves performing a second modification to the first random number. The second modification is comprised of a
modulo P operation. The second modification is performed subsequent to the first modification. The method further includes repeating the first and second
modification for one or more numbers comprising the first sequence of numbers to generate a second sequence of numbers.
[0009] According to an aspect of the invention, the method includes modifying a digital data stream with the second sequence of numbers. The generating step also
includes generating a pseudo-random sequence of numbers containing chosen statistical artifacts relating to the generating step. The statistical artifacts are
typically chosen to create a uniformly distributed sequence of random numbers on GF[P] by the first and second modification steps. As used herein, a “uniformly
distributed sequence of random numbers” is one where each numerical value in a sequence approximates a random draw from a perfect uniform distribution,
where all elements in the protection space are equally likely to be drawn. The generating step further includes exhaustively producing the first sequence of
numbers by using a mapping which is periodically repeated. The mapping is selected to include a combination of an additive mapping and a multiplicative
mapping. The additive mapping and multiplicative mapping are selected in combination to include repeated computations of an irreducible polynomial over a
Galois field, GF[M].
[0010] According to another aspect of the invention, the method includes selecting a value of M that is larger than P. The method also includes selecting a value of M
that is mutually prime with P and all of a plurality of prime factors of P including p1, p2, p3, . . . pk. The method further includes performing a third modification
step on the second sequence of numbers. The third modification step involves generating one or more output number sequences from the second sequence of
numbers. The third modification step also involves a modulo p operation performed upon each number in the second sequence of numbers to generate the
output number sequences, p includes a plurality of values selected from the group comprising p1, p2, p3, . . . pk.
[0011] According to yet another embodiment of the invention, the second number is immediately preceding the first number. The second number precedes the first
number by n positions, n is greater than one (1). The second sequence of numbers has statistical artifacts that are evenly distributed over two (2) or more
equivalence classes of the Galois field GF[P]. The equivalence classes include an equivalence class for each integer 0, 1, . . . , P−1.
[0012] A mixed radix number generator is also provided. The mixed radix number generator includes a number generator and a mixed radix accumulator. The number
generator is configured to generate a first sequence of numbers contained within a Galois field GF[M]. The mixed radix accumulator is configured to perform a
first modification to a first number in the first sequence of numbers. The first modification is achieved by summing the first number with a result of a modulo P
operation performed on a second number of the first sequence that proceeds the first number. M is relatively prime with respect to P. The mixed radix
accumulator is also configured to perform a second modification to the first random number. The second modification is comprised of a modulo P operation.
The second modification is performed subsequent to the first modification. The mixed radix accumulator is further configured to repeat the first and second
modifications for a plurality of numbers comprising the first sequence of numbers to generate a second sequence of numbers.
[0013] According to an aspect of the invention, the mixed radix number generator includes a means configured to use the second sequence of numbers to modify a
digital data stream. The number generator is comprised of a pseudo-random number generator. The pseudo-random number generator generates a pseudorandom sequence of numbers containing statistical artifacts relating to the generation of the first sequence of numbers. The statistical artifact is eliminated by
the mixed radix accumulator. The number generator is also configured to exhaustively produce the first sequence of numbers by using a mapping which is
periodically repeated. The mapping includes a combination of an additive mapping and a multiplicative mapping. The additive mapping and multiplicative
mapping in combination include repealed computations of an irreducible polynomial over a Galois field GF[M]. M is larger than P. The value of M is mutually
prime with respect to a value of P and all of a plurality of prime factors of P including p1, p2, p3, . . . pk.
[0014] According to another aspect of the invention, the mixed radix number generator is comprised of one or more arithmetic operator units. Each of the arithmetic
operator units is configured to perform a third modification on the second sequence of numbers. The arithmetic operator units are configured to generate one or
more output number sequences from the second sequence of numbers. The third modification includes a modulo p operation performed upon each number in
the second sequence of numbers to generate the output number sequences, p includes a plurality of values selected from the group comprising p1, p2, p3, . . . pk.
[0015] The second number precedes the first number by n positions, n is greater than one (1). The first sequence of numbers is limited to a finite number of elements M
defined by a Galois field GF[M]. According to another aspect of the invention, the second number precedes the first number by one (1) position. The second
sequence of numbers has statistical artifacts that are evenly distributed over two (2) or more equivalence classes of the Galois field GF[P]. The equivalence
classes include an equivalence class for each integer 0, 1, . . . , P−1.
[0016] According to yet another aspect of the invention, the first number sequence is operated by a filter structure whose calculations are performed inside a Galois
field of equal size to the second number sequence. The result of this filter is presented for combination with a subsequent value in the first number sequence. As
used herein, a “filter” means a mathematical structure having an output that is a time weighted sum of a previous succession of inputs.
BRIEF DESCRIPTION OF THE DRAWINGS
[0017] Embodiments will be described with reference to the following drawings figures, in which like numerals represent like items throughout the figures, and in which:
[0018] FIG. 1 is a conceptual diagram of a conventional mixed radix conversion algorithm that is useful for understanding the invention.
[0019] FIG. 2 is a conceptual diagram of a mixed radix ring generator for spreading statistical artifacts evenly over all equivalence classes of a Galois field GF[P].

https://patents.google.com/patent/US20080307024A1/en

3/15

2/14/26, 1:52 PM

US20080307024A1 - Mixed Radix Number Generator with Chosen Statistical Artifacts - Google Patents

[0020] FIG. 3A is a graph showing statistical simulations of a conventional mixed radix conversion algorithm that is useful for understanding the invention.
[0021] FIG. 38 is a graph showing statistical simulations of a mixed radix number generator method that is useful for understanding the invention.
[0022] FIG. 4 is a flow diagram of a mixed radix number generator method for spreading statistical artifacts evenly over all equivalence classes of a Galois field GF[P].
[0023] FIG. 5 is a flow diagram of a conventional method for altering a data stream that is useful for understanding the invention.
[0024] FIG. 6 is a flow diagram of a method for increasing the security of a communications system that is useful for understanding the invention.
[0025] FIG. 7 is a block diagram of a mixed radix number generator connected to a mechanism for combining a generator output with a data steam that is useful for
understanding the invention.
[0026] FIG. 8 is a block diagram of a mixed radix number generator that is useful for understanding the invention.
[0027] FIG. 9 is a block diagram of a mixed radix number generator that is useful for understanding the invention.
DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENTS
[0028] Referring now to FIG. 1, there is provided a conceptual diagram of a conventional mixed radix conversion algorithm that is useful for understanding the invention.
In communications systems, various algorithms are employed for combining a number sequence with a data stream. This combining process can be performed
for encrypting or masking the data stream prior to its transmission over a communications link. Such algorithms can include residue number system (RNS)
operations for expressing each number of the number sequence in a Galois field GF[M] base. The finite or Galois field GF[M] has a finite field size defined by the
Galois characteristic M. A Galois field GF[M] is a field that contains only a finite number of elements {0, 1, 2, . . . M−1}. As such, all arithmetic operations
performed in the finite or Galois field result in an element within that field. As such, a resulting sequence of a Galois field GF[M] operation can repeat every
(M+1)th element. These RNS operations are well known to persons skilled in the art, and therefore will not be described in great detail herein. However, it should
be understood that these RNS operations can require a mixed radix conversion. The phrase “mixed radix conversion” as used herein refers to a conversion of a
number sequence from a first number base (or radix) to a second number base (or radix). For example, a number sequence expressed in a Galois field GF[7]
base is converted to a number sequence expressed in a Galois field GF[3] base as depicted in FIG. 1. Typically, mixed-radix conversion produces statistical
artifacts whenever the destination radix is smaller than and does not evenly divide the starting radix.
[0029] A mixed radix conversion algorithm can include computations based on the Chinese Remainder Theorem. The Chinese Remainder Theorem is well known to
persons skilled in the art and therefore will not be described in great detail herein. However, if should be understood that these computations are performed to
uniquely combine numbers expressed in multiple small number bases (or radices). This combination is often achieved via a mapping operation. The mapping
operation of the Chinese Remainder Theorem involves mapping each of the numbers expressed in a small number base (or radices) to numbers expressed in a
large number base.
[0030] Notably, there is a statistical non-uniformity in the statistical distribution resulting from a number sequence conversion from a first Galois field GF[M1] base to a
second Galois field GF[M2] base when the two (2) number bases are not evenly divisible. For example, a random number sequence expressed in a Galois field
GF[7] base is mapped to a number sequence expressed in a Galois field GF[3] base. The random number sequence expressed in a Galois field GF[7] base is
defined by the set of elements {0, 1, 2, . . . , 6}. Similarly, the number sequence expressed in a Galois field GF[3] base is defined by the set of elements {0, 1, 2}.
Mapping the number sequence expressed in a Galois field GF[7] base to a number sequence expressed in the Galois field GF[3] base generally involves
segmenting each element {G< 1, 2, . . . , 6} by their corresponding equivalence class modulo three (3). Since the Galois field GF[3] is a finite field that contains
only a finite number of elements {0, 1, 2}, there is a corresponding equivalence class for the integers zero (0), one (1), and two (2).
[0031] The mapping operations of the elements from the Galois field GF[7] to elements in a Galois field GF[3] are listed in the following Table (1).
TABLE 1
Elements From A

Equivalent Elements in

Galois Field GF[7]:

Mapping Operatons:

A Galois Field GF[3]:

0

0 modulo 3

0

1

1 modulo 3

1

2

2 modulo 3

2

3

3 modulo 3

0

4

4 modulo 3

1

5

5 modulo 3

2

6

6 modulo 3

0

As illustrated in Table 1, the mapping operations result in a non-uniform distribution of the elements over the Galois field GF[3]. Specifically, the resulting
sequence of the mapping operations is defined as {0 1 2 0 1 2 0}. There are three elements {0, 3, 8} from the Galois field GF[7] in an equivalence class for the
integer zero (0). There are two (2) elements {1, 4} from the Galois field GF[7] in an equivalence class for the integer one (1). There are two (2) elements {2, 5}
from the Galois field GF[7] In an equivalence class for the integer two (2).
[0032] By utilizing a statistical analysis, an outside party can gain partial information from a system implementing the conventional mixed radix conversion algorithm
(described above in relation to FIG. 1) and can more easily identify an original number sequence from a data stream altered by a resulting number sequence of
the mapping operations. For example, if one knows the sizes of the two (2) number bases, then the attacker oar) use the statistical proportion of elements in the
various equivalence classes to more easily identify the original number sequence from the altered data stream. Moreover, knowledge of the data message
format will coincide in a statistically significant fashion with the statistical artifacts of the random number sequence. In effect, more information is provided in
the data message content. As used herein, the term “statistically significant” refers to a mathematical assurance of the validity of some piece of information. As
such, it is desirable to remove statistical artifacts from results derived by a mixed radix conversion algorithm so that identifying an original number sequence
from an altered data stream is relatively difficult.
[0033] Accordingly, some embodiments of the present invention provide a method for removing unwanted statistical artifacts in a mixed radix conversion. One method
generally includes spreading statistical artifacts evenly over all equivalence classes of a Galois field GF[P]. This even distribution of statistical artifacts can be
accomplished by using a mixed-radix ring generator process. The process involves (1) generating a first random number sequence utilizing a ring structure
defined by a Galois field GF[M], (2) modifying each random number of the first random number sequence by adding a previously computed remainder via a
modulo P operation, and (3) generating a second random number sequence utilizing the modified random numbers. The second random number sequence is
also generated utilizing a modulo P operation. The second random number sequence includes statistical artifacts that are distributed evenly over-all equivalence
classes of the Galois field GF[P].
[0034] It should be understood that such a mixed radix number generator process provides stochastic adherence to desired statistical properties rather than
unconditional adherence to desired statistical properties. The phrase “stochastic adherence” refers to a behavior that converges to an ideal. The phrase
“unconditional adherence” refers to a level of assurance provide by mathematical proof. It should also be understood that such a mixed radix number generator
process can be used in a variety of communications system applications. For example, such a mixed radix number generator process can be implemented in a
cryptographic system for altering a data stream. In such a scenario, the mixed radix number generator process provides an increased security feature to the
cryptographic system. It should be noted that this mixed radix number generator process produces a random number sequence that appears to be highly non-

https://patents.google.com/patent/US20080307024A1/en

4/15

2/14/26, 1:52 PM

US20080307024A1 - Mixed Radix Number Generator with Chosen Statistical Artifacts - Google Patents

deterministic in nature, in performing the modulo reduction, information from the original number sequence is intentionally destroyed, in effect unintended
reconstruction is made more difficult.
[0035] The present invention will now be described more fully hereinafter with reference to accompanying drawings, in which illustrative embodiments of the invention
are shown. However, this invention may be embodied in many different forms and should not be construed as limited to the embodiments set forth herein. For
example, the present invention can be embodied as a method, a data processing system, or a computer program product. Accordingly, the present invention can
take the form as an entirely hardware embodiment, an entirely software embodiment, or a hardware/software embodiment.
[0036] Referring now to FIG. 2, there is provided a conceptual diagram of a mixed radix number generator structure which is useful for spreading statistical artifacts
evenly over all equivalence classes of a Galois field GF[P]. As shown in FIG. 2, the mixed radix ring generator process begins with the generation of random
number sequence in random number generator 202. The random number sequence can be, but is not limited to, a pseudo-random number sequence or a
pseudo-chaotic sequence generated on a Galois field of characteristic M. Such a sequence is most easily viewed as a sequence of random elements chosen
from a Galois field GF[M], in order to map an element from the Galois field GF[M] to a desired Galois field of characteristic P, the Galois field characteristic M is
selected to be relatively prime to the Galois field characteristic P. The phrase “relatively prime” as used herein refers to a collection of numbers having a greatest
common divisor of one (1).
[0037] The random number sequence is communicated to an adder 204. To negate the statistical anomalies described above (in relation to FIG. 1), a previous output of
a modulo P operation is added to a next input from a Galois field GF[M] via a feedback structure. The feedback structure includes a delay unit 205. A result from
the adding operation is then communicated to the modulo P operator 208. The module P operator 206 performs a modulo P operation on the result from the
adding operation to generate an output value. The output of the modulo P operator is then used as the next addition input effectively rotating the entire ring
structure of GF[M]. In effect, the cumulative statistical deviation becomes significantly less noticeable since the rotation will converge to a steady-state value, it
is easy to show statistically, that faking numerous such samples from a Galois field GF[P] will distribute the statistical anomalies over all the equivalence classes
evenly, returning the output distribution to that of a uniform distribution. An additional option is to induce a constant rotation in addition to that of the feedback
path (ideally a value that is less than P and mutually prime with {M, P}) to ensure that there are no fixed points in the conversion. In mathematical parlance, a
“fixed point” is one that remains the same both at the input and the output of a mathematical operator, making repeated applications of the operator result in a
fixed value. For example, zero (0) is a fixed point of the traditional multiplication operator, since every number times zero (0) is zero (0).
[0038] A few numerical examples may help in seeing how the conversion works.
EXAMPLE 1
[0039] Let M=5*7=35, p=3, and an initial condition value of the unit delay be zero (0). It should be noted that the initial condition (initial output value) of the unit delay
can alternatively be any of zero (0), one (1), or two (2). Note that, absent the feedback mechanism described above, the outputs of the modulo P operation is a
stream of values that have a statistical artifact within a Galois field GF[P]. If the distribution of the outputs of the random number generation are truly uniform,
then the first two (2) equivalence classes of the Galois filed GF[P] will be larger by one (1) element than the third (3rd) equivalence class. This is easily seen from
the computation of 35 modulo 3=(3*11+2) modulo 3=2 modulo 3. The feedback (i.e., delay) in FIG. 2 spreads this statistical non-uniformity in the Galois field
GF[P] about all three (3) of its equivalence classes.
[0040] If the outputs of a first random number generation is a stream defined as {23 8 19 31 0 6 13 21 21 . . . }, then the corresponding output of a modulo three (3)
operation without feedback would be [2 2 1 1 0 0 1 0 0 . . . ]. Note that multiple inputs in this case map to the same output, which makes the reverse mapping
more difficult. The output of the modulo three (3) operation with unit delay feedback as shown in FIG. 2 is {2 1 2 0 0 0 1 1 1 . . . }. The difference of numbers on
this small scale may appear negligible, yet the feedback is spreading the non-uniformity of the mixed-radix conversion about the equivalence classes of GF[P].
[0041] In order to fully appreciate the non-uniformity which can exist with more conventional systems, and the improvement obtained with the arrangement described in
FIG. 2, consider a scenario in which the random number generator 202 in FIG. 2 generates 1,000,000 randomly chosen outputs of GF[M]. The Galois field GF[P] is
selected to be a Galois field GF[3]. The first random number sequence is comprised of one million (1,000,000) randomly drawn elements from the Galois field
GF[M]. If the conventional mixed radix conversion algorithm (described above in relation to FIG. 1) is employed, then the mapping operations result in a nonuniform distribution of the elements over the Galois field GF[3]. A graph is provided in FIG. 3A that illustrates the results of these mapping operations as obtained
from a MATLAB® simulation. MATLAB® is a common numerical simulation and analysis tool. The graph shows that the elements 0 and 1 appear more
frequently in the output as compared to the value 2. If the mixed radix number generator process (described above in relation to FIG. 2) is employed with a fixed
rotation offset of one then the statistical artifacts are spread almost evenly over all equivalence classes of a Galois field GF[3]. A graph is provided in FIG. 38 that
illustrates the results of the mixed radix number generator process of FIG. 2 as obtained from a MATLAB® simulation. The graph in FIG. 38 shows a uniform
distribution of the elements 0, 1 and 2 in the output sequence.
Mixed Radix Number Generator Method
[0042] Referring now to FIG. 4, there is provided a flow diagram of a mixed radix number generator method 400 for spreading statistical artifacts evenly over all
equivalence classes of a Galois field GF[F]. The flow diagram is an alternative expression of the concept which is shown in FIG. 2. As shown in FIG. 4, the method
400 begins with step 402 and continues with step 404. In step 404, a relatively large first Galois field GF[M] is selected. The relative sizes of M and P can take
any value and retain the statistical properties described in this application. The value of M is typically chosen to be orders of magnitude larger than P, but that is
not a requirement for the embodiment to function correctly. Step 404 also involves selecting a second Galois field GF[P] which is smaller than the first Galois
field GF[M]. Step 404 further involves selecting the Galois field characteristic M to be mutually prime with respect to the Galois field characteristic P. The phrase
“mutually prime” as used herein refers to two (2) or more integers having no common integer divisor except one (1).
[0043] After step 404, the method 400 continues with step 406. In step 406, a first random number sequence is generated utilizing a ring structure defined by the
relatively large Galois field GF[M]. Still, the invention is not limited in this regard. For example, the first random number sequence can also be generated utilizing a
ring structure defined by a punctured Galois field GF′[M]. As used herein, the term “punctured” means at least one element has been discarded in a Galois field
GF[M] that exceed an integer multiple of a desired characteristic.
[0044] Referring again to FIG. 4, the first random number sequence includes the random numbers RN1, RN2, . . . , RNN. The random number sequence can be, but is not
limited to, a pseudo-random number sequence or a pseudo-chaotic number sequence. In this regard, it should be understood that a random number generator
(RNG) can be employed for generating a random number sequence on the relatively large Galois field GF[M] or a punctured Galois field GF′[M]. RNGs are well
known to persons skilled in the art, and therefore will not be described in great detail herein. However, if should be understood that any RNG known in the art can
be used without limitation.
[0045] Subsequently, the method 400 continues with step 408. Step 408 and a subsequent step 410 (described below) collectively provide a means for removing
unwanted statistical artifacts in a mixed radix conversion. Step 408 and the subsequent step 410 (described below) also collectively provide a means for
spreading statistical artifacts evenly over all equivalence classes of a Galois field GF[P]. This even distribution of statistical artifacts provides stochastic
adherence to a desired statistical property, namely a uniform distribution of elements from the Galois field GF[M] over the Galois field GF[P]. Further, step 408
and the subsequent step 410 (described below) also collectively provide a means for inducing chosen statistical artifacts over the equivalence classed of a
Galois field GF[P].
[0046] In step 408, arithmetic operations are performed to combine each random number RN1, RN2, . . . , RNN of the first random number sequence with a result of a
modulo P operation. P is the Galois field characteristic of a Galois field GF[P]. The modulo P operation utilizes a preceding random number RN1, RN2, . . . , RNN of
the first random number sequence. The arithmetic operations can be generally defined by the mathematical Equations (1) through (4),

https://patents.google.com/patent/US20080307024A1/en

5/15

2/14/26, 1:52 PM
RN 1 ′=RN 1+0

US20080307024A1 - Mixed Radix Number Generator with Chosen Statistical Artifacts - Google Patents
(1)

RH 2 ′=RN 2 +RN 1 ′ modulo P

(2)

RN 3 ′=RN 3 +RN 2 ′ modulo P

(3)

RN N ′=RN N +RN N-1 ′ modulo P

(4)

where RN1′ is a modified first random number derived from a first arithmetic operation, RN2′ is a modified second random number derived from a second
arithmetic operation. RN3′ is a modified third random number derived from a third arithmetic operation, RNN′ is a modified Nth random number derived from an
Nth arithmetic operation. RNN-1′ is a second to last, modified random number derived from a second to last arithmetic operation. RN1 is a first random number of
the first random number sequence. RN2 is a second random number of the first random number sequence. RN3 is a third random number of the first random
number sequence. RNN is a last random number of the first random number sequence. P is a modulus having a value selected to be a positive integer defining a
finite field size of a Galois field GF[P].
[0047] An alternative embodiment of step 408 is to combine each random number RN1, RN2, . . . , RNN of the first random number sequence with a result of a modulo P
operation plus a fixed offset P is the Galois field characteristic of a Galois field GF[P]. The modulo P operation utilizes a preceding random number RN1, RN2, . . . ,
RNN of the first random number sequence. The arithmetic operations can be generally defined by the mathematical Equations (5) through (8).

RN 1 ′=RN 1 +C

(5)

RN 2 ′=RN 2 +RN 1 ′+C modulo P

(6)

RN 3 ′=RN 3 +RN 2 ′+C modulo P

(7)

...

RN N ′=RN N +RN N-1 ′+C modulo P

(8)

where RN1′ is a modified first random number derived from a first arithmetic operation, RN2′ is a modified second random number derived from a second
arithmetic operation, RN3′ is a modified third random number derived from a third arithmetic operation. RNN′ is a modified Nth random number derived from an
Nth arithmetic operation. RNN-1′ is a second to last modified random number derived from a second to last arithmetic-operation. RN1 is a first random number of
the first random number sequence. RN2 is a second random number of the first random number sequence, RN3 is a third random number of the first random
number sequence. RNN is a last random number of the first random number sequence, P is a modulus having a value selected to be a positive integer defining a
finite field size of a Galois field GF[P]. C is an arbitrary constant chosen to rotate the effective output in a manner to eliminate any fixed points.
[0048] After step 408, the method 400 continues with step 410. It should be understood that step 410 is performed to generate a second random number sequence.
This second random number sequence has evenly distributed statistical artifacts over all equivalence classes of the second Galois field GF[P]. Step 410 involves
performing arithmetic operations utilizing the modified random numbers RN1′, RN2′, RN3′, . . . , RNN′ derived from the arithmetic operations performed in step
408.
[0049] These arithmetic operations can be defined by the mathematical Equations (9) through (12).
R1=RN1′ modulo P

(9)

R2=RN2′ modulo P

(10)

R3=RN3′ modulo P

(11)

...
RN=RNN-1′ modulo P

(12)

where R1 is a result derived from a first arithmetic operation. R2 is a result derived from a second arithmetic operation. R3 is a result derived from a third
arithmetic operation. RN is a result derived from a last arithmetic operation, RN1′ is a modified first random number derived from a first arithmetic operation
performed in step 408, RN2′ is a modified second random number derived from a second arithmetic operation performed in step 408. RN3′ is a modified third
random number derived from a third arithmetic operation performed in step 408. RNN′ is a modified Nth random number derived from an Nth arithmetic operation
performed in step 408. P is a modulus having a value selected to be a positive integer defining a finite field size of a Galois field GF[P]. It should be understood
that each of the results R1, R2, . . . , RN is an element {0, 1, 2, . . . , P−1} from the Galois field GF[P]. It should be understood that the second random number
sequence is defined by a set of random numbers, namely R1, R2, . . . , RN.
[0050] Referring again to FIG. 4, the method 400 continues with step 412, in step 412, the method 400 ends. It should be understood that the method 400 is one
method for removing unwanted statistical artifacts in a conventional mixed radix conversion. However, the invention is not limited in this regard and any other
mixed radix number generator method configured for spreading statistical artifacts evenly over all equivalence classes of a Galois field GF[P] can be used
without limitation.
Referring now to FIG. 5, there is provided a flow diagram of a conventional method 500 for altering a data stream that is useful for understanding the invention.
As shown in FIG. 5, the method 500 begins with step 502 and continues with step 504. In step 504, a random number sequence is generated. It should be
understood that the sequence of random numbers are contained within a relatively large Galois field GF[M]. After generating the random number sequence, step
506 is performed where a portion of the random number sequence is selected.
[0051] After step 506, the method 500 continues with step 508, in step 508, the portion of the random number sequence is combined with the input data stream
thereby altering the input data stream. In this regard, it should be understood that the portion of the random number sequence has a size that is greater than or

https://patents.google.com/patent/US20080307024A1/en

6/15

2/14/26, 1:52 PM

US20080307024A1 - Mixed Radix Number Generator with Chosen Statistical Artifacts - Google Patents

equal to that of the input data stream, i.e., when they are expressed in the same number base (or radix). As such, the method 500 can be modified accordingly.
For example, the method 500 can include a conversion step prior to the step 508. The conversion step can involve converting the portion of the random number
sequence from a size GF[M] to a size n if the input data stream is of a size GF[n] or GF[n/d], where d is an even divisor of n. Subsequently, step 510 is performed
where the method 500 ends.
[0052] As should be understood, a relatively large Galois field GF[M] provides a certain degree of security to the conventional method 500. In this regard, it should be
appreciated that the Galois field GF[M] is a field that contains only a finite number of elements {0, 1, 2, . . . , M−1}. The Galois field GF[M] has a finite field size
defined by the Galois characteristic M. As such, an output sequence can repeat every Mth element. This repetitive behavior can produce correlations thereby
making a decoding of an altered data stream relatively easy when M is small. Consequently, it is desirable to select a relatively large Galois field GF[M].
[0053] It should also be appreciated that selecting a portion of the random number sequence also provides a certain degree of security to the conventional method
500. For example, a random number sequence is generated on a Galois field GF[M]. In this example, it is assumed that the random number sequence contains
five hundred (500) bits. A portion of the random number sequence is selected to include only sixteen (16) of the five hundred (500) bits. By using only sixteen
(16) bits of the random number sequence to alter a data stream, it becomes more difficult to determine the Galois field GF[M] employed for generating the
random number sequence. Still, it is desirable to further increase the security of the method.
[0054] Referring now to FIG. 8, there is provided a method 800 for increasing a security of a communications system. As shown in FIG. 8, the method 800 begins with
step 802 and continues with step 804. In step 804, a relatively large Galois field GF[M] is selected. As should be understood, a large Galois field can minimize the
likelihood that an attacker of a communications system can determine the Galois field GF[M] employed for generating an original random number sequence
from an altered data stream. In effect, the large Galois field GF[M] can provide a certain degree of security to a communications system implementing the
method 600. Stated in an alternate fashion, the security of a random number sequence is largely defined by the dynamic range of the output value (number of
bits or digits) and the apparent randomness.
[0055] Thereafter, step 808 is performed where a first random number sequence is generated utilizing a ring structure defined by the Galois field GF[M]. Still, the
invention is not limited in this regard. For example, the first random number sequence can also be generated utilizing a ring structure defined by a punctured
Galois field GF[M]. Each random number of the sequence is defined by an element of the Galois field GF[M] or the punctured Galois field GF′[M]. In step 808, a
portion of the first random number sequence is selected. This step provides a higher degree of security to a communications system implementing method 800.
In this regard, if should be appreciated that it becomes more difficult to determine the Galois field GF[M] when only a portion of the random number sequence is
used to alter an input data stream.
[0056] Step 610 also involves performing arithmetic operations to generate a second random number sequence. This second random number sequence has evenly
distributed statistical artifacts over all equivalence classes of the second Galois field GF[P]. According to a preferred embodiment of the invention, these
arithmetic operations can be the mixed radix number generator process described above in relation to FIG. 2. Still, it should be appreciated that the invention is
not limited in this regard. Any other suitable technique can be used for this purpose.
[0057] Referring again to FIG. 8, the method 800 continues with step 612. In step 612, the second random number sequence is communicated to a device, such as a
multiplier. The second random number sequence is combined with an input data stream to form an altered data stream. The input data stream is of a size GF(n)
or GF(n/d), where d is an even divisor of n. In this regard, it should be understood that the second random number sequence and the input data stream have the
same size, i.e., they are expressed in the same number base (or radix) and contain the same number of digits. Thereafter, step 618 is performed where the
method 600 ends.
[0058] A person skilled in the art will appreciate that method 600 is one method for increasing a security of a communications system. However, the invention is not
limited in this regard and any other method implementing the present invention can be used without limitation.
Hardware Implementation
[0059] There are a variety of ways to implement the method 400 (described above in relation to FIG. 4) for removing unwanted statistical artifacts in a conventional
mixed radix conversion algorithm. For example, the mixed radix number generator method 400 can be implemented utilizing a mixed radix accumulator
arrangement similar to the one shown in FIG. 2. The mixed radix number generator can be deployed in a communications system and/or a cryptographic system
for altering a data stream. In such a scenario, the mixed radix number generator can provide an increased security feature to the communications system and/or
cryptographic system. Such a mixed radix number generator is described below in relation to FIG. 7.
[0060] Referring now to FIG. 7, there is provided a block diagram of a mixed radix number generator 700. The mixed radix number generator 700 is comprised of a
random number generator 702, a mixed radix accumulator 750, and an external device 710. The random number generator 702 can be, but is not limited to, a
ring generator, a punctured ring generator, or a chaos generator. If the random number generator 702 is a ring generator, then the random number generator 702
is comprised of hardware and/or software configured to generate a random number sequence utilizing a ring structure defined by a Galois field GF[M]. If the
random number generator is a punctured ring generator, then the random number generator 702 is comprised of hardware and/or software configured to
generate a random number sequence utilizing a ring structure defined by a punctured Galois field GF′[M]. Accordingly, the output of the random number
generator 702 can be a random element from the Galois field GF[M] or a random element from the punctured Galois field GF′[M]. In order to map an element
from the Galois field GF[M] or the punctured Galois field GF′[M] to a desired Galois field characteristic F, the Galois field characteristic M is selected to be
relatively prime to the Galois field characteristic P. Also, the Galois field characteristic M is selected to be greater than the Galois field characteristic P.
[0061] The random number generator 702 is also comprised of hardware and/or software configured to communicate a random number of a random number sequence
to the mixed radix accumulator 750. The mixed radix accumulator 750 is configured to perform an arithmetic operation to generate a second random number.
The arithmetic operation involves computing a remainder value utilizing the random number received from the random number generator 702. Accordingly, the
mixed radix accumulator 750 is comprised of an adder 704, an arithmetic operator 706, and a delay 708.
[0062] The adder 704 is comprised of hardware and/or software configured to receive a random number from the random number generator 702 and a time delayed
remainder from the delay 708 (described below). The adder 704 is also comprised of hardware and/or software configured to perform an addition operation
using the random number received from the random number generator 702 and the time delayed remainder received from the delay 708 (described below). The
adder 704 is also comprised of hardware and/or software configured to communicate the sum of the addition operation to the arithmetic operator 708.
[0063] The arithmetic operator 708 is comprised of hardware and/or software configured to perform arithmetic operations. The arithmetic operations can involve
performing modulo operations. Modulo operations are well known to those skilled in the art, and therefore will not be described in great detail herein. However, it
should be appreciated that modulo operations can generally be defined by a mathematical equation: R=S modulo P, where R is a remainder derived from a
modulo operation, S is a random number input to the arithmetic operator 708. P is a modulus having a value selected to be a positive integer defining a finite
field size of a Galois field GF[F], it should be understood that the remainder R is an element from the Galois field GF[P].
[0064] The arithmetic operator 708 is further comprised of hardware and/or software configured to communicate the remainder R to the external device 710 and the
delay 708. The external device 710 can be a combiner configured for combing the remainder with input data or a digital data stream. For example, the external
device is a multiplier in one embodiment of the invention. The delay 708 is comprised of hardware and software configured to delay the remainder R received
from the arithmetic operator 708 by z−N or N clock cycles, where z−1 is a one (1) sample clock period delay or unit delay and N is a positive integer value. z−N is a
N clock period delay. For example, the delay 708 is configured to delay the remainder R by one (1) clock cycle. Still, the invention is not limited in this regard.
[0065] A person skilled in the art will appreciate that the mixed radix generator 700 is one architecture of a mixed radix generator implementing the present invention.
However, the invention is not limited in this regard and any other mixed radix generator architecture implementing the present invention can be used without
limitation.

https://patents.google.com/patent/US20080307024A1/en

7/15

2/14/26, 1:52 PM

US20080307024A1 - Mixed Radix Number Generator with Chosen Statistical Artifacts - Google Patents

[0066] It should be understood that the method and system for a mixed radix number generator described in relation to FIGS. 1-7 is not limited with regard to the size or
composition of the number P. For example, P can be selected so that P is equal to the product of p1′ p2′, . . . , ′ pk, where all of the k factors are mutually prime
with respect to M and each other. This characteristic of the system can facilitate certain alternative embodiments which provide for k individual outputs, each of
which can offer similar statistical behavior as compared to the system described above in reference to FIGS. 1-7. Such a mixed radix generator is provided in
FIG. 8.
A Mixed Radix Accumulator with Multiple Outputs
[0067] Referring now to FIG. 8, there is provided a block diagram of an alternative embodiment: of a mixed radix number generator 800 which provides multiple outputs.
The mixed radix number generator 800 is comprised of a random number generator 802 and a mixed radix accumulator 850. The random number generator 802
can be, but is not limited to, a ring generator, a punctured ring generator, or a chaos generator. If the random number generator 802 is a ring generator, then the
random number generator 802 is comprised of hardware and/or software configured to generate a random number sequence utilizing a ring structure defined by
a Galois field GF[M]. If the random number generator is a punctured ring generator, then the random number generator 802 is comprised of hardware and/or
software configured to generate a random number sequence utilizing a ring structure defined by a punctured Galois field GF′[M]. Accordingly, the output of the
random number generator 802 can be a random element from a Galois field GF[M] or a random element from a punctured Galois field GF′[M].
[0068] In order to map an element from the Galois field GF[M] or the punctured Galois field GF′[M] to a desired Galois field characteristic P, the Galois field characteristic
M is selected to be relatively prime to the Galois field characteristic P.
[0069] where P is equal to the product of p1′ p2′, . . . , ′ pk. The Galois field characteristic M is also selected to be mutually prime with the factors p1′ p2′, . . . , ′ pk of the
Galois field characteristic P. The Galois field characteristic M is further selected to be greater than the Galois field characteristic P.
[0070] The random number generator 802 is also comprised of hardware and/or software configured to communicate random numbers of a random number sequence
to the mixed radix accumulator 850. The mixed radix accumulator 860 advantageously has a configuration which is similar to the mixed radix accumulator 750
and performs similar functions. In this regard, the mixed radix accumulator is configured to perform an arithmetic operation to generate a second random
number. The arithmetic operation involves computing a remainder value utilizing the random number received from the random number generator 802.
Accordingly, the mixed radix accumulator 850 is also comprised of an adder 804, and a delay SOB.
[0071] The random number generator 802 also includes a plurality of arithmetic operators 810 1, 810 2, . . . 810 k. Each of the arithmetic operators 810 1, 810 2, . . . 810
k is comprised of hardware and/or software configured to perform arithmetic operations. The arithmetic operations can involve performing modulo operations.

According to a preferred embodiment, the modulo operations are defined by the mathematical equation R modulo p, where R is a remainder derived from a
modulo operation performed at the arithmetic operator 806, and p is one of the factors p1, p2, . . . , ′pk of the Galois field characteristic P. Each of the arithmetic
operators 810 1, 810 2, . . . 810 k is also comprised of hardware and/or software configured to produce one of k outputs. Each of the arithmetic operators 810 1,
810 2, . . . 810 k provides as an output an element of a Galois field GF[p1-k] which can be forwarded to an external device (not shown). The external device can be
any device configured for combing the remainder with input data. For example, in one embodiment the external device is a multiplier. Most significantly, each
sequence provided as one of the k outputs from arithmetic operators 810 1, 810 2, . . . 810 k will have uniformly distributed outputs which are free of unwanted
statistical artifacts.
[0072] A person skilled in the art will appreciate that the mixed radix generator 800 is one architecture of a mixed radix number generator implementing the present
invention. However, the invention is not limited in this regard and any other mixed radix generator architecture implementing the present invention can be used
without limitation. According to one such embodiment, the delay 808 can be replaced with a finite impulse response (FIR) or an infinite impulse response (IIR)
filter, where all operations are performed using modified GF arithmetic.
Multi-Rate Implementation of Mixed Radix Number Generator
[0073] Referring now to FIG. 9, there is provided a second alternative embodiment of the invention. The second alternative embodiment is a multi-rate implementation
of a mixed radix number generator 900. The multi-rate implementation can involve either periodically sampling the output from a random number generator or
sampling such output at a higher rate as compared to the set of desired outputs. Once again, this leads to an accumulation of values that cannot easily be
reconstructed by an observer.
[0074] As illustrated in FIG. 9, the mixed radix generator 900 is comprised of a random number generator 902 and a mixed radix accumulator 950. The random number
generator 902 and the mixed radix accumulator 950 are similar to the corresponding structures 802, 850 described above in relation to FIG. 8. Accordingly, the
mixed radix accumulator 950 can also be comprised of adder 908 and delay 918. A set of arithmetic operator units 912 1, 912 2, . . . 912 k can also be provided
for performing operations similar to those arithmetic operator units 810 1, 810 2, . . . 810 k in FIG. 8. Multi-rate processing is well understood by those skilled in
the art, and therefore will not be described in great detail herein.
[0075] The mixed radix generator 900 also includes adder 904 and delay 906. The adder 004 is comprised of hardware and/or software configured to receive a random
number from the random number generator 902 and a time delayed output from the delay 906 (described below). The adder 904 is also comprised of hardware
and/or software configured to perform an addition operation using the random number received from the random number generator 902 and the time delayed
output received from the delay 906. The adder 904 is also comprised of hardware and/or software configured to communicate the sum of the addition operation
to the delay 906.
[0076] The delay 906 is comprised of hardware and software configured to delay the sum received from the adder 904 by N clock cycles. Still, the invention is not
limited in this regard. The delay 906 is also comprised of hardware an software configured to communicate a time delayed output (i.e., a time delayed sum) to
the adders 904, 906.
[0077] A person skilled in the art will appreciate that the mixed radix generator 900 is one architecture of a mixed radix generator implementing the present invention.
However, the invention is not limited in this regard and any other mixed radix generator architecture implementing the present invention can be used without
limitation.
[0078] In light of the foregoing description of the invention, it should be recognized that the present invention can be realized in hardware, software, or a combination of
hardware and software. A method of generating an arbitrary permutation ordering of bits according to the present invention can be realized in a centralized
fashion in one processing system, or in a distributed fashion where different elements are spread across several interconnected processing systems. Any kind
of computer system, or other apparatus adapted for carrying out the methods described herein, is suited. A typical combination of hardware and software could
be a general purpose computer processor, with a computer program that, when being loaded and executed, controls the computer processor such that it carries
out the methods described herein. Of course, an application specific integrated circuit (ASIC), and/or an FPGA could also be used to achieve a similar result.
[0079] The present invention can also be embedded in a computer program product which comprises all the features enabling the implementation of the methods
described herein, and which, when loaded in a computer system, is able to carry out these methods. Computer program or application in the present context
means any expression, in any language, code or notation, of a set of instructions intended to cause a system having an information processing capability to
perform a particular function either directly or after either or both of the following a) conversion to another language, code or notation; b) reproduction in a
different material form. Additionally, the description above is intended by way of example only and is not intended to limit the present invention in any way,
except as set forth in the following claims.
[0080] The invention described and claimed herein is not to be limited in scope by the preferred embodiments herein disclosed, since these embodiments are intended
as illustrations of several aspects of the invention. Any equivalent embodiments are intended to be within the scope of this invention. Indeed, various

https://patents.google.com/patent/US20080307024A1/en

8/15

2/14/26, 1:52 PM

US20080307024A1 - Mixed Radix Number Generator with Chosen Statistical Artifacts - Google Patents

modifications of the invention in addition to those shown and described herein will become apparent to those skilled in the art from the foregoing description.
Such modifications are also intended to fail within the scope of the appended claims.

Patent Citations (114)
Publication number

Priority date

Publication date

Assignee

Title

US1556A *

1840-04-18

Tailor s measuring instrument

US16431A *

1857-01-20

John h

US31120A *

1861-01-15

Improvement in cotton-seed planters

US44004A *

1864-08-30

Improvement in machines for stretching and glossing silk

US95215A *

1869-09-28

Improvement in construction of vessels

US111296A *

1871-01-31

Improvement in adjustable reamers

US122926A *

1872-01-23

Improvement in croquet-balls from rubber

US123325A *

1872-02-06

Ebenezeb blackmail

US209932A *

1878-11-12

Improvement in grave-shield and body-protector

US274807A *

1883-03-27

Chaeles miller

US304553A *

1884-09-02

parsons

US323766A *

1885-08-04

Automatic car-brake

US3564223A *

1967-06-06

1971-02-16

Nat Res Dev

Digital differential analyzer

US4646326A *

1983-10-20

1987-02-24

Motorola Inc.

QAM modulator circuit

US4703507A *

1984-04-05

1987-10-27

Holden Thomas W

Noise reduction system

US5007087A *

1990-04-16

1991-04-09

Loral Aerospace Corp.

Method and apparatus for generating secure random numbers using
chaos

US5048086A *

1990-07-16

1991-09-10

Hughes Aircraft Company

Encryption system based on chaos theory

US5077793A *

1989-09-29

1991-12-31

The Boeing Company

Residue number encryption and decryption system

US5276633A *

1992-08-14

1994-01-04

Harris Corporation

Sine/cosine generator and method

US5297206A *

1992-03-19

1994-03-22

Orton Glenn A

Cryptographic method for communication and electronic signatures

US5297153A *

1989-08-24

1994-03-22

U.S. Philips Corporation

Method and apparatus for decoding code words protected wordwise
by a non-binary BCH code from one or more symbol errors

US5319735A *

1991-12-17

1994-06-07

Bolt Beranek And Newman Inc.

Embedded signalling

US5412687A *

1993-10-15

1995-05-02

Proxim Incorporated

Digital communications equipment using differential quaternary
frequency shift keying

US5598476A *

1995-04-20

1997-01-28

United Technologies Automotive,
Inc.

Random clock composition-based cryptographic authentication
process and locking system

US5754923A *

1995-10-20

1998-05-19

Samsung Electronics Co., Ltd.

Image forming apparatus with controlled warmup and related method

US5811998A *

1993-01-28

1998-09-22

Digital Equipment Corporation

State machine phase lock loop

US5852630A *

1997-07-17

1998-12-22

Globespan Semiconductor, Inc.

Method and apparatus for a RADSL transceiver warm start activation
procedure with precoding

US5900835A *

1998-07-09

1999-05-04

The United States Of America As
Represented By The Secretary Of
The Navy

Coherent hidden markov model

US5924980A *

1998-03-11

1999-07-20

Siemens Corporate Research, Inc.

Method and apparatus for adaptively reducing the level of noise in an
acquired signal

US5937000A *

1995-09-06

1999-08-10

Solana Technology Development
Corporation

Method and apparatus for embedding auxiliary data in a primary data
signal

US6014446A *

1995-02-24

2000-01-11

Motorola, Inc.

Apparatus for providing improved encryption protection in a
communication system

US6023612A *

1996-07-05

2000-02-08

Thomcast Communications, Inc.

Modular transmission system and method

https://patents.google.com/patent/US20080307024A1/en

9/15

2/14/26, 1:52 PM

US20080307024A1 - Mixed Radix Number Generator with Chosen Statistical Artifacts - Google Patents

US6038317A *

1997-12-24

2000-03-14

Magliveras; Spyros S.

Secret key cryptosystem and method utilizing factorizations of
permutation groups of arbitrary order 2l

US6078611A *

1997-09-16

2000-06-20

Motorola, Inc.

Rake receiver and finger management method for spread spectrum
communication

US6304216B1 *

1999-03-30

2001-10-16

Conexant Systems, Inc.

Signal detector employing correlation analysis of non-uniform and
disjoint sample segments

US6331974B1 *

1997-06-23

2001-12-18

The Regents Of The University Of
California

Chaotic digital code-division multiple access (CDMA) communication
systems

US6377782B1 *

1999-03-01

2002-04-23

Mediacell, Inc.

Method and apparatus for communicating between a client device
and a linear broadband network

US20020099746A1 *

1999-07-26

2002-07-25

Tie Teck Sing

T-sequence apparatus and method for general deterministic
polynomial-time primality testing and composite factoring

US20030044004A1 *

2001-05-02

2003-03-06

Blakley George Robert

Ring arithmetic method, system, and apparatus

US6570909B1 *

1999-07-09

2003-05-27

Nokia Mobile Phones

Interference suppression in a CDMA receiver

US6614914B1 *

1995-05-08

2003-09-02

Digimarc Corporation

Watermark embedder and reader

US20040059767A1 *

2002-09-20

2004-03-25

Pierre-Yvan Liardet

Masking of factorized data in a residue number system

US6744893B1 *

1999-08-25

2004-06-01

Southwest Research Institute

Receiver estimation engine for a chaotic system

US6754251B1 *

1998-03-09

2004-06-22

Texas Instruments Incorporated

Spread-spectrum telephony with accelerated code acquisition

US6766345B2 *

2001-11-30

2004-07-20

Analog Devices, Inc.

Galois field multiplier system

US20040196212A1 *

2001-10-25

2004-10-07

Fujitsu Limited

Display control device

US6842479B2 *

1998-10-02

2005-01-11

Ericsson Inc.

Method and apparatus for interference cancellation in a rake receiver

US20050050121A1 *

2003-09-02

2005-03-03

Udo Klein

Mapping pseudo-random numbers to predefined number ranges

US20050089169A1 *

2003-10-23

2005-04-28

Educational Corporation Pai Chai
Hak Dang

Encryption and communication apparatus and method using
modulated delay time feedback chaotic system

US20050274807A1 *

2004-06-09

2005-12-15

John Barrus

Embedding barcode data in an auxiliary field of an image file

US6980656B1 *

1998-07-17

2005-12-27

Science Applications International
Corporation

Chaotic communication system and method using modulation of
nonreactive circuit elements

US6986054B2 *

2001-03-30

2006-01-10

Hitachi, Ltd.

Attack-resistant implementation method

US7023323B1 *

1997-08-18

2006-04-04

X-Cyte, Inc.

Frequency hopping spread spectrum passive acoustic wave
identification device

US7027598B1 *

2001-09-19

2006-04-11

Cisco Technology, Inc.

Residue number system based pre-computation and dual-pass
arithmetic modular operation approach to implement encryption
protocols efficiently in electronic integrated circuits

US7069492B2 *

2002-03-13

2006-06-27

Canon Kabushiki Kaisha

Method of interleaving a binary sequence

US7076065B2 *

2001-05-11

2006-07-11

Lockheed Martin Corporation

Chaotic privacy system and method

US7078981B2 *

2004-07-27

2006-07-18

Lucent Technologies Inc.

16 QAM modulator and method of 16 QAM modulation

US7079651B2 *

1996-05-20

2006-07-18

Koninklijke Philips Electronics N.V.

Cryptographic method and apparatus for non-linearly merging a data
block and a key

US7095778B2 *

2002-01-18

2006-08-22

Mitsubishi Denki Kabushiki Kaisha

Spread spectrum transmitter and spread spectrum receiver

US7133522B2 *

2001-04-05

2006-11-07

International Business Machines
Corporation

Method and apparatus for encryption of data

US20060251250A1 *

2005-05-03

2006-11-09

Stmicroelectronics S.R.I

Method of generating successions of pseudo-random bits or
numbers

US7170997B2 *

2000-12-07

2007-01-30

Cryptico A/S

Method of generating pseudo-random numbers in an electronic
device, and a method of encrypting and decrypting electronic data

US7190681B1 *

1996-07-10

2007-03-13

Wu William W

Error coding in asynchronous transfer mode, internet and satellites

US7200225B1 *

1999-11-12

2007-04-03

Richard Schroeppel

Elliptic curve point ambiguity resolution apparatus and method

US20070121945A1 *

2005-11-29

2007-05-31

Samsung Electronics Co., Ltd.

Adjustable chaotic signal generator using pulse modulation for ultra
wideband (UWB) communications and chaotic signal generating
method thereof

https://patents.google.com/patent/US20080307024A1/en

10/15

2/14/26, 1:52 PM

US20080307024A1 - Mixed Radix Number Generator with Chosen Statistical Artifacts - Google Patents

US7233969B2 *

2000-11-14

2007-06-19

Parkervision, Inc.

Method and apparatus for a parallel correlator and applications
thereof

US7233970B2 *

2001-05-02

2007-06-19

Cipher Corporation Limited

Computational method, system, and apparatus

US7269198B1 *

2001-11-19

2007-09-11

Bbn Technologies Corp.

Systems and methods for beaconing in wireless networks with low
probability of detection

US7269258B2 *

2001-11-16

2007-09-11

Yazaki Corporation

Cryptographic key, encryption device, encryption/decryption device,
cryptographic key management device, and decryption device

US7272168B2 *

2003-04-01

2007-09-18

Nokia Siemens Networks Oy

Determining the correlation between received samples and available
replica samples

US7277540B1 *

1999-01-20

2007-10-02

Kabushiki Kaisha Toshiba

Arithmetic method and apparatus and crypto processing apparatus
for performing multiple types of cryptography

US20070230701A1 *

2006-03-28

2007-10-04

Samsung Electro-Mechanics Co.,
Ltd.

Chaotic signal transmitter using pulse shaping method

US20080198832A1 *

2007-02-15

2008-08-21

Harris Corporation

Low Level Sequence as an Anti-Tamper MEchanism

US20080263119A1 *

2007-04-19

2008-10-23

Harris Corporation

Digital Generation of a Chaotic Numerical Sequence

US20080294956A1 *

2007-05-22

2008-11-27

Harris Corporation

Encryption Via Induced Unweighted Errors

US20080294710A1 *

2007-05-22

2008-11-27

Harris Corporation

Extending a Repetition Period of a Random Sequence

US20080304666A1 *

2007-06-07

2008-12-11

Harris Corporation

Spread Spectrum Communications System and Method Utilizing
Chaotic Sequence

US20080307022A1 *

2007-06-07

2008-12-11

Harris Corporation

Mixed Radix Conversion with a Priori Defined Statistical Artifacts

US20090034727A1 *

2007-08-01

2009-02-05

Harris Corporation

Chaotic Spread Spectrum Communications System Receiver

US20090044080A1 *

2007-05-31

2009-02-12

Harris Corporation

Closed Galois Field Combination

US20090110197A1 *

2007-10-30

2009-04-30

Harris Corporation

Cryptographic system configured for extending a repetition period of
a random sequence

US7529292B2 *

2001-10-01

2009-05-05

Interdigital Technology Corporation

Code tracking loop with automatic power normalization

US20090196420A1 *

2008-02-05

2009-08-06

Harris Corporation

Cryptographic system incorporating a digitally generated chaotic
numerical sequence

US20090202067A1 *

2008-02-07

2009-08-13

Harris Corporation

Cryptographic system configured to perform a mixed radix conversion
with a priori defined statistical artifacts

US20090245327A1 *

2008-03-26

2009-10-01

Harris Corporation

Selective noise cancellation of a spread spectrum signal

US20090279688A1 *

2008-05-06

2009-11-12

Harris Corporation

Closed galois field cryptographic system

US20090279690A1 *

2008-05-08

2009-11-12

Harris Corporation

Cryptographic system including a mixed radix number generator with
chosen statistical artifacts

US20090296860A1 *

2008-06-02

2009-12-03

Harris Corporation

Adaptive correlation

US20090300088A1 *

2008-05-29

2009-12-03

Harris Corporation

Sine/cosine generator

US20090310650A1 *

2008-06-12

2009-12-17

Harris Corporation

Featureless coherent chaotic amplitude modulation

US20090309984A1 *

2006-06-29

2009-12-17

Thales

Hybrid image stabilization for video camera

US7643537B1 *

2007-01-23

2010-01-05

L-3 Communications, Corp.

Spread spectrum signal detection with inhibiting for known sidelobe
locations

US7779060B2 *

2002-11-12

2010-08-17

Stmicroelectronics, S.R.L.

Method of generating a chaos-based pseudo-random sequence and a
hardware generator of chaos-based pseudo random bit sequences

US7797060B2 *

2007-02-27

2010-09-14

Rockwell Automation Technologies,
Inc.

Prioritization associated with controller engine instances

FR1501059A

1966-09-26

1967-11-10

Csf

New key generator

JPS61193183A *

1985-02-22

1986-08-27

株式会社日立製作所

Random number generation circuit

JPS6319038A *

1986-07-12

1988-01-26

Sharp Corp

random number generator

US5757923A

1995-09-22

1998-05-26

Ut Automotive Dearborn, Inc.

Method of generating secret identification numbers

Family To Family Citations

https://patents.google.com/patent/US20080307024A1/en

11/15

2/14/26, 1:52 PM

US20080307024A1 - Mixed Radix Number Generator with Chosen Statistical Artifacts - Google Patents

US5963460A

1996-12-17

1999-10-05

Metaflow Technologies, Inc.

Apparatus for computing transcendental functions quickly

US6285761B1

1998-03-04

2001-09-04

Lucent Technologies, Inc.

Method for generating pseudo-random numbers

US6823068B1

1999-02-01

2004-11-23

Gideon Samid

Denial cryptography based on graph theory

US7596170B2

2000-02-28

2009-09-29

Aeroastro, Inc.

Coherent detection without transmission preamble

US7010055B2

2002-06-27

2006-03-07

Motorola, Inc.

System implementing closed loop transmit diversity and method
thereof

US7512645B2

2004-03-19

2009-03-31

Texas Instruments Incorporated

System and method for generating pseudorandom numbers

US7512647B2

2004-11-22

2009-03-31

Analog Devices, Inc.

Condensed Galois field computing system

US20060209932A1

2005-03-18

2006-09-21

Qualcomm Incorporated

Channel estimation for single-carrier systems

EP1959581B1

2005-12-07

2019-04-17

ZTE Corporation

Method and device for removing narrow band interference in
spreading frequency system

US7688878B2

2006-03-16

2010-03-30

The Boeing Company

Method and device of peak detection in preamble synchronization for
direct sequence spread spectrum communication

US9203438B2

2006-07-12

2015-12-01

Ternarylogic Llc

Error correction by symbol reconstruction in binary and multi-valued
cyclic codes

CN101653020B

2007-02-15

2013-03-27

皇家飞利浦电子股份有限公司

Coordination in a wireless network with devices using different
physical layer transport schemes

US7962540B2

2007-06-07

2011-06-14

Harris Corporation

Mixed radix number generator with chosen statistical artifacts

US20090122926A1

2007-11-13

2009-05-14

Texas Instruments Incorporated

Data throughput in an interference-rich wireless environment

US8145692B2

2008-05-29

2012-03-27

Harris Corporation

Digital generation of an accelerated or decelerated chaotic numerical
sequence

US8891756B2

2008-10-30

2014-11-18

Certicom Corp.

Collision-resistant elliptic curve hash functions

* Cited by examiner, † Cited by third party

Cited By (58)
Publication number

Priority date

Publication date

Assignee

Title

US20080263119A1 *

2007-04-19

2008-10-23

Harris
Corporation

Digital Generation of a Chaotic Numerical Sequence

US20080294710A1 *

2007-05-22

2008-11-27

Harris
Corporation

Extending a Repetition Period of a Random Sequence

US20080307022A1 *

2007-06-07

2008-12-11

Harris
Corporation

Mixed Radix Conversion with a Priori Defined Statistical Artifacts

US20080304666A1 *

2007-06-07

2008-12-11

Harris
Corporation

Spread Spectrum Communications System and Method Utilizing Chaotic Sequence

US20090034727A1 *

2007-08-01

2009-02-05

Harris
Corporation

Chaotic Spread Spectrum Communications System Receiver

US20090044080A1 *

2007-05-31

2009-02-12

Harris
Corporation

Closed Galois Field Combination

US20090060179A1 *

2007-08-29

2009-03-05

Red Hat, Inc.

Method and an apparatus to generate pseudo random bits from polynomials

US20090110197A1 *

2007-10-30

2009-04-30

Harris
Corporation

Cryptographic system configured for extending a repetition period of a random sequence

US20090214024A1 *

2008-02-21

2009-08-27

Schneider James
P

Block cipher using multiplication over a finite field of even characteristic

US20090245327A1 *

2008-03-26

2009-10-01

Harris
Corporation

Selective noise cancellation of a spread spectrum signal

US20090292752A1 *

2008-05-23

2009-11-26

Red Hat, Inc.

Mechanism for generating pseudorandom number sequences

US20090292751A1 *

2008-05-22

2009-11-26

James Paul
Schneider

Non-linear mixing of pseudo-random number generator output

US20090300088A1 *

2008-05-29

2009-12-03

Harris
Corporation

Sine/cosine generator

https://patents.google.com/patent/US20080307024A1/en

12/15

2/14/26, 1:52 PM

US20080307024A1 - Mixed Radix Number Generator with Chosen Statistical Artifacts - Google Patents

US20100054228A1 *

2008-08-29

2010-03-04

Harris
Corporation

Multi-tier ad-hoc network communications

US20100091700A1 *

2008-10-09

2010-04-15

Harris
Corporation

Ad-hoc network acquisition using chaotic sequence spread waveform

US20100135486A1 *

2008-11-30

2010-06-03

Schneider James
P

Nonlinear feedback mode for block ciphers

US20110002460A1 *

2009-07-01

2011-01-06

Harris
Corporation

High-speed cryptographic system using chaotic sequences

US20110019817A1 *

2009-07-22

2011-01-27

Harris
Corporation

Permission-based tdma chaotic communication systems

US7962540B2

2007-06-07

2011-06-14

Harris
Corporation

Mixed radix number generator with chosen statistical artifacts

US8064552B2

2008-06-02

2011-11-22

Harris
Corporation

Adaptive correlation

US8068571B2

2008-06-12

2011-11-29

Harris
Corporation

Featureless coherent chaotic amplitude modulation

US8139764B2

2008-05-06

2012-03-20

Harris
Corporation

Closed galois field cryptographic system

US8145692B2

2008-05-29

2012-03-27

Harris
Corporation

Digital generation of an accelerated or decelerated chaotic numerical sequence

US8180055B2

2008-02-05

2012-05-15

Harris
Corporation

Cryptographic system incorporating a digitally generated chaotic numerical sequence

US8265272B2

2007-08-29

2012-09-11

Red Hat, Inc.

Method and an apparatus to generate pseudo random bits for a cryptographic key

US8312551B2

2007-02-15

2012-11-13

Harris
Corporation

Low level sequence as an anti-tamper Mechanism

US8320557B2

2008-05-08

2012-11-27

Harris
Corporation

Cryptographic system including a mixed radix number generator with chosen statistical
artifacts

US8345725B2

2010-03-11

2013-01-01

Harris
Corporation

Hidden Markov Model detection for spread spectrum waveforms

US8351484B2

2008-12-29

2013-01-08

Harris
Corporation

Communications system employing chaotic spreading codes with static offsets

US8363700B2

2009-07-01

2013-01-29

Harris
Corporation

Rake receiver for spread spectrum chaotic communications systems

US8363830B2

2008-02-07

2013-01-29

Harris
Corporation

Cryptographic system configured to perform a mixed radix conversion with a priori
defined statistical artifacts

US8369377B2

2009-07-22

2013-02-05

Harris
Corporation

Adaptive link communications using adaptive chaotic spread waveform

US8369376B2

2009-07-01

2013-02-05

Harris
Corporation

Bit error rate reduction in chaotic communications

US8379689B2

2009-07-01

2013-02-19

Harris
Corporation

Anti-jam communications having selectively variable peak-to-average power ratio
including a chaotic constant amplitude zero autocorrelation waveform

US8385385B2

2009-07-01

2013-02-26

Harris
Corporation

Permission-based secure multiple access communication systems

US8406276B2

2008-12-29

2013-03-26

Harris
Corporation

Communications system employing orthogonal chaotic spreading codes

US8406352B2

2009-07-01

2013-03-26

Harris
Corporation

Symbol estimation for chaotic spread spectrum signal

US8428103B2

2009-06-10

2013-04-23

Harris
Corporation

Discrete time chaos dithering

US8428102B2

2009-06-08

2013-04-23

Harris
Corporation

Continuous time chaos dithering

US8428104B2

2009-07-01

2013-04-23

Harris
Corporation

Permission-based multiple access communications systems

US8457077B2

2009-03-03

2013-06-04

Harris
Corporation

Communications system employing orthogonal chaotic spreading codes

US8611530B2

2007-05-22

2013-12-17

Harris
Corporation

Encryption via induced unweighted errors

https://patents.google.com/patent/US20080307024A1/en

13/15

2/14/26, 1:52 PM

US20080307024A1 - Mixed Radix Number Generator with Chosen Statistical Artifacts - Google Patents

US11075888B2

2017-12-04

2021-07-27

Nicira, Inc.

Scaling gateway to gateway traffic using flow hash

US11095617B2

2017-12-04

2021-08-17

Nicira, Inc.

Scaling gateway to gateway traffic using flow hash

US11277343B2

2019-07-17

2022-03-15

Vmware, Inc.

Using VTI teaming to achieve load balance and redundancy

US11347561B1 *

2018-04-30

2022-05-31

Vmware, Inc.

Core to resource mapping and resource to core mapping

US11509638B2

2019-12-16

2022-11-22

Vmware, Inc.

Receive-side processing for encapsulated encrypted packets

US11863514B2

2022-01-14

2024-01-02

Vmware, Inc.

Performance improvement of IPsec traffic using SA-groups and mixed-mode SAs

US11956213B2

2022-05-18

2024-04-09

VMware LLC

Using firewall policies to map data messages to secure tunnels

US12107834B2

2021-06-07

2024-10-01

VMware LLC

Multi-uplink path quality aware IPsec

US12113773B2

2021-06-07

2024-10-08

VMware LLC

Dynamic path selection of VPN endpoint

JP2010518464A *

2007-02-01

2010-05-27

株式会社東芝

Semiconductor memory device

US9569771B2

2011-04-29

2017-02-14

Stephen
Lesavich

Method and system for storage and retrieval of blockchain blocks using galois fields

US9137250B2

2011-04-29

2015-09-15

Stephen
Lesavich

Method and system for electronic content storage and retrieval using galois fields and
information entropy on cloud computing networks

US9361479B2

2011-04-29

2016-06-07

Stephen
Lesavich

Method and system for electronic content storage and retrieval using Galois fields and
geometric shapes on cloud computing networks

US9037564B2

2011-04-29

2015-05-19

Stephen
Lesavich

Method and system for electronic content storage and retrieval with galois fields on
cloud computing networks

US8654819B2

2011-06-22

2014-02-18

Harris
Corporation

Systems and methods for pulse rotation modulation encoding and decoding

US9997233B1

2015-10-08

2018-06-12

Rambus Inc.

Memory module with dynamic stripe width

Family To Family Citations

* Cited by examiner, † Cited by third party, ‡ Family to family citation

Similar Documents
Publication

Publication Date

Title

US7962540B2

2011-06-14

Mixed radix number generator with chosen statistical artifacts

US8320557B2

2012-11-27

Cryptographic system including a mixed radix number generator with chosen statistical artifacts

US7970809B2

2011-06-28

Mixed radix conversion with a priori defined statistical artifacts

US8363830B2

2013-01-29

Cryptographic system configured to perform a mixed radix conversion with a priori defined statistical artifacts

EP2000900B1

2015-04-29

Extending a repetition period of a random sequence

US7995749B2

2011-08-09

Cryptographic system configured for extending a repetition period of a random sequence

US7995757B2

2011-08-09

Closed galois field combination

EP2279579A2

2011-02-02

A closed galois field cryptographic system

CN110912674B

2020-09-01

Image encryption method and device, electronic equipment and readable storage medium

Priority And Related Applications
Priority Applications (5)
Application

Priority date

Filing date

Title

US11/759,276

2007-06-07

2007-06-07

Mixed radix number generator with chosen statistical artifacts

JP2008140677A

2007-06-07

2008-05-29

Mixed radix generator with selected statistical artifacts

EP08010174A

2007-06-07

2008-06-04

Galois field number generator

DE602008005519T

2007-06-07

2008-06-04

Galois-number generator

https://patents.google.com/patent/US20080307024A1/en

14/15

2/14/26, 1:52 PM

US20080307024A1 - Mixed Radix Number Generator with Chosen Statistical Artifacts - Google Patents

CA2633923A

2007-06-07

2008-06-05

Mixed radix number generator with chosen statistical artifacts

Applications Claiming Priority (1)
Application

Filing date

Title

US11/759,276

2007-06-07

Mixed radix number generator with chosen statistical artifacts

Legal Events
Date

Code

Title

Description

2007-06-07

AS

Assignment

Owner name: HARRIS CORPORATION, FLORIDA
Free format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:MICHAELS, ALAN J.;CHESTER, DAVID
B.;REEL/FRAME:019393/0871
Effective date: 20070531

2011-05-25

STCF

Information on status: patent grant

Free format text: PATENTED CASE

2014-12-15

FPAY

Fee payment

Year of fee payment: 4

2018-12-14

MAFP

Maintenance fee payment

Free format text: PAYMENT OF MAINTENANCE FEE, 8TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE:
M1552); ENTITY STATUS OF PATENT OWNER: LARGE ENTITY
Year of fee payment: 8

2022-12-14

MAFP

Maintenance fee payment

Free format text: PAYMENT OF MAINTENANCE FEE, 12TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE:
M1553); ENTITY STATUS OF PATENT OWNER: LARGE ENTITY
Year of fee payment: 12

Data provided by IFI CLAIMS Patent Services

About

Send Feedback

https://patents.google.com/patent/US20080307024A1/en

Public Datasets

Terms

Privacy Policy

Help

15/15

2/14/26, 1:55 PM

US8725786B2 - Approximate SRT division method - Google Patents

Patents
Back to results

4 of 125,048

high radix

(high radix);

Approximate SRT division method
Abstract
The invention relates to a program storage device readable by a machine, tangibly embodying a
program of instructions executable by a specific semiconductor-based computational device

US8725786B2
United States

situated in the machine to perform the steps of a partial SRT (PSRT) division of a dividend X by a
divisor D to obtain a quotient Q. The steps include: causing a computer to obtain the dividend X and

Download PDF

Find Prior Art

Similar

the divisor D; representing the dividend X and the divisor D as a digital representation having a
plurality of bits; and performing iteratively a series of steps until a desired accuracy of the quotient Q
is achieved. The invention also relates to an article of manufacture including a computer usable
medium having computer readable program code embodied therein for causing a partial SRT (PSRT)

Inventor: Makia Powell
Current Assignee : University of Massachusetts Amherst

division of a dividend X by a divisor D to generate a quotient Q.
Worldwide applications

Images (18)

2010 US

Application US12/770,197 events

Classifications
G06F7/5375 Non restoring calculation, where each digit is either negative, zero or positive, e.g.
SRT

G06F2207/5351 Multiplicative non-restoring division, e.g. SRT, using multiplication in quotient
selection

2010-04-29

Application filed by University of Massachusetts
Amherst

2010-04-29

Priority to US12/770,197

2010-04-29

Assigned to UNIVERSITY OF MASSACHUSETTS

2010-11-04

Publication of US20100281087A1

2014-05-13

Application granted

2014-05-13

Publication of US8725786B2

Status

Expired - Fee Related

2032-09-28

Adjusted expiration

G06F2207/5356 Via reciprocal, i.e. calculate reciprocal only, or calculate reciprocal first and
then the quotient from the reciprocal and the numerator

Info: Patent citations (10), Non-patent citations (11) , Cited by
(16), Legal events, Similar documents, Priority and Related

G06F7/487 Multiplying; Dividing

Applications

Hide more classifications

External links: USPTO, USPTO PatentCenter, USPTO
Assignment, Espacenet, Global Dossier, Discuss

Landscapes

Physics & Mathematics
General Physics & Mathematics
Show more

Claims (18)

Hide Dependent

What is claimed is:
1. A non-transitory programmable storage device readable by a machine, tangibly embodying a program of instructions executable by a specific semiconductor-based
computational device situated in the machine to perform the method steps of a partial SRT (PSRT) division of a dividend X by a divisor D to obtain a quotient Q, said method
steps comprising:
causing a computer to obtain said dividend X and said divisor D;
representing said dividend X and said divisor D as a digital representation having a plurality of bits; and
performing iteratively the following steps until a desired accuracy of said quotient Q is achieved:
entering said divisor into a partial quotient select table (PQST), said PQST configured to output an inverse of said divisor 1/X′ to n+3 most significant digits, wherein n
equals logb R, b equals a number of the base system, and R equals a Radix, said PQST having a ROM size given by [2(n+2)×(n+2)] for b=2, and having a ROM size given by
[2(n+3)×(n+3)] for all other values of b;

https://patents.google.com/patent/US8725786B2/en?q=(high+radix)&oq=high+radix

1/20

2/14/26, 1:55 PM

US8725786B2 - Approximate SRT division method - Google Patents

multiplying said 1/X′ by a selected one P′ selected from:
1) on a first iteration: X, and
2) on a successive iteration: a partial remainder P from a previous cycle to compute as output a Q′;
truncating said Q′ to n most significant bits to yield a quotient Q″;
multiplying Q″ by said divisor D and subtracting the resultant Q″ D from P′ to compute said partial remainder P;
shifting said partial remainder P and said quotient Q″ by n digits; and
comparing said most recent Q″ to a previously computed Q″ to determine whether said desired accuracy has been achieved;
in the event that said desired accuracy has been achieved, performing a selected step of recording, outputting and displaying said quotient Q=said most recent Q″ in a digital
form; and
in the event that said desired accuracy has not been achieved, performing the steps identified above as iterative steps again until said desired accuracy has been achieved
and a selected step of recording, outputting and displaying said quotient Q=said most recent Q″ in a digital form is performed.
2. The programmable storage device of claim 1, wherein at least one calculation of one of said method steps is performed using a synchronous hardware logic element.
3. The programmable storage of claim 2, wherein at least one of said synchronous hardware logic elements is clocked on a positive edge of a clock and at least one of
said synchronous hardware logic elements is clocked on a negative edge of said clock.
4. The programmable storage of claim 2, wherein at least one of said synchronous hardware logic elements is clocked on both a positive edge of a clock and a negative
edge of said clock.
5. The programmable storage of claim 4, wherein at least one of said synchronous hardware logic elements is configured to provide said partial remainder on a clock
edge of a first slope so that as said partial remainder is available to calculate a next quotient bit on a successive clock edge complimentary to said first slope.
6. The programmable storage device of claim 1, wherein at least one calculation of one of said method steps is performed using an asynchronous hardware logic
element.
7. The programmable storage device of claim 6, wherein a two stage asynchronous divider multiplies 1/X′ by said partial remainder.
8. The programmable storage device of claim 1, wherein said method steps are performed using a linear convergence algorithm.
9. The programmable storage of claim 8, wherein said linear convergence algorithm converges a digit precision of said quotient to a pre-determined number of bits.
10. The programmable storage of claim 1, wherein said step of entering said divisor into a partial quotient select table (PQST) comprises entering said divisor into a
partial quotient select table (PQST) having entries of size n+3 bits and a following step of multiplying uses a n+3-bit multiplier.
11. The programmable storage of claim 1, wherein said divisor complies with the IEEE-754r standard.
12. The programmable storage device of claim 1, wherein said semiconductor-based computational device is an FPGA (field programmable logic array).
13. The programmable storage device of claim 1, wherein said semiconductor-based computational device is a microprocessor.
14. The programmable storage device of claim 13, wherein said PQST is disposed in a look up table on said microprocessor.
15. The programmable storage device of claim 13, wherein said PQST is calculated by software and stored on memory accessible by said microprocessor.
16. The programmable storage device of claim 1, wherein said PQST is calculated according to the following equation:

PQST[j]=b n+2 /a for a=b n+3 to b n+4−1, and j=a−b n+3.
17. An article of manufacture comprising:
a non-transitory computer usable medium having computer readable program code embodied therein in a non-transitory manner for causing a partial SRT (PSRT) division of
a dividend X by a divisor D to generate a quotient Q, the computer readable program code in said article of manufacture comprising:
computer readable program code for causing a computer to obtain said dividend X and said divisor D and to represent said dividend X and said divisor D as a digital
representation having a plurality of bits; and
computer readable program code configured to cause a computer to iteratively perform the following calculations until a desired accuracy of said quotient Q is achieved:
computer readable program code configured to cause a computer to obtain said dividend X and said divisor D, and to enter said divisor D into a partial quotient select
table (PQST), said PQST configured to output an inverse 1/X′ of said divisor to n+3 digits, wherein n equals logb R, b equals a number of the base system, and R equals a
Radix, said PQST having a ROM size given by [2(n+2)×(n+2)] for b=2, and having a ROM size given by [2(n+3)×(n+3)] for all other values of b;
computer readable program code configured to cause said 1/X′ to be multiplied by a selected one P′ of:
1) on a first iteration: X, and
2) on a successive iteration: a partial remainder P from a previous iteration to compute as output a Q′;
computer readable program code configured to cause said Q′ to be truncated to n most significant bits to yield a quotient Q″;
computer readable program code configured to cause said quotient Q″ to be multiplied by said divisor to produce a partial product Q″ D;
computer readable program code configured to cause said partial product Q″ D to be subtracted from P′ to compute said partial remainder P;

https://patents.google.com/patent/US8725786B2/en?q=(high+radix)&oq=high+radix

2/20

2/14/26, 1:55 PM

US8725786B2 - Approximate SRT division method - Google Patents

computer readable program code configured to cause said partial remainder P and said quotient Q″ to be shifted by n digits; and
comparing said most recent Q″ to a previously computed Q″ to determine whether said desired accuracy has been achieved:
in the event that said desired accuracy has been achieved, performing a selected step of recording, outputting and displaying said quotient Q=said most recent Q″ in a digital
form; and
in the event that said desired accuracy has not been achieved, performing the steps identified above as iterative steps again until said desired accuracy has been achieved
and a selected step of recording, outputting and displaying said quotient Q=said most recent Q″ in a digital form is performed.
18. A microprocessor-based computational device, comprising:
an input port and an output port in communication with a logic unit, said logic unit configured to control a memory, a multiplier, an adder/subtractor, a truncation register, a
shift register, and a comparator;
said input port configured to accept a dividend X and a divisor D, said dividend X and said divisor D each having a digital representation having a plurality of bits;
said output port configured to provide as output a quotient Q representing X/D to a desired accuracy;
said memory configured to hold a partial quotient select table (PQST), said PQST configured to output an inverse of said divisor 1/X′ to n+3 most significant digits, wherein n
equals logb R, b equals a number of the base system, and R equals a Radix in response to receipt of said divisor D, said PQST having a ROM size given by [2(n+2)×(n+2)] for
b=2, and having a ROM size given by [2(n+3)×(n+3)] for all other values of b;
said multiplier configured to multiply said 1/X′ by a selected one P′ selected from: on a first iteration: X, and on a successive iteration: a partial remainder P from a previous
cycle to compute as output a Q′;
said truncation register configured to truncate said Q′ to n most significant bits to yield a quotient Q″;
said multiplier configured to multiply Q″ by said divisor D to provide a resultant Q″ D;
said adder/subtractor configured to subtract the resultant Q″ D from P′ to compute said partial remainder P;
said shift register configured to shift said partial remainder P and said quotient Q″ by n digits;
said comparator configured to compare said most recent Q″ to a previously computed Q″ to determine whether said desired accuracy has been achieved: and
said logic unit configured to control the operation of the microprocessor-based computational device to cause it to perform a selected one of:
in the event that said desired accuracy has been achieved, terminating said calculation and causing said quotient Q=said most recent Q″ to be output from said output
port in a digital form; and
in the event that said desired accuracy has not been achieved, causing the performance of repeated iterative cycles of calculation until said desired accuracy has been
achieved, and then terminating said calculation and causing said quotient Q=said most recent Q″ to be output from said output port in a digital form.

Description
CROSS-REFERENCE TO RELATED APPLICATIONS
This application claims priority to and the benefit of co-pending U.S. provisional patent application Ser. No. 61/214,888, filed Apr. 29, 2009, which application is
incorporated herein by reference in its entirety.
FIELD OF THE INVENTION
The invention relates in general to a computer SRT division system and method and more particularly to a computer SRT division system and method using a new
approach for approximation.
BACKGROUND OF THE INVENTION
Floating point division systems and methods continue to be of interest in the art of computer processor design. Without floating point division, even the most capable
current computer processors could typically take 8 to 32 times more time to perform a routine division operation. Using less powerful modern processors, that time
could increase as much as one thousand times.
The Sweeny, Robinson and Tocher (SRT) floating point division method is widely used today to perform floating point division in commercial processors, including
microprocessors. The SRT method is relatively easy to implement and can calculate more than one digit of the answer for each computation cycle. However, to achieve
greater speeds using existing SRT methods, the size of a SRT component, the Quotient Selection Table (QST), needs to increase exponentially. Thus, according to the
present art, an increase in SRT speed also leads to an exponential increase in the need for memory.
What is needed therefore, is a more efficient SRT system and method that can yield increased floating point divisions speeds.
SUMMARY OF THE INVENTION
In one aspect, the invention relates to a program storage device readable by a machine, tangibly embodying a program of instructions executable by a specific
semiconductor-based computational device situated in the machine to perform the method steps of a partial SRT (PSRT) division of a dividend X by a divisor D to obtain
a quotient Q. The method steps include: causing a computer to obtain the dividend X and the divisor D; representing the dividend X and the divisor D as a digital
representation having a plurality of bits; and performing iteratively the following steps until a desired accuracy of the quotient Q is achieved: entering the divisor into a
partial quotient select table (PQST) table, the PQST table configured to output an inverse of the divisor 1/X′ to n+3 most significant digits, wherein n equals logb R, b
equals a number of the base system, and R equals a Radix; multiplying the 1/X′ by a selected one P′ selected from: 1) on a first iteration: X, and 2) on a successive
iteration: a partial remainder P from a previous cycle to compute as output a Q′; truncating the Q′ to n most significant bits to yield a quotient Q″; multiplying Q″ by the
divisor D and subtracting the resultant Q″D from P′ (P′−Q″D) to compute the partial remainder P; shifting the partial remainder P and the quotient Q by n digits; and
comparing the most recent Q″ to a previously computed Q″ to determine whether the desired accuracy has been achieved: in the event that the desired accuracy has been

https://patents.google.com/patent/US8725786B2/en?q=(high+radix)&oq=high+radix

3/20

2/14/26, 1:55 PM

US8725786B2 - Approximate SRT division method - Google Patents

achieved, performing a selected step of recording, outputting and displaying the quotient Q in a digital form; and in the event that the desired accuracy has not been
achieved, performing the steps identified above as iterative steps again until the desired accuracy has been achieved and a selected step of recording, outputting and
displaying the quotient Q in a digital form is performed.
In one embodiment, the step of entering the divisor into a partial quotient select table (PQST) table includes entering the divisor into a partial quotient select table (PQST)
table having about 2n+3 entries.
In another embodiment, at least one calculation of one of the method steps is performed using a synchronous hardware logic element.
In yet another embodiment, at least one of the synchronous hardware logic elements is clocked on a positive edge of a clock and at least one of the synchronous
hardware logic elements is clocked on a negative edge of the clock.
In yet another embodiment, at least one of the synchronous hardware logic elements is clocked on both a positive edge of a clock a negative edge of the clock.
In yet another embodiment, at least one of the synchronous hardware logic elements is configured to provide the partial remainder on a clock edge of a first slope so that
as the partial remainder is available to calculate a next quotient bit on a successive clock edge complimentary to the first slope.
In yet another embodiment, at least one calculation of one of the method steps is performed using an asynchronous hardware logic element.
In yet another embodiment, a two stage asynchronous divider multiplies 1/X by the partial remainder.
In yet another embodiment, the method steps are performed using a linear convergence algorithm.
In yet another embodiment, the linear convergence algorithm converges a digit precision of the quotient to a pre-determined number of bits.
In yet another embodiment, the step of entering the divisor into a partial quotient select table (PQST) table includes entering the divisor into a partial quotient select table
(PQST) having entries of size n+3 bits and a following step of multiplying uses a n+3-bit multiplier.
In yet another embodiment, the divisor complies with the IEEE-754r standard.
In yet another embodiment, the semiconductor-based computational device is an FPGA (field programmable logic array).
In yet another embodiment, the semiconductor-based computational device is a microprocessor.
In yet another embodiment, the PQST is disposed in a look up table on the microprocessor.
In yet another embodiment, the PQST is calculated by software and stored on memory accessible by the microprocessor.
In yet another embodiment, the PQST is calculated according to the following equation: PQST[j]=bn+2/a for a=bn+3 to bn+4−1, and j=a−bn+3.
In another aspect, the invention relates to an article of manufacture including: a computer usable medium having computer readable program code embodied therein for
causing a partial SRT (PSRT) division of a dividend X by a divisor D to generate a quotient Q, the computer readable program code in the article of manufacture including:
computer readable program code for causing a computer to obtain the dividend X and the divisor D and to represent the dividend X and the divisor D as a digital
representation having a plurality of bits; and computer readable program code configured to cause a computer to iteratively perform the following calculations until a
desired accuracy of the quotient Q is achieved: computer readable program code configured to cause a computer to obtain the dividend X and the divisor D, and to enter
the divisor D into a partial quotient select table (PQST), the PQST configured to output an inverse 1/X′ of the divisor to n+3 digits, wherein n equals logb R, b equals a
number of the base system, and R equals a Radix; computer readable program code configured to cause the 1/X′ to be multiplied by a selected one P′ of: 1) on a first
iteration: X, and 2) on a successive iteration: a partial remainder P from a previous iteration to compute as output a Q′; computer readable program code configured to
cause the Q′ to be truncated to n most significant bits to yield a quotient Q″; computer readable program code configured to cause the quotient Q″ to be multiplied by the
divisor to produce a partial product Q″D; computer readable program code configured to cause the partial product Q″D to be subtracted from P′ (P′−Q″D) to compute the
partial remainder P; computer readable program code configured to cause the partial remainder P and the quotient Q″ to be shifted by n digits; and comparing the most
recent Q″ to a previously computed Q″ to determine whether the desired accuracy has been achieved: in the event that the desired accuracy has been achieved,
performing a selected step of recording, outputting and displaying the quotient Q in a digital form; and in the event that the desired accuracy has not been achieved,
performing the steps identified above as iterative steps again until the desired accuracy has been achieved and a selected step of recording, outputting and displaying
the quotient Q in a digital form is performed.
The foregoing and other objects, aspects, features, and advantages of the invention will become more apparent from the following description and from the claims.
BRIEF DESCRIPTION OF THE DRAWINGS
The objects and features of the invention can be better understood with reference to the drawings described below, and the claims. The drawings are not necessarily to
scale, emphasis instead generally being placed upon illustrating the principles of the invention. In the drawings, like numerals are used to indicate like parts throughout
the various views.
FIG. 1 shows Pan's table of SRT QST size in terms of ROM size and PLA area.
FIG. 2 shows the results of an exemplary simulation for a Radix-64 approximate SRT Divider having a multiplier stage.
FIG. 3 shows one exemplary SRT architecture.
FIG. 4 shows a block diagram of an overlapped QST architecture.
FIG. 5 shows a block diagram of overlapped Partial remainder selection architecture.
FIG. 6 shows a simplified block diagram of one exemplary embodiment of a synchronous PSRT divider.
FIG. 7 shows a simplified block diagram of one exemplary embodiment of an asynchronous PSRT divider.
FIG. 8 shows a more detailed block diagram of a clocked type PSRT divider similar to that shown in FIG. 6.
FIG. 9 shows a block diagram of the 16-bit Radix-256 PSRT divider block of FIG. 8.
FIG. 10 shows a graphical representation of the PQST compared to Pan's study and traditional QST sizes.

https://patents.google.com/patent/US8725786B2/en?q=(high+radix)&oq=high+radix

4/20

2/14/26, 1:55 PM

US8725786B2 - Approximate SRT division method - Google Patents

FIG. 11 shows a comparison between PSRT size and Pan's combined RQST+QHT only.
FIG. 12 shows a graphical representation of the data presented in Table 5.
FIG. 13 shows a graphical representation of the data presented in Table 6.
FIG. 14 shows a graphical representation of the data presented in Table 7.
FIG. 15 shows a graphical representation of the data presented in Table 8.
FIG. 16 shows a graphical representation of the data presented in Table 9.
FIG. 17 shows a graphical representation of the data presented in Table 10.
FIG. 18 shows a graphical representation of the data presented in Table 11.
FIG. 19 shows a floor plan for a routed Virtex-II Pro Design.
FIG. 20 shows a floor plan for a routed Spartan-3 design.
FIG. 21 shows an exemplary Modelsim® wave window.
FIG. 22 shows a screenshot of a Modelsim® simulation illustrating an average case.
FIG. 23 shows a screenshot of a Modelsim® simulation illustrating a worst-case scenario.
FIG. 24 shows a screenshot of a Modelsim® simulation illustrating what occurs when the dividend and divisor are equal.
DETAILED DESCRIPTION OF THE INVENTION
This detailed description is presented in six parts. Part I presents a brief overview of SRT methods including an introduction to the inventive SRT system and method, the
partial SRT or “PSRT”. Part II describes division algorithms in more detail. Part III explains the SRT division method in more detail. Part IV describes a theoretical analysis
(proofs) and error bounds for the inventive PSRT system and method, part V describes the PSRT system and method in further detail including architectural approaches
for implementation, and part VI describes exemplary PSRT Test Results and Comparisons.
As was noted in the background, division by SRT methods and other methods is well known in the art. The inventive PSRT method described in parts IV and V of the
detailed description discloses a new method that has several beneficial attributes. However, while perhaps not benefiting from the several beneficial attributes of the
PSRT method, other pre-existing means to accomplish floating point division, including a wide variety of SRT methods, remain available to those not choosing to use the
new PSRT method. It is thus emphasized that there are presently numerous ways to accomplish floating point multiplication and division using well known computer
circuits and programmed computers. The present invention is a system for performing improved computation using specific hardware and software operating thereon,
that provides advantages in terms of cost, speed and improved operation.
Part I: Introduction to SRT Methods and the Inventive SRT Method
One problem with existing SRT methods is that in order to divide much faster, or to get more digits of the answer at a time, the size of the Quotient Table “increases unmanageably.” (Pan, et. al., “High-Radix SRT Division with Speculation of Quotient Digits”, Proceedings of the 1995 International Conference on Computer Design: VLSI in
Computers and Processors table of contents, Page 479, IEEE Computer Society, Washington, D.C.). For example, in the table of FIG. 1, it can be seen that the size of the
QST increases from ½ kB for 2 digits at a time (Radix-4), to 16 MB for 5 digits at a time, an exponential rate of increase. Pan's table of FIG. 1 shows how the exponential
size rate increases as the Radix, or effective bits computed at a time increases (B=2n where n=the number bits computed at a time). In the table of FIG. 1, β is the Radix, a
is the number of digits at a time, Np is the number of bits of the partial remainder, ND is the number of bits of the divisor, Nq is the number of bits of the quotient, ROM
(read only memory) bits is the size of the QST, and PLA (programmable logic array) np is the number of product lines, in the PLA Area. Where the QST increases
exponentially in size as in the SRT method of FIG. 1, increasing, the speed of division can be problematic in both embedded systems and modern processors.
One way to reduce the size of the Quotient Selection Table (QST) is to use fewer bits. Since most methods use convergence to approximate, the QST often needs many
extra bits in the dividend/divisor field known as a redundant-digit representation. For example, 10/9 might read 1.11100101. Both the dividend and divisor often need
more bits in their representation since the SRT divider must “anticipate” how the quotient may be rounded. Instead of 10 and 9, we might also have 10.83 and 9.57 in the
QST to help with rounding. Convergence does not yield results that are exact. Using the convergence method, the percentage error is bounded by a certain degree.
(Kapur, et. al., “Mechanizing verification of arithmetic circuits: SRT division. In FSTTCS”, Volume 1346 of LNCS, pg. 103-122, 1997). Therefore, convergence alone fails to
significantly reduce the size of this QST table.
The recent availability of built-in multipliers in many Field Programmable Gate Array (FPGA) technologies is one reason for a renewed interest in multiplicative division
algorithms. While division is a rather slow operation, multiplicative algorithms offer the benefits of lower delay, usually at the expense of increased area.
The inventive apparatus and method described herein offers the benefits of low latency, but not at the expense of increased area. Instead, it uses a combination of linear
convergence and reciprocal methods to significantly reduce hardware complexity for high radix division. For example, in a Radix-64 implementation of an asynchronous
16-bit floating-point unit, the method uses only 201 Look Up Tables (LUTs) with built-in multipliers, and 410 LUTs without built-in multipliers. The new PSRT system and
method was tested using a low-end FPGA hardware configuration that costs, about $30 running an exemplary algorithm. One exemplary embodiment was found to have
a worst-case delay of under 38 ns, and a peak power consumption of only 60 mW. Such results are relatively fast by today's standards, especially given that a current 32bit fixed-point divider typically has an end-to-end delay of 169 ns and needs at least 1690 LUTs. (Sorokin, “Implementation of high-Speed Fixed-Point Dividers on FPGA”,
Journal of Computer Science and Technology, Vol. 6 No. 1, pgs. 8-11, April 1996).
The new SRT system and method, the partial SRT (PSRT) system and method using a new approach for approximation, is described herein. As described and proven
below, if two numbers are known out to their first N digits, then the first N-2 digits of their product or division can also be known. This statement allows an assumption
that the dividend is always 1, and thus a multiplier can be used to multiply the quotient from the QST to get the correct quotient.
EXAMPLE 1
Consider two numbers a=123.450 and b=456.780, and their product, a*b=56389.491. We want to know the product of c=123.456 and d=456.780 or c*d. Since we know
that the most significant five digits of a and b are equal to c and d respectively, we can assume the first three digits of a*b are equal to c*d, or that c*d=563XX.XXX. The
product of c*d is 56392.231680.

https://patents.google.com/patent/US8725786B2/en?q=(high+radix)&oq=high+radix

5/20

2/14/26, 1:55 PM

US8725786B2 - Approximate SRT division method - Google Patents

The algorithm used in the new system and method lets us speculatively round up, and correct later. The rounding is relatively simple because the algorithm can round up
only in a certain decimal place. If the result is incorrect (i.e. the new trial dividend is negative), then the divisor is shifted and added back to the new trial dividend before
the next stage. Thus the QST can be reduced from a table with 4(2N*2) entries, to only 2N+3 entries, an exponential difference (an exponential reduction in the size of the
QST).
In addition, the algorithm used in the new system and method can also reduce the number of bits in the QST (reducing the size of the QST), and the only added cost is a
low-order multiplier, usually 10 bits or less. The additional delay for the multiplier is very low, and the QST size can be reduced from 16 MB, to under 3 kB (2.56 kB), a
factor of reduction of over 6000. Pan's relatively sophisticated hardware in “High-Radix SRT Division with Speculation of Quotient Digits” reduced a QST table to 22 kB.
For the same Radix, the inventive PSRT system and method uses a 2.56 kB QST, a QST more than 7 times smaller than is needed in Pan's design. As the number of bits
increases, a PSRT implementation becomes much smaller compared to both existing and theoretical SRT implementations since the QST size does not increase
exponentially. Thus, the approximation algorithm used in the PSRT system and method greatly simplifies the approximations used in SRT, resulting in a much smaller chip
size (area) and potentially much greater speeds because of the ability to compute more digits of the answer at a time.
Simulations
Exemplary feasibility tests were performed using a hardware simulator. The simulation was performed for a Radix-64 approximate SRT Divider having a multiplier stage
using Taiwan Semiconductor Manufacturing Company (“TSMC”) 0.35 μm technology. TSMC also provides as mainstream technology 0.25 μm, 0.22 μm, 0.18 μm and
0.15 μm technology, and as advanced technology 0.45 nm, 0.55 nm, 0.65 μm and 0.90 μm technology. FIG. 2 shows the results of the simulation. Using dated technology
(0.35 micron), the extra delay from the multiplier is low. It is further contemplated that by using more recent technology (65 nm), the speed will improve significantly.
Using various sizes, the additional delays ranged from 5.6-10 ns, for Radix-64 (6 bits) to Radix-1024 (11 bits).
This issue of QST exponential increase has previously limited the speed of SRT. For example, until recently, Pentium processors manufactured by the Intel Corporation
computed only 2 digits at a time, before computing three digits of the answer at a time. By contrast, the QST can be 1000 or more times smaller than prior art QSTs by
using the inventive PSRT system and method since it uses a different approximation scheme. Because an algorithm used in the inventive system and method knows the
relative error in digits, as a digit-by-digit algorithm, it has a simpler implementation than existing convergence-based schemes, and thus results in a smaller chip size
(smaller area). The improved SRT (PSRT) design can be used to improve the speed of current processors. The PSRT design can also be used in embedded systems
applications including cryptography because of its ability to produce more digits of the solution while using less physical chip (integrated circuit) area.
Thus, it can be seen that the approximation algorithm used in the PSRT system and method results in a much smaller QST. While in the prior art, as the divider processes
more bits at a time, the QST table increases exponentially, using the new PSRT approximation algorithm, the size of the QST increases linearly, providing an exponentially
smaller QST. Since the size of the QST is a main factor in cost and speed, the new technology can potentially greatly reduce the cost, and improve the speed of SRT
division. Prior art (SRT) is currently used in many commercial processors ranging from Intel, to embedded systems. Currently the speed is limited to the number of bits
which can be processed at a time, usually, 2-3 bits at a time. By contrast, PSRT technology can be used to implement high-speed division with relatively low area. Also, it
is contemplated that cryptographic communications protocols could benefit from a technology which can process 10 or more bits at a time, due to bit encoding and
communication rates.
Part II: Division Algorithms and Linear Convergence Algorithms
There are currently several classes of division algorithms each with their own advantages and disadvantages. There are two main categories of division algorithms:
quadratic convergence and linear convergence algorithms. Both classes of algorithms have a function that is repeated to yield the quotient. Since the function is
recursive, current literature, including Sorokin, refers to each repetition of the function as an iteration. The algorithm used in the PSRT system and method described
herein is a linear convergence algorithm.
Quadratic convergence algorithms double the amount of quotient digits calculated for each iteration. Current literature typically refers to each quotient digit calculated as
a retired digit. The algorithms start from an approximation of the divisor's reciprocal, and repeat a recursive function that equals zero when the divisor is accurate to a
given precision. Then, the divisor's reciprocal is multiplied by the dividend to yield the quotient. (Oberman, et. al., “Division Algorithms and Implementations”, IEEE
Transactions on Computer, vol. C-46, pgs. 833-854, 1997).
Two commonly used quadratic convergence algorithms are the Newton-Rhapson, and Goldschmidt's algorithm. The Newton-Rhapson algorithm uses Newton's method.
Newton's method is based upon the equation D*1/D−1=0; where D is the divisor. Therefore, if X0 is the initial divisor, the function of Eq. 1 (the Newton-Rhapson method) is
repeated until F(X)=0.

X i+1 =X i−(F(X i)/F′(X i))

Eq. (1)

where F(Xi)=Xi*1/Xi−1.
Goldschmidt's algorithm is similar to Newton-Rhapson, but it uses a series expansion. Instead of finding the divisor reciprocal, both the dividend and divisor are
multiplied by a scaling factor until the divisor converges to 1, and the dividend converges to the quotient.
There are several pros and cons regarding quadratic convergence methods. For example, while quadratic convergence algorithms compute the quotient relatively fast,
they require many multiplications. Usually, the additional multipliers require additional hardware. In addition, the precision of the quotient is not exact, since the
algorithms use convergence and the final result is correct only within a certain range. Typically such algorithms are modified so that this error is within 1 Unit in the Last
Place (ULP), also known in the art as “1 ULP.” Therefore many designs that use quadratic convergence algorithms add extra error correction hardware to detect if the
quotient's error is out of bounds, and then to correct the quotient.
Linear convergence algorithms calculate the quotient one digit at a time. The number of bits calculated at a time is directly proportional to the Radix. The higher the
Radix, the more bits of the quotient the algorithm calculates per iteration. The relationship between the Radix, R, and number of bits per digit is:

R=2r

Eq. (2)

In Eq. 2, r is the number of bits per digit for Radix-R. For example, for Radix-64, R is 64, and since 64=26, we know that r=6, and that the divider will calculate 6 bits of the
quotient each iteration.
There are two types of linear convergence algorithms: restoring and non-restoring. In both cases, the general formula for division can be shown as the SRT Equation:

P j+1 =rP j −Q j+1 *d

Eq. (3)

where Pj+1 is the next partial remainder, Pj is the current partial remainder, r is the number of bits in the Radix, Qj is the current quotient digit, and d is the divisor. The first
Partial Remainder, P0, is the dividend, and the final quotient, Q, is the sum of all Qj.

https://patents.google.com/patent/US8725786B2/en?q=(high+radix)&oq=high+radix

6/20

2/14/26, 1:55 PM

US8725786B2 - Approximate SRT division method - Google Patents

During each iteration, the divisor is multiplied by the quotient digit, and subtracted from the partial remainder. Before subtraction, the partial remainder is shifted left by r
bits (i.e. one digit), where r is the number of bits in the Radix. (Harris, et. al., “SRT division architectures and implementations”, Proceedings of the 13th IEEE Symposium
on Computer Arithmetic, July 1997, pgs. 18-25). This process is repeated N times, where N is the length of the dividend, divided by the length of the Radix in bits. For
example, a 32-bit divider using Radix-256 division would take 32/8=4 iterations. Often times, the quotient digit is estimated, so the next partial remainder may be
negative. For example, in base 10, Radix 10, if the dividend is 120, the divisor is 13, and the quotient digit is I then the first partial remainder would be 120−(10*13)=−10. In
restoring division, the divisor is added back to the partial remainder, and the quotient, Qj is decreased by one. So for the restoring case, 10*13 would be added back to
−10 to yield 120. In non-restoring division, the next partial remainder remains negative, but in the next iteration, the quotient digit, Qj+1 will also be negative. Allowing the
quotient digit to take negative values instead adds the product of the quotient digit and divisor to the partial reminder, Pj. As a result, the final quotient Qj is positive.
(Fenwick, “High-Radix Division with Approximate Quotient Digit Estimation”, Journal of Universal Computer Science, vol. 1, no. 1, pgs 2-22, January 1995). In the example
above, if the first partial remainder is −10, then in the next iteration, rPj=−100, and Qj+1=−8. So the final partial remainder would be calculated as −100−(13*−8)=4, which
is the remainder of 100 divided by 13. The final quotient, Q, is the sum of all quotient digits in their respective places. In this non restoring case Q=10+−0.8=9. Notice that
Q was rounded down to the nearest integer, and that the second partial remainder −8 was shifted right by one since quotient digits are added in their respective places.
The most popular linear convergence division algorithm used today is the SRT method, developed it in the 1950s and named after its three inventors Sweeney, Robertson
and Tocher. The SRT method is non-restoring, and uses a Quotient Selection Table (QST) to select each quotient digit based upon both the dividend and divisor. During
each iteration, the dividend is the Partial Remainder, Pj, the divisor is d, and the quotient digit is Qj.
Although SRT is relatively fast when compared to other linear convergence methods, the size of the QST grows quadratically as the Radix increases. The QST expands
exponentially since it is dependent upon both the dividend and divisor. Often, SRT measures only the first few bits of the dividend and divisor, usually log2R bits for RadixR. However, for Radix-R (e.g. for Radix-64, R=64, r=6 where r=log2R), this results in a table with R2 entries (4,096 for R=64). In addition, each entry must have a certain
number of bits for the quotient digit for accuracy. Knuth shows that for Radix-4, we at least need r+2 bits of the quotient, where r=2; the number of bits in the Radix.
Part III: SRT Implementations
The basic SRT algorithm (Eq. 3) was described above in Part I. Several implementations and improvements to the basic SRT method are now described. At the most
basic level, SRT resembles long division, using an arbitrary base, an arbitrary radix, and an arbitrary number of bits, rather than base 10 standard division that we learned
in school as children. At the beginning of each iteration, the quotient digit is selected from the QST. The QST is typically implemented on a read only memory (ROM), or as
a Look-Up-Table (LUT). After the quotient digit is selected, it is used to select the quotient-divisor product Qj*d using a multiplexer. Next, the quotient-divisor product is
subtracted from the partial remainder using an adder. The process is then repeated until the partial remainder is less than the divisor.
The block diagram of a basic SRT divider of FIG. 3 shows one exemplary SRT architecture. In FIG. 3, the quotient select logic (“QLSC”) is the QST, and the quotient digit,
q[i+1] is used to select the next partial remainder, PR[i+1], using a multiplexer. This figure uses redundant digit notation. If the maximum value for Q Radix-R is α, then the
multiplexer can select any multiple of the divisor, D, from −αD to αD to subtract from the partial remainder PR[i] to form the next partial remainder, PR[i+1].
For example, if the divided is 1024 base 2, and the divisor is 544 base 2, then using Radix-4 division, the maximum value of Q Radix-4 is 2. Since the maximum value for
Radix-4 is 2, a=2, and since the divisor is 544, D=544. Depending upon the quotient digit, the multiplexer can select any value from −2*544 to 2*544 to subtract from the
partial remainder PR[i]. Assuming this is the first iteration, the partial remainder PR[0] is the dividend, 1024, the first quotient digit, q[1] is 1. Therefore, using the
multiplexer pictured in FIG. 3, subtract 544*1 from 1024 to yield the next partial remainder PR[1], which equals 1024−544*1=480.
Over the years, designers have made improvements to SRT that make it faster, and more efficient. For example, one improvement on the basic SRT method uses prescaling of the dividend and divisor. To reduce the size of the QST, many designs pre-scale both the dividend and divisor to a certain range. Usually, the range for the
dividend is 1<P<2, and the divisor's range is 0.5<D<1. Pre-scaling can be done using a priority encoder and a left shifter. The priority encoder and shifter left shift the
dividend to the leftmost position, and shift the divisor to the next to leftmost position. (Ercegovac, et. al., “Very high Radix division with selection by rounding and
prescaling”, IEEE Transactions on Computers, vol. 43, pgs. 909-918, August 1994). For example, assume that the operand width is 4 bits, and that using this system, 2 is
1000, and 1 is 0100, and ½ is 0010 in binary. If the dividend is 0010, the divisor is 0011, and the width of the operands is 4 bits, then the dividend would be shifted to
0100, and the divisor will remain at 0011. Afterwards, the result is right shifted by the difference of the number of positions the dividend was shifted, and the number of
positions the divisor was shifted. In the previous example, the quotient will be shifted by 1−0=1 position.
Left shifting both the dividend and divisor decreases the size of the QST significantly since the quotient entries in the QST are the same precision. With left shifting, the
quotient can have a range of only 2≦Q<4, numbers that have the same binary length. In addition, since the leftmost bit of the dividend and divisor are always ‘1’, the
leftmost bits can be omitted from the QST. However, omitting the leftmost ‘1’ from the QST adds extra delay since the most significant ‘1’ of the quotient is added back
later.
Another improvement on the basic SRT method uses redundant digit representation. If the SRT divider uses full-width carry-propagate adders, the SRT divider has a high
latency when the product of the divisor and quotient is subtracted from the dividend. Redundant digit notation reduces this delay by allowing the divider to use carry save
adders instead, reducing the delay caused by carry propagation. Redundant digit notation uses signed 2's complement numbers, and contains more bits than required in
Radix-R. For example, a Radix-4 redundant digit notation would contain at least 3 bits of the quotient. Because of the added precision, the quotient can be approximated,
reducing the size of the QST.
There are two types of redundant digit notation. For Radix-R, where R=2r:
1) A maximally redundant digit set contains {−(R-1), −(R-2) . . . 0 . . . (R-2), (R-1)}.
2) A minimally redundant digit set contains {−(R/2-1), −(R/2-2) . . . 0 . . . (R/2-2), (R/2-1)} where all digits in the set are in signed-2's complement form.
Harris et al. show that using a maximally redundant digit set in Radix-4 division is 20% faster, and 50% smaller than a minimally redundant digit set at the expense of
additional delay and area of the QST.
Another improvement on the basic SRT method uses Quotient Digit Speculation. In 1995, Pan et al. published a study which shows how the size of the QST increases
with Radix. (“High-Radix SRT division with speculation of quotient digits”, Proceedings of IEEE International Conference on Computer Design: VLSI in Computers and
Processors, pages 479-484, October 1995). Table 1 shows the results from the studies for radices 8-32:
TABLE 1
Size of ROM
Radix

(in bits)

https://patents.google.com/patent/US8725786B2/en?q=(high+radix)&oq=high+radix

7/20

2/14/26, 1:55 PM

US8725786B2 - Approximate SRT division method - Google Patents

4

0.5K

8

48K

16

1M

32

16 M

In the study, the authors split the QST into two smaller tables called the RQST and QHT that estimated the quotient digit, and then corrected the prediction if the quotient
digit was incorrect. Table 2 shows the results from the studies for Radix 8-32 using combined RQST and QHT Size:
TABLE 2
Size of ROM
Radix

(in bits)

8

0.5K

16

4K

32

22K

Pan showed that the max error bound from approximating the quotient is −1. So, while the quotient is calculated, the correction hardware runs in parallel. Then, before
retiring the quotient digit, a multiplexer is used to select between the speculated quotient Q, and the corrected quotient, Q+1. Although the size of the QST in the study
decreased, its size still increases quadratically with Radix.
Another improvement on the basic SRT method uses quotient overlapping. Some designs speculate the quotient, then select the correct quotient based upon the partial
remainder, Pj. This approach decreases delay at the expense of area since the QST must return more than one entry. Returning more than one entry requires either
duplicating the QST, or enlarging the QST so that it returns more than one entry. FIG. 4 from Harris shows a block diagram of an overlapped QST architecture.
Another improvement on the basic SRT method uses partial remainder overlapping. Some designs speculate the partial remainder, then select the correct partial
remainder based upon the quotient digit, Qj. All of the possible partial remainders are computed, then a multiplexer is used to select the correct partial remainder. This
saves critical path delay at the expense of added hardware due to computation of all possible partial remainders. FIG. 5 from Harris shows a block diagram of an
architecture using overlapped partial remainder selection. Yet another improvement on the basic SRT method uses hybrid overlapping. A combination of quotient digit
speculation, quotient overlapping and partial remainder overlapping can further reduce the latency of SRT computations.
All of the SRT methods and improvements discussed above make trade-offs between area and latency. Duplicating the QST would cause an increase in area (where area
can be defined in terms of units) as is done in quotient overlapping. Also, for example, while a redundant digit set can decrease delay (since the SRT divider can use carry
save adders) it increases the area of the QST. And, while speculation of the quotient and partial remainder reduce critical path delay, speculation of the quotient and
partial remainder requires additional error correction hardware for the quotient digit. In addition, the size of the QST in bytes still increases quadratically with Radix. Since
increased area and components usually leads to increased power consumption using VLSI and FPGA technologies, power is also an issue with existing SRT designs.
By contrast, the inventive improved SRT system and method and associated algorithms as presented below and herein, improve both the speed and area of SRT by using
different estimation logic. Instead of approximating with respect to the divisor or dividend, the algorithm approximates with respect to the number of bits in the Radix.
Knowing the exact quotient reduces hardware complexity since the algorithm does not need as much of the correction hardware as required in other linear convergence
and quadratic convergence algorithms. Instead, the algorithm increases the quotient digit precision so that it is correct to a certain number of bits. Furthermore, the size
of the QST decreases by a factor of at least r, where r is the number of bits in the Radix. So instead of a table with (2r)2 entries, the QST has only 2r+3 entries, and a small
r+3-bit multiplier.
Part IV: PSRT Theoretical Analysis
Since most approximation algorithms estimate error with respect to the divisor, this description first gives a theoretical analysis for the algorithm, and shows that the
error of an approximation can be stated in respect to the number of bits in the Radix, r. The algorithm used in the inventive system and method is named “Partial SRT”
(PSRT) since it approximates the dividend, divisor and quotient. In the theoretical analysis which follows, a partial number refers to the leftmost n digits of a number,
where n is a positive integer. After the theoretical analysis, an additional assumption is described that eliminates the need for a redundant digit set. Lastly, an
implementation of a PSRT system and method, including how PSRT implementation differs from SRT, is described.
Theorem 1: The Leftmost Equality Theorem
Theorem 1, the Leftmost Equality Theorem (LET), is a new theorem. LET differs from the byte division approximation since it gives the error in digits, instead of as a
percentage, or fraction of the dividend or divisor. LET states that if the leftmost n digits of two numbers are known, then we at least know the leftmost n−2 bits of the
product. In Radix-R, which has r bits per quotient digit, PSRT uses LET to approximate the reciprocal of the divisor to r+3 bits, and then multiplies the divisor's reciprocal
by the first r+3 bits of the dividend to yield the quotient digit. The resulting quotient digit has at least r correct digits which is the Radix width. After each iteration, the
PSRT method shifts the partial quotient left by exactly r bits with no need for correction cycles since the quotient has at least r bits that are correct.
The second theorem, Theorem 2, modifies the partial quotient so that the estimated quotient is always less than or equal to the actual quotient. Then, the second
theorem shows that the max error of the quotient is equivalent to the error in the LET. Therefore, the estimated quotient for Radix R is still correct to r bits. Both theorems
can be proved as follows below.
Beginning with Theorem 1, the Leftmost Equality Theorem (LET) is proven as follows:
Statement:
If we know the leftmost n digits of two numbers, at least the leftmost n−2 digits of the product are known.
Proof:
First, the dividend and divisor reciprocal are written in a sum of products form, also known as digit decomposition. Each digit pre-multiplies the base, b, raised to some
power. Now, each number is split into left and right parts.

Let⁢
: 𝑃 = 𝑃𝐿 + 𝑃𝑅 ⁢

𝑋 = 𝑋𝐿 + 𝑋𝑅 𝑃 𝐿 = ∑
⁢

𝑆𝑝 + 𝐿𝑝 - 1
𝑘 = 𝑆𝑝 + 𝐿𝑝 - 𝑛

𝑘

𝑃𝑘 ⁢
⁢
𝑏 ⁢

𝑋𝐿 = ∑
⁢

𝑆𝑥 + 𝐿𝑥 - 1
𝑘 = 𝑆𝑥 + 𝐿𝑥 - 𝑛

𝑋𝑘 ⁢
⁢
𝑏

𝑘

𝑃𝑅 = ∑

𝑆𝑝 + 𝐿𝑝 - 𝑛 - 1
𝑘 = 𝑆𝑝

𝑘

𝑃𝑘 ⁢
⁢
𝑏 ⁢

𝑋𝑅 = ∑
⁢

𝑆𝑥 + 𝐿𝑥 - 𝑛 - 1
𝑘 = 𝑆𝑥

𝑋𝑘 ⁢
⁢
𝑏

𝑘

Here P is the dividend, X is the divisor's reciprocal, Sp and Sx are the start digit places of P and X respectively, Lp and Lx are the lengths of P and X in digits, and b is the

https://patents.google.com/patent/US8725786B2/en?q=(high+radix)&oq=high+radix

8/20

2/14/26, 1:55 PM

US8725786B2 - Approximate SRT division method - Google Patents

base. All numbers are integers. Each Pk and Xk is the digit pre-multiplying the kth power of b, and has an absolute value strictly less than the base, b. Next, there is an
estimated dividend and divisor's reciprocal P′ and X′. The first n digits of P′ and X′ are equal to the first n digits of P and X respectively. In other words, the left parts of P′
and X′ are equal to the leftmost parts of P and X.

′

′

′

Let⁢
: 𝑃 = 𝑃𝐿 + 𝑃𝑅 ⁢
′

′

𝑋 = 𝑋𝐿 + 𝑋𝑅 𝑃 𝐿 = ∑
⁢

𝑆

𝑝

+𝐿

𝑝

-1

𝑘

𝑘 = 𝑆𝑝 + 𝐿𝑝 - 𝑛

𝑃𝑘 ⁢
⁢
𝑏 ⁢

𝑋𝐿 = ∑
⁢

𝑆

𝑥

+𝐿

-1

𝑥

𝑘 = 𝑆𝑥 + 𝐿𝑥 - 𝑛

𝑋𝑘 ⁢
⁢
𝑏

𝑘

′

𝑃𝑅 = ∑

𝑆

𝑝

+𝐿

𝑝

-𝑛-1

𝑘 = 𝑆𝑝

′

𝑘

′

𝑃𝑘 ⁢
⁢
𝑏 ⁢

𝑋𝑅 = ∑
⁢

𝑆

𝑥

+𝐿

𝑘 = 𝑆𝑥

𝑥

-𝑛-1

′

𝑋𝑘 ⁢
⁢
𝑏

𝑘

′

𝑃𝑅 ≠ 𝑃𝑅 ,

𝑋𝑅 ≠ 𝑋𝑅
⁢

The quotient is the product of the dividend, and divisor reciprocal, or Q=PX, where Q is the quotient. Next, Q, and Q′, the products of PX and P′X′ respectively are calculated
so that some comparisons can be made.

Let⁢
: 𝑄 = PX = ( 𝑃𝐿 + 𝑃𝑅 ) ⁢( 𝑋𝐿 + 𝑋𝑅 ) 𝑄 = ( 𝑃𝐿 ⁢
𝑋𝐿 ) + ( 𝑃 𝐿 ⁢
𝑋𝑅 + 𝑋𝐿 ⁢
𝑃𝑅 ) + ( 𝑃𝑅 ⁢
𝑋𝑅 )
′

′

′

′

′

′

′

Left⁢
⁢

′

′

Middle⁢
⁢
Right
⁢

′

Likewise, ⁢
𝑄 = 𝑃 ⁢
𝑋 = ( 𝑃𝐿 + 𝑃𝑅 ) ⁢( 𝑋𝐿 + 𝑋𝑅 ) 𝑄 = ( 𝑃𝐿 ⁢
𝑋𝐿 ) + ( 𝑃 𝐿 ⁢
𝑋𝑅 + 𝑋𝐿 ⁢
𝑃𝑅 ) + ( 𝑃𝑅 ⁢
𝑋𝑅 )

Left⁢
⁢

Middle⁢
⁢
Right
⁢

Notice that Q, and Q′ are split into left, middle, and right terms. When Q is compared to Q′, the left parts are equal. However, the middle and right terms of Q′ and Q differ.
To determine the maximum error, first determine the worst case values for the difference between Q and Q′, the estimated quotient. The worst case difference for Q and
Q′ occurs when the difference between each digit in the middle and right terms of P and P′ and X and X′ are equal to b−1 since the maximum value for each digit base b is
b−1. There are two cases where the worst case occurs. In the first case, each digit in the middle and right terms of both P and X is zero, and each digit in the middle and
right terms of both P′ and X′ is equal to b−1, where b is the base. In this case, the maximum difference is the sum of the middle and right terms of Q′, the estimated
quotient. In the second case, each digit in the middle and right terms is equal to—(b−1). Now, the right terms of P′ and X′ are simplified by substituting the worst case. In
the first case where every middle and right digit in P′ and X′ is equal to (b−1).
′

𝑃𝑅 = ∑
′

𝑃𝑅 < 𝑏

𝑆𝑝 + 𝐿𝑝 - 𝑛 - 1
𝑘 = 𝑆

𝑘

′

⁢( 𝑏 - 1 ) ⁢
𝑏 ⁢

𝑋𝑅 = ∑
⁢

𝑝

𝑆𝑝 + 𝐿𝑝 - 𝑛 + 1

-𝑏

𝑆𝑝 + 𝐿𝑝 - 𝑛

′

,

𝑋𝑅 < 𝑏
⁢

𝑆𝑥 + 𝐿𝑥 - 𝑛 - 1
𝑘 = 𝑆

⁢( 𝑏 - 1 ) ⁢
𝑏

𝑘

𝑥

𝑆𝑥 + 𝐿𝑥 - 𝑛 + 1

-𝑏

′

𝑃𝑅 = ∑

𝑆𝑝 + 𝐿𝑝 - 𝑛 - 1
𝑘 = 𝑆

𝑏
⁢

𝑘+1

𝑘

′

-𝑏 ⁢

𝑋𝑅 = ∑
⁢

𝑝

𝑆𝑥 + 𝐿𝑥 - 𝑛 - 1
𝑘 = 𝑆

𝑏
⁢

𝑘+1

-𝑏

𝑘

𝑥

𝑆𝑥 + 𝐿𝑥 - 𝑛

Since,

P′ R <b S p +L p −n+1 −b S p +L p −n, and X′ R <b S x +L z −n+1 −b S x +L x −n
we can use

P′ R ≈b S p +L p −n+1, and X′ R ≈b S x +L x −n+1
to provide a max-bound to the error estimate for Q′. If the values of the max bound are substituted for the worst-case P′R, and X′R into Q′, then Q′ is equal to:

′

′

′

𝑄 = 𝑃 ⁢
𝑋 = ( 𝑃𝐿 + 𝑏

𝑆𝑝 + 𝐿𝑝 - 𝑛 + 1

) ⁢( 𝑋𝐿 + 𝑏

𝑆𝑥 + 𝐿𝑥 - 𝑛 + 1

′

) 𝑄 = ( 𝑃𝐿 ⁢
𝑋𝐿 ) +

(

𝑃𝐿 ⁢
𝑏

𝑆𝑥 + 𝐿𝑥 - 𝑛 1

𝑋 ⁢
𝑏

+

)

𝑆𝑝 + 𝐿𝑝 - 𝑛 + 1

+ (𝑏

𝑆𝑝 + 𝐿𝑝 - 𝑛 + 1

𝑏
⁢

𝑆𝑥 + 𝐿𝑥 - 𝑛 + 1

)

𝐿

Left⁢
⁢

Middle⁢
⁢

′

𝑄 = ( 𝑃𝐿 ⁢
𝑋𝐿 ) + ( ∑

𝑆

𝑝

+𝐿

𝑝

-1

𝑘 = 𝑆𝑝 + 𝐿𝑝 - 𝑛

Right
⁢

𝑘

𝑃𝑘 ⁢
⁢
𝑏 ⁢
𝑏

𝑆

𝑥

+𝐿

𝑥

-𝑛 1

+ ∑

𝑆

𝑥

+𝐿

𝑥

-1

𝑘

𝑘 = 𝑆𝑥 + 𝐿𝑥 - 𝑛

𝑋𝑘 ⁢
⁢
𝑏 ⁢
𝑏

𝑆

𝑝

+𝐿

𝑝

-𝑛+1

) + (𝑏

𝑆

𝑝

+𝐿

𝑝

-𝑛+1

𝑏
⁢

𝑆

𝑥

+𝐿

𝑥

-𝑛+1

)

Left⁢
⁢
Middle⁢
⁢

Right
⁢

Knowing that,
∑

𝑆𝑝 + 𝐿𝑝 - 1
𝑘 = 𝑆𝑝 + 𝐿𝑝 - 𝑛

𝑃𝑘 ⁢
⁢
𝑏

𝑘

< 𝑏

𝑆𝑝 + 𝐿𝑝

, and⁢ ⁢∑

𝑆𝑥 + 𝐿𝑥 - 1
𝑘 = 𝑆𝑥 + 𝐿𝑥 - 𝑛

𝑋𝑘 𝑏
⁢

𝑘

< 𝑏

𝑆𝑥 + 𝐿𝑥

the maximum power of the sum of the middle and right terms is less than:

b S p +L p +S x +L x −n+1 +b S p +L p +S x +L x −n+1=2b S p +L p +S x +L x −n+1
Therefore, the worst case for the sum of the middle and right terms of P′ and X′ error is strictly less than ‘2’ in the Sp+Sx+Lp+Lx−n+1 place.
Next, this proof or theoretical analysis examines the worst case in terms of the number of accurate digits in the estimated quotient. The worst case occurs when the
leftmost digits have a minimal product, and the middle and right terms have maximal products. Also, the worst case for the leftmost parts occurs when the product is
Sp+Sx+Lp+Lx−1 digits long or, in other words, there is no carry when P and X are multiplied. So, the expression for the leftmost bits is:
𝑃𝐿 = ∑

𝑆𝑝 + 𝐿𝑝 - 1
𝑘 = 𝑆𝑝 + 𝐿𝑝 - 𝑟

𝑃𝑘 ⁢
⁢
𝑏

𝑘

= 𝑏

𝑆𝑝 + 𝐿𝑝 - 1

⁢

𝑋𝐿 = ∑
⁢

In the worst-case PL and XL are equal to b

𝑆𝑥 + 𝐿𝑥 - 1
𝑘 = 𝑆𝑥 + 𝐿𝑥 - 𝑟

S p +L p −1

, and b

𝑋𝑘 ⁢
⁢
𝑏

𝑘

= 𝑏

S x +L x −1

𝑆𝑥 + 𝐿𝑥 - 1

respectively since all other digits except for the leftmost digit are zero.

The product of the worst-case PL and XL is:

P L X L =b S p +L p −1 b S x +L x −1 =b S p +S x +L p +L x −2
So, at worst, the leftmost parts have a 1 in the Sp+Sx+Lp+Lx−2 place.
Next, this proof or theoretical analysis compares the number of digits between the left terms of Q and Q′ and the middle terms of Q, and Q′ to determine how many digits
are exactly known. Comparing Q and Q′ yields:

Q−Q′<b S p +S x +L p +L x −2−2b S p +L p +S x +L x −n+1
When the absolute number of digits between the leftmost digit of the left parts, which are equal, and the leftmost digit of the middle and right parts which are unequal are
counted the total number of digits is:

S p +L p +S x +L x−2+1−(S p +L p +S x +L x −n+1)=n−3
Now, the difference between the number of digits in the leftmost and middle terms is n−3. If the leading ‘1’ in the leftmost places of Q, and Q′ is counted, the number of
known digits is n−3+1=n−2 digits. So, if the number of accurate digits of Q′ compared to Q is counted, the actual quotient, then there is, a difference strictly less than ‘−2’
in the n−2th place from the left. The ‘1’ was added to the leftmost place to account for the left-most ‘1’ since in the worst case for PL XL, there is a ‘1’ in the bS p +S x +L p +L
x −2

place.

https://patents.google.com/patent/US8725786B2/en?q=(high+radix)&oq=high+radix

9/20

2/14/26, 1:55 PM

US8725786B2 - Approximate SRT division method - Google Patents

Conversely, if instead each bit in the middle and right terms of Q is 0, and each bit in the middle and right terms of Q′ is equal to −(b−1), where b is the base, then the
maximum error is a difference of +1 in the n−2 place. This can be seen by simply replacing every (n−1) term in the preceding equations with a−(b−1) term. This theorem
asserts that when two numbers are multiplied that at least the first n+2 digits of both numbers must be known to achieve n digits of accuracy for their product. Since the
SRT divider uses binary numbers, the base, b is 2 and each digit is a bit.
Examples of Leftmost Equality:
It has been shown above, if the first n+2 leftmost digits two numbers are known, X′ and P′, then the first n leftmost digits of their product Q is also known.
For example, if we multiply 12399*45799 (P′=12399, X′=45799) and compare it with 12300*45700 (P′=12300, X′=45700), then their products are
12399*45699=567861801, and 12300*45700=562110000. In both cases the equal digits are highlighted in bold. In this case, since the first 3 digits of each product is
equal, at least the leftmost digit of their products is equal.
In base 2, 12399*45799=567861801 is
11000001101111*1011001011100111=100001110110001110001000101001
Also in base 2, 12300*45700=562110000 is
110000000011004011001010000100=100001100000010001111000110000
In this case, at least the first 7 bits of P′ and X′ are equal (highlighted in bold), so at least the first 5 bits of their product are equal. Although there are more digits equal in
this case, the theorem was proven at the worst case. These two operands, P′ and X′ can be operands to a multiplier, or the divisor, P′ and the divided reciprocal, X′ as done
in the PSRT divider.
Theorem 2: Quotient Approximation
The previous proof or theoretical analysis shows that if the dividend and quotient reciprocal are known to n+2 digits then their product, the quotient, will be correct to at
least n digits, ±1. If the base, b, is set to binary, then b=2, and n=r, the number of bits in the Radix. Although this information may be helpful for some implementations,
implementing this estimation alone would require correcting the quotient digit since the partial remainder may be negative. Recall that from the general formula for
division as shown by the SRT Equation of Eq. 3, in order for the partial remainder to be negative, the estimated quotient digit must differ from the actual quotient digit by
+1. If the partial remainder is positive, then the speculated quotient digit is always less than or equal to the actual quotient digit. If the partial remainder is never negative,
and the dividend and quotient reciprocal are known to r+3 bits, then the quotient digit will be known to at least r+1 bits−1. This is equal to knowing r bits−½. The extra
precision eliminates the need for correcting the quotient digit prior to shifting since the partial remainder's first r bits would be zero before it is shifted left by r bits.
Statement:
Given the divisor's reciprocal, X, and dividend, P, to at least r+3 bits: the product Q″=(X′−1)(P′) has the same error as the product Q′=X′P′ if the dividend and divisor have the
constraints 1≦X<2, and 1≦P<2. In other words, if the dividend and divisor are pre-scaled to a certain range, and one extra bit of precision is added, then the estimated
quotient has a difference of at most−½.
Proof:
First round down the divisor estimate, D′, to r+3 bits. This means that after the first r+3 significant bits, the rest of the divisor estimate is zero. Therefore X′≧X since D′≦D.
Since the rest of X to from X′ are truncated, the difference of X and X, is X−X′<1. This value which is less than ‘1’ in the last place happens to be the same worst case
difference for LET which was proven earlier. This is because assuming that numbers are in the form 1.XXXXXX, the worst case difference between X′ and X is the same
as the difference between 1.XXXXXX11111, and 1.XXXXXX00000, which is at worst in the form 0.00000011111. In other words, less than ‘1’ in the last significant place
of the Radix.
LET is a general approximation theorem for multiplication which can also be used for division. If it is proven that adding a ‘1’ to the last significant divisor has the same
error assumptions as LET, then LET can be used to justify truncating the divisor's reciprocal, X. Refer back to the proof or theoretical analysis for the leftmost equality
theorem. A substitution was made for the sum of the middle and right terms before determining the max error for the estimated quotient digit, and the actual quotient
digit.

b S p +L p +S x +L x −n+1 +b S p +L p +S x +L x −n+1=2b S p +L p +S x +L x −n+1
Given the expression for Q′ in that proof or theoretical analysis, notice that each bS p +L p +S x +L s −n+1 term is equivalent to a ‘1’ in the n−2th place of the left term.

′

′

′

𝑄 = 𝑃 ⁢
𝑋 = ( 𝑃𝐿 + 𝑏

𝑆𝑝 + 𝐿𝑝 - 𝑛 + 1

) ⁢( 𝑋𝐿 + 𝑏

𝑆𝑥 + 𝐿𝑥 - 𝑛 + 1

′

) 𝑄 = ( 𝑃𝐿 ⁢
𝑋𝐿 ) +

𝑃 ⁢
𝑏

(

𝑆𝑥 + 𝐿𝑥 - 𝑛 + 1

𝐿

𝑋𝐿 ⁢
𝑏

Left⁢
⁢
′

𝑄 = ( 𝑃𝐿 ⁢
𝑋𝐿 ) + ( ∑

Middle⁢
⁢
𝑆

𝑝

+𝐿

𝑝

-1

𝑘 = 𝑆𝑝 + 𝐿𝑝 - 𝑛

𝑆

𝑝

+𝐿

𝑝

+

)

-𝑛+1

+ (𝑏

𝑆𝑝 + 𝐿𝑝 - 𝑛 + 1

𝑏
⁢

𝑆𝑥 + 𝐿𝑥 - 𝑛 + 1

)

Right
⁢
𝑘

𝑃𝑘 ⁢
⁢
𝑏 ⁢
𝑏

𝑆

𝑥

+𝐿

Left⁢
⁢
Middle⁢
⁢

since the left term, PLXL, begins at power b

𝑥

-𝑛+1

+ ∑

𝑆

𝑥

+𝐿

𝑥

-1

𝑘 = 𝑆𝑥 + 𝐿𝑥 - 𝑛

𝑘

𝑋𝑘 ⁢
⁢
𝑏 ⁢
𝑏

𝑆

Right where⁢
⁢
: 𝑃𝐿 = ∑

S p +L p S x +L x −2n

𝑝

+𝐿

𝑝

-𝑛+1

) + (𝑏

𝑆𝑝 + 𝐿𝑝 - 1
𝑘 = 𝑆

𝑝

𝑘

+𝐿

𝑝

-𝑛

𝑃𝑘 𝑏 ⁢
⁢

, and ends at the power b

𝑆

𝑝

+𝐿

𝑝

-𝑛+1

𝑏
⁢

𝑆

𝑋𝐿 = ∑
⁢

𝑥

+𝐿

𝑥

-𝑛+1

𝑆𝑥 + 𝐿𝑥 - 1
𝑘 = 𝑆

S p +L p +S x +L x −1

𝑥

+𝐿

𝑥

-𝑛

)

𝑋𝑘 ⁢
⁢
𝑏

𝑘

. There is a ‘1’ in the bS p +L p +S x +L x −n+1 place. If the number of

decimal places between where the left term ends, and the bS p +L p +S x +L x −n+1 place is counted, the total amount of terms is n−1 terms. If the leading ‘1’ is included,
there are at least n correct digits in the left term. In this case, the left term ends wherever the divisor's reciprocal, X was truncated. In this example, the left term would be
1.XXXXXX, and there would be seven digits in the left term.
Now, leave the dividend as P in the theoretical analysis, and substitute X and X′ for the approximate and estimated divisor's reciprocal. Making these substitutions, then
LET states that if P and X′ are known to n digits, that Q′, the estimated quotient digit has n−2 correct digits when compared to the actual quotient digit, Q. So, for base b=2
for binary, and n=r+3, the number of correct quotient digits is r+3−2 (r+1) correct digits in Q′ when it is compared to the actual quotient digit, Q. Since the significant digits
have a decimal point after the rth digit, r+1 precision means that at most a difference of ½ in the rth decimal place, which is the last significant decimal place in the Radix.
Part V: PSRT System and Method and Implementation

https://patents.google.com/patent/US8725786B2/en?q=(high+radix)&oq=high+radix

10/20

2/14/26, 1:55 PM

US8725786B2 - Approximate SRT division method - Google Patents

The PSRT system and method can cause a computer to accept a divisor and a dividend in a digital representation and to transform the divisor and dividend into a
quotient that can also be recorded, provided as output and/or displayed in a digital form. The division operation according to the inventive PSRT system and method can
be performed iteratively, such as by linear convergence or other suitable convergence method, until the quotient is accurate to n places. The PSRT system and method
can be used for any base number system and any Radix, including, for example, the widely used base 10 and base 2 number systems. The PSRT system and method is
now described in more detail.
Given the left equality theorem, and quotient approximation, the modified SRT algorithm used in the PSRT system and method can use the basic SRT represented by Eq.
3. One difference between PSRT and basic SRT, however, is that the QST contains only the difference between the divisor's reciprocal (to r+3 digits) and one, i.e. the
entries X′−1. PSRT then multiplies the entry from the Partial Quotient Selection Table (PQST) by the first r+3 digits of the dividend, to yield the estimated quotient, which
differs from the actual quotient by at most −½. As in SRT, the partial remainder is right shifted r digits before the next iteration. The additional multiplier is a r+3 digit
multiplier. There are many ways to implement PSRT. Two exemplary implementations described below do not use specialized hardware and instead focus on the
algorithm itself. The first exemplary embodiment uses one stage of a synchronous, or clocked, PSRT divider. The second exemplary embodiment unrolls divider stages to
make an asynchronous PSRT divider. However, in actual practice, one would use specific hardware, for example as is illustrated hereinbelow.
FIG. 6 shows a simplified block diagram of the first exemplary embodiment showing one stage of a synchronous, or clocked, PSRT divider. While the diagram of FIG. 6
has some resemblance to that of FIG. 3, the multiplexer that selects the product QD in FIG. 3 has been replaced with a multiplier in FIG. 6. Although either implementation
can work, the multiplier works if D has relatively few bits. Since the quotient is within 1 ULP of the real quotient, a second stage would not need to subtract QD from the
partial remainder. Therefore using only one extra multiplier, a two-stage asynchronous version of the divider can multiply 1/X by the partial remainder Pi+1 to determine
Q.
FIG. 7 shows one exemplary stage of a PSRT asynchronous version of the synchronous, or clocked, PSRT divider of FIG. 6. The asynchronous version was selected for
testing since it only use one extra multiplier and adder in the second stage, and this asynchronous version does not require duplication of the PQST.
In some embodiments, the divider can follow the 16-bit IEEE-754r standard for floating point numbers, which is slightly different from the standard 32-bit IEEE 754
format. This format was selected since it has approximately the same dynamic range as 32-bit fixed-point numbers that are currently used in many FPGA divider
implementations. (Wang, et. al., “Decimal Floating-point Division Using Newton-Raphson Iteration”, Proceedings of IEEE International Conference on Application-Specific
System, Architectures and Processors, pgs 84-95, September 2004). In addition, using the 16-bit format, a Radix-64 divider can compute the entire quotient in two
iterations. Furthermore, since the second iteration is within 1 ULP of the final quotient, the last stage only needs a multiplier to determine the quotient digit, Qi. So instead
of a synchronous design, in some embodiments, the divider can be made asynchronous at the expense of one extra multiplier.
The IEEE 754r floating point standard has 1 sign bit, 5 exponent bits, and 10 mantissa bits. In normalized form, the 11th bit of the mantissa is a leading ‘1’, and is not
included in the floating point number. Since all numbers use signed magnitude format, and not 2's complement format, all values must be positive. So, the sign bit
indicates sign, and the exponent is biased by −15. Table 3 is a bit representation table of the IEEE 754r format is shown below. This format is Big-endian, with the most
significant bit to the left, and the least significant bit at the right:
TABLE 3
15

14 13 12 11 10

9876543210

Sign

Exponent

Mantissa

According to this format, the Mantissa is at most, 11 bits long when it is normalized. For example, a binary number 1 01111 0000000001 in IEEE 754r normalized format
is −1.00098e0 in decimal form. The sign bit is one. The next five bits, are 15−15=0. Assuming a ‘1’ in the 11th position, the next 10 bits equal 0/2+014+ . . . 1/1024, or
0.00098. So the total number is −1.00098e0. Similarly, to convert a decimal number, 2.25e1 base 10 into IEEE 754r format, the mantissa 2/1+0/1+0/2+¼, or 0
0010000000. Then the leading ‘1’ is dropped since it is implied. Next, since the exponent is 1, the bias of 15 is added to write the exponent as 16, or 10000. Since 2.25e1
is positive, the sign is 0. Therefore, the representation for 2.25e1 base 10 in IEEE-754r format is: 0 10000 0010000000.
One exemplary embodiment of a synchronous, or clocked, PSRT divider is described in more detail. FIG. 8 shows a more detailed block diagram of a clocked type PSRT
divider similar to that shown in FIG. 6. The PSRT divider shown in FIG. 8 is a 16-bit Radix-256 divider using a floating point format. The flow of FIG. 8 will be seen to
correspond to the detailed general method for PSRT division as outlined below. Each rectangle in FIG. 8 represents a register. FIG. 9 shows a block diagram of the 16-bit
Radix-256 PSRT divider of FIG. 8 as performed on the mantissa. Data is in a big-endian format (sign exponent mantissa). The dividend and divisor are pre-scaled, and fed
into the divider as operands X, and D respectively at the top of FIG. 9.
The divisor is spliced and entered into the PQST table. The PQST table outputs the inverse of the divisor to n+3 bits. The inverse of the divisor is then multiplied by the
foremost n+3 bits of the partial remainder, which is selected from either the divided, or the partial reminder computer from the previous cycle by a control circuit which in
the exemplary block diagram of FIG. 9, is depicted as a counter. The selected value is P′. The output of this multiplication stage is Q′, or an extended version of the
quotient bit. Note that Q is normalized so that the first bit is 1. At most, Q is shifted left by 1.
Continuing with the block diagram of FIG. 9, the quotient bit is truncated to n bits (Q[7:0]) and multiplied by the divisor, D and subtracted from the current partial
remainder (P′) selected in the previous stages. The next partial remainder is P′−QD, and called P. P is shifted right by 8, and stored as the next partial reminder.
Generally, the components shown from the top of FIG. 9 through Q′ are positive-edge clocked, and the lower components are negative edge clocked. The counter can be
clocked on both edges, so its rate is twice that of other components. By clocking the counter on both edges, the next partial remainder is available to calculate the next
quotient bit by the next positive edge.
General Formula for PSRT Division
In PSRT X′ is the divisor reciprocal to N+3 digits, where N is the number of digits in radix-R. Generally, N=logb R where b is the base of computation. For example, for a
radix-64 divider in base 2, b=2, R=64, N=logb R=log2 64=6, and N+3=6+3=9. PSRT uses N+3 precision to prevent the quotient digit from going negative. If the quotient digit
went negative, PSRT would require a redundant digit set, which generally takes up twice the resources in the quotient selection logic. PSRT is non-restorative. Unlike SRT,
the quotient digit never goes negative. The reason why the quotient digit never goes negative was shown above by theorem 2 (quotient approximation).
The section shows that if we truncate the divisor reciprocal towards zero (that means just truncate to the first n+3 bits), and have n+3 digits precision, then the quotient
digit will always be equal to the actual quotient digit on the range (0, −½) in the last place. Since an acceptable error in a divider is one unit in the last place, the result is
valid. Secondly, PSRT is normalized to eliminate the need for correction hardware. PSRT assumes IEEE-754 normalized format. This step can be done using pre-shifters
and post-shifters or another technique already published to shift the numbers. Unlike other products, PSRT operands do not have to be scaled, which would have required
extra multiplication.
One general method for PSRT division now follows:

https://patents.google.com/patent/US8725786B2/en?q=(high+radix)&oq=high+radix

11/20

2/14/26, 1:55 PM

US8725786B2 - Approximate SRT division method - Google Patents

1) Let b be the base of computation, the divisor be D, the dividend be X, Q[i] be the current quotient bit, and Q be the final quotient, and Pi be the partial remainder, and n
be the number of digits in Radix-R, where n is the number of digits of the quotient computed at a time, and n, b and r have the relationship n=logb R. Set Q to zero initially.
In short Let D=divisor, X=dividend, Q[i]=ith quotient digit (left-to-right), Q=quotient, n=logb R where b=base of computation n=# of digits computed/iteration, R=radix of
divider.
2) Calculate PQST[j]=bn+2/a for a=bn+3 to bn+4−1, and j=a−bn+3. PQST[ ] is always all (b−1) in each place. You may either store the result in the PQST or, if you are working
in base 2 and want to save space, since the first bit of PQST[j] is always ‘1’ take the first n+3 digits of this result, store the last n+2 digits of the result's leftmost n+3 digits
in the PQST logic, so when a value is retrieved from the PQST, a ‘1’ is appended to the left-most bit.
3) Let the first partial remainder be the dividend, X. Let i be the number of iterations. Set i to 0. In short, P[0]=X. P[i]=ith partial remainder, i=0.
4) Let X′ be the first n+3 digits of the dividend, X. Calculate PQST[X′] and multiply it by the first n+3 digits of the Partial remainder, P[i]′, and store this value as the quotient
digit Q′[i]. Take the leftmost n digits of Q′[i] and store it in Q[i]. Shift Q[i] right by n+2 places. In short Q′[i]=P[i]′*PQST[X] where X′=leftmost n+3 digits of X. P[i]′=leftmost n+3
digits of P[i]′ and Q[i]=leftmost n digits of Q′[i]/bn°2.
5) Subtract the product of the quotient digit and dividend Q[i]*D from the full partial remainder, P[i], and store this value as the next partial remainder P[i+i]. In short
P[i+1]=P[i]−Q[i]*D.
6) Let Q=Q+Q[i]. Shift Q left by n digits. In short Q=(Q+Q[i])*bn.
7) Shift the partial remainder P[i+1] left by n digits. In short P[i+1]=P[i+1]*bn.
8) Increment i. In short (i=i+1).
9) Repeat steps 4-7 until i=ceil(W/n), where W is the number of digits in the dividend. At this step the partial remainder is the actual remainder, and the quotient is Q. The
precision of accuracy of the quotient, Q, is the number of digits in W. In order to get the same accuracy for the remainder, add an additional ceil(W/n) repetitions of steps
4-7.
10) Optional: Post-scale the result (shift) so that it is normalized to the range (0,b).
Note that pre-scaling of operands is not included in the general formula. The operands are assumed to be of the same order, but this can be done using pre-shifters/postshifters or some other method. The PSRT system and method can be implemented in part or in whole either hardware of software.
EXAMPLE OF GENERAL METHOD (BASE-10)
In the following examples each step of the general formula is shown:
1) Divisor, D=1.99020103; Divided, X=1.51563451; base b=10; Radix R=1000; n=logbR=3; Q=quotient. We scale D to 199020103, and scale X to 151563451.
2) Since n=3, the width of the PQST is n+3=6 digits. PQST[j]=105/a for a=b6 to b7−1, and j=a−b6. In short PQST[0]=0.999999. PQST[1]=100000/1000001=0.999990,
PQST[3]=100000/1000003=0.999970, all the way up to PQST[999999]=0.0500000.
3) The first partial remainder is X=1.51563 and i=0, so P[0]=X=151563451
4) The leftmost n+3=6 digits of the dividend, P[0]=X is P[0]′=X′=151563. Q[0]=X′*PQST[D′]=151563*100000/199020=76154.6. and Q[0]=leftmost n digits of Q[0]=76100.
Shift Q[0] right by n+2 places so Q[0]=0.76100
5) Next calculate the partial remainder, P[i+1]=P[i]−Q[i]*D.

P[1]=P[0]−Q[0]*D→P[1]=151563451−0.76100*199020103=109153.
6) Add bits to quotient. Q=Q+Q[i]Q=0+Q[0]=0+0.76100=0.76100. Shifting Q left by n places, shifts q left by 3 places, so that Q=0.761*1000=761.
7) Shift the partial remainder left by n=3 places. P[1] now is 109153000.
8) Incrementing i so that i=1. There are 9 digits in each operands so the operand width, W=9. Next we will repeat steps 4-7 until i=ceil(W/N)=ceil(9/3)=3.
4) Since i is now 1, P[i] is now P[1]=109153. The leftmost n+3=6 digits of the current dividend, P[1] is now P[1]′=109153.
Q′[1]=X′*PQST[D′]=109153*100000/199020=54845.2 and Q[1] is the leftmost n=3 digits of Q[1]=548000. Shift Q[1] right by n+2=5 places so that Q[1]=0.548.
5) Next calculate the partial remainder, P[i+1]=P[i]−Q[i]*D.

P[2]=P[1]−Q[1]*D→P[2]=109153000−0.54800*199020=89983.6.
6) Add bits to quotient. Q=Q+Q[i]Q=0761+Q[1]=761+0.548=761.548. Shifting Q left by n places, shifts q left by 3 places, so that Q=761.548*1000=761548.
There can be one more iteration in this example for full precision, but at this step, if we normalize Q=761548 to Q=0.761548, we can see that Q resembles the actual
quotient which is 1.51563451/1.99020103=0.761548450208057 by 6 digits. The extra iteration is included for worst-case divisions.
Part VI: PSRT Implementation Results and Comparisons
PSRT was implemented as a 16-bit floating point divider in Verilog, and verified using Modelsim® (available from Mentor Graphics, Wilsonville, Oreg.). For a Verilog
implementation, a program called QSTgen was created in C++ and used to create the PQST tables. Screenshots of some of the functional verification tests in Modelsim®
are shown in FIG. 21 through FIG. 24. An Appendix lists the source code for QSTgen. After verification, PSRT was implemented using Xilinx ISE® Design Suite 10.1 and
targeted for both the low-end Spartan-3 XCS200 FPGA, and the high-end Virtex™-II Pro FPGAs (both available from Xilinx, Inc., 2100 Logic Drive, San Jose, Calif. 951243400). Xilinx Tools were used to extract test results, which included area, maximum delay, and power consumption. Test results are presented below. To simplify design,
all numbers in the tests are normalized.
Table 4 compares the results of Pan et al. for the estimated QST and the PQST used by PSRT. Depending upon the Radix, the size of the PQST is 3-25 times smaller than
the size of the RQST and QHT combined, and 307 to 18724 times smaller than the traditional QST.
TABLE 4
Comparison of PQST Size

Radix

Size of

Size of

Size of PQST (in

RQST + QHT (in

Conventional

bits)

bits)

QST (in bits)

8

160

0.5 k

16

384

4k

1M

32

896

22k

16M

https://patents.google.com/patent/US8725786B2/en?q=(high+radix)&oq=high+radix

48K

12/20

2/14/26, 1:55 PM

US8725786B2 - Approximate SRT division method - Google Patents

FIG. 10 shows a graphical representation of the PQST compared to Pan's study and traditional QST sizes. It can be seen that both Pan's results and the PQST are much
smaller than a traditional QST.
FIG. 11 shows a comparison between PSRT size and Pan's combined RQST+QHT only. FIG. 11 shows that while Pan's combined RQST and QHT are smaller than a
traditional QST, Pan's combined RQST and QHT still exhibits quadratic growth with Radix. By contrast, the PQST size in bits exhibits a roughly linear growth with Radix and
is therefore much smaller. Furthermore, the size difference between the PQST and other QSTs increases with Radix, making PSRT suitable for higher radices. Therefore,
using PSRT decreases the size of the QST significantly.
We turn now to growth of the PQST table with Radix. As stated in PART III, the Quotient Selection Table (QST) grows quadratically with the Radix. The size of the PQST is
smaller than a traditional QST, and grows roughly linearly with Radix. The tables below show linear growth both in terms of ROM size, and FPGA area as well as how the
size of the PQST grows with Radix. The increase factor is the size of the PQST for the current Radix in bits divided by the size of the PQST for the previous Radix in bits.
As the Radix increases, the increase factor approaches, but is never less than 2. Each two-fold increase in R produces a roughly two-fold increase in PQST size, so the
PQST grows approximately linearly with Radix. The multiplier size in bits shown in Table 5 below indicates that extra multiplier bits are required after the PQST to
calculate the quotient digit, Qi.
TABLE 5
Growth of PQST Table in ROM
Increase Factor

Radix

Size of PQST in

Rom

Multiplier

of PQST ROM

ROM

Dimensions

Size

Size

4

64

16 × 4

5

1

8

160

32 × 5

6

2.50

16

384

64 × 6

7

2.40

32

896

128 × 7

8

2.33

64

2K

256 × 8

9

2.29

128

4.5K

512 × 9

10

2.25

256

10K

1024 × 10

11

2.22

512

22K

2048 × 11

12

2.20

1024

48K

4096 × 12

13

2.18

2048

104K

8192 × 13

14

2.17

The size of the multiplier in Table 5 grows only logarithmically with Radix, and is therefore relatively small. For example, a Radix-64 divide requires a 9-bit multiplier.
FIG. 12 shows a graphical representation of Table 5, and shows how the size of the PQST grows with Radix. It can be seen that the percent increase in size of the PQST
for a given Radix over the size of the PQST for a previous Radix decreases as the Radix increases.
Table 6 and Table 7 below show the size of the PQST in both LUT block and Slices for the Spartan-3 and Virtex-II FPGAs. The size of the PQST on FPGA increases slightly
sub-linearly with Radix. The delay column shows the worst case delay of the units in nanoseconds. The results of both FPGAs agree with the result from Oberman and
Flynn, and the delay on the QST increases roughly logarithmically with Radix. FIG. 13 and FIG. 14 are graphical representations of the data presented in Table 6 and Table
7 respectively. FIG. 13 and FIG. 14 visually demonstrate the roughly linear growth of the size of the PQST with Radix, and the roughly logarithmic growth of the delay of
the PQST with Radix.
TABLE 6
PQST Size on Spartan-3
Radix

LUTS

Slices

Delay (ns)

4

4

4

7.41

8

8

4

7.76

16

21

11

9

32

39

23

8.53

64

108

59

10.63

128

213

118

12.4

256

417

234

14.74

512

736

401

17.48

1024

1397

749

20.73

2048

2463

1296

24.59

TABLE 7
PQST Size on Virtex-II Pro
Radix

LUTs

Slices

Delay (ns)

4

4

3

5.41

8

8

5

5.23

16

21

11

7.18

32

39

21

6.61

64

108

56

8.71

128

220

125

10.95

256

431

247

13.07

512

759

428

14.34

https://patents.google.com/patent/US8725786B2/en?q=(high+radix)&oq=high+radix

13/20

2/14/26, 1:55 PM

US8725786B2 - Approximate SRT division method - Google Patents

1024

1453

847

21.44

2048

2450

1344

21.01

Surprisingly, the differences between the delays of the low-end Xilinx Spartan 3, and the Xilinx Virtex-II Pro FPGAs decrease with Radix. For Radix 4, the Virtex-II Pro FPGA
is 30% faster than the Spartan-3. For Radix 2048, this difference drops to 14%.
Table 8 and Table 9 show the multiplier size, and delay as the Radix increases for the Spartan-3 and Virtex-II Pro FPGAs for two special cases: 1) the column LUT delay
shows the delay if no Xilinx-Specific multipliers are used. This option is useful since a design can implemented on other FPGA platforms that do not contain built-in
multipliers. 2) The column Block Delay shows the multiplier delay if the Xilinx Built-in 18×18 bit multipliers are used instead.
FIG. 15 and FIG. 16 show graphical representations of Table 8 and Table 9 respectively. From both figures it can be seen that delay and the total number of LUTs in the
PSRT divider has a logarithmic relationship to Radix.
TABLE 8
Multiplier Size on Spartan-3

Radix

LUTs

Slices

LUT

Block

Delay (ns)

Delay (ns)

4

27

16

14.49

10.42

8

39

21

14.17

11.09

16

56

30

15.66

11

32

72

37

15.48

11.16

64

93

49

17.97

11.44

128

113

58

20.11

12.45

256

139

72

20.62

12.37

512

164

83

19.06

12.75

1024

192

99

21.09

12.51

2048

220

112

20.64

12.66

TABLE 9
Multiplier Size on Virtex-II Pro
LUT
Radix

LUTs

Slices

Block

Delay (ns)

Delay (ns)

4

27

16

10.99

7.44

8

39

21

11.66

8.28

16

56

30

11.62

8.32

32

72

37

12.24

8.87

64

93

49

14.14

9.99

128

113

58

15.26

8.87

256

139

72

15

9.41

512

164

83

14.85

9.8

1024

192

99

16.59

9.92

2048

220

112

15.97

10.14

The area of the multiplier increases only logarithmically with Radix for both FPGAs. On average, using a built-in multiplier decreases delay from 28-38% over not using any
built in multipliers. FIG. 15 and FIG. 16 show that as the Radix increases that the multiplier delay, LUT delay, and the area of the multiplier increase logarithmically on both
Spartan-3 and Virtex-II Pro FPGAs.
The next two tables, Table 10 and Table 11 show the combined area and delay for the PQST and its multiplier for the Spartan-3 and Virtex-II Pro FPGAs. Radix-64 was
chosen in both cases since the size of the multipliers and the PQST in LUTs are approximately equal as shown in Table 6, Table 7, Table 8, and Table 9. For Radix 64, the
PQST and Multiplier take approximately 100 LUTs each for the Spartan-3 FPGA, and the Virtex-II Pro FPGA. FIG. 17 and FIG. 18 tom are graphical representations of Table
10 and Table 11 respectively.
The total area of the PQST and Multiplier increases slightly sub-linearly with Radix, while the Delay increases logarithmically. In Table 10 and Table 11, the LUT delay is the
delay of the design not using multiplier blocks, and the Block delay is the delay using multiplier blocks. On average, the total number of LUTs decreases significantly using
the multiplier blocks for lower radices such as Radix-64 and less, with a total area savings of 50%. The delay decrease from using multiplier blocks decreases from 21%
to 15% for radices 4-2048. These results indicate that multiplier blocks should be used for low to high Radix (Radix-64) PSRT dividers. Very high Radix PSRT dividers, or
dividers Radix 1024 and above, could use either multiplier blocks or LUT only implementations.
TABLE 10
PQST + Multiplier Size and Delay on Spartan-3
LUT Total

Radix

Without

LUT Total

LUT

Block

Blocks

with Blocks

Delay

Delay

4

31

4

21.9

17.83

8

47

8

21.93

18.85

16

77

21

24.66

20

32

111

39

24.01

19.69

64

201

108

28.6

22.07

https://patents.google.com/patent/US8725786B2/en?q=(high+radix)&oq=high+radix

14/20

2/14/26, 1:55 PM

US8725786B2 - Approximate SRT division method - Google Patents

128

326

213

32.51

24.85

256

556

417

35.36

27.11

512

900

736

36.54

30.23

1024

1589

1397

41.82

33.24

2048

2683

2463

45.23

37.25

TABLE 11
PQST + Multiplier Size and Delay on Virtex-II Pro
LUT Total

Radix

Without

LUT Total

LUT

Block

Blocks

With Blocks

Delay

Delay

4

31

4

16.4

12.85

8

47

8

16.89

13.51

16

77

21

18.8

15.5

32

111

39

18.85

15.48

64

201

108

22.85

18.7

128

333

220

26.21

19.82

256

570

431

28.07

22.48

512

923

759

29.19

24.14

1024

1645

1453

38.03

31.36

2048

2670

2450

36.98

31.15

From FIG. 17 and FIG. 18 tom, it can be seen that the total area of the PQST and Multiplier increases slightly sub-linearly with Radix, while the Delay increases
logarithmically.
Turning now to area delay and power for 16-bit Floating Point PSRT Divider, to get the most accurate measurements, the Area, Delay, and Power statistics for the PSRT
were taken post Place and Route, or the step before the design is programmed on to a selected FPGA. FIG. 19 and FIG. 20 show the floor plan for the Virtex-II and
Spartan-3 FPGAs. FIG. 19 shows a floor plan for a routed Virtex-II Pro Design. FIG. 20 shows a floor plan for a routed Spartan-3 design. In both cases, the PSRT divider
uses relatively little area.
Table 12 and Table 13 show the area and delay statistics for the 16-bit floating point divider.
TABLE 12
Implementation Spartan-3
Multiplier
Option

Slices

Utilization

LUT

410

LUTS

223

10%

53.37

Delay

Block

201

114

5%

37.9

TABLE 13
Implementation Virtex-II Pro
Multiplier
Option

LUTs

Slices

Utilization

Delay

LUT

395

215

1%

45.08

Block

206

113

1%

28.87

In both cases, the Block implementation has half the area and 30% delay reduction over a LUT only implementation. The utilization of chip resources (in LUT area) is
relatively low; it decreases from 5-10% on the Spartan-3 to 1% on the Virtex-II Pro FPGA.
Table 14 shows the power statistics and junction temperature for the PSRT divider on the Spartan-3 FPGA.
TABLE 14
Power Statistics Spartan-3
Active

Total

Multiplier

Idle

Power

Power

Junction

Option

Power

(mW)

(mW)

Temp (° C.)

LUT

41.5

84

125.5

28.9

Block

41.1

21.4

62.5

26.9

The Xilinx setting assumed an ambient temperature of 25° C. The power consumed by the Block implementation is approximately half the power consumed by the LUT
only implementation. The power consumption results in Table 14 agree with the area results shown in Table 12 since the LUT only implementation contains twice the
area of the Block implementation, and power consumption is proportional to area. Interestingly, the block implementation consumes only 62.5 mW.
Turning now to a comparison of the PSRT divider with SRT dividers, Nikolas Sorokin did a study in 2006 experimenting with using a multiplicative method for 32 bit fixed
point to improve the speed of SRT on Xilinx Virtex-II Pro FPGAs. This study was chosen since the decimal point is fixed in both fixed point and IEEE standard normalized
numbers, so the results can be compared by the mantissa size. In the fixed point version, the mantissa is 32 bits long, and in the PSRT divider, the mantissa is 11 bits
long. (Sorokin).

https://patents.google.com/patent/US8725786B2/en?q=(high+radix)&oq=high+radix

15/20

2/14/26, 1:55 PM

US8725786B2 - Approximate SRT division method - Google Patents

Table 15 compares the commercial Xilinx IP Core in Sorokin's study statistics to the 16-bit PSRT divider. The divisor has a width of 32 in the 32 bit divider, and a width of
11 in the 16-bit PSRT divider. Both dividers are targeted for Virtex-II Pro FPGAs, and do not use any Xilinx-specific parts such as built-in multipliers. Although the operand
width of the divider presented here is ⅓ the size of the operand in the study, both the size of the PSRT divider decreases by a factor of 5.58, and the latency of the PST
divider decreases by factor of 7.76. These size and speed improvements are due, in part, to the fact that the 16-bit PSRT divider's operand width is ½ that of the divider
presented in Sorokin's study. However, if we assume that both the size and latency of the PSRT divider would increase by a factor of 4 for the 32-bit implementation, the
PSRT divider would still have a noticeable speed and area difference. In the 32 bit case, a 4× slower, and 4× larger 32-bit PSRT divider would have an area 1.40 times
smaller, and 1.94 times faster than the divider presented in Sorokin's study.
TABLE 15
Area and Delay Comparison of Xilinx 32-bit SRT and 16-bit
PSRT divider
Total
Delay
Implementation

LUTS

Xilinx IP Core

2240

(ns)
350

16-bit PSRT

401

45

Table 16 compares the delay of Sorokin's proposed divider to the delay of the 16-bit PSRT divider.
TABLE 16
Area and Delay Comparison of Sorokin's and 16-bit PSRT divider
Total
Implementation

Delay (ns)

SRT Standard

854

Sorokin's Restoring Algorithm

597

Sorokin's Non-Restoring

265

Algorithm
PSRT (Virtex-II/Spartan-3)

45/53

When compared to the standard 32-bit SRT, the 16-bit divider delay decreases by a factor of 19 using the Virtex-II and a factor of 16 using the Spartan-3. When compared
to the non-restoring algorithm presented in Sorokin's study, the delay of a 16-bit PSRT divider decreases by a factor of 5.9 using the Virtex-II and a factor of 5 using the
Spartan-3. For a better comparison, if we assume that both the size and latency of the PSRT divider, would increase by a factor of 4 for the 32-bit implementation, delay
of 16-bit PSRT divider decreases by a factor of 1.48 using the Virtex-II and a factor of 1.25 using the Spartan-3. In other words, the 32-bit PSRT divider would be 20-32%
faster. So the area and delay improvements from using PSRT are significant, even when using a much cheaper FPGA.
We turn now to a comparison of PSRT with quadratic convergence dividers. SRT is a linear convergence method, and is usually much slower than quadratic convergence
methods. However this comparison shows how PSRT relates to other division methods besides SRT. In 2005, Liu et al. conducted a study of a hybrid quadratic
convergence algorithm using a reciprocal table of the divisor, and Taylor-series expansion. (Liu, et. al., “An Iterative Division Algorithm for FPGAs”, ACM/SIGDA
International Symposium on Field-Programmable Gate Arrays, pgs. 83-89, 2006). Table 17 Compares the throughput, or number of divisions per second for the nonpipelined 32-bit divider in the 2005 study versus the asynchronous PSRT divider. While Liu's divider uses an Altera FPGA, the PSRT here uses the Spartan-3 FPGA.
TABLE 17
Comparison of Non-Pipelined Version (Liu/PSRT)
Memory

DSP/

Power

Application

Throughput
M Div/s

LUTs

(Bytes)

Mult

(mW)

LUT

24.4/18.7

1437/410

768/0

0/0

378/125.5

Block

24.3/26.4

213/201

768/0

28/3

350/62.5

When compared to 32-bit quadratic convergence algorithms, 16-bit PSRT is slower. The PSRT divider is slower since although the divider has a higher throughput as
shown in Table 17, the operand width for the divider in Liu's study is twice as long, so unless the speed gain for the PSRT divider is much faster, it is safe to conclude that
due to extra hardware, the PSRT divider would perform slower than Liu's divider. This assumption is based upon the conservative estimate that a 32-bit PSRT divider
would have 4× the size, and 4× the latency as a 16-bit PSRT divider. However, PSRT still consumes fewer resources and consumes much less power with the Spartan III
implementation having a peak power consumption of only 62.5 mW.
If both designs use LUTs only, the 16-bit PSRT divider consumes 71.5% less area. The PSRT divider also consumes up to 66.8% less power using LUTs only. However, if
both designs use Multiplier/DSP blocks, the PSRT divider uses relatively the same number of blocks, uses over 99% less RAM, and consumes 82.2% less power than the
non-pipelined version of the 2005 divider. So although the PSRT divider actually performs relatively slower in terms of delay, the difference in terms of FPGA resources
and power consumption improvements are still significant. In addition, Table 17 does not account for the fact that the PSRT divider does not use any bytes of RAM. If the
768 Bytes of Internal RAM used by the divider in the 2005 study were accounted for, then the area difference for the PSRT divider would be greater.
Although test results have shown that PSRT consumes much less area and has a smaller delay than traditional SRT, PSRT is slightly slower that other quadratic
convergence division methods based upon the conservative estimate that a 32-bit PSRT divider would be 4× as slow, and 4× larger than a 16-bit PSRT divider. Also, PSRT
is well-suited for high Radix division such as Radix-64, since the Quotient Selection Table (QST) required by PSRT still increases slightly sub-linearly with Radix, versus
quadratically with Radix as in other SRT implementations. The PSRT divider also leads to designs with low power consumption in part due to the area savings. The
Spartan III implementation only consumed 5% of the resources of the low-cost $30 FPGA, and had a peak power consumption of 62.5 mW.
Exemplary computer code useful for performing the PSRT method on a computer system is attached hereto on a CDROM medium. Two identical copies of a PSRT Divider
computer source code on two identical compact discs labeled “Copy 1” and “Copy 2” are incorporated by reference herein in their entirety. Each of the identical compact
discs is IBM-PC compatible. Each of the identical compact discs includes the following files: File no. 1: File name: PSRT DIVIDER SOURCE CODE.doc; File size: 94 Kbytes;
Date of file creation: Apr. 29, 2009, readable using Microsoft Word. File no. 2: File name: PSRT DIVIDER SOURCE CODE.txt; File size: 20 Kbytes; Date of file creation: Apr.
29, 2009, readable using any ASCII compatible program, such as Microsoft Windows Notepad or Microsoft Word.

https://patents.google.com/patent/US8725786B2/en?q=(high+radix)&oq=high+radix

16/20

2/14/26, 1:55 PM

US8725786B2 - Approximate SRT division method - Google Patents

The PSRT system and method represents a compromise between traditional SRT and quadratic convergence algorithms. It is contemplated that PSRT implementations
using design optimization techniques such as pipelining, and carry-save adders, can be made comparable in speed and efficiency to quadratic convergence algorithms
for lower operand widths. However, PSRT performs better in terms of both speed and area than traditional SRT algorithms. While the relative simplicity and alternative
approximation scheme of the PSRT method and system results in a low-area design, with relatively low power, and low latency compared to SRT, multiplicative algorithms
such as Taylor-series expansion can alternatively be used with some advantage for dividers with long operand widths where speed, as opposed to area and power
consumption, are among the top design priorities.
The PSRT can be implemented using a program storage device readable by a machine, tangibly embodying a program of instructions executable by a specific
semiconductor-based computational device situated in the machine to perform the steps of a partial SRT (PSRT) division of a dividend X by a divisor D to obtain a
quotient Q. The program storage device can be the memory of a computer, a removable program storage device such as a magnetic disk or CD-ROM, a memory
accessible over a communication systems such as the internet, or even paper tape or cards punched with holes (e.g., Hollerith or “IBM” cards such as are described in
U.S. Pat. No. 395,782). The semiconductor-based computational device can be any conventional microprocessor, FPGA or other semiconductor-based device capable of
performing the computational steps described hereinabove.
Many functions of electronic computer apparatus can be implemented in hardware (for example, hard-wired logic or other integrated circuitry), in software (for example,
using instructions encoded in a program operating on a general purpose processor or on a specially designed processor), and in firmware (for example, using
instructions encoded in a non-volatile memory that is accessed for operation on a processor as required). The present invention contemplates the substitution of one
implementation of hardware, firmware and/or software for another implementation of the equivalent functionality using a different one of hardware, firmware and/or
software. To the extent that an implementation can be represented mathematically by a mathematical function, that is, a specified response is generated at one or more
output terminals for a specific input or inputs applied to one or more input terminals of a “black box” exhibiting the mathematical function, any implementation of the
mathematical function, including any combination of hardware, firmware and/or software implementations of portions or segments of the mathematical function, is
contemplated herein.
Recording the results from a PSRT operation, such as, for example, recording results of the division or multiplication of two numbers is understood to mean and defined
herein as “writing” output data to a storage element or device. For example, recording the data of the division or multiplication of two numbers for later use as output or
as data for display to a user can include, but is not limited to, writing data to random access memory (“RAM”) (volatile or non-volatile), SRAM, DRAM, and EEPROM. Such
digital memory elements or chips can be standalone memory devices, or can be incorporated within a programmable array, such as a field programmable array (“FPGA”),
or within a microcomputer. “Writing output data” is defined herein as including writing transformed data to registers within a microcomputer. Memory elements or
devices can also include other writing means, such as writing digital data to a magnetic storage device, such as a hard drive, magnetic technology disc, or other disc
media, such as an optical CD media, or DVD media.
“Microcomputer” is defined herein as synonymous with microprocessor, microcontroller, and digital signal processor (“DSP”). It is understood that memory used by the
microcomputer, including for example an algorithm to perform a PSRT operation can be coded as “firmware” that can reside in memory physically inside of a
microcomputer chip or in memory external to the microcomputer or in a combination of internal and external memory. It is also understood that field programmable array
(“FPGA”) chips or application specific integrated circuits (“ASIC”) chips can perform microcomputer functions, either in hardware logic, software emulation of a
microcomputer, or by a combination of the two. Apparatus having any of the inventive features described herein can operate entirely on one microcomputer or can
include more than one microcomputer.
Simulation Screenshots
Modelsim® screenshots were recorded for some of the verification test conducted on the 16-bit floating point PSRT divider. FIG. 21 shows an exemplary Modelsim®
wave window. Signals are to the left, and the bus data is shown to the right.
In FIG. 22, Modelsim® screenshot demonstrating average cases, the mantissa for the divided was 1.51563. The divisor's mantissa is listed from left to right. The divisors
mantissa took on the values 1.51465, 1.00, 1.9902, and 1.28125. Xin is the value of the dividend's mantissa multiplied by 210. Similarly, Din is the value of the divisor's
mantissa multiplied by 210. The results mantissa were 1.00000, 1.51563, 0.758789, and 0.989258, which are equivalent to the precise results 1.00064, 1.51563,
0.758553, and 0.989796 by one unit in the last place (1 ULP) which is equal to 0.000977 base 2, if the quotient is greater than or equal to 1, or 0.000488 is the quotient is
less than 1. The actual answers were calculated by first multiplying the mantissas of the dividend and divisor by 210, dividing the dividend by the divisor, then multiplying
the quotient by 2−10.
FIG. 23, a Modelsim® screenshot demonstrates a worst case scenario, what occurs when the dividend is at its maximum value, and the divisor is at its minimum value.
When the mantissa of the dividend is 2047e-10, and the mantissa of the divisor is 1024e-10, the result is 2046e-10, which is correct. In decimal, this is equivalent to
1.9902/1=1.9902, or all 1 s in the mantissa. In a second worst case, the dividend and divisor differ by 1ULP. The mantissa of the dividend is 2047e-10, and the divisor's
mantissa is 2046e-10. The result is 1025e10 which is also correct. 1025e-10 is equivalent to 1.00098 base 10.
FIG. 24 demonstrates what occurs when the dividend and divisor are equal. In this case, the mantissas for both the dividend and divisor is 1024e-10, or 1.00. The
mantissa of the quotient is 1024e10, or 1.00 which is correct.
Theoretical Analysis
Although the theoretical description, including proofs, given herein is thought to be correct, the operation of the devices described and claimed herein does not depend
upon the accuracy or validity of the theoretical description. That is, later theoretical developments that may explain the observed results on a basis different from the
theory presented herein will not detract from the inventions described herein.
While the present invention has been particularly shown and described with reference to the structure and methods disclosed herein and as illustrated in the drawings, it
is not confined to the details set forth and this invention is intended to cover any modifications and changes as may come within the scope and spirit of the following
claims.

Patent Citations (10)
Publication number

Priority date

Publication date

Assignee

Title

US5046038A *

1989-07-07

1991-09-03

Cyrix Corporation

Method and apparatus for performing division using a rectangular aspect ratio
multiplier

US5140179A *

1990-07-18

1992-08-18

Sony Corporation

Master-slave type flip-flop circuit

https://patents.google.com/patent/US8725786B2/en?q=(high+radix)&oq=high+radix

17/20

2/14/26, 1:55 PM

US8725786B2 - Approximate SRT division method - Google Patents

US5307303A *

1989-07-07

1994-04-26

Cyrix Corporation

Method and apparatus for performing division using a rectangular aspect ratio
multiplier

US5475630A *

1991-09-05

1995-12-12

Cyrix Corporation

Method and apparatus for performing prescaled division

US20030050950A1 *

2001-09-07

2003-03-13

Gregory Agami

Method and apparatus to perform division in hardware

US6594681B1 *

1998-11-04

2003-07-15

Sun Microsystems,
Inc.

Quotient digit selection logic for floating point division/square root

US20040186873A1 *

2003-03-21

2004-09-23

Intel Corporation

Narrow data path for very high radix division

US20090006509A1 *

2007-06-28

2009-01-01

Alaaeldin Amin

High-radix multiplier-divider

US20120166512A1 *

2007-11-09

2012-06-28

Foundry Networks,
Inc.

High speed design for division & modulo operations

US8255448B1 *

2008-10-02

2012-08-28

Altera Corporation

Implementing division in a programmable integrated circuit device

Family To Family Citations
* Cited by examiner, † Cited by third party

Non-Patent Citations (11)
Title
A. A. Ibrahem, H. Elsimary, and A. E. Salama, "FPGA implementation of modified radix 2 SRT division algorithm," 2003 IEEE 46th Midwest Symposium on Circuits and Systems, vol.
3, pp. 1419-1422, Dec. 2003. *
D. Das Sarma and D. W. Matula, "Faithful Bipartite ROM Reciprocal Tables", Technical Report, Computer Science Department, Southern Methodist University, May 1995. *
D. DasSarma and D. Matula, "Measuring the Accuracy of ROM Reciprocal Tables," IEEE Trans. Computers, vol. 43, No. 8, pp. 932-940, Aug. 1994. *
D. L. Fowler and J. E. Smith, "An accurate high speed implementation of division by reciprocal approximation," in Proc. 9th IEEE Symp. Comput. Arithmetic, 1989, pp. 60-67. *
D.L. Harris, S.F. Oberman, and M.A. Horowitz, "SRT Division Architectures and Implementations," Proc. 13th IEEE Symp. Computer Arithmetic, Jul. 1997. *
H. Suzuki, H.Makino, K.Mashiko, and H. Hamano, "A floating-point divider using redundant binary circuits and an asynchronous clock scheme," in Proc. International Conf.
Computer Design (ICCD), pp. 685-689, Oct. 1997. *
M. D. Ercegovac, T. Lang and P. Montuschi, "Very high radix division with prescaling and selection by rounding," IEEE Transactions on Computers, vol. 43, No. 8, pp. 909-918, Aug.
1994. *
Pan, Kay, Chun, and Wey, "High-radix SRT division with speculation of quotient digits," Proc. International Conference of Computer Design (ICCD), Austin, Texas, 1995, pp. 479-482.
*
S. Upadhyay and J.E. Stine, "Pipelining high-radix SRT division algorithms," 50th Midwest Symposium on Circuits and Systems, pp. 309-312, Aug. 2007. *
Stuart F. Oberman and Michael J. Flynn, "An Analysis of Division Algorithms and Implementations," Technical Report: CSL-TR-95-675, Jul. 1995. *
T. E. Williams and M. Horowitz, "SRT Division Diagrams and Their Usage in Designing Custom Integrated Circuits for Division," Technical Report: CSL-TR-87-328, Nov. 1986. *
* Cited by examiner, † Cited by third party

Cited By (16)
Publication number

Priority date

Publication date

Assignee

Title

US10127015B2

2016-09-30

2018-11-13

International Business Machines
Corporation

Decimal multiply and shift instruction

US10209957B2

2015-05-04

2019-02-19

Samsung Electronics Co., Ltd.

Partial remainder/divisor table split implementation

US10235137B2

2016-09-30

2019-03-19

International Business Machines
Corporation

Decimal shift and divide instruction

US8103712B2 *

2007-09-28

2012-01-24

Lexmark International, Inc.

Method for performing a division operation in a system

US8407274B2 *

2010-05-21

2013-03-26

The Board Of Regents Of The
University Of Texas System

Machine division

US20110289128A1 *

2010-05-24

2011-11-24

Chih-Ta Star Sung

Method of performing discrete cosine transform

US9348796B2

2013-09-19

2016-05-24

International Business Machines
Corporation

Arithmetic operation in a data processing system

Family To Family Citations

https://patents.google.com/patent/US8725786B2/en?q=(high+radix)&oq=high+radix

18/20

2/14/26, 1:55 PM

US8725786B2 - Approximate SRT division method - Google Patents

JP2016062404A *

2014-09-19

2016-04-25

サンケン電気株式会社

Arithmetic processing method and arithmetic processor

US10353671B2 *

2016-01-13

2019-07-16

Arm Limited

Circuitry and method for performing division

CN109669667B *

2017-10-17

2023-09-19

电信科学技术研究院

Data processing method and device implemented on fixed-point DSP
chip

US10447983B2 *

2017-11-15

2019-10-15

Nxp Usa, Inc.

Reciprocal approximation circuit

US10503473B1 *

2018-05-30

2019-12-10

Apple Inc.

Floating-point division alternative techniques

JP2022121055A *

2021-02-08

2022-08-19

キオクシア株式会社

Arithmetic device and arithmetic circuit

CN117149133B *

2023-09-05

2024-05-24

上海合芯数字科技有限公司

Floating point division and square root operation circuit lookup table
construction method and operation method

CN117785117B *

2023-12-26

2024-08-20

合芯科技(苏州)有限公司

Realizing the division circuit of SRT16 based on SRT4

CN118819467B *

2024-07-16

2025-07-08

上海壁仞科技股份有限公司

Method, device, equipment and medium for operating coordinates in
artificial intelligent operator

* Cited by examiner, † Cited by third party, ‡ Family to family citation

Similar Documents
Publication

Publication Date

Title

US8725786B2

2014-05-13

Approximate SRT division method

EP4042270B1

2025-03-19

Hardware algorithm for complex-valued exponentiation and logarithm using simplified sub-steps

Obermann et al.

2002

Division algorithms and implementations

Mohan et al.

2016

Residue number systems

US6360241B1

2002-03-19

Computer method and apparatus for division and square root operations using signed digit

Ercegovac et al.

2002

Reciprocation, square root, inverse square root, and some elementary functions using small multipliers

Pineiro et al.

2003

High-speed double-precision computation of reciprocal, division, square root, and inverse square root

Lang et al.

2007

A radix-10 digit-recurrence division unit: algorithm and architecture

US20110231468A1

2011-09-22

High-radix multiplier-divider

Wang et al.

2005

Decimal floating-point square root using Newton-Raphson iteration

Bruguera

2019

Low latency floating-point division and square root unit

Omondi

2020

Cryptography arithmetic

WO2013102588A1

2013-07-11

Range check based lookup tables

Raveendran et al.

2020

A novel parametrized fused division and square-root POSIT arithmetic architecture

Sreedhar et al.

2022

A fast large-integer extended gcd algorithm and hardware design for verifiable delay functions and modular inversion

Pinckney et al.

2007

Parallelized radix-4 scalable Montgomery multipliers

Patankar et al.

2020

Division algorithms-from past to present chance to improve area time and complexity for digital applications

Bruguera

2023

Radix-64 floating-point division and square root: Iterative and pipelined units

US20090006509A1

2009-01-01

High-radix multiplier-divider

Nannarelli

2018

Tunable floating-point for energy efficient accelerators

Ushasree et al.

2013

VLSI implementation of a high speed single precision floating point unit using verilog

US7174015B1

2007-02-06

Methods and apparatus for variable radix scalable modular multiplication

Sutter et al.

2004

Comparative study of SRT-dividers in FPGA

Panda et al.

2015

A novel vedic divider architecture with reduced delay for VLSI applications

Pineiro et al.

2008

A radix-2 digit-by-digit architecture for cube root

Priority And Related Applications

https://patents.google.com/patent/US8725786B2/en?q=(high+radix)&oq=high+radix

19/20

2/14/26, 1:55 PM

US8725786B2 - Approximate SRT division method - Google Patents

Priority Applications (1)
Application

Priority date

Filing date

Title

US12/770,197

2009-04-29

2010-04-29

Approximate SRT division method

Applications Claiming Priority (2)
Application

Filing date

US21488809P

2009-04-29

US12/770,197

2010-04-29

Title

Approximate SRT division method

Legal Events
Date

Code

Title

Description

2010-04-29

AS

Assignment

Owner name: UNIVERSITY OF MASSACHUSETTS, MASSACHUSETTS
Free format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:POWELL,
MAKIA;REEL/FRAME:024311/0345
Effective date: 20100429

2014-04-23

STCF

Information on status: patent grant

Free format text: PATENTED CASE

2017-12-25

FEPP

Fee payment procedure

Free format text: MAINTENANCE FEE REMINDER MAILED (ORIGINAL EVENT CODE: REM.)

2018-04-25

FEPP

Fee payment procedure

Free format text: SURCHARGE FOR LATE PAYMENT, SMALL ENTITY (ORIGINAL EVENT CODE: M2554)

2018-04-25

MAFP

Maintenance fee payment

Free format text: PAYMENT OF MAINTENANCE FEE, 4TH YR, SMALL ENTITY (ORIGINAL EVENT CODE:
M2551)
Year of fee payment: 4

2022-01-03

FEPP

Fee payment procedure

Free format text: MAINTENANCE FEE REMINDER MAILED (ORIGINAL EVENT CODE: REM.); ENTITY
STATUS OF PATENT OWNER: SMALL ENTITY

2022-06-20

LAPS

Lapse for failure to pay maintenance fees

Free format text: PATENT EXPIRED FOR FAILURE TO PAY MAINTENANCE FEES (ORIGINAL EVENT
CODE: EXP.); ENTITY STATUS OF PATENT OWNER: SMALL ENTITY

2022-06-20

STCH

Information on status: patent discontinuation

Free format text: PATENT EXPIRED DUE TO NONPAYMENT OF MAINTENANCE FEES UNDER 37 CFR
1.362

2022-07-12

FP

Lapsed due to failure to pay maintenance fee

Effective date: 20220513

Data provided by IFI CLAIMS Patent Services

About

Send Feedback

Public Datasets

Terms

https://patents.google.com/patent/US8725786B2/en?q=(high+radix)&oq=high+radix

Privacy Policy

Help

20/20

2/14/26, 1:59 PM

US8774625B2 - Method and systems for implementing high-radix switch topologies on relatively lower-radix switch physical…

Patents
Back to results

12 of 125,048

high radix

(high radix);

Method and systems for implementing high-radix switch topologies on relatively lower-radix switch physical
networks
Abstract
Embodiments of the present invention are directed to implementing high-radix switch topologies on
relatively lower-radix physical networks. In one embodiment, the method comprises constructing the

US8774625B2
United States

physical network (702) composed of one or more optical switches connected via one or more
Download PDF

waveguides. A desired switch topology (704) is then designed for implementation on the physical

Find Prior Art

Similar

network. The switch topology is then overlain on the switch network by configuring the optical
switches and waveguides (706) to implement the switch topology on the physical network. The
optical switches can be reconfigured following a transmission over the physical network and can be
configured to implement circuit switching or packet switch.

Inventor: Nathan L. Binkert, Alan L. Davis, Moray McLaren
Current Assignee : Valtrus Innovations Ltd , Hewlett Packard
Development Co LP

Images (19)
Worldwide applications
2008 WO CN EP US

Application US13/058,024 events

Classifications
G02B6/356 Switching arrangements, i.e. number of input/output ports and interconnection
types in an optical cross-connect device, e.g. routing and switching aspects of interconnecting
different paths propagating different wavelengths to (re)configure the various input and output
links

G02B6/02042 Multicore optical fibres
G02B6/3512 Optical coupling means having switching means involving stationary waveguides
with moving interposed optical elements the optical element being reflective, e.g. mirror

G02B6/3556 NxM switch, i.e. regular arrays of switches elements of matrix type constellation
Hide more classifications

2008-08-08

Application filed by Hewlett Packard
Development Co LP

2011-06-09

Assigned to HEWLETT-PACKARD DEVELOPMENT
COMPANY, L.P.

2011-07-21

Publication of US20110176804A1

2014-07-08

Application granted

2014-07-08

Publication of US8774625B2

2021-01-26

Assigned to OT PATENT ESCROW, LLC

2022-08-05

Assigned to VALTRUS INNOVATIONS LIMITED

Status

Active

2029-07-12

Adjusted expiration

Info: Patent citations (14), Non-patent citations (7) , Cited by
(58), Legal events, Similar documents, Priority and Related
Applications

Landscapes

External links: USPTO, USPTO PatentCenter, USPTO
Assignment, Espacenet, Global Dossier, Discuss

Physics & Mathematics
Engineering & Computer Science
Show more

Claims (18)

Hide Dependent

The invention claimed is:
1. A method for implementing a higher-radix switch topology on a lower-radix physical switch network, the method comprising:
providing the lower-radix physical switch network comprising hybrid packet/circuit switches connected via links, the hybrid packet/circuit switches including at least one
circuit switch and at least one packet switch optically coupled to the at least one circuit switch;
configuring the hybrid packet/circuit switches to implement a first higher-radix switch topology on the physical switch network, wherein configuring the hybrid packet/circuit
switches comprises configuring the hybrid packet/circuit switches with a number of hops based on a number of the hybrid packet/circuit switches and a radix of the hybrid
packet/circuit switches; and
reconfiguring the hybrid packet/circuit switches to implement a second, different higher-radix switch topology on the physical switch network, the reconfiguring comprising
changing circuit routes of the at least one circuit switch, wherein each of the first and second higher-radix switch topologies has a higher radix than the lower-radix physical
switch network.

https://patents.google.com/patent/US8774625B2/en?q=(high+radix)&oq=high+radix&page=1

1/12

2/14/26, 1:59 PM

US8774625B2 - Method and systems for implementing high-radix switch topologies on relatively lower-radix switch physical…

2. The method of claim 1 further comprising configuring the hybrid packet/circuit switches to carry out a hybrid combination of circuit switching and packet switching.
3. The method of claim 1 wherein the hybrid packet/circuit switches further comprise one of more micro-electromechanical system mirror farm-based circuit switches
including micro-electromechanical mirrors.
4. The method of claim 1 wherein the links comprise one of:
a multi-core fiber; and
a multi-core photonic crystal fiber.
5. The method of claim 1 wherein configuring the hybrid packet/circuit switches comprises splicing cores of multi-core optical fibers to cores of other multi-core optical
fibers and switches.
6. The method of claim 1 wherein the configuring comprises orienting mirrors of a micro-electromechanical system micromirror switch, and the reconfiguring comprises
reorienting the mirrors of the micro-electromechanical system micromirror switch.
7. The method of claim 1 wherein configuring the hybrid packet/circuit switches comprises configuring each of the hybrid packet/circuit switches to transmit optical
signals that are destined for other hybrid packet/circuit switches.
8. The method of claim 1, wherein the at least one packet switch is to receive input signals from the at least one circuit switch, and to send output signals to the at least
one circuit switch, the input and output signals corresponding to data communicated for a computing device connected to the at least one packet switch.
9. The method of claim 1, wherein each of the first and second higher-radix switch topologies has a higher numbers of input or output ports than the lower-radix physical
switch network.
10. The method of claim 1, wherein the first higher-radix switch topology is selected from among a cross-bar switch topology, X-mesh switch topology, hex-mesh switch
topology, cubic mesh switch topology, ring switch topology, and folded Clos switch topology, and the second higher-radix switch topology is a different topology selected
from among a cross-bar switch topology, X-mesh switch topology, hex-mesh switch topology, cubic mesh switch topology, ring switch topology, and folded Clos switch
topology.
11. The method of claim 1, wherein the number of hops comprises a number of paths traversed from a source to a destination.
12. A method for implementing a higher-radix switch topology on a lower-radix physical switch network, the method comprising:
providing the lower-radix physical switch network comprising hybrid packet/circuit switches connected via links, the hybrid packet/circuit switches including at least one
circuit switch and at least one packet switch optically coupled to the at least one circuit switch;
configuring the hybrid packet/circuit switches to implement a first higher-radix switch topology on the physical switch network, wherein configuring the hybrid packet/circuit
switches comprises configuring the hybrid packet/circuit switches with a hop count on the order of logr(N) where N represents a number of the hybrid packet/circuit
switches and r is the radix of the hybrid packet/circuit switches; and
reconfiguring the hybrid packet/circuit switches to implement a second, different higher-radix switch topology on the physical switch network, the reconfiguring comprising
changing circuit routes of the at least one circuit switch, wherein each of the first and second higher-radix switch topologies has a higher radix than the lower-radix physical
switch network.
13. A system comprising:
a lower-radix physical switch network comprising hybrid packet/circuit switches connected via links, the hybrid packet/circuit switches including at least one circuit switch
and at least one packet switch optically coupled to the at least one circuit switch,
wherein the hybrid packet/circuit switches are configurable to implement a first higher-radix switch topology on the physical switch network, wherein configuring the hybrid
packet/circuit switches comprises configuring the hybrid packet/circuit switches with a hop count based on a number of the hybrid packet/circuit switches and a radix of
the hybrid packet/circuit switches, and
wherein the hybrid packet/circuit switches are reconfigurable to implement a second, different higher-radix switch topology on the physical switch network, the reconfiguring
comprising changing circuit routes of the at least one circuit switch, wherein each of the first and second higher-radix switch topologies has a higher radix than the lowerradix physical switch network.
14. A system of claim 13:
wherein the packet switch is to route packets using addresses in headers of the packets, and
the packet switch is to convert optical signals received by the circuit switch on input optical links into electrical signals, and to convert electrical signals into optical
signals that are sent to the circuit switch for output on output optical links,
the circuit switch including mirrors configured to be reoriented to change circuit routes of the circuit switch, the mirrors having a first orientation to implement the first
higher-radix switch topology, and the mirrors configured to reorient to a second, different orientation to implement the second, higher-radix switch topology.
15. The system of claim 14 wherein each of the input and output optical links comprises one of:
a multi-core fiber; and
a photonic crystal fiber.
16. The system of claim 14, wherein the mirrors include reorientable micro-electromechanical mirrors to direct optical signals between optical links.
17. The system of claim 14, wherein the first higher-radix switch topology is selected from among a cross-bar switch topology, X-mesh switch topology, hex-mesh switch
topology, cubic mesh switch topology, ring switch topology, and folded Clos switch topology, and the second higher-radix switch topology is a different topology selected
from among a cross-bar switch topology, X-mesh switch topology, hex-mesh switch topology, cubic mesh switch topology, ring switch topology, and folded Clos switch
topology.

https://patents.google.com/patent/US8774625B2/en?q=(high+radix)&oq=high+radix&page=1

2/12

2/14/26, 1:59 PM

US8774625B2 - Method and systems for implementing high-radix switch topologies on relatively lower-radix switch physical…

18. The system of claim 13, wherein the hybrid packet/circuit switches are configurable with a hop count on the order of logr(N), where N represents a number of the
hybrid packet/circuit switches and r is the radix of the hybrid packet/circuit switches.

Description
CROSS-REFERENCE TO RELATED APPLICATION
This application is a national stage application under 35 U.S.C. §371 of PCT/US2008/009524, filed Aug. 8, 2008.
TECHNICAL FIELD
Embodiments of the present invention relate to optical devices, and, in particular, to constructing optical-based, high-radix switch topologies that are based on low-radix
switch physical networks.
BACKGROUND
In order for massively parallel systems to achieve their full performance potential, the processing power, memory capacity, and communication resources should be
balanced. Communication performance can be measured by considering bisection bandwidth and average communication latency. Bisection bandwidth refers to the
bandwidth between two substantially equal parts of a network. Average communication latency refers to the average time delay between the moment information is sent
to the moment it is received. Latency can be measured as the sum of fall-through delay and payload transport time. The payload transport time is the number of bytes
being transported divided by the aggregate bandwidth of the physical link which may consist of one or more wires in the case of electrical communication or waveguides
in the case of optical communication. Fall-through delay is the time it takes 1 bit on any path to transit from a source to a destination for a particular source to
destination route and can be determined as the sum of the time on the wire or waveguide plus any propagation delay through any intervening logic, plus any control delay
incurred in the routing mechanism.
Two fundamentally different switching networks are possible: packet switched and circuit switched. Circuit switched networks consist of a set of circuit switches
connected by communication links. A link may be a wire, an optical fiber, or any other suitable device for transmitting information in electrical or optical signals. The
circuit switches can be configured to directly connect input links to output links to create a desired path from a sender to the desired receiver. Circuit switched networks
must be configured prior to a communication event. Circuit switched networks therefore have a physical path topology which changes based on the configuration of the
network. Packet switched networks have a fixed physical topology consisting of a set of routers which are interconnected with links. When routing a packet from a
source to a destination in a packet switched network, a routing decision must be made at each router. When a packet arrives at a router, the router examines the
destination address portion of the packet header. The router then places the packet on an appropriate link that leads to the next router on a path to the destination or on
the link that actually reaches the destination.
High-radix switch networks reduce all fall-through delay components by reducing the average number of hops which a message must pass through, but suffer from a
cost perspective due to the increased wiring complexity. In the subsequent description, the term “switch” will refer to either a circuit switch in circuit switched networks or
a router in packet switched networks. For switches with an equal number of input and output links the term “radix” refers to the number of input or output links
associated with each switch. Each link comprises 1 or more wires or waveguides. Each wire is capable of carrying 1 bit of information per clock cycle. A number of
different wavelengths can be wave division multiplexed on a single waveguide. Thus, a waveguide capable of carrying n wavelengths is capable of carrying n bits of
information per clock cycle. The term “link width” refers to the number of bits that can be transported on a link per clock cycle. The term “hop” refers to the number of
paths traversed from a source to a destination. If a message or packet traverses m hops then m−1 switches will be involved in making routing decisions. Circuit switched
networks reduce latency by removing the need to make a routing decision at each hop but incur configuration delays which are problematic if reconfiguration is done
often.
The cost of a communication network is based on the number of switches, routers, repeaters, and the associated system level integration and fabrication effort, all of
which are heavily influenced by wiring complexity. Wiring complexity refers to the number of links that must be connected to form the network. Due to pin bandwidth
limitations, most high performance communication fabrics are built from high-radix topologies which employ many low bandwidth connections rather than fewer high
bandwidth connections. The use of electrical high bandwidth connections is problematic in terms of excess power consumption and the difficulty in insuring that
sufficient signal integrity is present to provide reliable communication.
Computer systems with very large numbers of nodes combined with high-radix switch networks present a significant wiring challenge at installation time and significant
cost in terms of physical connectors and cables. Networks with a large number of nodes can also be problematic when adding resources to the network due to the need
to reconfigure large numbers of cables and wires.
In recent years, a number of high-radix switch topologies have been proposed for implementation of massively parallel computing systems. For example, fat-trees are a
high-radix switch topology that were used in Connection Machines, such as the CM-5, and are currently used in the Black Widow switch of systems produced by Cray, Inc.
Numerous other contributions have been made which use high-radix switch topologies on physical switch networks, such as the flattened Butterfly by John Kim, James
Balfour, William Daily “Flattened Butterfly Topology for On-chip Networks” In the proceedings of the 40th Annual IEEE/ACM International Symposium on Micro-architecture
(MICRO), Chicago, Ill. December 2007, and dilated path multistage switches by Frederic Chong, Erin Egozy, and Andre DeHon “Fault Tolerance and Performance of
Multipath Multistage Interconnection Networks” In the proceedings of Advanced Research in VLSI and Parallel Systems, MIT press, March 1992, and multistage Banyan
networks by L. Rodney Goke and G. J. Lipovski “Banyan networks for partitioning multiprocessor systems” In the Proceedings of the International Symposium on Computer

Architecture (ISCA), ACM, New York, 1973.
Accordingly, systems and methods for economically implementing high-radix switch topologies and efficiently utilizing low-radix switch physical networks and hybrid
packet and circuit switched control approaches are desired.
SUMMARY
Embodiments of the present invention are directed to implementing high-radix switch topologies on relatively low-radix switch physical networks. In one method
embodiment, the method comprises constructing the switch physical network comprising hybrid packet/circuit switches connected via links. A desired high-radix packet
switch topology is then designed for implementation on the relative physical network. The packet switch topology is then overlain on the physical network by configuring
the hybrid packet/circuit switches to implement the logical packet switch topology on the physical network. The hybrid packet/circuit switches can be reconfigured
following a transmission over the physical network and can be configured to implement alternate packet switching topologies.
In one system embodiment, a hybrid packet/circuit switch includes an optical circuit switch which is optically coupled to one or more input optical links and one or more
output optical links, and a packet switching device optically coupled to the optical circuit switch. The packet switching device converts optical signal inputs to the circuit

https://patents.google.com/patent/US8774625B2/en?q=(high+radix)&oq=high+radix&page=1

3/12

2/14/26, 1:59 PM

US8774625B2 - Method and systems for implementing high-radix switch topologies on relatively lower-radix switch physical…

switch on the input optical links into electrical signals that are buffered, analyzed, routed, and converted back into optical signals that are sent to the optical circuit switch
which places the signals on the output optical links.
BRIEF DESCRIPTION OF THE DRAWINGS
FIG. 1 shows a single switch device configured in accordance with embodiments of the present invention.
FIG. 2A shows a cross-sectional view of a 16-core fiber configured in accordance with embodiments of the present invention.
FIG. 2B shows a cross-sectional view of a photonic crystal fiber configured in accordance with embodiments of the present invention.
FIG. 3 shows an exemplary circuit switch configured in accordance with embodiments of the present invention.
FIG. 4A shows a circuit switch network composed of nine circuit switches.
FIG. 4B shows a circuit switched network configured to maintain connectivity when an optical link fails.
FIG. 5 shows an example of a passive network.
FIGS. 6A-6C shows three hybrid packet/circuit switches configured in accordance with embodiments of the present invention.
FIG. 7 shows a flowchart representing a number steps in a method for implementing a high-radix switch topology on a low-radix switch physical network in accordance
with embodiments of the present invention.
FIG. 8 shows a schematic representation of a ring-shaped, switch physical network 800 configured in accordance with embodiments of the present invention.
FIG. 9 shows a unidirectional ring switch topology that can be implemented on the network shown in FIG. 8 in accordance, with embodiments of the present invention.
FIG. 10A shows the output directions of optical signals output from a switch 0 of the switch network shown in FIG. 8 in accordance with embodiments of the present
invention.
FIG. 10B shows a chordal ring switch topology configured in accordance with embodiments of the present invention.
FIG. 11 shows a schematic representation of radix 5 switch of the switch topology shown in FIG. 8B in accordance with embodiments of the present invention.
FIG. 12 shows a schematic representation of waveguides of a link that are dedicated to transmitting optical signals to and from a switch in accordance with
embodiments of the present invention.
FIG. 13 shows a diagram representing paths optical signals take on the switch topology, shown in FIG. 10B, in accordance with embodiments of the present invention.
FIG. 14 shows a cross-sectional view and schematic representation of a 16-core fiber configured to route optical signals around a packet switch in accordance with
embodiments of the present invention.
FIG. 15 shows an exemplary switch topology configured in accordance with embodiments of the present invention.
FIG. 16 shows an exemplary physical Clos network configured in accordance with embodiments of the present invention.
FIG. 17 shows a micromirror switch configured to operate as an intermediate switch in the Clos switch network shown in FIG. 16 in accordance with embodiments of the
present invention.
DETAILED DESCRIPTION
Method and system embodiments of the present invention are directed to implementing high-radix communication switch topologies on relatively lower radix physical
networks. Embodiments of the present invention can be implemented using optical technology.
Definition of Terms
As used herein, the term “optical signal” refers to electromagnetic radiation of a particular wavelength that has been amplitude modulated. In other words, an optical
signal can be composed of high and low amplitude patterns, where, for example, a “high” amplitude represents the bit “1” and a “low” amplitude represents the bit “0.”
The term “waveguide” refers to an optical fiber, a core, or any suitable light transmitting medium surrounded by a confinement layer of lower dielectric constant.
The term “link” as used herein refers to one or more waveguides.
The term “switch topology” used herein refers to a configuration or arrangement of switches and interconnecting communications links forming a communication
network.
The term “physical” as used herein refers to items having substance or material existence in the real material world, rather than as an idea or notion, and are able to be
touched and seen.
The term “radix” as used herein refers to the number of input or output ports of a switch.
The term “switch” as used herein as a general term to refer to circuit switches and hybrid packet/circuit switches, which are described in greater detail below in
subsections I and II, respectively.
Advantages
Embodiments of the present invention have a number of advantages over conventional electrical based systems and methods for configuring switch networks. In all
optical implementations, the circuit switched mechanism significantly minimizes the energy and component overheads that would be incurred in an electrical-based
network. In addition, dense wave division multiplexing (“DWDM”) can be used to further increase the cross-section bandwidth. In contrast, DWDM on wires is simply not
feasible and the only way to increase the cross-sectional bandwidth of electrical switches is to increase the number of wires in each channel or increase the speed of
each wire. Increasing wire speed is fundamentally limited by signal integrity and power problems. Increasing the number of wires also incurs additional cost and is
problematic due to electrical component input-output pin limitations.

https://patents.google.com/patent/US8774625B2/en?q=(high+radix)&oq=high+radix&page=1

4/12

2/14/26, 1:59 PM

US8774625B2 - Method and systems for implementing high-radix switch topologies on relatively lower-radix switch physical…

Many high-radix switch networks, once designed, are static in their instantiation. By contrast, embodiments of the present invention are directed to systems and methods
for designing various high-radix switch topologies that can be implemented on a low-radix switch physical network. In other words, the circuit switch topology can be
changed to meet the needs of changing traffic patterns or to compensate for link or switch failure. Note this is useful only in the case where the traffic patterns persist
long enough to amortize the configuration time. This allows the option of splitting the cross section bandwidth on a per channel granularity between one or more specific
circuit switched routes.
Embodiments of the present invention also achieve a low source to destination hop count and the high cross-section bandwidth associated with high-radix switch
topologies while achieving the low cost, low link count and simple interconnect complexity of low-radix switch physical networks.
Circuit switches and passive networks are described in subsection I. Hybrid packet/circuit switches are described in subsection II. Methods for implementing high-radix
communication switch topologies on relatively lower radix switch networks using circuit switches are described for two exemplary networks in subsection III.
I. Circuit Switches and Passive Networks
Switched networks include numerous instances of switch devices and links. A physical network is configured by interconnecting switch devices with links to implement a
particular network topology. FIG. 1 shows a single switch device 102 connected to input optical links 104-106 and output optical links 107-109 in accordance with
embodiments of the present invention. As shown in the example of FIG. 1, each of the input optical links 104-106 transmits four input optical signals to the switch device
102, where the input optical signals are represented by directional arrows pointing to the switch device 102. Each of the output optical links 107-109 transmits four
optical signals away from the switch device 102, where the output optical signals are represented by directional arrows pointing away from the switch device 102. The
optical links can be configured with 4 waveguides or waveguides that each carry 1 optical signal, or using DWDM, each optical link 104-109 can be configured with a
single waveguide configured to carry multiple optical signals.
Multi-core fibers (“MCFs”) and photonic-crystal fibers (“PCFs”) are just two examples of optical links. MCFs and PCFs contain multiple waveguides where each
waveguide can transmit one or more optical signals. FIG. 2A shows a cross-sectional view of a 16-core fiber 200 configured in accordance with embodiments of the
present invention. The 16-core fiber 200 includes 16 waveguides called “cores,” such as core 202, extending the length of the fiber 200. The cores are surrounded by a
relatively lower refractive index cladding material 204 that forms a cladding layer around each core. Although the multi-core fiber is shown as having a circular cross
section, the cores can be arranged to have a planar configuration to produce multi-core optical fiber ribbons. Embodiments of the present invention are not limited to 16core fibers. The fibers can be configured with any suitable number of cores, and the cores can be configured to support one or more modes of electromagnetic radiation.
FIG. 2B shows a cross-sectional view of a PCF 210 configured in accordance with embodiments of the present invention. The PCF 210 is composed of a hexagonal
lattice of holes, such as air hole 212, or another suitable relatively low refractive index material that extends the length of the fiber in a relatively higher refractive index
material 214, such as silica. As shown in FIG. 2B, the holes are arranged to form 19 cores, such as core 216, where light is guided. Other PCFs can be configured with
fewer or more cores and can be configured with concentric rings of two or more materials that operate as Bragg reflectors to confine light to a central core.
Returning to FIG. 1, the switch device 102 can be used to connect particular input waveguides of the input optical links 104-106 to particular output waveguides of the
output optical links 107-109. For example, the switch device 102 can be configured to direct an optical signal 110 input to the switch device 102 on a waveguide of the
optical link 104 to a particular waveguide of the optical link 109. FIG. 1 also shows how a high radix switch topology can be constructed from a physically low radix
switch topology. In this case, the radix of this switch device 102 is 3 since there are 3 input optical links 104-106 and 3 output optical links 107-109.
The switch device 102 can be either a circuit switch or a passive network. A circuit switch needs to be configured to achieve a desired connectivity between links. A
passive network consists of wires or optical waveguides and contains no switches that can be configured. A passive network therefore implements an interconnect
topology that is static. The advantage of a circuit switch is that it can be reconfigured as needed to more efficiently handle changing demands of a network or to maintain
connectivity in networks where either a link or switch device or both have failed. Other advantages of the circuit switch include that there is no routing delay contribution
to the fall through delay which helps with both latency and power. A disadvantage of circuit switches is that while the switches are being configured no communication
traffic can be carried out. Hence the cost associated with the flexibility benefits of a circuit switch is due to reduced network availability during the reconfiguration time. If
reconfiguration is frequent then a significant increase in average packet latency will be observed, whereas if reconfiguration is rare the average packet latency will be
reduced since the reconfigured network advantage will outweigh the availability loss due to reconfiguration.
FIG. 3 shows an exemplary circuit switch 300 configured in accordance with embodiments of the present invention. The circuit switch 300 includes a microelectromechanical system (“MEMS”) mirror farm 302, a first lens array 304, and a second lens array 306. Sixteen waveguides, also called cores, of an incoming 16-core
fiber 308 and 16 cores of an outgoing 16-core fiber 310 are each capped by an associated lens in the lens arrays 304 and 306. For example, incoming core fiber 312 is
capped by a lens 314 in the lens array 304, and outgoing core fiber 316 is capped by a lens 318 in the lens array 306. The MEMS mirror farm 302 is composed of an array
of 16 individual, mechanically controlled silicon micromirrors. The lenses in the lens array 304 can each be oriented to direct light onto a particular micromirror. The
lenses in the lens array 306 can be configured to collect light reflected from the micromirrors in the mirror farm 302 into a corresponding outgoing fiber. The circuit
switch 300 can be used as a circuit switch by orienting the micromirrors to direct optical signals input on particular incoming cores into particular outgoing cores. The
incoming cores can be directly connected to a first computing device such as a packet switch or computer, or to another circuit switch within a switch network, and the
outgoing cores can be directly connected to a second computing device such as a packet switch or computer, or another circuit switch within the switch network. For
example, consider an optical signal originated from a circuit switch on a network connected to the circuit switch 300 via core 312. The optical signal entering on the core
312 is directed by the lens 314 onto the micromirror 320. The micromirror 320 was pre-oriented to reflect the optical signal to the lens 318, which directs the light out of
the switch 300 along the core 316. The core 316 can lead directly to another circuit switch on the network or to a computational device. The micromirrors can be
reoriented to break old connections and make new ones in order to implement a variety of different switch topologies on the same physical network. Micromirror
switches are not limited to the square 4×4 mirror farm 302. In other embodiments, micromirror switches can be used with any number of rows and columns of
micromirrors and lens arrays to provide switching for any number of incoming and outgoing multi-core optical fibers.
FIG. 4A shows a circuit switch network 400 composed of nine circuit switches 401-409. Directional arrows represent the physical instantiation of the optical links.
Dashed lines represent the logical, direct circuit routes between circuit switches. The circuit switches 401-409 can be mirror-farmed based switches such as those
described in FIG. 3. The circuit switches are configured so that each circuit switch can transmit directly to another circuit switch in the x and y directions. In other words,
the circuit switches 401-409 are configured so that optical signals logically travel in the directions identified by the dashed lines, but physically travel on the optical links
represented by the directional arrows. For example, circuit switch 406 can transmit optical signals directly to circuit switch 405 as indicated by dashed line 410, and
circuit switch 405 is also configured so that circuit switch 406 can transmit directly to circuit switch 404, as indicated by dashed line 411.
Circuit switch networks can be configured to create a variety of possible circuit routes and, as described above, can be reconfigured to direct signals around optical links
that fail or to meet the changing demands of traffic. FIG. 4B shows how the circuit switched network 400 can be reconfigured to maintain full connectivity when an
optical link fails. In the example of FIG. 4B, dashed-line directional arrow 412 represents a failed optical link. As a result, the logical paths represented by dashed lines
410 and 411, in FIG. 4A, no longer exist. Using excess capacity on the existing optical links and reconfiguring circuit switches 402-409, circuit switch 406 can send
optical signals to circuit switch 405 via the logical path represented by dot-dash line 414, and the circuit switch 406 can send optical signals to the circuit switch 404 via
the logical path represented by dot-dash line 416. By building this new circuit the logical topology is maintained and the packet routing protocol can proceed oblivious to

https://patents.google.com/patent/US8774625B2/en?q=(high+radix)&oq=high+radix&page=1

5/12

2/14/26, 1:59 PM

US8774625B2 - Method and systems for implementing high-radix switch topologies on relatively lower-radix switch physical…

the failure of the physical link 412. When there is no excess capacity then the remaining capacity can be repartitioned to maintain full connectivity with the original hop
count properties but where the bandwidth of each route gets reduced since its share of the capacity has been reduced due to the failed link capacity loss.
A passive network, on the other hand, is one in which the input waveguides are physically connected to output waveguides. Once configured the topology of the passive
network does not change. Benefits of a passive network include that the implementation is less costly than a circuit switch and there is no latency penalty associated
with reconfiguration since the topology is fixed. A disadvantage is that the topology is inflexible and cannot dynamically adapt to new traffic requirements or component
failures on the network.
FIG. 5 shows an example of a passive network 500. Optical signals are received on 4-core optical fibers 501-504, and optical signals are output on 4-core optical fibers
505-508. As shown in the example of FIG. 5, the shuffle network 500 is configured by connecting each core of the 4-core optical fibers 501-504 to one particular core in
each of the four 4-core optical fibers 505-508. FIG. 5 reveals one particular embodiment for making such connections. The cores of each optical fiber are all labeled 1
through 4. Lines connecting cores in the optical fibers 501-504 to cores in the optical fibers 505-508 can represent optical fibers. The cores of the optical fiber 501 are all
optically connected to the cores labeled “1” in the optical fibers 505-508. The cores of the optical fiber 402 are all connected to the cores labeled “2” in the optical fibers
505-508. The cores of the optical fiber 503 are all connected to the cores labeled “3” in the optical fibers 505-508. The cores of the optical fiber 504 are all connected to
the cores labeled “4” in the optical fibers 505-508.
II. Hybrid Packet/Circuit Switches
One purpose of an interconnection network is to connect computational endpoints which are sources and destinations for the network's message traffic. As described
above, networks can be inflexible when configured with passive networks or costly when configured with only circuit switch devices. In addition, information is typically
transmitted over a network in packets. Packet switching involves breaking a message up into a number of packets. Each packet includes a header that is examined at
each switch along the path to decide which switch output the packet should be sent on in order to route the packet to the appropriate switch. The packet header can be
modified as part of the routing process and not all packets composing a given message have to take the same route from a source switch to a destination switch. The
most common form of packet switched networks is to connect a set of packet switch devices with links in a desired network topology. When packets arrive at a switch,
the packet switch examines the packet header to determine the destination address and then determines onto which waveguide of an output optical link to place the
packet. Packet switches also contain a variety of buffers to improve overall performance and also contain other storage resources as well as compute resources. In
general, a packet switch is just a specialized computing device.
System embodiments of the present invention are directed to implementing a packet switched network on top of a circuit switched network to create a hybrid
interconnection fabric with the advantage of packet switching and the reconfiguration and low latency benefits of circuit switching. These hybrid packet switched/circuit
switched networks are implemented with a hybrid combination of packet and circuit switches that are described as follows.
FIG. 6A shows a schematic representation of a first hybrid packet/circuit switch 600 configured in accordance with embodiments of the present invention. The hybrid
switch 600 includes a packet switch 602 optically coupled to a circuit switch 604, which, in turn, is optically coupled to input and output optical links 606-611, as
described above with reference to FIG. 1. As shown in the example of FIG. 6A, input optical signals 612 and output optical signals 614 are transmitted over waveguides
between the packet switch 602 and the circuit switch 604. The input optical signals undergo an optical-to-electrical (“OE”) conversion at the packet switch 602 so that
the information in each packet can be buffered, analyzed, and routed by the packet switch 602 to the appropriate destination. Once routed, the packet switch 602
converts the information into output optical signals using electrical-to-optical (“EO”) conversion. The output optical signals are sent to the circuit switch 604 and the
appropriate waveguide of the output optical links 604-611.
Alternatively, FIG. 6B shows a schematic representation of a second hybrid packet/circuit switch 620 configured in accordance with embodiments of the present
invention. The hybrid switch 620 is nearly identical to the hybrid switch 600, but the packet switch is replaced by a computer 622. The computer 622 receives input
optical signals from the circuit switch 604 and sends output optical signals to the circuit switch 604. The computer 622 employs OE conversion to convert the input
optical signals into electrical signals for processing and employs EO conversion to convert electrical signals generated by the computer 622 into output optical signals
that are sent to the circuit switch 604. When the computer 622 sends information it must choose the appropriate output waveguide of the output optical links 609-611.
In certain embodiments, an interconnect network can consist of hybrid packet/circuit switches, where a number of the packet switch ports are each connected to a local
computing device. The local computing device can be a computer, a processor, memory, sensor, or any other device. FIG. 6C shows the hybrid packet/circuit switch 600
connected to a local computing device 626 in accordance with embodiments of the present invention. In this embodiment, the packet switch 602 is configured to
determine whether or not packets are destined for the local computing device 626 or destined for a different switch. When the packets include the address of the local
computing device 626, the packet switch delivers the packet over electrical or optical links 628 which connect the packet switch 602 to the local computing device 626.
When the local computing device 626 needs to send a packet, the local computing device 626 sends the packet to the packet switch 626 on electrical links 630. The
packet switch 602 then examines the destination address in the packet to determine which of the output waveguides 614 is to be used to inject the packet into the circuit
switch 604, where the packet can be transmitted over one of the appropriate waveguides of the optical links 609-611 to reach the destination.
III. Implementing Switch Topologies
In the interest of brevity, method embodiments are described below for implementing network topologies on ring and Clos networks having 16 switches. These switch
topologies are merely exemplary of the many different kinds of switch topologies that methods of the present invention can be employed to implement and are by no
means intended to be exhaustive. Examples of other switch topologies for which methods of the present invention can be employed include cross-bars, X-mesh, hexmesh, and cubic mesh topologies. Meshes can also have wrap or twisted wrap topologies. Other topologies include chordal rings, a variety of multistage networks, such
as folded Clos, Banyan, fat-trees, and various forms of hypercubes, such as k-ary n-cubes, where k and n are integers.
FIG. 7 shows a flowchart representing a number steps in a method for implementing a topology of high-radix switches on a physical network of low-radix switches in
accordance with embodiments of the present invention. In step 702, a physical network composed of optical switches interconnected via one or more links is
constructed. Each switch can in turn be connected to any number of computing devices, which are sources and/or destinations of information. In step 704, based on the
switch and waveguide components comprising the physical switch network, a switch topology is then designed for transmitting information between switches within the
physical network. The switch topology provides a plan for configuring the switches to transmit information between switches on the physical network. Typically, the
switch topology with the fewest number of hops between switches receiving the most traffic is used to configure the physical network. Examples of switch topologies
that can be implemented on various physical networks of the present invention include a ring, a chordal ring, a mesh, a skinny tree, a Clos network, or any other suitable
switch topology that can be implemented using the available switches and waveguides of the physical network. In step 706, the switches and waveguides are then
configured within the physical network to implement the selected switch topology. In certain embodiments, the optical switches can be configured to implement packet
switching, circuit switching, or a combination of packet and circuit switching. In other words, depending on how the information is transmitted, the optical switches can
be circuit switches or hybrid packet/circuit switches. For example, if information is transmitted using packets, the physical network can be configured with hybrid
packet/circuit switches, otherwise circuit switches can be used. In addition, the physical network can be constructed with switches and waveguides that can be
reconfigured to meet changing switch topologies that are selected to meet the needs of changing traffic patterns on the physical network.

https://patents.google.com/patent/US8774625B2/en?q=(high+radix)&oq=high+radix&page=1

6/12

2/14/26, 1:59 PM

US8774625B2 - Method and systems for implementing high-radix switch topologies on relatively lower-radix switch physical…

A. A Ring-Shaped, Physical Network
FIG. 8 shows a schematic representation of a ring-shaped, physical network 800 configured in accordance with embodiments of the present invention. The physical
network 800 includes 16 optical switches connected by links, where each switch is represented by a dot and is identified by a number ranging from 0-15. For example,
switch 0 is connected to switches 15 and 1 via links 802 and 804, respectively. Each switch can in turn be connected to a number of different nodes (not shown). If
information is sent over the physical network using packets, then the switches 0-15 can be hybrid packet/circuit switches, and the switch network 800 is a packet
switched network implemented on a circuit switch network. In other embodiments, when information is not sent in packets, the switches 0-15 can be circuit switches.
Next, a switch topology can be designed to have the fewest number of hop counts given the switch network by employing high radix switches. The kinds of switches
selected and the number of available waveguides limits the kinds of switch topologies that can implemented on a particular switch network. Typically, a larger variety of
switch topologies can be implemented on physical networks having high radix switches. Physical networks configured with high radix switches can typically be
configured with fewer hop counts than physical networks employing relatively lower radix switches. For example, a physical network employing N high radix switches
typically has a hop count on the order of log2(N). In contrast, an analogous physical network employing N relatively lower radix switches can have a hop count on the
order of N.
In order to show how selecting the switch topology can be limited by the kinds of switches of the physical network, consider first a simple unidirectional, ring switch
topology for the network 800 where the switches in the network 800 are assumed to be radix 2. FIG. 9 shows a simple unidirectional ring switch topology 900 for the
switches 0-15 that can be implemented on the physical network 800 in accordance with embodiments of the present invention. In FIG. 9, single waveguides are
represented by curved line segments between switches, and information is transmitted in a unidirectional clockwise manner represented by directional arrow 902.
Curved segment 904 represents a single waveguide connecting switches 0 and 1. Switches 0-15 are radix 2 switches including two input ports and two output ports. For
example, switch 0 comprises two input ports and two output ports. Switch 0 receives optical signals sent from switch 15 in waveguide 906 at a first input port and
receives optical signals generated by a node (not shown) in a second input port represented by directional arrow 908. The node can be a computer, a circuit or packet
switch that provides a bridge to another ring. Switch 0 sends optical signals to switch 1 through a first output port in waveguide 904 and removes optical signals that are
destined for the node from the physical network 800 by sending these optical signals through a second output to the node, as represented by directional arrow 910.
The switch topology 900 can be implemented on the network 800 with radix 2 switches 0-15 that are connected to other switches via a single core optical fiber or a
single core fiber. In order for switch 0 to transmit information to switch 2, the information is first transmitted to switch 1. The information can be carried in packets that
include a header identifying the destination switch 2. The switch 1 converts the optical signals into electrical signals that are read by an electronically connected packet
switch which directs switch 1 to convert electrical signals encoding the same information into optical signals and transmit the optical signals via waveguide 912 to
switch 2. The number of packet switch router hops needed to get the information from the switch 0 to the switch 2 is two. The maximum number of packet switch router
hops for the ring switch topology 900 is 15. In general, a ring switch topology implemented on a ring network composed of N radix 2 switches connected via single
waveguides has a worst case hop count, also called the “switching diameter,” of N−1.
Consider now a high port count, unidirectional, chordal ring, switch topology that can be implemented on the physical network 800 where the network 800 is
implemented with switches 0-15, each of which is a radix 5 switch. FIG. 10A shows the output paths of switch 0 configured to transmit optical signals directly to
switches 1, 2, 4, and 8 in a unidirectional manner and a node (not shown) in accordance with embodiments of the present invention. Each separate transmission from the
switch 0 to the switches 1, 2, 4, and 8 is accomplished in a single hop on four separate waveguides 1001-1004. The switch 0 extracts optical signals directed to the node
as represented by directional arrow 1005. FIG. 10B shows a chordal ring switch topology 1010 configured in accordance with embodiments of the present invention. The
chordal ring switch topology 1010 is constructed by repeating the same pattern of input and output waveguides for switch 0, shown in FIG. 10A, at each switch. For the
sake of clarity, waveguides leading to nodes connected to each switch are not shown. For example, examination of switch topology 1010 reveals that switch 15 is
connected to waveguides 1011-1014 for transmitting optical signals to switches 0, 1, 3, and 7.
Each switch in the switch topology 1010 receives optical signals From four different switches in a single hop, extracts the optical signals that are destined for processing
at a node connected to the switch, and transmits optical signals to four different switches in a single hop. As a result, each switch has five input and live output ports for
a radix of 5. For example, FIG. 11 shows that based on the chordal ring switch topology of FIG. 10B, switch 0 receives optical signals on four separate waveguides from
switches 8, 12, 14, and 15, transmits optical signals on four separate waveguides to switches 1, 2, 4, and 8, and transmits and receives optical signals from a node on
one input and one output waveguide. Thus, switch 0 has five input and five output ports for a radix of 5.
The following description reveals how the high radix switches of the switch topology 1010 can be implemented on a physical network. As a described above, the
switches 0-15 of the physical network 800 can be implemented with circuit switches 300 or hybrid packet/circuit switches 600 and 620. Based on the switch topology
1010, it must be determined how each switch can be configured to extract optical signals destined for a node connected to the switch and direct optical signals that are
destined for other switches. FIG. 12 shows waveguides of a link that are dedicated to transmitting optical signals to and from switch 0 in accordance with embodiments
of the present invention. Directional arrow 1200 represents the direction optical signals travel on the network. Solid curves 1201-1204 represent separate waveguides of
the link that are dedicated to direct transmission of optical signals to the switches 1, 2, 4, and 8, and dashed curves 1205-1208 represent separate waveguides of the link
that are dedicated to direct transmission of optical signals to switch 0 from the switches 8, 12, 14, and 15. Thus, four waveguides of a link are needed to transmit optical
signals to and from each switch. However, this is the case for all 16 switches. Thus, each of the switches 0-15 must also be configured to allow a number of optical
signals that are not directed to a node connected to a switch to pass undisturbed.
The number of optical signals that need to pass through each switch can be determined as follows. FIG. 13 shows a diagram representing the path optical signals take
on the switch topology 1010 and can be used to configure each switch of the physical network 800, shown in FIG. 8, in accordance with embodiments of the present
invention. In FIG. 13, the 17 parallel lines running parallel to x-axis 1302 represent the 16 switches where the switch 8 has been repeated at the top and bottom.
Directional arrows running parallel to y-axis 1304 represent the unidirectional flow of optical signals that start and end at switches revealed by the switch topology 1010.
FIG. 13 reveals that each switch needs to be configured so that 11 optical signals pass each switch undisturbed. For example, based on the switch topology 1010,
directional arrows 1305-1308 represent optical signals sent from switch 0 to switches 1, 2, 4, and 8, directional arrows 1309-1312 represent optical signals sent from
switches 8, 12, 14, and 15 to switch 0, and directional arrows 1313-1324 represent 11 optical signals that pass switch 0 on their way to other switches. These 11 optical
signals can be circuit switched through switch 0. The optical signals that terminate or originate at a switch are packet switched. The switches 0-15 can be implemented
with two different types of hybrid packet/circuit switches.
In one embodiment, the switches 0-15 are implemented using the hybrid packet/circuit switches 600 or 620, where the circuit switch portion of the hybrid packet/circuit
switches can be accomplished using the MEMS mirror farm-based circuit switch 300 as described above with reference to FIG. 6. The switches 0-15 can be configured to
receive all of the optical signals transmitted over the links of the physical network 800. In other words, each optical switch has at least a radix of 15 since there are 15
input and 15 output waveguides. The mirrors in the mirror farm of each of the switches can be oriented as described above with reference to FIG. 3 to let optical signals
carried by 11 waveguides pass undisturbed. However, the mirrors can be oriented to direct optical signals carried by 4 waveguides to be sent to a node for packet
switching.

https://patents.google.com/patent/US8774625B2/en?q=(high+radix)&oq=high+radix&page=1

7/12

2/14/26, 1:59 PM

US8774625B2 - Method and systems for implementing high-radix switch topologies on relatively lower-radix switch physical…

In a second embodiment, the switches 0-15 are implemented using hybrid packet/circuit switches comprising packet switches in combination with passively shuffling
optical signals carried by 11 waveguides around the packet switch. In other words, each of the switches 0-15 of the physical network 800 is a packet switch that packet
switches the optical signals carried by 4 waveguides that terminate at the packet switch leaving optical signals carried by the remaining 11 waveguides undisturbed. FIG.
14 shows a cross-sectional view and schematic representation of a hybrid packet switch/passive shuffle network in accordance with embodiments of the present
invention. The passive shuffling is constructed by splicing four cores 1401-1404 from a 16-core fiber 1406 to a switch 0. The numbers in each core represents the
number of the switches of the physical network 800 connected at the other end of a continuous unbroken core. For example, core 1410 provides a continuous unbroken
connection for transmitting optical signals from the switch 13 to the switch 5. Although all the cores are shown in FIG. 14 as severed at the switch 0, in practice, the
cores connected by solid directional arrows are not severed and represent optical signals that pass switch 0, and the dashed-line directional arrows represent spliced
cores 1401-1404 that carry optical signals to and from switch 0. The 11 solid directional arrows correspond to the 11 waveguides 1313-1324 that carry optical signals
passed switch 0 as described above with reference to FIG. 13. For example, directional arrow 1412 represents the undisturbed transmission of optical signals along the
core 1410 connecting the switch 13 to the switch 5, which corresponds to the waveguide 1319, shown in FIG. 13. Dashed-line directional arrows 1414-1417 represent the
transmission of optical signals from the switches 15, 14, 12, and 8 to the switch 0, and dashed-line arrows 1418-1421 represent the transmission of optical signals to the
switches 1, 2, 4, and 8. The switch 0 is configured as a packet switch in order to extract optical signals transmitted on optical fibers 1401-1404 that are directed to the
node 1424 and place optical signal generated by the node 1424 for processing at other nodes into waveguides 1401-1404. Note that the same set of four cores 14011404 are used to send optical signals to and from switch 0. The 16 core-fiber 1406 also includes an extra unused core that can be used in the event one of the other
cores fails or to provide extra bandwidth. For example, if core 1403 cannot support all of the traffic between switch 0 and switch 1, then core 1422 can be spliced to
switch 0 and used to provide additional bandwidth.
B. A Folded Clos Networks
In alternate embodiments, a switch network can be composed of rows of switches, where each switch in a given row is configured to transmit information to any switch
in an adjacent row. FIG. 15 shows an exemplary high radix switch topology 1500. As shown in the example of FIG. 16 each of the switches 1501-1508 in the first row has
eight waveguides that are each connected to a different switch in the second row. Information can be unidirectionally or bidirectionally transmitted between the row of
switches 1501-1508 and the row of switches 1509-1516.
These high radix switch topology 1600 can be implemented as a “Clos network.” FIG. 16 shows an exemplary folded Clos network 1600 configured in accordance with
embodiments of the present invention. The Clos network 1600 is composed of the first row of eight switches 1501-1508 and the second row of eight switches 15091516. The Clos network 1600 also includes the four intermediate shuffle networks 1601-1604. The shuffle networks 1601-1604 can be implemented using passive
networks, circuit switches, or hybrid packet/switches. Each switch is connected to two different intermediate shuffle networks via a 4-core optical fiber. For example,
switch 1508 is connected to the intermediate shuffle network 1602 via a 4-core fiber 1606 and is connected to the shuffle network 1604 via a 4-core fiber 1607. Four-core
optical fibers provide the minimum number of cores needed to implement the Clos network 1500 on the physical network 1600. By employing intermediate shuffle
networks and 4-core optical fibers, the physical network 1600 has ¼ the number of optical fibers running between switches as is the case with the switch topology 1600
(although the fibers are multicore) Thus, the switch network 1500 provides a lower-radix switch network on which the relatively higher radix switch topology 1500 can be
implemented.
The MEMS switch 300 can be used as the switch in the shuffle networks 1601-1604. For example, in FIG. 17, the cores 1701-1704 can represent the cores of one of the
4-core optical fibers 1601-1604. The micromirrors 1705-1708 of the mirror farm 302 are oriented to direct the optical signals output from the cores 1701-1704 to the
cores 1709-1712, where each of the cores 1709-1712 is a core in one of the 4-core optical fibers 1605-1608. The micromirrors of the mirror farm 302 can analogously be
oriented to carry out the remaining optical interconnections represented in FIG. 16.
The foregoing description, for purposes of explanation, used specific nomenclature to provide a thorough understanding of the invention. However, it will be apparent to
one skilled in the art that the specific details are not required in order to practice the invention. The foregoing descriptions of specific embodiments of the present
invention are presented for purposes of illustration and description. They are not intended to be exhaustive of or to limit the invention to the precise forms disclosed.
Obviously, many modifications and variations are possible in view of the above teachings. The embodiments are shown and described in order to best explain the
principles of the invention and its practical applications, to thereby enable others skilled in the art to best utilize the invention and various embodiments with various
modifications as are suited to the particular use contemplated. It is intended that the scope of the invention be defined by the following claims and their equivalents:

Patent Citations (14)
Publication number

Priority date

Publication date

Assignee

Title

US5457556A *

1993-04-16

1995-10-10

Nec Corporation

Optical cross-connect system with space and wavelength division switching
stages for minimizing fault recovery procedures

US5541914A

1994-01-19

1996-07-30

Krishnamoorthy; Ashok
V.

Packet-switched self-routing multistage interconnection network having
contention-free fanout, low-loss routing, and fanin buffering to efficiently realize
arbitrarily low packet loss

WO1999013606A1

1997-09-05

1999-03-18

Telcordia Technologies,
Inc.

Hitless reconfiguration of a wavelength division multiplexed optical
communication network

US6230252B1

1997-11-17

2001-05-08

Silicon Graphics, Inc.

Hybrid hypercube/torus architecture

US20020145778A1

2001-03-16

2002-10-10

Photuris, Inc.

Wavelength division multiplexed optical communication system having a
reconfigurable optical switch and a tunable backup laser transmitter

WO2002089401A2

2001-04-26

2002-11-07

Marconi Uk Intellectual
Property Ltd

Improvements in and relating to telecommunications networks

US6512612B1 *

1999-06-25

2003-01-28

Lucent Technologies
Inc.

Intelligent optical router

US6741552B1

1998-02-12

2004-05-25

Pmc Sierra
Inertnational, Inc.

Fault-tolerant, highly-scalable cell switching architecture

US6873796B1 *

1999-07-28

2005-03-29

Oki Electric Industry
Co., Ltd.

Node device and optical path setting method

https://patents.google.com/patent/US8774625B2/en?q=(high+radix)&oq=high+radix&page=1

8/12

2/14/26, 1:59 PM

US8774625B2 - Method and systems for implementing high-radix switch topologies on relatively lower-radix switch physical…

US7088919B2 *

2002-11-04

2006-08-08

Nortel Networks
Limited

Modular photonic switch with wavelength conversion

CN1859382A

2005-12-16

2006-11-08

华为技术有限公司

Communication device for supporting multiple service and its method

CN1993915A

2001-03-16

2007-07-04

福图瑞斯有限公司

Method and apparatus for interconnecting a plurality of optical transducers with a
wavelength division multiplexed optical switch

US20080212157A1 *

2007-01-15

2008-09-04

Fujitsu Limited

Optical switching system and control method for micro mirror

US7440448B1

2001-07-02

2008-10-21

Haw-Minn Lu

Systems and methods for upgradeable scalable switching

Family To Family Citations
* Cited by examiner, † Cited by third party

Non-Patent Citations (7)
Title
Chong et al., "Fault Tolerance and Performance of Multipath Multistage Interconnection Networks" In the proceedings of Advanced Research in VLSI and Parallel Systems, MIT
press, Mar. 1992 (16 pages).
European Patent Office, EP Application No. EP 08795141, Extended European Search Report, May 29, 2012 (6 pages).
Goke et al., Banyan Networks for Partitioning Multiprocessor Systems, In the Proceedings of the International Symposium on Computer Architecture (ISCA), ACM, New York, 1973
(8 pages).
International Search Report, PCT/US2008/009524, Apr. 14, 2009, 11 pages.
Kim et al., "Flattened Butterfly Topology for On-chip Networks" In the proceedings of the 40th Annual IEEE/ACM International Symposium on Micro-architecture (MICRO), Chicago,
IL. Dec. 2007 (11 pages).
Korean Intellectual Property Office, International Search Report and Written Opinion for PCT/US2008/009524 dated Apr. 14, 2009 (11 pages).
The International Bureau of WIPO, Preliminary Examination Report for PCT/US2008/009524 dated Feb. 17, 2011 (6 pages).
* Cited by examiner, † Cited by third party

Cited By (58)
Publication number

Priority date

Publication date

Assignee

Title

US20160088376A1 *

2008-04-16

2016-03-24

Ciena Corporation

Network controller, a multi-fabric shelf and, a method of processing traffic
in a transport network

US20160301996A1 *

2013-10-22

2016-10-13

Hewlett Packard Enterprise
Development Lp

Hybrid circuit-packet switch

US9668037B2

2014-02-24

2017-05-30

Rockley Photonics Limited

Detector remodulator and optoelectronic switch

US9706276B2

2015-11-05

2017-07-11

Rockley Photonics Limited

Optoelectronic switch

US9781059B2

2014-09-30

2017-10-03

Rockley Photonics Limited

Optoelectronic switch

US9900672B2

2015-04-24

2018-02-20

Rockley Photonics Limited

Optoelectronic switch architectures

US9946042B2

2014-11-11

2018-04-17

Rockley Photonics Limited

Electronic/photonic chip integration and bonding

US10028041B2

2015-04-24

2018-07-17

Rockley Photonics Limited

Optical switch architecture

US10034069B2

2015-09-29

2018-07-24

Rockley Photonics Limited

Optoelectronic switch

US10088643B1

2017-06-28

2018-10-02

International Business
Machines Corporation

Multidimensional torus shuffle box

US10133094B1

2017-07-05

2018-11-20

Rockley Photonics Limited

Optoelectronic device

US10169048B1

2017-06-28

2019-01-01

International Business
Machines Corporation

Preparing computer nodes to boot in a multidimensional torus fabric
network

US10191350B2

2015-03-05

2019-01-29

Rockley Photonics Limited

Waveguide modulators structures

US10205664B2

2015-09-29

2019-02-12

Rockley Photonics Limited

System and method for routing

US10216059B2

2015-03-05

2019-02-26

Rockley Photonics Limited

Waveguide modulator structures

US10222677B2

2014-02-24

2019-03-05

Rockley Photonics Limited

Optoelectronic device

https://patents.google.com/patent/US8774625B2/en?q=(high+radix)&oq=high+radix&page=1

9/12

2/14/26, 1:59 PM

US8774625B2 - Method and systems for implementing high-radix switch topologies on relatively lower-radix switch physical…

US10356008B2

2017-06-28

2019-07-16

International Business
Machines Corporation

Large scale fabric attached architecture

US10491973B2

2015-04-24

2019-11-26

Rockley Photonics Limited

Optoelectronic switch

US10571983B2

2017-06-28

2020-02-25

International Business
Machines Corporation

Continuously available power control system

US10678115B2

2015-03-05

2020-06-09

Rockley Photonics Limited

Waveguide modulator structures

US10811848B2

2017-06-14

2020-10-20

Rockley Photonics Limited

Broadband arbitrary wavelength multichannel laser source

US10921616B2

2016-11-23

2021-02-16

Rockley Photonics Limited

Optoelectronic device

US10928659B2

2014-02-24

2021-02-23

Rockley Photonics Limited

Optoelectronic device

US11036006B2

2016-12-02

2021-06-15

Rockley Photonics Limited

Waveguide device and method of doping a waveguide device

US11101256B2

2016-11-23

2021-08-24

Rockley Photonics Limited

Optical modulators

US11105975B2

2016-12-02

2021-08-31

Rockley Photonics Limited

Waveguide optoelectronic device

US11150494B2

2015-03-05

2021-10-19

Rockley Photonics Limited

Waveguide modulator structures

US20230204849A1 *

2021-12-28

2023-06-29

Sterlite Technologies Limited

Trench assisted multi-core optical fiber with reduced crosstalk

US12368283B2

2017-06-14

2025-07-22

Rockley Photonics Limited

Broadband arbitrary wavelength multichannel laser source

EP2774338A4

2011-10-31

2015-06-03

Hewlett Packard Development
Co

IMPLEMENTING A PROPORTIONAL ENERGY NETWORK ARCHITECTURE
BASED ON DESIRED NETWORK CRITERIA

JP5838748B2 *

2011-11-15

2016-01-06

富士通株式会社

Optical transmission system, pumping light supply control method, and
pumping light supply apparatus

US8842575B2 *

2012-06-21

2014-09-23

Alcatel Lucent

Method and apparatus for providing a non-overlapping ring-mesh network
topology

WO2014070137A1

2012-10-30

2014-05-08

Empire Technology
Development Llc

Waved time multiplexing

US9197356B2

2012-11-16

2015-11-24

At&T Intellectual Property I, L.P.

Distributed spatial mode processing for spatial-mode multiplexed
communication systems

US9197509B1

2013-03-15

2015-11-24

Google Inc.

Logical topology in a dynamic data center network

US9692639B1 *

2013-03-15

2017-06-27

Google Inc.

Achieving full bandwidth usage and max-min fairness in a computer
network

US9246760B1

2013-05-29

2016-01-26

Google Inc.

System and method for reducing throughput loss responsive to network
expansion

US9166692B1 *

2014-01-28

2015-10-20

Google Inc.

Network fabric reconfiguration

US9537714B1 *

2014-05-09

2017-01-03

Google Inc.

Randomized rotation striping for direct connect networks

CN103986673A *

2014-05-19

2014-08-13

江苏新瑞峰信息科技有限公司

Improved network switch

US9602431B2

2015-03-20

2017-03-21

International Business
Machines Corporation

Switch and select topology for photonic switch fabrics and a method and
system for forming same

JP6480601B2 *

2015-11-26

2019-03-13

日本電信電話株式会社

Communication system and failure detection method

US20180375579A1 *

2015-11-26

2018-12-27

Nippon Telegraph And
Telephone Corporation

Communication system and connector

US10382843B2 *

2016-08-24

2019-08-13

Verizon Patent And Licensing
Inc.

Colorless, directionless, contentionless, spaceless, and flexible grid
reconfigurable optical node

US11153105B2 *

2017-06-29

2021-10-19

Intel Corporation

Technologies for densely packaging network components for large scale
indirect topologies

US10225632B1

2017-12-12

2019-03-05

International Business
Machines Corporation

Planar photonic switch fabrics with reduced waveguide crossings

US10244296B1 *

2017-12-12

2019-03-26

International Business
Machines Corporation

Planar photonic switch fabrics with reduced waveguide crossings

US10397671B2

2017-12-12

2019-08-27

International Business
Machines Corporation

Planar photonic switch fabrics with reduced waveguide crossings

Family To Family Citations

https://patents.google.com/patent/US8774625B2/en?q=(high+radix)&oq=high+radix&page=1

10/12

2/14/26, 1:59 PM

US8774625B2 - Method and systems for implementing high-radix switch topologies on relatively lower-radix switch physical…

US11650849B2 *

2018-09-25

2023-05-16

International Business
Machines Corporation

Efficient component communication through accelerator switching in
disaggregated datacenters

WO2021207097A1 *

2020-04-07

2021-10-14

Nec Laboratories America, Inc.

Network sensing topologies for fiber optic sensing

CN111596412A *

2020-04-12

2020-08-28

桂林电子科技大学

Multi-core optical fiber programmable multifunctional device based on
array MEMS reflector

CN111562653A *

2020-04-12

2020-08-21

桂林电子科技大学

A multi-core optical fiber switch based on array MEMS reflector

CN111596411A *

2020-04-12

2020-08-28

桂林电子科技大学

Multi-core optical fiber fan-in fan-out device based on array MEMS reflector

US12401423B2

2020-09-14

2025-08-26

Nec Corporation

Optical network, network management device, and network management
method

CN114363272B *

2020-09-27

2023-03-31

华为技术有限公司

Configuration method of switch and related equipment

US11765103B2 *

2021-12-01

2023-09-19

Mellanox Technologies, Ltd.

Large-scale network with high port utilization

JP2025527340A *

2022-08-18

2025-08-20

Ｎｔｔ株式会社

Switching device and switching method

US12355679B1

2024-01-02

2025-07-08

Morgan Stanley Services Group
Inc.

Packet switched control, circuit switched data apparatus orchestrated by a
low-latency digital programmable controller

* Cited by examiner, † Cited by third party, ‡ Family to family citation

Similar Documents
Publication

Publication Date

Title

US8774625B2

2014-07-08

Method and systems for implementing high-radix switch topologies on relatively lower-radix switch physical networks

US20220150607A1

2022-05-12

Photonic switches, photonic switching fabrics and methods for data centers

US20150098700A1

2015-04-09

Distributed Optical Switching Architecture for Data Center Networking

CN102281478A

2011-12-14

On-chip optical router for hybrid switching

CN106533993A

2017-03-22

Optical network on chip based on five-port optical router

CN108604940B

2020-06-19

optoelectronic switch

US11190860B2

2021-11-30

Switch with a shuffle

Gu et al.

2009

Design of 3D optical network on chip

Hao

2020

Research of Optical Interconnection Technology in Datacenter Networks

US9031407B2

2015-05-12

Bidirectional optical data packet switching interconection network

Yang et al.

2022

A Non-blocking Network Design for Terabit Capacity Optical Interconnects

Wang et al.

2009

A fault-tolerant backbone network architecture targeting time-critical communication for avionic WDM LANs

Chan et al.

2010

Tools and methodologies for designing energy-efficient photonic networks-on-chip for highperformance chip multiprocessors

Lee et al.

1997

PACKET SWITCHING PHOTONIC NETWORK SWITCH DESIGN AND ROUTING ALGORITHMS

Vaidyanathan et al.

2005

On mapping multidimensional weak tori on optical slab waveguides

Woesner

1998

PrimeNet: network design based on arrayed waveguide grating multiplexers

Bregni et al.

2001

Optical packet switching for IP-over-WDM transport networks

Mao et al.

2004

A scheme of optical interconnection for super high speed parallel computer

Zhang et al.

2008

On segment-shared protection for dynamic connections in multi-domain optical mesh networks

Priority And Related Applications
Applications Claiming Priority (1)
Application

Filing date

Title

PCT/US2008/009524

2008-08-08

Methods and systems for implementing high-radix switch topologies on relatively lower-radix switch physical networks

https://patents.google.com/patent/US8774625B2/en?q=(high+radix)&oq=high+radix&page=1

11/12

2/14/26, 1:59 PM

US8774625B2 - Method and systems for implementing high-radix switch topologies on relatively lower-radix switch physical…

Legal Events
Date

Code

Title

Description

2011-06-09

AS

Assignment

Owner name: HEWLETT-PACKARD DEVELOPMENT COMPANY, L.P., TEXAS
Free format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:BINKERT, NATHAN LORENZO;DAVIS,
ALAN LYNN;MCLAREN, MORAY;SIGNING DATES FROM 20110411 TO 20110428;REEL/FRAME:026418/0151

2014-06-18

STCF

Information on status: patent grant

Free format text: PATENTED CASE

2015-11-09

AS

Assignment

Owner name: HEWLETT PACKARD ENTERPRISE DEVELOPMENT LP, TEXAS
Free format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:HEWLETT-PACKARD DEVELOPMENT
COMPANY, L.P.;REEL/FRAME:037079/0001
Effective date: 20151027

2017-12-26

MAFP

Maintenance fee payment

Free format text: PAYMENT OF MAINTENANCE FEE, 4TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE:
M1551)
Year of fee payment: 4

2021-01-26

AS

Assignment

Owner name: OT PATENT ESCROW, LLC, ILLINOIS
Free format text: PATENT ASSIGNMENT, SECURITY INTEREST, AND LIEN AGREEMENT;ASSIGNORS:HEWLETT
PACKARD ENTERPRISE DEVELOPMENT LP;HEWLETT PACKARD ENTERPRISE
COMPANY;REEL/FRAME:055269/0001
Effective date: 20210115

2021-12-16

MAFP

Maintenance fee payment

Free format text: PAYMENT OF MAINTENANCE FEE, 8TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE:
M1552); ENTITY STATUS OF PATENT OWNER: LARGE ENTITY
Year of fee payment: 8

2022-08-05

AS

Assignment

Owner name: VALTRUS INNOVATIONS LIMITED, IRELAND
Free format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:OT PATENT ESCROW,
LLC;REEL/FRAME:061244/0298
Effective date: 20220803
Owner name: VALTRUS INNOVATIONS LIMITED, IRELAND
Free format text: ASSIGNMENT OF ASSIGNOR'S INTEREST;ASSIGNOR:OT PATENT ESCROW,
LLC;REEL/FRAME:061244/0298
Effective date: 20220803

2025-12-29

MAFP

Maintenance fee payment

Free format text: PAYMENT OF MAINTENANCE FEE, 12TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE:
M1553); ENTITY STATUS OF PATENT OWNER: LARGE ENTITY
Year of fee payment: 12

Data provided by IFI CLAIMS Patent Services

About

Send Feedback

Public Datasets

Terms

Privacy Policy

https://patents.google.com/patent/US8774625B2/en?q=(high+radix)&oq=high+radix&page=1

Help

12/12

2/14/26, 1:53 PM

WO2004095260A1 - Very high radix division - Google Patents

Patents
Very high radix division
Abstract
Methods, machines, and systems are provided for very high radix division using narrow data paths.
A numerator and denominator are received for a very high radix division calculation. An approximate

WO2004095260A1
WIPO (PCT)

reciprocal of the denominator is obtained from a data structure. The numerator and denominator are
Download PDF

pre-scaled by the reciprocal. The denominator is decomposed to an equivalent expression that

Find Prior Art

Similar

results in a number of leading insignificant values. Next, modifying a current remainder by forming a
first product and subtracting the equivalent expression iteratively assembles a quotient.

Other languages: French
Inventor: Ping Tang, Warren Ferguson

Classifications

Current Assignee : Intel Corp

G06F7/535 Dividing only
Worldwide applications

Landscapes

2003 US 2004 CN WO

Physics & Mathematics

Application PCT/US2004/005664 events

General Physics & Mathematics
Show more

2004-02-25

Application filed by Intel Corp

2004-11-04

Publication of WO2004095260A1

2005-09-21

Anticipated expiration

Status

Ceased

Info: Patent citations (12), Cited by (10), Legal events, Similar
documents, Priority and Related Applications
External links: Espacenet, Global Dossier, PatentScope, Discuss

Claims

Hide Dependent

CLAIMS What is claimed is:
1. A method to perform high radix division in a microprocessor architecture, comprising: pre-scaling a numerator and denominator by a factor to produce a scaled numerator
and a scaled denominator; and iteratively assembling a portion of a quotient by multiplying a radix by a current remainder and subtracting an integer portion of the current
remainder and adding the integer portion of the current remainder multiplied by a sum of one minus the scaled denominator.
2. The method of claim 1 further comprising, acquiring the factor by approximating a reciprocal of the denominator.
3. The method of claim 2 wherein in acquiring, a data structure lookup operation provides the reciprocal of the denominator.
4. The method of claim 2 wherein in pre-scaling, the scaled numerator includes the numerator multiplied by the factor and the scaled denominator includes the denominator
multiplied by the factor.
5. The method of claim 1 wherein in iteratively assembling, the integer portion of the current remainder includes truncating a first digit associated with the current remainder.
6. The method of claim 1 wherein in iteratively assembling, the sum results in a reduction in a data width when dividing the numerator by the denominator. .
7. The method of claim 1 further comprising producing the quotient once reaching a desired precision or once obtaining a current remainder of zero.
8. A method to perform high radix division in a microprocessor architecture, comprising: establishing an equivalent mathematical equation for an original division that includes
a scaled divisor, wherein the scaled divisor when used in the equivalent mathematical equation reduces a number of significant digits that are required to resolve the division;
and iteratively selecting a quotient digit and using the scaled divisor in the equivalent mathematical equation until a desired number of quotients digits are resolved.
9. The method of claim 8 wherein in establishing, the equivalent mathematical equation equals a second sum subtracted from a first sum, and wherein the first sum is an
integer first portion of a current remainder multiplied by a radix, and wherein the second sum a negative value for the current remainder added to a product of the integer first
portion of the current remainder multiplied by a sum of 1 minus the scaled divisor.
10. The method of claim 8 wherein in establishing, the scaled divisor equals an original divisor multiplied by an approximate reciprocal of the original divisor.
11. The method of claim 8 wherein in establishing, the equivalent mathematical equation includes restating an original numerator and an original divisor as a scaled numerator
divided by the scaled denominator.
12. The method of claim 8 wherein in iteratively selecting, the quotient digit is rounded based on a trailing portion of a current remainder.
13. A method to perform high radix division in a microprocessor architecture, comprising: restating a division in an equivalent expression using a scaled divisor; selecting a first
quotient digit from a first digit of a numerator represented in the equivalent expression; processing the division by separating a first portion of a current remainder from a trailing
portion of the current remainder and using the portions with the scaled divisor in the equivalent expression until a desired number of quotient digits for the division is reached.

https://patents.google.com/patent/WO2004095260A1/en

1/8

2/14/26, 1:53 PM

WO2004095260A1 - Very high radix division - Google Patents

14. The method of claim 13 wherein in selecting and processing, the quotient digits are rounded when the trailing portion is greater than a predefined threshold.
15. The method of claim 13 wherein in restating, the scaled divisor equals an approximate reciprocal of an original divisor multiplied by the original divisor.
16. The method of claim 15 wherein in restating, the scaled divisor is further modified to equal one minus the scaled divisor.
17. A method to perform high radix division in a microprocessor architecture, comprising: decomposing a division calculation with a scaled divisor, wherein the decomposition
results in a mathematically equivalent representation of the division calculation; selecting a quotient digit for the division; calculating the next remainder by using the scaled
divisor with the mathematically equivalent representation, and wherein the scaled divisor is one minus an approximate reciprocal factor for an original divisor; and repeating the
processing of selecting and calculating until a desired number of quotient digits are determined for the division calculation.
18. The method of claim 17 wherein in calculating, the approximate reciprocal factor is acquired from a reciprocal data structure using a radix associated with the division
calculation.
19. The method of claim 17 wherein in decomposing, the mathematically equivalent representation is a sum subtracted from a first product, the first product is the current
remainder multiplied by a radix, and the sum is a positive first portion of the current remainder added to a negative second product, wherein the second product is the scaled
divisor multiplied by the first portion of the current remainder.
20. An article having a machine accessible medium having associated instructions, wherein the instructions, when executed, produce a quotient for high radix division, the
machine comprising at least one component performing: inverting a denominator to obtain an approximate factor; multiplying a numerator by the factor to acquire a scaled
numerator; multiplying the denominator by the factor to acquire a scaled denominator; and using a first digit portion of the scaled numerator as a first portion of the quotient
and setting a current remainder as the scaled numerator; decomposing the scaled denominator as an equivalent expression used for the very high radix division that results in a
number of leading insignificant digits; and iteratively using the current remainder by multiplying a radix by the current remainder to acquire a first product and subtracting from
the first product the equivalent expression, wherein a resulting sum is used to modify the current remainder.
21. The article of claim 20 further comprising, instructions for obtaining the factor to a desired precision from a table.
22. The article of claim 20 wherein in decomposing, the equivalent expression is a first integer portion of the current remainder minus a second product represented as the first
integer portion of the current remainder multiplied by a second sum of one minus the scaled denominator.
23. The article of claim 22 further comprising, instructions for resolving and saving the second sum before iteratively using the current remainder.
24. The article of claim 20 further comprising, instructions for iteratively assembling a quotient using the iteratively modified current remainder until reaching a desired precision
or until obtaining a zero value for the current remainder.
25. The article of claim 20 wherein in iteratively using, space utilization of the machine is not increased beyond a desired precision when performing the division of the
numerator by denominator.
26. A high radix division system, comprising: a data structure having approximate factors at a desired precision for reciprocal numbers; a processor having access to the data
structure; logic processing on the processor for receiving a numerator and a denominator associated with a high radix division; and wherein the logic uses the data structure to
acquire a denominator factor and restates the high radix division into a division of the numerator multiplied by the denominator factor divided by the denominator multiplied by
the denominator factor, and the logic iteratively forms a quotient by subtracting an integer first portion of a changing current remainder from a first product and adding a second
product, the first product acquired by multiplying the current remainder by a radix, and the second sum acquired by multiplying an integer first portion of the changing current
remainder by a sum of one minus the denominator multiplied by the denominator factor.
27. The high radix division system of claim 26 wherein the processor retains the sum before the logic iteratively forms the quotient.
28. The high radix division system of claim 26 wherein the logic includes hardwired components associated with the processor.
29. The high radix division system of claim 26 wherein the logic includes firmware or software associated with the processor.
30. The high radix division system of claim 26 wherein the logic performs the high radix division using a standard cell library adder and multiplier.
31. The high radix division system of claim 26 wherein the second product has a bit width of L, where L is a data bit width associated with a desired precision of the quotient.
32. The high radix division system of claim 26 wherein a desired precision for the denominator factor is configurable for the data structure

Description

VERY HIGH RADIX DIVISION
Technical Field [0001] Embodiments of the present invention relate generally to radix division, and more particularly to reducing a data path for implementations of radix
division in microprocessor architectures.
Background Information [0002] Floating point performance is a key focus of modern microprocessor architecture. Among the four basic floating-point operations
addition, subtraction, multiplication, and division, division is the most resource intensive operation for microprocessing architectures. Recently, advances have been
achieved in making very high radix (number base, e.g. 2 (binary), 10 (decimal), 16 (hexadecimal), and the like) digit recurrence algorithms practical to implement. By very
high radix, we mean that the number of quotient digits generated by each iteration of the algorithm is much larger than the typical traditional algorithms that yield 1 bit
(radix-2), 2 bits (radix-4), 3 bits (radix-8), or 4 bits (radix-16). It is practical for these very high radix division algorithms to generate on the order of 10 bits (radix-1024) or
20 bits (radix-1048576) during an algorithm's iteration. One common drawback of these algorithms, however, is that the internal data width grows in a somewhat
unnatural way.
[0003] For example, traditional digit recurrence division algorithms have as their central computational step the update of the remainder: Rj+ 1 = rx Rj - q,-+ 1 x V. Here, R is
the remainder, r is the radix, gy + 1 is the quotient digit, and Y is the divisor (e.g., denominator). The bulk of the work is in computing the product qJ + 1 x Y. The width of Y
remains fixed while the width of q,-+ 1 grows with the radix. Generally, the radix is an integral power of 2, so r= 2m for some integer m. When this is the case, the multiplier
needs to handle an -by- multiplication, where L is the data width of the precision in question (e.g., L = 53 for Institute of Electrical and Electronics Engineers (IEEE)
standards of double precision). In other words, the depth of the multiplier is the number of additional quotient data bits the algorithm generates per iteration, and the
width of the multiplier is fixed at the data width of the precision in question. [0004] For traditional digit recurrence division algorithms, only the depth of the multiplier

https://patents.google.com/patent/WO2004095260A1/en

2/8

2/14/26, 1:53 PM

WO2004095260A1 - Very high radix division - Google Patents

grows with the radix of the algorithm. The unnatural growth with recently developed very-high-radix division algorithms occurs because the multiplier for these operations
must instead be able to handle an m-by- ( + m) multiplication. That is, the width of the multiplier grows as well. This requirement is a direct outcome of a crucial "prescaling" step (e.g., divisor or denominator reciprocal, discussed in the Detailed Discussion Section below) in this class of algorithms that make them practical to
implement. While it is generally accepted that only the depth of the multiplier affects division operation speed, the growth of the width leads, nevertheless, to a number of
drawbacks. The more obvious drawbacks are increased space and increased power consumption. The less obvious but ever growing important drawback is the need for
a customized multiplier and/or adder rather than those most naturally found in standard cell libraries related to the precision width L in question. [0005] Therefore, there
is a need for improved implementations and techniques for radix division. These implementations and techniques should be as fast as some of the recent radix division
algorithms, but capable of maintaining the width of the multiplier such that space usage and power consumption is minimized and thus reduced when compared to
existing and conventional radix division implementations.
Brief Description of the Drawings
[0006] FIG. 1 is a flow diagram of a method for performing very high radix division, in accordance with one embodiment of the invention. [0007] FIG. 2 is a diagram
depicting a machine implementation for performing very high radix division, in accordance with one embodiment of the invention. [0008] FIG. 3 is a diagram of a very high
radix division system, in accordance with one embodiment of the invention. Description of the Embodiments
[0009] Novel methods, machines, and systems for very high radix division are described. In the following detailed description of the embodiments, reference is made to
the accompanying drawings, which form a part hereof, and in which is shown by way of illustration, but not limitation, specific embodiments of the invention that may be
practiced. These embodiments are described in sufficient detail to enable one of ordinary skill in the art to understand and implement them, and it is to be understood
that other embodiments may be utilized and that structural, logical, and electrical changes may be made without departing from the spirit and scope of the present
disclosure. The following detailed description is, therefore, not to be taken in a limiting sense, and the scope of the embodiments of the inventions disclosed herein is
defined only by the appended claims. [0010] FIG. 1 illustrates a flow diagram of one method 100 for performing very high radix division, in accordance with one
embodiment of the invention. The method 100 is implemented in microprocessor architectures. Moreover, the method 100 can be implemented in hardware, software,
firmware, or in combinations of hardware, software, and firmware within or accessible to microprocessor architectures.
[0011] As will be demonstrated to one of ordinary skill in the art, in the description that follows, embodiments of method 100 permit very high radix division calculations
to be decomposed in a novel manner that result in narrow data paths. In contrast, existing and conventional decompositions perform very high radix divisions where wide
data paths are necessary. Thus, microprocessor architectures implementing embodiments of method 100 experience a decrease in space utilization and accordingly a
decrease in power consumption when performing very high radix division. This is achieved without adversely impacting processor throughput. This is a significant
advantage over conventional techniques, since as microprocessor architectures become increasingly smaller in the industry; the ability to conserve power is a growing
concern. [0012] At 110 a numerator and denominator (e.g., divisor) are received as operands for a very high radix division calculation. The division calculation is prescaled (e.g., preprocessed) into an equivalent mathematical calculation before a quotient for the calculation is determined. This is achieved by attempting to make the
denominator of the division calculation approximately 1. By making the denominator or divisor 1 , the division calculation becomes straightforward, since any numerator
divided by 1 will equal the numerator.
[0013] Thus, in order to recast the original division calculation such that the denominator is approximately 1 , an approximate reciprocal of the denominator is necessary,
since the reciprocal of any number N (MN) when multiplied by the number is equal to 1 (e.g. 1/Λ/X A/ = 1). However, because the reciprocal of a number N may not
terminate (e.g., continues to infinity), some desired and configurable precision of the approximate reciprocal can be used. Accordingly, at 120, a factor representing a
desired precision for an approximate reciprocal of the denominator is acquired. In one embodiment, at 122, an approximate factor is obtained by performing a lookup in a
data structure (e.g., table or other data structure). This permits rapid and efficient acquisition of the factor as the original division calculation is being restated in a more
efficient format for processing. [0014] As one of ordinary skill in the art appreciates, the data structure having the reciprocal value for the denominator need not store all
number possibilities, since a subset of numbers can be used and then calculated to the proper reciprocal value. For example, a data structure may house only the
approximate reciprocal values for number digits 1-9 (where the radix is decimal (base 10)). Of course, in actuality only 2-9 may be stored since it is readily known the
reciprocal of 1 is 1. Reciprocal values for all other number possibilities can be readily calculated based on this small data structure, because these remaining possibilities
are only off by a factor of the radix (e.g., decimal (base 10)). For example, the reciprocal of 1 is 1 , and the reciprocal of 10 is 0.1. Moreover, the reciprocal of 7 is
approximately 0.14, and the reciprocal of 71 is approximately 0.014. Of course any desired level of precision can be configured to the requirements or desires of the
underlying microprocessor architecture. Thus, to increase precision the data structure could store reciprocals for numbers 2-99, 11- 99, 10-99, or other subsets of
numbers. Moreover, the number of significant digits for the stored reciprocal values can be tailored to the requirements or desires of the underlying architecture. Thus, if
only digits 2-9 were stored in the data structure, the actual values can include more than 2 significant digits. For example, the reciprocal of 7 can be stored as
0.142857142 having 8 significant digits.
[0015] Next, at 130, the original division calculation is pre-scaled (e.g., preprocessed or decomposed) to a new processing format where the original denominator is
approximately 1. This is achieved by multiplying the original numerator and the original denominator by the acquired approximate denominator reciprocal factor. Thus,
the new processing format for the division calculation is recast, at 140, to a mathematical equivalent represented by a scaled numerator (e.g., original numerator X
factor) divided by a scaled denominator (e.g., original denominator X factor). The multiplication is applied to both the numerator and the denominator so the restated
division calculation is mathematically equivalent to the original division calculation. [0016] At this point the first digit of the scaled numerator is a first portion of a
quotient for results associated with the original division calculation. This is so, because the scaled denominator is approximately 1 , and any number divided by 1 is the
number itself. Thus, at 150, the first portion of the quotient can be set as the first digit of the scaled numerator. [0017] By way of illustration only and for purposes of
understanding, consider an original division calculation of 3/7. Using the description detailed above, the reciprocal of 7 is approximately 0.14 (e.g., where our reciprocal
data structure is accurate to approximately 2 significant digits). Thus, when we recast the calculation, the scaled numerator becomes 0.42 (3 X 0.14) and the scaled
denominator becomes 0.98 (7 X 0.14). The newly restated calculation is thus 0.42/0.98; the first portion of the quotient for the original calculation of 3/7 is 0.4. One of
ordinary skill in the art appreciates, that the above is presented for purposes of illustration only, since very high radix division would most likely entail more precision than
presented in the this example. Yet, the example assists in more easily understanding how various embodiments of the present invention decompose an original division
calculation by restating the calculation in a format more easily and efficiently processed by microprocessor architectures. [0018] Next, embodiments of the present
invention begin to decompose the restated division calculation in a novel manner. Accordingly, at 160, a running variable defined as a current remainder is initially set to
be the scaled numerator. At 170, method 100 enters a processing loop that continues until the quotient reaches a desired level of precision (in IEEE this is /_=53 for
double precision, where L is the data bit width for the precision) is reached or until the current remainder is zero, indicating a termination has occurred in the division
calculation. Correspondingly, at 170, the quotient is iteratively assembled using a portion of the then current remainder. The first time into the processing loop, the
processing at 170 does not need to occur since this was initially handled before entering the loop at 150. Alternatively, the processing at 150 can be removed and
handled by the processing at 170. In this case, the processing at 170 need not make a check to determine if it is processing a current remainder for a first time within the
loop. [00019] At 172, a first product is a radix (number base being used, e.g., binary, decimal, hexadecimal, and others) multiplied by the current remainder. This first
product is then altered further at 174 by subtracting an integer portion of the current remainder. In one embodiment, the integer portion is the first digit of the current
remainder and made a whole integer number. This can be done because our scaled denominator is nearly or approximately 1 , and the division calculation can be
decomposed in a novel manner. [0020] For example, in traditional manual division one would determine a present remainder for a developing quotient by taking a first
product of the then existing current remainder multiplied by the radix and then subtracting a second product representing the denominator (e.g., divisor) multiplied by

https://patents.google.com/patent/WO2004095260A1/en

3/8

2/14/26, 1:53 PM

WO2004095260A1 - Very high radix division - Google Patents

some new number N. Thus, for example, when dividing 3/7 we would manually solve this calculation by forming a first product represented by multiplying 3 (e.g., current
remainder) X 10 (e.g., radix is base 10 (decimal)) and subtracting a second product represented by 7 (e.g., the divisor or denominator) X N (some integer number). We
intuitively know that N is equal 4, in order to make this second product (e.g., 7 X 4 = 28) close to, but less than our first product (e.g., 3 X 10 = 30). [0021] However, in
embodiments of the present invention, the division calculation is restated as a scaled numerator divided by scaled denominator (which is approximately 1 ), thus, N can
be automatically selected as the first digit integer portion of the current remainder, which we did in our example of 0.42/0.98 (e.g., 3/7 restated to a mathematical
equivalent). Thus, our first digit integer value (0.4 for the present example) is readily obtained from the current remainder. [0022] Moreover, the original calculation used
to solve for a quotient digit is presented as a second product subtracted from a first product. The first product is the current remainder multiplied by the radix, and the
second product is some number N multiplied by our scaled denominator (e.g., divisor). The second product is decomposed in a novel manner, which decreases the data
width required to solve for a value of the second product, for microprocessor architectures that implement embodiments of the present invention. [0023] This is achieved
by decomposing the divisor (e.g., scaled denominator) to be represented as a mathematically equivalent expression. Since, the divisor (e.g., scaled denominator) is
approximately 1, if the scaled denominator is subtracted from 1 , then the sum of this subtraction will include one or more initial or leading digits that are 0 or 1. Leading
insignificant 1 's can occur when the subtractions results in a negative number, such as when the scaled denominator was greater than 1 when using an approximate
factor that was greater than 1. The leading zeros or ones are insignificant numbers and will not occupy additional data width within microprocessor architectures.
Therefore, the leading zeros or ones will decrease the data width of the second product, if the second product is also restated in a mathematically equivalent manner.
The equivalent expression for the divisor can be stated as 1 - Y', where Y' is some number that when subtracted from 1 equals the scaled denominator. For example, in
the previous example the scaled denominator was 0.98 and if we set an equation to be 1-Y' = 0.98 and solve for Y'we find that Y' is .02, but if we express the equation in
terms of the Y, we find Y' = 1 - 0.98 (e.g., where 0.98 is the scaled denominator or the divisor). Thus, the scaled denominator equals 1 - (1-Y1) and Y' = 1 - scaled
denominator.
[0024] The second product can now appear as the equation of the first integer portion of the current remainder (since scaled denominator is approximately 1) multiplied
by the sub equation represented by 1-Y' (this is how the scaled denominator is being expressed), where Y' was 1 - scaled denominator. The second product can be further
decomposed to another mathematically equivalent equation by distributing the first portion of the current remainder over the sub equation 1-Y'. Thus, the second product
will now be represented as the first integer portion of the current remainder (e.g., current remainder multiplied by 1 the first part of the sub equation) minus the product of
the first integer portion of the current remainder multiplied by Y'. The product will include less significant digits and occupy less data width within microprocessor
architectures, than it would have if we had multiplied the first integer portion of the current remainder directly against the scaled denominator or divisor, because Y'
includes one or more insignificant leading zeros or ones, whereas the raw scaled denominator did not include any insignificant leading zeros or ones. Thus, the
decomposition of the scaled denominator to be restated in a mathematically equivalent manner is space efficient.
[0025] It has now been demonstrated that the division calculation to determine quotient digits can be stated as a first product (radix multiplied by the first integer portion
of the current remainder) minus a second product. The second product is equal to the first integer portion of the current remainder minus the product of the first integer
portion of the current remainder multiplied by Y', and Y' is know as 1 - scaled denominator. If we distribute the negative sign across the second product (since the second
product must be subtracted from the first product), the equation becomes the first product added to a negative first integer portion of the current remainder and a
positive product value (the negative sign becomes positive) of the first integer portion of the current remainder multiplied by Y'.
[0026] Thus, the entire equation for solving for digits of a quotient can be expressed as subtracting the first integer portion of the current remainder from a first product
represented as the first integer portion of the current remainder multiplied by the radix and then subtracting a product represented by multiplying the first integer portion
of the current remainder against Y', and Y' is equivalent to 1 - scaled denominator. [0027] Having restated and decomposed the original division calculation, we can return
to the discussion of FIG. 1. The first product in the iterative loop is the current remainder multiplied by the radix as depicted at 172, next at, 174 we subtract the first
integer portion of the current remainder from the first product.
Then, at 176, we add the product of Y' multiplied by the first integer portion of the current remainder, where Y' is equal to 1 - scaled denominator. Accordingly, at 178, the
resulting sum is updated as a new value for the current remainder, processing then continues again at 170 until a desired precision is reached or the current remainder is
equal to zero. Once this occurs, all the digits for the quotient are provided at 180 to the desired or configured precision required.
[0028] When doing high radix division the processing that requires the most space or area within a microprocessor, and correspondingly the most power consumption, is
the computation of the divisor multiplied by some number N. Traditional architectures may recast the original division calculation in terms of a scaled numerator divided
by scaled denominator in order to acquire processing efficiency, such that N is the first portion of a current remainder, but these traditional architectures do not
decompose the scaled denominator in the novel manner (1 - Y') presented with the embodiments of the present invention. As a result, in traditional architectures, the data
width of this calculation is more than what is necessary to solve the division calculation. This is so because the scaled denominator that is not decomposed into a
mathematical equivalent (1- Y1) will include no leading insignificant ones or zeros. Thus, with conventional techniques the scaled denominator (which is has significant
digits to a desired precision) when multiplied by the first portion of the current remainder will exceed the desired level of precision.
[0029] In contrast, the embodiments of the present invention decompose the processing for determining the product of the scaled denominator multiplied by the first
portion of the current remainder, such that the data width required in performing the division calculation is reduced. This is so because Y' is represented as 1 - the scaled
denominator (which is approximately 1), and thus Y' will include one or more insignificant leading digits that are 0 or 1. Therefore, when this multiplication is performed
less data width is required to obtain the resulting product. The data width of the multiplication is the data width of the desired precision. Correspondingly, microprocessor
architectures that implement embodiments of the present invention can perform high radix division with less space with less power consumption than what has been
conventionally required.
As one of ordinary skill in the art appreciates, power efficiency is directly proportional to area or space used, and since embodiments of the present invention require less
area or space, less power consumption is needed. [0030] Moreover, the novel decomposition and processing of the embodiments of the present invention are more
amenable to cell-based microprocessor designs. With the growth of microprocessor cell complexity cell- based designs have become important design considerations
that are needed to reduce time to market latencies. For example, rounding operations at the end of division iterations require carry-propagate addition to be performed on
a remainder. The embodiments of the present invention can use an adder cell that handles normal data width associated with a desired precision for high radix division
with microprocessor architectures.
[0031] Conversely, traditional architectures require adder cells that need to handle data widths in excess of the desired precision level, because the product of the current
remainder multiplied by the scaled denominator results in significant digits in excess of the desired precision level. As another example, many microprocessor
architectures require special multiplier cells for performing multiplication when high radix division is required. The embodiments of the present invention require no such
special multiplier cells, in order to perform high radix division. [0032] FIG. 2 illustrates a diagram 200 depicting one machine implementation for performing very high
radix division, in accordance with one embodiment of the invention. The machine architecture can include standard cell libraries associated architectures for adders and
multipliers when performing high radix division. The machine implementation depicted in diagram 200 is presented for purposes of illustration and not by way of
limitation. Accordingly, a variety of other configurations or combinations of hardware, software, and/or firmware can also be used without departing from embodiments
of the present invention. [0033] A first operand X having a bit width of L bits is received by multiplexer (Mux) 201. Simultaneously, in parallel, or subsequently Mux 202
receives a second operand Y having a data width of L bits. Operand X is associated with a numerator of a very high radix division, and operand Yis associated with a

https://patents.google.com/patent/WO2004095260A1/en

4/8

2/14/26, 1:53 PM

WO2004095260A1 - Very high radix division - Google Patents

denominator of the very high radix division. L is the number of significant digits used by the machine architecture to perform the very high radix division calculation at a
desired precision. The radix (r) at the machine level is a binary power of 2, where the power is represented as m (e.g., 2m). [0034] Yis fed to an approximate reciprocal
table data structure 203, in order to acquire an approximate reciprocal factor for Y. Concurrently, Xand Yare fed to Mux 204. Also, the approximate reciprocal factor is
provided to Mux 205. Next, the sum of the partial products of X and Y are produced by Mux or adder 206(can also be a multiplier), using the approximate reciprocal factor
acquired from Mux 205. Mux 205 feeds m bits of the approximate reciprocal at a time to Mux or adder 206 for processing. Multiplier 206 (can be a Mux or adder as well)
multiplies m bits of the approximate reciprocal factor against the Xand Yto restate the original division calculation as scaled X divided by scaled Y, where scaled Y is now
approximately or near a value of 1 , since the approximate reciprocal of Yis now multiplied by Y.
[0035] Multiplier 206 is designed to handle calculations for an m by L bit width array. In the case where multiplier 206 is an adder 206, the adder is designed to handle
adding L bits plus L bits. Scaled Yis then adjusted in multiplier 206 to be represented as Y' = 1 - B multiplied by scaled Y, where B is the approximate reciprocal factor of
Y). This is used to decompose the division calculation by expressing scaled Yas Y' in terms that will results in leading insignificant bits (discussed above with FIG. 1 as
leading insignificant zeros or ones). Because this is done, the architecture of diagram 200 can process high division radix division with standard architectures associated
with cell multipliers and adders designed to process calculations having bit array dimensions of L by m or adding L bits plus L bits, respectively. Conversely, traditional
architectures would need a multiplier capable of handling m by (m + L) bit width arrays or adders capable of handling adding data bit widths of m + L. X' is then multiplied
in multiplier 206 by r (e.g., radix represented as 2m). This reduction in significant bits will be realized in subsequent iterations through diagram 200. However, one of
ordinary skill in the art appreciates this is not really a multiplication at all, all that is needed is a movement of the binary decimal point or simple bit alignment when
multiplication of some number N is multiplied by radix r.
[0036] The leading digits (discussed as the first integer portion of the current remainder above with FIG. 1) are stripped in adder 207 and stored as p in register 208. The
trailing digits are stripped in adder 207 and provided to Mux 209. These trailing digits are represented as R'. Next, in order to adjust p to account for rounding such as
when the first few bits of R' are greater than some threshold (e.g., decimal fractional numbers greater than or equal to 0.5 are rounded to 1) an adjustment can be made,
in some embodiments, such that a rounding occurs to approximate R'to a higher value in Mux or adder 210. For example, with a decimal r having current remainder
represented as 0.478, p can be thought of as 4 (the first integer portion) and R' can be thought of as 0.78. In this example, the 0.78 can be rounded in Mux or adder 210 to
be 1. The adjusted value of R' is stored as d in register 211. This rounding is used to adjust p in adder 212 to a potentially rounded (although not always required or
needed) value for a quotient digit q, which is stored in register 213.
[0037] The discussion thus far has been to determine the first and initial digit q0 for a desired quotient Q. The architecture in diagram 200 is designed to iterate a number
of times to produce a full Q at a desired precision or until a current remainder is 0. The number of iterations for a Q that is populated to the desired precision will be Urn,
where L is the number of significant digits required for a desired precision and m is the number of bits per iteration that diagram 200 is designed to process for high radix
division calculations. [0038] Subsequent iterations to populate Q can proceed as follows. Y' is fed back to Mux 202 for each iteration, this falls through Mux 204 and
multiplier 206 unmodified. Mux or adder 206 then provides Y'to adder 207 and the previous selected digit q. Adder 207 also receives a previously potentially rounded
representation of the trailing remainder R' represented in the diagram as d, and adder 207 receives the previous R'. [0039] With iterations that occur after the first or initial
iteration, adjustments in adder 207 may need to be made to the previously determined value for Q digit q, since q may have been rounded. Thus, adder 207 compares the
previous R' against d to see if a rounding to q has occurred, and if it has then q is adjusted back to an non rounded version of q. This is done to ensure accuracy in the
very high radix division for all iterations. Next, adder 207 assembles the current remainder as q + R' and multiplies this against the radix (r) to acquire the first product for
the next cycle of the division. Adder 207 also multiplies q against Y' to form a second product. Next, q is subtracted from the first product and the result is added to the
second product. At this point, we now have the information necessary for determining the next Q digit qJ+1.
[0040] Accordingly, adder 207 strips the first few bits of the resulting calculation and stores this in register 208 as pJ+ι, the trailing portion of the result becomes the next
trailing remainder R'J+ι and is provided to Mux 209. Next, in
210 R' is processed to a value for d and the result is stored in register 211 as dJ+ι.
Note, that if R'J+ι did not require rounding, then the value for dJ+ι is 0 and still processed. Adder 212 then adds d J+ι to pJ+ι to produce the next quotient digit q j+i, which
is stored in register 213. [0041] As was previously discussed the high radix division processing continues until a desired level of precision is reached (e.g., L bits for Q or
Urn iterations) or until a current remainder has a zero value.
[0042] The processing of diagram 200 can also be expressed in a formal notation to reflect processing logic of the various components of FIG. 2. One example notation
is as follows. In a preprocess iteration where we determined that Y' should be decomposed or re-expressed in a format where we have leading insignificant bits can be
expressed as B = the approximate reciprocal of Yand Y'
= 1 - B x Y.
[0043] The initialization or first iteration can be expressed as Initialization: p0 + Ro = X', where X' is the scaled X (e.g., numerator), Q0 = 0, ;' «- 0, r = 2m = radix, where the
subscript 0 indicates an initial or first iteration such that the value for; = 0, identifying a first iteration. The Quotient digit selection can then be expressed as δv = Approx
round to integer (rx Rj"), q, + -i = rp, + δ7 . The quotient digit selection begins the main processing loop in diagram 200. As was described above, a quotient digit qj+1 can
be rounded thus δ, reflects this adjustment that is being made in diagram 200 to quotient digits q J+ . Also, within the main processing loop the remainder processing can
be expressed as pJ + 1 + RJ' + 1 = rx R'j + qJ + - x Y'- δj . Next, the update processing of the quotient with the next selected quotient digit q J+ι can be expressed as Qy + 1 =
Q, + q, + i/r' + 1. ; <- j + 1. Finally, iterations proceed back to the quotient digit selection until the quotient has obtained L bits (Urn iterations). [0044] It has now been
demonstrated how very high radix division can be decomposed in a novel manner to permit processing that can be handled by microprocessor architectures using
narrow data paths. Moreover, embodiments of the present invention permit very high radix division to be processed using standard cell libraries. As a result,
microprocessor architectures implementing the techniques of the embodiments of the present invention experience a more space efficient solution resulting in a
decrease in power consumption. [0045] Further, it is to be understood that the diagram 200 is presented for purposes of illustration only. Thus, other cell library
configurations are foreseen that implement the novel decomposition of the embodiments of the present invention. Therefore, the embodiments of the present invention
are intended to include any such configuration that implements the novel decomposition of radix division presented herein.
[0046] FIG. 3 illustrates a diagram of a very high radix division system 300, in accordance with one embodiment of the invention. The high radix division system includes
a data structure 310 and a processor 320. The processor includes logic 321 , and space for housing a numerator 322, a denominator 323, and a quotient associated with
a high radix division calculation. FIG. 3 is presented for purposes of illustration only and is not intended to be limiting, since it is readily apparent that in some
embodiments the data structure 310 can also be included within a register or area of the processor 320. In fact, any embodiment where the data structure 310 is
accessible to the logic 321 is intended to fall within the broad scope of the present invention. Moreover, the logic 321 can be hardware components, firmware
components, software components, or a combination of hardware, firmware, and software. [0047] The data structure 310 is an approximate reciprocal table for integer
numbers. The number of significant digits represented in the data structure 310 is configurable. Moreover, as was discussed above with FIG. 1 , the data structure 310

https://patents.google.com/patent/WO2004095260A1/en

5/8

2/14/26, 1:53 PM

WO2004095260A1 - Very high radix division - Google Patents

need not store all possible integers since factors can be readily derived based on the radix being used for the very high radix division. Thus, for a decimal radix only
integers 2-9 (no need for 1 since the reciprocal of 1 is 1) need to be represented in the data structure 310, since all other integer possibilities can be derived from factors
for these integers using the radix. Of course it is readily apparent that to obtain a higher degree of precision the data structure 310 can include integer possibilities for
large sets of integers (e.g., 2-99, 10-99, 11-99, or other combinations for a decimal radix). Additionally, the data structure 310 need not be a table in all embodiments,
since any data structure 310 that is accessible to the logic 321 can be used. [0048] Initially, the logic 321 accesses the numerator 322 and the denominator 323. Next, the
logic 321 accesses the data structure 310 to acquire an approximate reciprocal factor for the denominator 323. In some embodiments, the logic 321 may need to derive
the approximate reciprocal factor from a value obtained from the data structure 310 by using a radix associated with the very high radix division, such as when the
denominator is not directly represented directly in the data structure 310.
[0049] Once an approximate factor is obtained or derived, the logic 321 multiplies the numerator 322 and the denominator 323 by the factor in order to restate the
division in a more efficient manner. At this point, the numerator 322 is represented as a scaled numerator and the denominator is represented as a scaled denominator.
The scaled denominator is approximately 1 , which simplifies the division. Next, the scaled denominator is decomposed into an equivalent expression that is used for
processing the division. [0050] The equivalent expression represents the fact that the division will be iteratively processed to solve for quotient 324 digits. Thus, as was
discussed above with FIGs. 1 and 2, the division will be processed by the logic 321 to obtain new values for the current remainder, and a first integer portion of the current
remainder will be a new or next quotient digit. The iterative processing produces a new quotient 324 digit for each processing cycle. Each new current remainder is
determined by the logic 321 by evaluating the expression of a first product and subtracting the sum of a first integer portion of the current remainder minus a product
represented by the first integer portion of the current remainder multiplied by 1 minus the scaled denominator. [0051] Thus, the logic 321 , in some embodiments for
efficiency purposes, restates the scaled denominator as 1 minus the scaled denominator. The logic 321 then stores this value within the processor 320 or memory
accessible to the processor 320 during initialization. Since the revised scaled denominator is approximately 1 , when the logic 321 stores the sum of 1 minus the scaled
denominator one or more leading digits of the sum will be insignificant (e.g., zeros or ones by a factor of the radix). Thus, by performing this processing the required data
width for calculating the high radix division is reduced from what has been conventionally achieved. This results in more efficient use of space within the processor 320
and reduces power consumption. Moreover, this permits microprocessor architectures to use standard cell libraries to perform very high radix division. Conversely,
conventional architectures require specialized cell libraries to process very high radix division. [0052] Next, the logic 321 enters a processing loop to iteratively determine
the quotient 324 for the very high radix division. During the first iteration, the first quotient digit is the first integer portion of the scaled numerator, since the scaled
denominator is approximately 1. In some embodiments, the logic 321 can adjust the iteratively determined quotient digits by rounding them upward/downward based on
digit values associated with the trailing portion of a current remainder (trailing portion of the scaled numerator for the first iteration).
[0053] On each processing iteration following the first iteration, the logic
321 determines a new value for a current remainder. The logic 321 takes a first integer portion of the previous current remainder and multiplies it by the radix to acquire a
first product. Next, the first integer portion of the previous current remainder is subtracted from the first product. Then, a second product is acquired by multiplying the
saved and restated scaled denominator (1 - scaled denominator) by the first integer portion of the previous current remainder. Finally, the second product is added to the
first product to acquire a new value for the current remainder. This new current remainder will then have its first integer portion used as a next quotient digit for the
quotient 324 that is being iteratively assembled. Moreover, the new current remainder is recycled back in the logic 321 for another processing cycle where the new
current remainder becomes the previous current remainder. [0054] The logic 321 continues this processing until a desired precision is reached or until a current
remainder value of 0 is obtained. Moreover, as was previously discussed and in some embodiments, during each processing cycle the first integer portion of the current
remainder can be rounded upward/downward based on the trailing portion of the current remainder before providing the next quotient digit for the quotient 324.
[0055] One of ordinary skill in the art now appreciates upon reading the above disclosure how very high radix division can be achieved with narrow data paths using the
embodiments of the present invention. The narrow data path results in less space requirements and less power consumption than what is needed with conventional
approaches. Moreover, the techniques discussed in various embodiments of the present invention maintain conventional processing throughput of conventional systems
and in some case improve processing throughput. Additionally, the tenets of the various embodiments of the present invention can be used with standard
microprocessor architectures, unlike conventional techniques that require specialized cell libraries to perform very high radix division.
[0056] It is to be understood that the above description is intended to be illustrative, and not restrictive. Many other embodiments will be apparent to those of skill in the
art upon reviewing the above description. The scope of embodiments of the invention should, therefore, be determined with reference to the appended claims, along with
the full scope of equivalents to which such claims are entitled. [0057] It is emphasized that the Abstract is provided to comply with 37 C.F.R. §1.72(b) requiring an
Abstract that will allow the reader to quickly ascertain the nature and gist of the technical disclosure. It is submitted with the understanding that it will not be used to
interpret or limit the scope or meaning of the claims. [0058] In the foregoing Description of the Embodiments, various features are grouped together in a single
embodiment for the purpose of streamlining the disclosure. This method of disclosure is not to be interpreted as reflecting an intention that the claimed embodiments of
the invention require more features than are expressly recited in each claim. Rather, as the following claims reflect, inventive subject mater lies in less than all features of
a single disclosed embodiment. Thus the following claims are hereby incorporated into the Description of the Embodiments, with each claim standing on its own as a
separate exemplary embodiment.

Patent Citations (12)
Publication number

Priority date

Publication date

Assignee

Title

US4337519A *

1979-02-01

1982-06-29

Tetsunori Nishimoto

Multiple/divide unit

GB2296350A *

1994-12-21

1996-06-26

Advanced Risc Mach Ltd

Data processing divider enabling reduced interrupt latency

US5020017A *

1989-04-10

1991-05-28

Motorola, Inc.

Method and apparatus for obtaining the quotient of two numbers within
one clock cycle

EP0530936B1 *

1991-09-05

2000-05-17

Cyrix Corporation

Method and apparatus for performing prescaled division

US6240338B1 *

1995-08-22

2001-05-29

Micron Technology, Inc.

Seed ROM for reciprocal computation

US6141670A *

1997-09-30

2000-10-31

Intel Corporation

Apparatus and method useful for evaluating periodic functions

US6078939A *

1997-09-30

2000-06-20

Intel Corporation

Apparatus useful in floating point arithmetic

JP3551113B2 *

2000-02-07

2004-08-04

日本電気株式会社

Divider

Family To Family Citations

https://patents.google.com/patent/WO2004095260A1/en

6/8

2/14/26, 1:53 PM

WO2004095260A1 - Very high radix division - Google Patents

US6782405B1 *

2001-06-07

2004-08-24

Southern Methodist University

Method and apparatus for performing division and square root functions
using a multiplier and a multipartite table

FI20011610A0 *

2001-08-07

2001-08-07

Nokia Corp

Method and apparatus for performing a division calculation

US7127483B2 *

2001-12-26

2006-10-24

Hewlett-Packard Development
Company, L.P.

Method and system of a microprocessor subtraction-division floating
point divider

US7167891B2

2003-03-21

2007-01-23

Intel Corporation

Narrow data path for very high radix division

* Cited by examiner, † Cited by third party

Cited By (10)
Publication number

Priority date

Publication date

Assignee

Title

US7167891B2

2003-03-21

2007-01-23

Intel Corporation

Narrow data path for very high radix division

US7499962B2 *

2004-12-21

2009-03-03

Intel Corporation

Enhanced fused multiply-add operation

US20060294177A1 *

2005-06-27

2006-12-28

Simon Rubanovich

Method, system and apparatus of performing division operations

US7830905B2 *

2007-04-20

2010-11-09

Cray Inc.

Speculative forwarding in a high-radix router

US8725786B2 *

2009-04-29

2014-05-13

University Of
Massachusetts

Approximate SRT division method

CN102156625B *

2011-03-31

2012-11-21

北京大学

Method for performing division calculation by utilizing rheostatic element

CN104731551B *

2013-12-23

2018-02-16

浙江大华技术股份有限公司

The method and device of divide operations is carried out based on FPGA

US10209959B2 *

2016-11-03

2019-02-19

Samsung Electronics Co.,
Ltd.

High radix 16 square root estimate

US10447983B2 *

2017-11-15

2019-10-15

Nxp Usa, Inc.

Reciprocal approximation circuit

CN111813372B *

2020-07-10

2021-05-18

上海擎昆信息科技有限公司

Method and device for realizing 32-bit integer division with high precision and
low time delay

Family To Family Citations

* Cited by examiner, † Cited by third party, ‡ Family to family citation

Similar Documents
Publication

Publication Date

Title

US8719322B2

2014-05-06

Floating point format converter

US5046038A

1991-09-03

Method and apparatus for performing division using a rectangular aspect ratio multiplier

Ko et al.

2011

Design and application of faithfully rounded and truncated multipliers with combined deletion, reduction, truncation, and rounding

US9753695B2

2017-09-05

Datapath circuit for digital signal processors

US5307303A

1994-04-26

Method and apparatus for performing division using a rectangular aspect ratio multiplier

US20040098440A1

2004-05-20

Multiplication of multi-precision numbers having a size of a power of two

US4949296A

1990-08-14

Method and apparatus for computing square roots of binary numbers

US5951629A

1999-09-14

Method and apparatus for log conversion with scaling

US6782405B1

2004-08-24

Method and apparatus for performing division and square root functions using a multiplier and a multipartite table

Murillo et al.

2021

Energy-efficient MAC units for fused posit arithmetic

US7167891B2

2007-01-23

Narrow data path for very high radix division

US5060182A

1991-10-22

Method and apparatus for performing the square root function using a rectangular aspect ratio multiplier

WO2021217034A1

2021-10-28

Design of high-performance and scalable montgomery modular multiplier circuits

KR102581403B1

2023-09-21

Shared hardware logic unit and method for reducing die area

US20040167956A1

2004-08-26

Method and apparatus for executing division

Murillo et al.

2023

A suite of division algorithms for posit arithmetic

https://patents.google.com/patent/WO2004095260A1/en

7/8

2/14/26, 1:53 PM

WO2004095260A1 - Very high radix division - Google Patents

KR100433131B1

2004-05-28

A pipelined divider with small lookup table and a method for dividing the same

CN108334304B

2023-06-23

Digital recursive division

US7711764B2

2010-05-04

Pipelined real or complex ALU

KR102332323B1

2021-11-29

Radix 16 pd table implemented with a radix 4 pd table

Pizano-Escalante et al.

2015

Fast bit-accurate reciprocal square root

US5159566A

1992-10-27

Method and apparatus for performing the square root function using a rectangular aspect ratio multiplier

Bajger et al.

2008

Low-error, high-speed approximation of the sigmoid function for large FPGA implementations

US9612800B2

2017-04-04

Implementing a square root operation in a computer system

Walke

1997

High sample-rate Givens rotations for recursive least squares

Priority And Related Applications
Applications Claiming Priority (2)
Application

Filing date

US10/394,952

2003-03-21

US10/394,952

2003-03-21

Title

Narrow data path for very high radix division

Legal Events
Date

Code

Title

Description

2004-11-04

AK

Designated states

Kind code of ref document: A1
Designated state(s): AE AG AL AM AT AU AZ BA BB BG BR BW BY
BZ CA CH CN CO CR CU CZ DE DK DM DZ EC EE EG ES FI GB GD GE
GH GM HR HU ID IL IN IS JP KE KG KP KR KZ LC LK LR LS LT LU LV
MA MD MG MK MN MW MX MZ NA NI NO NZ OM PG PH PL PT RO
RU SC SD SE SG SK SL SY TJ TM TN TR TT TZ UA UG US UZ VC VN
YU ZA ZM ZW

2004-11-04

AL

Designated countries for regional patents

Kind code of ref document: A1
Designated state(s): BW GH GM KE LS MW MZ SD SL SZ TZ UG ZM
ZW AM AZ BY KG KZ MD RU TJ TM AT BE BG CH CY CZ DE DK EE
ES FI FR GB GR HU IE IT LU MC NL PT RO SE SI SK TR BF BJ CF CG
CI CM GA GN GQ GW ML MR NE SN TD TG

2004-12-29

121

Ep: the epo has been informed by wipo that ep was designated in this application

2005-09-21

WWE

Wipo information: entry into national phase

Ref document number: 20048077421
Country of ref document: CN

2006-05-10

122

Ep: pct application non-entry in european phase

Data provided by IFI CLAIMS Patent Services

About

Send Feedback

https://patents.google.com/patent/WO2004095260A1/en

Public Datasets

Terms

Privacy Policy

Help

8/8

2/14/26, 1:57 PM

WO2019213070A1 - Systems and methods for matrix multiplication, normalization, and reverse conversion - Google Patents

Patents
Back to results

7 of 125,048

high radix

(high radix);

Systems and methods for matrix multiplication, normalization, and reverse conversion
Abstract
Arithmetic circuits and methods that perform efficient matrix multiplication for hardware
acceleration of neural networks, machine learning, web search and other applications are disclosed

WO2019213070A1
WIPO (PCT)

herein. Various arrays of multiplier-accumulators may be coupled to form a matrix multiplier which
processes data using high precision, fixed point residue number arithmetic.

Download PDF

Find Prior Art

Similar

Other languages: French

Classifications

Inventor: Eric Olsen
G06F7/729 Methods or arrangements for performing computations using a digital nondenominational number representation, i.e. number representation without radix; Computing

Current Assignee : Individual

devices using combinations of denominational and non-denominational quantity representations,
e.g. using difunction pulse trains, STEELE computers, phase computers using residue arithmetic
using representation by a residue number system

Worldwide applications
2018 US US US US 2019 WO

G06F17/15 Correlation function computation including computation of convolution operations
G06F17/16 Matrix or vector computation, e.g. matrix-matrix or matrix-vector multiplication,
matrix factorization
G06F7/72 Methods or arrangements for performing computations using a digital nondenominational number representation, i.e. number representation without radix; Computing
devices using combinations of denominational and non-denominational quantity representations,
e.g. using difunction pulse trains, STEELE computers, phase computers using residue arithmetic

Application PCT/US2019/029901 events
2019-04-30

Application filed by Individual

2019-11-07

Publication of WO2019213070A1

2020-11-04

Anticipated expiration

Status

Ceased

G06N3/0464 Convolutional networks [CNN, ConvNet]
G06N3/063 Physical realisation, i.e. hardware implementation of neural networks, neurons or

Info: Patent citations (24), Cited by (54), Legal events, Similar
documents, Priority and Related Applications

parts of neurons using electronic means

G06N3/045 Combinations of networks

External links: Espacenet, Global Dossier, PatentScope, Discuss

Hide more classifications

Landscapes

Engineering & Computer Science
Physics & Mathematics
Show more

Claims

Hide Dependent

CLAIMS What is claimed is:
1. A residue number matrix multiplier comprising:
a plurality of digit processing elements, each of the digit processing elements comprising a modular accumulator associated with a modulus and responsive to a control input,
wherein:
the modular accumulator operates in a modular accumulation mode for an initial accumulation responsive to assertion of a control input;
the modular accumulator operates in a binary accumulation mode for a final accumulation responsive to de-assertion of a control input; and
at least a subset of the modulus are unique;
wherein said modular accumulation is transferred from the modular accumulator to an output register before overflow of the modular accumulator.
2. The residue number matrix multiplier of claim 1, wherein the modular accumulator performs operations on congruent operands.
3. A matrix multiplier that performs matrix multiplication on a plurality of input matricies in residue number format, the matrix multiplier comprising:
a plurality of digit processing units that receive said plurality of input matricies, each of said plurality of digit processing units associated with a distinct modulus; and
one or more normalization units in communication with each of said plurality of digit processing units;

https://patents.google.com/patent/WO2019213070A1/en?q=(high+radix)&oq=high+radix

1/38

2/14/26, 1:57 PM

WO2019213070A1 - Systems and methods for matrix multiplication, normalization, and reverse conversion - Google Patents

wherein each of said plurality of digit processing units perform a modular matrix multiplication on said plurality of input matricies resulting in a matrix of non-normalized dot
products;
wherein said one or more normalization units generate a plurality of normalized dot products based on a plurality of non-normalized dot products from said matrix of nonnormalized dot products;
wherein said plurality of normalized dot products form a single normalized matrix product.
4. The matrix multiplier of claim 3, wherein when at least one of said plurality of normalized dot products is negative said one or more normalization units subtract a correction
constant from said at least one of said plurality of normalized dot products.
5. A method for normalizing signed numbers comprising digits having a whole range and a fractional range with a normalization unit, the method comprising:
receiving a non-normalized intermediate product at an input of the normalization unit; converting the non-normalized intermediate product to a plurality of mixed-radix digits
associated with a whole range and a fractional range;
converting the plurality of mixed radix digits associated with the whole range to a residue format, producing an uncorrected normalized residue product; and
when the uncorrected normalized residue product is a negative value, generating a normalized residue product by applying a correction constant to the uncorrected normalized
residue product;
wherein a signed normalized product is generated and transmitted to an output of the normalization unit.
6. The method of claim 5, wherein a rounding value is added to the signed normalized product when one or more digits of the fractional range equal or exceed a threshold.
7. A residue number reverse conversion apparatus comprising:
an input register that receives an RNS value having a fractional portion and a whole number portion; and
an output register where a fully signed converted value is generated by:
scaling the RNS value by a fractional range of the output register thereby creating a scaled RNS value;
converting the scaled RNS value to a mixed-radix format thereby creating a scaled mixed-radix value having a fractional portion and a whole portion;
truncating the fractional portion of the scaled mixed-radix value thereby creating a truncated mixed-radix value; and
converting the truncated mixed-radix value to an un-corrected binary value; and subtracting a correction constant from the un-corrected binary value when the un corrected
binary value is negative.
8. The residue number reverse converter of claim 7, further comprising adding a rounding value to the fully signed converted value if the truncated mixed-radix value is greater
than a predetermined threshold value.
9. The residue number reverse converter of claim 7, wherein the correction constant is equal to the range of the whole number portion of the RNS value.

Description

wo 2oi?/2i307o s and methods for matrix multiplication, ncpcr/us2oi9/o299oi
and reverse conversion
BACKGROUND OF THE INVENTION
1. Field of the Invention
[0001] The invention relates to matrix multipliers and in particular to a residue number matrix multiplier and methods therefor.
2. Related Art
[0002] The use of Convolutional Neural Networks (CNN’s) has exploded due to emerging technologies such as autonomous vehicles and cloud-based AI. Unfortunately,
the intense numerical processing demands of CNN’s place heavy workload on servers using general purpose CPU and GPU technology; this translates to high power
consumption and cost. Factors such as the slowing of Moore’s law, the need to save power, and the ever-increasing demand for compute capability create opportunities
for hardware accelerators that are streamlined to solve specific problems.
[0003] One type of circuit for AI acceleration is a so-called hardware matrix multiplier, i.e., a systolic array of multiplier- accumulators coupled to perform matrix
multiplication. The advantage of the matrix multiplier is derived from the massive parallelism afforded by a two-dimensional array of processing elements and is also due
to the streamlined flow of matrix data to the many processing elements.
[0004] The mapping of neural network algorithms to systolic array architectures was proposed and analyzed by S. Y. Kung and others in the early l990’s. S. Y. Kung reformulates the retrieving phase of neural networks by mapping it to consecutive matrix multiplication interleaved with a non-linear activation function. In another
adaptation, 2D -convolution used in AI pattern recognition is mapped to matrix multiplication by re-ordering input data flow.
[0005] Recently, a systolic architecture for processing CNN’s called the Tensor Processing Unit (TPU) was developed by Google Inc. The TPU uses a 256x256 element
matrix multiplier coupled to circuits enabling data pooling, normalization, and application of a non-linear activation function. The TPU significantly accelerates the
inference phase of CNN’s by supporting a minimal operand precision, but it does not support the precision required for training phases. The problem is exasperated when
developing neural network weights during training phases of the CNN’ s, since the same TPU hardware cannot be used to train the network.
[0006] Moreover, convolution algorithms have been found to be sensitive to limited numerical precision. [0007] From the discussion that follows, it will become apparent
that the present invention addresses the deficiencies associated with the prior art while providing numerous additional advantages and benefits not contemplated or
possible with prior art constructions

https://patents.google.com/patent/WO2019213070A1/en?q=(high+radix)&oq=high+radix

2/38

2/14/26, 1:57 PM

WO2019213070A1 - Systems and methods for matrix multiplication, normalization, and reverse conversion - Google Patents

SUMMARY OF THE INVENTION
[0008] Arithmetic circuits and methods that perform efficient matrix multiplication for hardware acceleration of neural networks, machine learning, web search and other
applications are disclosed herein. A two-dimensional systolic array of multiplier-accumulators may be coupled to form a matrix multiplier which processes data using
high precision, fixed point residue number arithmetic.
[0009] Nicknamed RNS TPU, the matrix multiplier takes advantage of carry-free RNS arithmetic for processing of dot products. As a result, the circuit is partitioned so
each RNS digit is processed using a dedicated matrix multiplier unit. By operating a sufficient number of digit matrix multipliers in parallel, a suitable RNS word size is
realized. After dot product summation is complete, the RNS TPU transfers each summation to a word normalization pipeline unit, wherein all digits of the RNS word are
processed together. The systolic architecture favors a small RNS digit bit- width, so each digit matrix multiplier is realized in minimal IC area, using fast adders and
multipliers. These features and others result in greater speed and efficiency especially as arithmetic precision is increased.
[0010] Using an FPGA based implementation and analysis, the RNS TPU performs matrix multiplication of 32.32 fixed point arithmetic 7 to 9 times more efficiently than a
binary matrix multiplier provided the dimension of the matrices multiplied are sufficiently large. This FPGA based implementation uses 18-bit wide RNS digits and is
introduced in this specification as a working example. It is anticipated that reduction of the RNS encoding width to 7-bits and adoption of ASIC technology will result in
significantly higher efficiency of the order of 25 to 80 times higher than an equivalent high-precision binary format using similar technologies and standard design layout.
[0011] To further increase efficiency and precision, a matrix multiplier of the present invention is disclosed which operates using the residue number system (RNS).
Traditional motivations to use RNS include exploiting carry-free properties and decreased power consumption. But new motivations have recently emerged, including
a“true” fixed point RNS arithmetic that enables efficient processing of fractional product summations, i.e., the most common operation in AI.
[0012] Other systems, methods, features and advantages of the invention will be or will become apparent to one with skill in the art upon examination of the following
figures and detailed description. It is intended that all such additional systems, methods, features and advantages be included within this description, be within the scope
of the invention, and be protected by the accompanying claims
BRIEF DESCRIPTION OF THE DRAWINGS
[0013] The components in the figures are not necessarily to scale, emphasis instead being placed upon illustrating the principles of the invention. In the figures, like
reference numerals designate corresponding parts throughout the different views
[0014] Figure 1 is a block diagram of an exemplary systolic matrix multiplier implemented using binary arithmetic;
[0015] Figure 2a is a block and flow diagram of an exemplary systolic matrix multiplier logic element;
[0016] Figure 2b is a block diagram of an exemplary multiplier- accumulator element for a systolic matrix multiplier;
[0017] Figure 3a illustrates a generalized RNS fixed-point number format and digit range definitions;
[0018] Figure 3b illustrates exemplary RNS fixed-point values in decimal notation;
[0019] Figure 3c illustrates exemplary RNS fixed-point values in hexa-decimal notation;
[0020] Figure 3d illustrates exemplary RNS moduli sets;
[0021] Figure 3e illustrates an example of RNS fixed-point multiplication;
[0022] Figure 4 is a block diagram of an exemplary RNS matrix multiplier in a TPU accelerator card application;
[0023] Figure 5 is a block diagram of an exemplary 2x2 array of multiplier- accumulator elements;
[0024] Figure 6 is a block diagram of an exemplary matrix multiplier with plurality of digit matrix multiplier units;
[0025] Figure 7 is a block diagram of an exemplary RNS matrix multiplier and elements thereof;
[0026] Figure 8a is a block diagram of an exemplary binary multiplier-accumulator (BMAC);
[0027] Figure 8b is a block diagram of an exemplary binary multiplier-accumulator (BMAC) symbol and I/O;
[0028] Figure 9a is a block diagram of an exemplary modular multiplier- accumulator (MMAC);
[0029] Figure 9b is a block diagram of an exemplary modular multiplier-accumulator (MMAC) symbol and I/O with binary multiplier resource.
[0030] Figure lOa is a block diagram of an exemplary modular accumulator arithmetic circuit synthesized using binary resources;
[0031] Figure lOb is a block diagram of an exemplary modular reduction circuit synthesized using binary resources; [0032] Figure 11 is a block diagram of an exemplary
faulty pipelined modular accumulator circuit design;
[0033] Figure 12 is a block diagram of an exemplary modular accumulator arithmetic circuit with synthesized parallel adders;
[0034] Figure 13 is a block diagram of an exemplary pipelined binary accumulator circuit;
[0035] Figure l4a is a block diagram of an exemplary modular accumulator with truncation enable control;
[0036] Figure l4b illustrates an exemplary signal waveform showing control of a modular accumulator with ENA_TRUNC and accumulator reset control inputs;
[0037] Figure l4c illustrates an exemplary signal waveform showing control of modular accumulator with ENA_TRUNC and accumulator load control inputs;
[0038] Figure l5a is a block diagram of an exemplary 36-bit to 21 -bit modular reduction unit connected to a modular accumulator with ENA_TRUNC and reset control
inputs;
[0039] Figure l5b illustrates exemplary modular product summation data;
[0040] Figure l5c illustrates an exemplary waveform of modular multiplier- accumulator with ENA_TRUNC and reset control inputs;
[0041] Figure 16 is a block diagram of an exemplary pipelined modular accumulator with congruent output and ENA_TRUNC and reset control inputs;

https://patents.google.com/patent/WO2019213070A1/en?q=(high+radix)&oq=high+radix

3/38

2/14/26, 1:57 PM

WO2019213070A1 - Systems and methods for matrix multiplication, normalization, and reverse conversion - Google Patents

[0042] Figure l7a is a block diagram of an exemplary pipelined product normalization with dual mixed -radix conversions;
[0043] Figure 17b is a block diagram of an exemplary pipelined product normalization with dual mixed -radix conversions;
[0044] Figure l8a is a block diagram of an exemplary pipelined product normalization unit with single mixed-radix conversion and negative value correction;
[0045] Figure 18b is a block diagram of an exemplary pipelined product normalization unit with single mixed-radix conversion and negative value correction;
[0046] Figure l8c is a block diagram of an exemplary modular subtract then multiply |SM| element;
[0047] Figure l8d is a block diagram of an exemplary modular multiply then add |MA| element;
[0048] Figure l8e is a block diagram of an exemplary pipeline comparator C element;
[0049] Figure 18f illustrates exemplary product normalization data;
[0050] Figure 19 is a block diagram of an exemplary modular subtract then multiply |SM| element schematic using binary resources;
[0051] Figure 20 is a block diagram of an exemplary modular multiply then add |MA| element schematic using binary resources; [0052] Figure 21 is a block diagram of an
exemplary pipeline comparator element;
[0053] Figure 22a is a block diagram of an exemplary streamlined product normalization pipeline with negative value correction;
[0054] Figure 22b is a block diagram of an exemplary arithmetic positive value product normalization pipeline;
[0055] Figure 22c is a block diagram of an exemplary arithmetic negative value product normalization pipeline;
[0056] Figure 23a is a block diagram of an exemplary base (digit) extension pipeline unit with sign extend;
[0057] Figure 23b is a block diagram of an exemplary arithmetic positive value base extend pipeline;
[0058] Figure 23c is a block diagram of an exemplary arithmetic negative value base extend pipeline;
[0059] Figure 24 is a block diagram of an exemplary floating-point number to RNS fixed -point converter pipeline block;
[0060] Figure 25 is a block diagram of an exemplary signed fixed -point binary to fixed-point RNS converter pipeline block;
[0061] Figure 26 is a block diagram of an exemplary forward integer conversion pipeline;
[0062] Figure 27a is a block diagram of an exemplary forward binary fraction value to RNS fraction value conversion pipeline;
[0063] Figure 27b is a block diagram of an exemplary forward binary fraction to RNS fraction value conversion pipeline;
[0064] Figure 28 is a block diagram of an exemplary 2-digit forward fractional value converter pipeline arithmetic circuit for a TPU;
[0065] Figure 29 is a block diagram of an exemplary complete 32.32 fixed-point binary to RNS forward converter schematic for a TPU;
[0066] Figure 30 is a block diagram of an exemplary reverse RNS fixed-point to binary fixed-point converter pipeline block diagram with sign magnitude requirement;
[0067] Figure 31 is a block diagram of an exemplary reverse RNS fixed-point to binary fixed-point converter with sign detect and negative value correction;
[0068] Figure 32a is a block diagram of an exemplary initial scaling stage for reverse RNS fixed- point to binary fixed -point converter pipeline;
[0069] Figure 32b is a block diagram of an exemplary RNS to mixed-radix conversion stage for reverse converter pipeline; [0070] Figure 32c is a block diagram of an
exemplary mixed-radix to binary conversion stage for reverse converter pipeline;
[0071] Figure 33 is a block diagram of an exemplary reverse conversion of a positive fractional RNS value to binary pipeline;
[0072] Figure 34 is a block diagram of an exemplary reverse conversion of a negative fractional RNS value to binary pipeline; and
[0073] Figure 35 is a block diagram of an exemplary pipelined MOD function circuit.
DETAILED DESCRIPTION OF THE INVENTION
[0074] This disclosure introduces a new and novel approach to implementing high precision arithmetic for matrix multiplication which uses residue number arithmetic
instead of binary arithmetic. The residue number system (RNS) used is a carry free number system that supports signed fixed point numbers; this new version of RNS
was disclosed in U.S. Patent No. 9,081,608.
[0075] A unique property of RNS provides an opportunity to organize the matrix multiplier into separate digit slices. Each digit slice supports its own matrix multiplier unit,
called a digit matrix multiplier. A complete RNS word is processed by operating a plurality of digit matrix multipliers in parallel. Because the width of each RNS digit is
comparatively small, each digit matrix multiplier is optimized to be fast, efficient and consume less circuit area. By operating a plurality of efficient digit matrix multipliers
in parallel, a high precision matrix multiplier is realized. Of important note is that increasing the precision of the matrix multiplier does not slow the circuit, since there is
no carry between digits during the product summation operation due to the use of RNS.
[0076] Another major advantage of the RNS matrix multiplier is that multiplier resources grow in a linear manner with respect to increasing data precision (in bits). In
contrast, binary multiplier resources grow geometrically with respect to increasing bit precision. Therefore, as target precision is increased, there is a point when the RNS
matrix multiplier is more efficient than a binary matrix multiplier. Viewed in another way, greater efficiency can be realized by choosing a smaller RNS digit width for most
applications. Changing the RNS digit width and number of RNS digits allows the RNS matrix multiplier to be tuned for a particular performance and precision. This means
the RNS matrix multiplier can be implemented with less circuitry, operate with less power, and yet operate at much higher speed than a matrix multiplier employing binary
arithmetic of the same approximate data precision. In existing tests, the RNS TPU is shown to be as much as 10 to 25 times more efficient than binary arithmetic at high
precision.
[0077] Furthermore, this patent disclosure introduces several new advances in RNS circuit design which facilitate high speed general purpose computation in RNS. These
advances are reflected in specific apparatus that comprise preferred solutions to the matrix multiplier of the present invention. One such advancement is the use of
“congruent modular arithmetic”. This advancement makes use of the fact that residue digit values are often derived from the output of multipliers having twice the

https://patents.google.com/patent/WO2019213070A1/en?q=(high+radix)&oq=high+radix

4/38

2/14/26, 1:57 PM

WO2019213070A1 - Systems and methods for matrix multiplication, normalization, and reverse conversion - Google Patents

number of bits. In the prior art, it is often deemed necessary to decompose or reduce the multiplier output to a residue digit value of“legal range” by taking a complete
MOD function of the multiplier output. In one preferred embodiment of the present invention, the output of a binary multiplier is not decomposed entirely but is only
reduced to a value congruent to the correct modular result. The full modular result is thus delayed as long as practical. This technique has a dramatic effect on the speed
and efficiency of RNS circuits, especially for high performance multiply-accumulate processing. In several preferred embodiments, the width of the congruent digit result
will only be a few bits wider than the fully reduced residue digit. Therefore, there is enough reduction to maintain the efficiency of small RNS digit precision while at the
same time eliminates a final MOD operation which saves circuitry and several pipeline stages.
[0078] Moreover, the present invention discloses new techniques and associated circuitry providing for high speed modular accumulation. In this novel approach,
modular accumulation is sped up by accumulating a value congruent to the modular summation. This is particularly important if the width of the residue digit is relatively
large, which allows supporting much higher precision using less number of RNS digits. This novel and unique invention allows a single residue digit accumulator to be
pipelined like a standard binary accumulator, resulting in very high-speed operation. This is not possible or at least very difficult with prior art modular accumulators since
the full MOD function inhibits or restricts pipelining of the accumulator loop.
[0079] Another new advancement disclosed herein is the design of fully pipelined normalization and conversion circuits. The new pipelined normalization units use less
than 30% of the resource of the prior normalization designs. Moreover, the use of advanced normalization pipelines reduces or eliminates any need to support sign bits,
i.e., sign magnitude notation in RNS.
[0080] Another advancement disclosed is the method of truncating RNS data for storage of results, and base extending stored RNS data to a fully extended state prior to
processing to reduce memory access and storage costs.
[0081] Another advantage of the RNS matrix multiplier is in the application of Web Search. In some search algorithms, a rating to the importance of a web site is used to
list search results. To find this rating, a transition probability matrix is built from the hyper-link structure of the web and is used to rank web sites using the relationship of
the probability of surfing from any given web node to any other web node. One such popular search algorithm is called“Page Rank”. Page Rank applies the power method,
which requires vector multiplication of each row of a probability matrix to the page rank vector.
[0082] Because page rank iterations can be processed similar to matrix multiplication, a specially modified RNS matrix multiplier (systolic architecture with stationary
product summation) is devised to converge the page rank vector using RNS calculations. In this special modification, the RNS matrix multiplier product normalization
pipelines are modified to integrate the page rank recursive formula, thereby efficiently updating the page rank vector without a full multiply for each element. The multiply
of the constant 0.85 is replaced with a suitable quick divide (by modulus factor) and then quick multiply of constant factor implementing the desired ratio of “0.85”. The
systolic architecture also addresses the issue of huge data sets by managing final summation of partial product summations in hardware. There are many advantages of
this arrangement including speed, efficiency, and high accuracy. In addition, because RNS data can be separated into digits, and each digit can be operated on
independently during the process of product summation, this allows the storage of the page rank vector to be efficiently partitioned into multiple memories. This provides
a means to store a larger page rank vector into multiple memory banks, each memory bank directly connected to a product summation apparatus.
[0083] Another advantage of the RNS matrix multiplier of the present invention is its very high numerical accuracy for matrix multiplication. During processing of RNS
product summations of fixed-point data, the entire word size of the RNS value is used since there is no penalty of carry and since overflow would result otherwise.
Moreover, there is incentive to accumulate the full word size of non-normalized products because the cost of normalizing each product in RNS is high, therefore, product
normalization is delayed until after product summation is complete according to equation (16). As a result, there is no loss of accuracy until the final step of
normalization, which is applied only once after all product summation is complete (dot products are processed).
[0084] A floating-point unit cannot emulate the precision of the RNS matrix multiplier because it cannot operate without loss of precision since the floating-point unit
does not store or operate on non-normalized values; instead, the floating point unit normalizes on each product multiply thereby resulting loss of precision.
[0085] A matrix multiplier implemented using fixed-point or floating-point binary arithmetic may be forced to normalize or round up for each product or for each
summation, or both, since the cost of carry delays, partial product summation and high resource requirements of accumulating double width binary products is high. In
the case of binary arithmetic, there is significant loss of information due to truncation and rounding error since truncation results in every multiply or summation
operation. In contrast, RNS matrix multiplication performs zero truncation during product summation, and is fast, efficient, and highly accurate.
[0086] Another significant advantage of the RNS TPU of the present invention is the high-speed operation resulting from the reduced digit encoding width of RNS, and
lack of carry from digit- to- digit in RNS arithmetic. Because RNS digit encoding width is very small, say of the order of 7 to 8 bits in some cases, it follows that RNS
operand buses, multipliers, reduction units and modular accumulators also shrink, and this affects the internal operation of the matrix multiplier array 400 of Figure 4
since signal paths within the array are much shorter as well. The methods and inventions disclosed herein provide further means to enhance RNS speed by pipelined
modular accumulation using advanced techniques disclosed herein. Therefore, digit encoding width is not only small in RNS, but may be further reduced by partitioning
the digit accumulator into a plurality of smaller accumulation stages cascaded in time for highest speed possible.
[0087] Another significant advantage of the RNS TPU of the present invention is the low-power operation resulting from the high-efficiency of RNS arithmetic. One reason
this is justified is that RNS matrix multiplication requires a linear increase in unit resources as arithmetic precision is increased; on the other hand, a conventional matrix
multiplier comprising binary arithmetic, such as floating-point arithmetic, will require a square increase in multiplier resources as the arithmetic precision is increased.
This precision tradeoff can be controlled in RNS by selecting a suitable modulus set for a given application and ensuring the ratio of RNS multiplier resources to
equivalent binary multiplier resources is minimized, all else being equal. Low multiplier resource efficiency coupled with slow speed and low precision places the matrix
multiplier based on binary arithmetic at a significant disadvantage to the performance, precision and efficiency of a matrix multiplier based on RNS fixed -point
arithmetic.
[0088] The subject of the present invention is a hardware-based matrix multiplier which uses residue number arithmetic for its processing. Such hardware matrix
multipliers are known in the prior art as highly pipelined systolic architectures since many hardware processing elements are tightly inter-connected enabling efficient
flow of data and highly parallel processing. In the prior art, binary fixed point or floating-point arithmetic units are commonly used to implement a matrix multiplier since
binary arithmetic is a standard, nearly universal solution for machine computation.
[0089] However, a new alternative to binary arithmetic has recently emerged which is based on an improved version of the residue number system (RNS). The new
arithmetic allows continuous processing of fractional data values in RNS format while preserving the carry free properties of integer RNS arithmetic. The new RNS
arithmetic was first introduced in U.S. Patent No. 9,081,608 and is referred to as“modular computation in RNS” in the present disclosure. The present invention discloses
a unique approach to matrix multiplier design by using modular computation in RNS to implement matrix multiplication. The primary computation required is fractional
product summation to process each matrix dot product; therefore, modular computation in RNS appears ideally suited for this task.
[0090] In one high performance design, a square matrix of multiplier- accumulators is interconnected to process matrix multiplication so that each multiply-accumulate
element solves the dot product corresponding to its position. When using binary arithmetic circuits, each multiplier-accumulator must be completely located within the
matrix position so that carry is easily accommodated during accumulation. However, with RNS arithmetic, each RNS digit is processed using a high speed modular

https://patents.google.com/patent/WO2019213070A1/en?q=(high+radix)&oq=high+radix

5/38

2/14/26, 1:57 PM

WO2019213070A1 - Systems and methods for matrix multiplication, normalization, and reverse conversion - Google Patents

accumulator which“accumulates” without generating carry to another RNS digit. Therefore, RNS digits may be separated and their corresponding circuits may be isolated
from each other. This means each RNS digit sub-system is relatively small, and more efficient than a larger binary circuit which cannot be so organized.
[0091] One novel and unique circuit element disclosed in the present invention speeds up modular accumulation by operating on values which are congruent to a“fully”
reduced RNS digit result. By so doing, a running MOD function is implemented which is fast, and allows the modular accumulator to be pipelined in the same manner as
binary accumulators. This novel circuit has implications, as it changes the nature of both RNS multiplier and RNS accumulator arithmetic, and furthermore reduces
hardware requirements and provides for faster operation than prior art RNS methods.
[0092] The matrix multiplier of the present invention may exist in many different forms and be integrated into various processing systems. For example, the matrix
multiplier of the present invention can be implemented as an arithmetic sub-system in custom and semi-custom ICs; in this case, the matrix multiplier provides on-chip
acceleration of matrix multiplication for CPU’s and GPU’s. Alternatively, the matrix multiplier of the present invention can be implemented as an “accelerator card”
typically housed within a rack mount chassis and suitably connected to other servers and computers which utilize the accelerator card’s functions. The matrix multiplier
may be implemented as a hard resource within IC’s or may also be implemented within one or more FPGA’s as a soft configurable system. It should be clear to those
skilled in the art of computer design that the invention of the present disclosure can be implemented in different ways, with different technologies, and may be applied to
a wide range of applications and problems.
[0093] The disclosure will also briefly discuss the problems and issues of implementing a hardware matrix multiplier using binary arithmetic to illustrate and contrast the
operation of the RNS matrix multiplier against known prior art.
[0094] To be concise, the present invention is disclosed using illustrations of only one preferred embodiment, which includes the matrix multiplier as a component within
an accelerator card application as shown in the block diagram of Figure 1. For example, the accelerator card of Figure 1 can be used to accelerate the inference phase of
convolutional neural networks (CNN). In this embodiment, a hardware matrix multiplier 100 of the present invention is shown as a principal component of a neural
network accelerator card 106. The accelerator card supports typical interfaces such as a PCIe bus interface 140, and typical DRAM memory interface 135 and DDR4
memory bank(s) 145.
[0095] Detailed description of the many operations present and required on an accelerator card 106 of Figure 1 is not necessary, as these details are well known to those
skilled in the art of computer design. However, a basic overview of the general operations of each component is provided for clarity.
[0096] One general aim of the circuitry of accelerator card 106 of Figure 1 is to maintain a constant, high rate of matrix data flow to the hardware matrix multiplier 100. A
host computer, not shown in Figure 1, is responsible for issuing commands to the accelerator card via PCIe bus interface 140. For example, accelerator card commands
may instruct the hardware matrix multiplier 100 to multiply two matrices in memory. In one embodiment, accelerator card commands and instructions are stored in DDR4
memory system 145 and directly transferred and executed in the matrix control unit 150.
[0097] Central to the accelerator card is the hardware matrix multiplier 100 which in one embodiment consists of a two-dimensional array of multiplier-accumulator
elements or processors, such as multiplier- accumulator element 101, 102, 103 and 104. In one embodiment, matrix data is fed into the hardware matrix multiplier 100
along two axis, each axis fed by an array of high speed FIFO or dual port memory 105, 115. As shown in Figure 1, a matrix A data source 115 is fed to the rows of the
matrix multiplier 100 while a matrix B data source 105 is fed to the columns of the matrix multiplier 100. To maintain high speed operation, matrix data is typically
accessed from DDR4 memory 145 via high-speed memory controller interface 135. Matrix data is transferred from the DDR4 memory to its respective matrix port in one
embodiment via data cache 125, 130 which is in turn coupled to its respective matrix data routing circuitry 110, 120. The data cache memory allows frequently used
matrix data to be queued and stored without the need to re access through DDR4, which can be slower than accessing cache memory.
[0098] The circuitry for the accelerator card 106 of Figure 1 will typically include matrix routing circuitry 110, 120 to re-organize stored data delivered by each respective
data cache 130, 125 to each respective matrix data port 105, 115. For instance, matrix B data routing circuit 110 will re organize matrix data by columns and will deliver
matrix data to the column-oriented matrix B data port 105. Likewise, the matrix A data routing circuits 120 will re-organize matrix data by rows and will deliver matrix data
to the row-oriented matrix A data port 115. Matrix data ports 105, 115 may be comprised of suitable high-speed memory circuits, such as FIFO registers, or dual-ported
RAM. Matrix control unit 150 coordinates the delivery of matrix data from matrix A data port 115 and matrix B data port 105 to the row inputs and column inputs of
matrix multiplier 100 respectively.
[0099] The output of the matrix multiplier 100 is a product matrix, which in Figure 1 is shown exiting from the right of each row of the systolic array 100 by means of
illustration. The data rate of matrix products exiting the systolic array 100 may be equal to the data flow of matrix operand input, and therefore a matrix product result
store 160 is needed to temporarily store output matrix data before it is transferred back to memory or back to the matrix multiplier data input. The output of the hardware
matrix multiplier 100 may not be fully normalized, and therefore a global normalize pipeline circuit 155 may be present to normalize each dot product which exits a row of
matrix multiplier 100. Additional normalization pipelines may be present for each additional row of the multiplier-accumulator array 100 as shown in Figure 1. Normalized
dot product values are received via pipelined normalization units, such as normalize unit 155, by the matrix product result port 160. The matrix product result store 160
will consist of suitable high-speed memory, such as high-speed FIFO register or dual-ported RAM memory. The matrix product port 160 may also include routing circuitry
and other support circuitry to assist in transmission of matrix product results back into data cache memory 130 or fed back directly into matrix data routing circuitry 110
for re -processing.
[0100] A post function unit 165 may exist in the processing pipeline of the matrix multiplier to facilitate other required functions, such as matrix addition and scalar or
vector matrix multiplication. The post function is typically a pipelined circuit, and more than one pipeline may be present. The post function 165 can be applied to the
normalized matrix product result received from the matrix product store 160 as shown in Figure 1. Alternatively, other special functions can be applied elsewhere using
circuits and data paths not shown in Figure 1. For neural network applications, the post function 165 incorporates a non-linear function, such as a ReLU or Sigmoid
function, to facilitate the inference phase of convolutional neural network processing as suggested by Rung.
[0101] Figure 2a illustrates a diagram describing a typical two-dimensional systolic architecture suitable for matrix multiplication, i.e., a two-dimensional array lOOb of
multiplier- accumulator elements, such as element 101, which in a preferred embodiment of the present invention comprises the matrix multiplier 100 of Figure 1. It is
well known that other systolic architectures exist for processing of matrix multiplication. Figure 2b shows a typical block representation of a single multiplieraccumulator element 270. The multiplier- accumulator 270 has two inputs, Aln and Bin, which illustrate row data and column data input from a prior element, and two
outputs Aout and Bout, which illustrate data flowing out to the next row and column elements in the array respectively. Figure 2b also shows a third output, Yout, which is
the current summation of the products of Am and Bln in the prior clock cycle.
[0102] In Figure 2b, a typical multiplier-accumulator element 270 may be defined with a series of recurrence equations 271, 272, 273, 274 describing data movement
between the input ports and output ports for each transition of the clock, and equation 271 relating internal state variables to their next transition state by means of an
arithmetic expression. Equation 271 illustrates that element 270 stores a summation Y, and that the summation Y is updated on each clock cycle by adding the product
of the inputs Am and Bin to the current summation value Y. Thus, it can be easily deduced that the element 270 is indeed a multiplier-accumulator arithmetic circuit.

https://patents.google.com/patent/WO2019213070A1/en?q=(high+radix)&oq=high+radix

6/38

2/14/26, 1:57 PM

WO2019213070A1 - Systems and methods for matrix multiplication, normalization, and reverse conversion - Google Patents

[0103] Some details are missing in Figure 2b for a practical design. One such omission are the control signals needed to coordinate each element 270, i.e., such as an
accumulator load function, or an accumulator clear function. Later sections of this disclosure will describe control circuits in more detail where pertinent to the subject
invention, but otherwise control circuits are deemed well understood in the prior art. Figure 2b is included to provide completeness, however, significant detail is missing,
such as notation indicating pipeline delay which changes the time subscript between the input and output variables of equation 271. For example, omitted detail may
include time state subscripts to indicate the number of clocks delay between summation output Y and its corresponding last data summed Am and Bin.
[0104] In one embodiment of the present invention illustrated by Figure 2a, all elements comprising matrix multiplier lOOb are the same multiplier- accumulator element
270 of Figure 2b. For example, element 101 of Figure 2a is identical to the element 270 of Figure 2b. Figure 2a also shows the matrix data A port 115 and the matrix B
data port 105 in more detail; the matrix data ports are shown as FIFO data structures by means of example which are directly coupled to the matrix multiplier lOOb. In
one embodiment, as shown in Figure 2a, the matrix data A port 115 is comprised of a series of FIFO registers 201, 205 210, 215, where each FIFO register is connected to
a matrix input row by data bus 202, 206, 211, 216. Likewise, matrix data B port 105 is comprised of a series of FIFO registers 220, 225, 230, 235, where each FIFO register
is connected to a matrix input column by data bus 221, 226, 231, 236.
[0105] Figure 2a also shows more detail for the matrix product result store 160. The product result store 160 is comprised of a series of FIFO registers 240, 245, 250, 255
which each receive dot product data from a row of the matrix multiplier lOOb transferred over output readout bus 241, 246, 251, 256. The output readout bus 241 is
diagrammatic since there are a number of methods to implement data transfer of dot products from each multiplier- accumulator element of a matrix row. For example,
semi-systolic methods exist which allow the readout bus 241 to be driven by each element output Y using tri-state logic implementations. Alternatively, systolic shift
register based readout circuitry can be implemented which typically requires summations for each element Y be buffered, and then transferred to the shift register
readout circuit when ready. In this case, the single line bus 241 of Figure 2a represents a chain of parallel load shift registers.
[0106] Figure 2a also illustrates the classic diagonal wave-front of column matrix data by dotted lines 260, 261. In fact, all matrix data is staggered in this way to facilitate
the proper timing of matrix multiplication within the matrix multiplier array lOOb. Matrix control unit 150 of Figure 1 provides the control information to synchronize each
successive row and column data port FIFOs 115, 105 to ensure the proper matrix data staggering for each matrix operation. Not shown in Figure 2a are normalize
pipeline units 155 of Figure 1 for means of clarity.
[0107] Representation of Fixed-Point RNS Values
[0108] This section re-introduces the new fixed point RNS arithmetic as first disclosed in U.S. Patent No. 9,081,608. The fixed-point RNS format is important to the
underlying operation of the RNS based matrix multiplier and is therefore briefly reviewed and referred to for completeness.
[0109] A real value x is represented in RNS using the following digit nomenclature,

[0110] In this digit nomenclature, we define the modulus associated to each digit using the same subscript in (1) as,

[0111] The new nomenclature includes a period which defines two groups of moduli, one group defining a fractional range RF, the other a whole number range Rw- The
period has no other significance, and may treated as a comma; therefore, the value in (1) is treated as a single residue value. Both digits and modulus are positive
integers; see prior art for information regarding standard integer RNS operations and definitions.
[0112] The total range RT for a fixed-point residue number system is the product of all modulus,

[0113] The fractional range RF is defined as the product of the first F number of modulus,
RF = m1 * m2 *...* mf (4)
[0114] The whole number range Rw is the product of the last N number of modulus,
R

w = mF+ * mF+2 * - * mF+N (5a)

[0115] Some operations in RNS require an extended machine word Y thus, the digits associated to the whole number range Rw of equation (5a) may be partitioned into a
group of digits representing an integer number range, Ri, and the remaining digits into an extended range, RE, SO that,
Rw = Ri * RE (5b)
[0116] The integer number range, Ri, provides a means to define the range Rs of the short RNS format word as,

[0117] Returning to equations (1), (2), (3), the total range of the RNS word Y is therefore,
RT = RF * RW (6)
[0118] The real value JC of (1) may be defined as a sum of its whole part and its fractional part,

[0119] where w is the whole portion, and U/RF is the fractional portion of x, so that,

[0120] A positive real x is encoded by multiplying with the fractional range RF, and truncating and rounding to the closest integer. Therefore, encoding the value x
produces a machine integer Y,

https://patents.google.com/patent/WO2019213070A1/en?q=(high+radix)&oq=high+radix

7/38

2/14/26, 1:57 PM

WO2019213070A1 - Systems and methods for matrix multiplication, normalization, and reverse conversion - Google Patents

[0121] The quantity w * RF is an integer since w and RF are integers, so that (9) reduces to,
Y = w*RF +[n ] (10)
[0122] For fixed-point representations, using equations (8), (9) and (10), the value x is encoded to a machine number Yx using,

[0123] The astute reader will notice the mathematics for encoding fixed -point RNS is like fixed- radix number systems.
[0124] Multiplication of Fixed-point Residue Numbers
[0125] Multiplication of fixed-point RNS fractions is briefly introduced by examining the case for positive fractions. Using results of the previous section, the product of
two real numbers v and is represented in machine number notation and computed by,

[0126] The numerator of the last term of equation (12) represents a normalized product result mathematically but is a rational value. The value RF in the denominators is
implied by the fraction point position. Extending the machine number notation using equation (12), we define an intermediate product Yip of two fixed-point RNS formats:

[0127] The intermediate product Yip is obtained by performing an integer RNS multiplication, with each machine number Y treated as an integer. From equation (12),
substituting YIP and truncating the rational numerator to the closest integer, we have:

[0128] The ceiling bracket operator denotes the INT function with round up. Equation (15) formalizes our final fixed -point product result in terms of machine number
representation YR,

[0129] Division by RF and the INT operation of equation (15) is the mathematics of normalization of the intermediate product Yip.
[0130] Summation of fixed point products of the form of equation (14) may be performed separate from normalization, and normalization may be delayed as a final
operation since,

[0131] Equation (16) forms the basis for why it is possible to separate matrix multiplier digits into their own digit matrix multiplier. Because integer multiplication and
addition in RNS is carry free, the summation of Yip may be computed without carry, with the final process of normalization, i.e., division by RF and rounding, performed in
a final step as suggested by the last term of equation (16). Moreover, product summation in the last quantity of equation (16) is more accurate. The second quantity of
equation (16) applies a normalization on every multiplication, but the third quantity applies a normalization only after summation, thus minimizing error due to truncation;
therefore, product summation in RNS is fast and accurate.
[0132] RNS Fixed Point Multiply Procedure
[0133] A basic procedure for fixed point fractional multiply of two positive fixed-point RNS fractions is described. The first step is to multiply the fixed-point values as if
they are integers according to equation (13). The next steps involve performing a division by the fractional range RF according to equation (15). One way to perform this
division is to convert the intermediate product Yip to a mixed-radix number Mip, MIP = MRC F.w (Yjp ) (17)
[0134] where MRCF.-W( ) denotes mixed-radix conversion, with fractional digits converted before whole digits. The last step produces a normalized RNS result YR by first
truncating fractionally associated digits of Mip, and converting the remaining mixed-radix digits back to RNS, denoted as,

[0135] The expression in the parenthesis of equation (18) denotes the value of Mip after truncation of fractionally associated digits (and their radices), and the function
MR2R denotes conversion from mixed-radix back to residue format. Several operations are performed by equation (18), including division by RF, and base extension of
the result during re-conversion to RNS. Rounding of the result is performed in RNS by adding one (unit) if the truncated mixed-radix digits are equal or exceed half the
fractional range RF.
[0136] Fixed Point Multiply Example
An example of RNS fixed point multiplication is provided in Figure 3e. In this example, our new fractional representation will assume the modulus:
TWj = 4, m2 = 3, m3 = 5 , m4 = 7, m5 = 11, m6 = 13, m7 = P, hi = 19
[0137] Our fractional representation will encode the fractional portion using the first four digits, therefore, according to (4),
RF = 4*3*5 *7 = 420 (19)
Using (11) and our example modulus, we encode decimal values 3.2 and 0.25 and compute the product which is 0.8 as shown in the example calculation bounded by box
399 of Figure 3e.

https://patents.google.com/patent/WO2019213070A1/en?q=(high+radix)&oq=high+radix

8/38

2/14/26, 1:57 PM

WO2019213070A1 - Systems and methods for matrix multiplication, normalization, and reverse conversion - Google Patents

[0138] A generalized formulation for fixed point normalization is given by,

[0139] Figure 3 a is included to provide an overview of the new fixed -point residue format, including an example fixed-point residue machine register formats which uses
18-bit residue digits. In Figure 3a, an RNS machine word 350 consists of a plurality of residue digits, where in one preferred embodiment each digit is stored in a digit
register, such as digit register 300. For fixed- point operation of the present invention, a plurality of RNS digits 300, 305, 310 is grouped into the fractional digit group,
where digit register 305 with“continuation dots” represents any number of digits, and represents the number of digits in the fractional range RF as described by equation
(4). Likewise, a total of‘w’ digits 315, 320, 325 is grouped into the integer number range Ri, and a total of digits 330, 335, 340 is grouped into an extended digits range RE,
wherein the product of both ranges constitutes the whole range Rw as described by equations (5a) and (5b).
[0140] In one preferred embodiment, each RNS digit is encoded in a binary word of Q bits wide as illustrated by digit register Di 300 of Figure 3a. Nomenclature typically
uses Dj (or Rj to denote residue digit) to denote the jth digit value, and Mj to denote the modulus of the jth digit. The nomenclature also uses the letter‘/T to denote the
total number of digits in the machine word 350, and the variable Y represents the entire machine word 350, typically interpreted as an unsigned integer.
[0141] The RNS fixed-point type 351, denoted as Z, is defined as a subset of the overall RNS machine word 350, Y, and consists of‘w’ number of digits comprising the
whole number range, and number of digits comprising the fractional number range. The fixed-point type 351 represents the RNS digits which store the entire value of a
single data operand. The RNS digits of the fixed-point type 351 define the fractional precision and define the overall fixed-point value range. It is acceptable terminology
to describe digits assigned to the fractional digit group as “fractionally associated digits”, or that such digits belong to the“fractional range”. The same terminology may
apply to the digits of the whole number range, as well as digits of the extended digit range as shown in Figure 3a. The reader should note that different RNS range
assignments can be defined, and different equations asserted, but these differences do not influence the underlying principles when the fractional range is comprised of
a product of some number of RNS moduli as in equation (4), and that the range definitions provided in equations (3) through (6) and Figures 3a are defined.
[0142] Figure 3b is provided to illustrate some typical fixed-point RNS values using a sample RNS number system as defined by the parameters within dotted lines 380. In
this example, p=8 and Q=l8, so there are eight, eighteen-bit RNS digits total. Also, f=2 indicates there are two digits defined for the fractional range RF, W=2 indicates there
are 2 digits for the integer number range Ri, and e=4 indicates the remaining digits are assigned to the extended number range RE. The modulus for the sample are listed
in row 381 of the modulus column 360 of Figure 3b. The RNS fixed-point digits for several decimal values of column 364 are indicated on the table of Figure 3b, with the
machine word Y column 362 listing the equivalent integer value of the fixed-point RNS values in Row 382 through Row 386. Using equation (11), the fixed-point decimal
value “3.14159265” is encoded in table Row 382, the decimal value of“2.71828182” is encoded in the table Row 383a, and the product of these fixed-point RNS values is
shown in the table Row 384a as the RNS value for the equivalent decimal value“8.53973422” in Figure 3b.
[0143] Many operations involving debugging and data dumps of complex computer systems use hexadecimal instead of decimal for the listing and printing of residue
digits. Therefore, Figure 3c is provided which lists the hexadecimal equivalents for the decimal residue digit values 370, 372, 374 in Figure 3b. In the many data listing of
example apparatus to follow, the data values listed in the table of Figure 3c is used.
[0144] In the prior art, designers of RNS circuits often take advantage of the properties of specific moduli. In some cases, the choice of modulus is so specific that the
circuit and the modulus properties exploited do not work for different sets of moduli. This strategy is faulty in the sense it will likely discourage the use of RNS for general
purpose computation. However, the use of specific moduli does have advantages, and should not be completely ignored in the present invention. But it is the goal of the
present invention that general moduli sets be supported in most cases. This is particularly important if modular computation by RNS is to support variable word size
designs, or very large word size requirements found in applications such as cryptography. By general purpose moduli sets, this disclosure is generally referring to a
significant sub-set of legal moduli combinations of a given range. In other words, the design cannot guarantee that every moduli set be supported, but that a wide
selection of moduli sets is supported.
[0145] Figure 3d provides a table showing three sample moduli sets under consideration which have common properties encompassed by dotted line 390 by means of
example. The common fixed-point residue number system parameters 390 are a good match for FPGA’s which support embedded 18x18 bit“hard” multiplier resources.
The common fixed-point RNS number system parameters 390 of Figure 3d include eight total RNS digits, with two digits being assigned to the fractional range, two digits
assigned to the whole range, and the last four digits assigned to the extended range as shown in Figure 3 a. The digit encoding width for all digits is eighteen bits by
means of example. This encoding width was chosen to make efficient use of FPGA 18x18 bit multipliers for example. The example parameters 390 will be used in the
continuing design example throughout the disclosure, however, it should be clear that many other RNS number system variations are possible, and that designs based on
alternative moduli sets may use other resources, such as 9x9 bit embedded multiplier resources.
[0146] As stated earlier, choice of specific modulus can have an impact on the RNS system. For example, the first moduli set row 396 of Figure 3d has specific properties
since one fractional digit is a power of two, while the other fractional RNS digit is a power of five. Therefore, the RNS fractional range (RF) contains powers of ten so that
fractions supported by these moduli are very decimal-like. The modulus set 396 also allows easy conversion to decimal notation because of this fact. Column 392
entitled“Equiv Binary Bits” lists the effective precision of the moduli set in terms of binary bits. For example, the moduli set of row 396 supports 33.25 bits for the
fractional precision, and about 35.99 bits of precision for the whole precision. The closer a given modulus is to the maximum binary value of 2Q, the more efficient the
modulus. For example, the moduli set of Row 397 is very efficient in terms of precision (in bits), since it supports nearly the full binary fixed-point precision of 36.36 bits
(denoted“35.99/35.99” in Figure 3d).
[0147] The last moduli set of Row 398 is a pure power based RNS fixed-point number system. The advantage of the moduli set of Row 398 is its ease of divisibility using
RNS integer techniques. The disadvantage is its lack of encoding efficiency as shown in Column 393 entitled“Overall Efficiency” and column 394 entitled“Format
efficiency”. Overall efficiency 393 provides a measure of encoding efficiency for the entire extended RNS word, whereas the format efficiency 394 provides a measure of
encoding efficiency for the actual fixed -point RNS format, which is a measure using the short format range Ri, or the first four moduli (Mi thru M4) in the example
configuration 390. For any given set of‘/F number of moduli Mi, the encoding efficiency, E, is given by:

[0148] In the example designs and example moduli sets disclosed herein, the moduli are listed in order of ascending value, left to right. In terms of hardware design, one
advantage of this arrangement is it allows simpler implementation of mixed radix conversion, since a smaller range digit is always converted before larger RNS digit
ranges, which simplifies subtractor circuitry as shown later. However, this is only a design simplification, so it is apparent other non-ordered moduli sets can be supported
by the present invention. (The order of RNS digits is not particularly important, but the order of mixed-radix conversion is important.)

https://patents.google.com/patent/WO2019213070A1/en?q=(high+radix)&oq=high+radix

9/38

2/14/26, 1:57 PM

WO2019213070A1 - Systems and methods for matrix multiplication, normalization, and reverse conversion - Google Patents

[0149] In summary of the overview and background, the matrix multiplier block diagram of Figure 1 is a baseline design intended to provide context for the invention at
hand. Moreover, the block diagram of Figure 1 represents basic functions of a matrix multiplier but does not show details of the underlying arithmetic. The systolic array
of Figure 2a is known in the prior art and is included to highlight the systolic architecture in the context of a high-speed matrix multiplier of the present invention.
Equations (1) through (19) are relatively new disclosures from U.S. Patent No. 9,081,608, they illustrate basic mathematics for positive fixed-point RNS. Negative number
encoding will be discussed later when pertinent to the invention details. Figures 3a, 3b, and 3c provide sample fixed-point RNS register formats, sample RNS value
encodings and example RNS number system moduli sets. Next, the disclosure will disclose the present invention, and highlight key differences of the present invention
versus the known prior art.
[0150] Description of Typical Apparatus of Invention [0151] Figure 4 shows a block diagram for a matrix multiplier which is similar in application to the matrix multiplier of
Figure 1, i.e., a matrix multiplier as part of a neural-network PCIe card accelerator. However, in Figure 4, some details specific to the present invention are disclosed. It
should be noted that many common functional details of Figure 4 are for example only, as Figure 4 is provided to clearly describe pertinent aspects of the invention at
hand. For example, another embodiment will include a high-speed fiber network connection, such as lO-Gigabit Ethernet, implemented independently or in conjunction
with a high speed PCIe interface 440 for transferring information into and out of the accelerator card 406. Variations of the block diagram of Figure 4 are well known to
those skilled in the art of accelerator card design and are not discussed in detail so as to focus on the inventions pertinent of this disclosure.
[0152] In Figure 4, a systolic array of multiplier-accumulators 400a performs high-speed matrix multiplication of fixed -point RNS values received from matrix A data port
415a and matrix B data port 405a. In one systolic embodiment, the product of two matrices is computed with their row and column data staggered in a diagonal wave
front pattern as illustrated in Figure 2a by dotted lines 260, 261. Product matrices exit the systolic array 400a from each row path, such as the first matrix-row path 453,
as non-normalized dot products. In one embodiment, the non-normalized dot-products are passed through a pipelined MOD function 454a, which is required to reduce
the congruent value of RNS digits to a fully modular value before being passed to a word normalization pipeline 455a.
[0153] It should be noted the MOD function 455a is not required if the output of the multiplier- accumulators of the systolic array 400a produce RNS digits in legal range,
i.e., a“fully” reduced modular value less than the modulus. However, one problem with modular computation in RNS is that“ideal” modular multipliers and accumulators
seldom exist in practice, so that prior art modular circuits are typically comprised of binary arithmetic, comparator and other logic circuits. A problem results since
modular circuits comprised of binary arithmetic and logic circuits tend to operate more slowly than binary arithmetic circuits alone, and of the same precision (bit width).
In contrast, methods disclosed herein for implementing high-speed modular arithmetic rely on novel techniques which do not, by design, fully reduce RNS digits until
absolutely required. These novel techniques result in faster operation and require less circuitry than circuits producing fully modular results. More about these novel
techniques are introduced later.
[0154] For high speed designs, every row output of the systolic array 400a will support a separate MOD function and normalize pipeline in hardware. However, it is
possible that less number, or more number, of MOD and normalize pipeline units be implemented depending on the needs of the application. Also, different systolic
architectures can be supported by the present invention, however, the present invention primarily focuses on those systolic architectures which provide for a stationary
accumulation result Y 271 of Figure 2b.
[0155] In Figure 4, a fully normalized matrix product is transmitted from the normalization pipelines, such as normalize pipeline 455a, to the matrix product store 460. In
one embodiment for neural network acceleration, the matrix product store 460 includes data routing circuitry to stage and route matrix product data to a non-linear RELU
function pipeline unit 465 before the result matrix is passed back to the matrix data routing circuitry 410 for re -processing. For web search applications, such as a
hardware accelerator card for accelerating the Page-rank algorithm, the matrix product store 460 routes the product result matrix to a vector accumulator unit 465 to
update the next page rank vector for the next iteration of the page rank formula. In this application, a fully or partially accumulated page rank vector is accumulated by
vector accumulator 465 and can be transmitted back to memory 445 for storage until needed.
[0156] Another novel and unique feature of the RNS matrix multiplier of the present invention is the forward converter pipeline unit 44 la and reverse converter pipeline
unit 442a. Figure 4 shows the conversion pipelines 44 la, 442a exist in the path between the PCIe bus interface 440 and the high-speed memory interface 435 by means
of example. In this example design, binary formatted data, such as double precision floating-point values, are transmitted from the PCIe interface 440 to the pipelined
binary to RNS converter 44 la. The binary to RNS“forward” converter 44 la translates the value of a floating-point number into an RNS value comprising four digits if using
the example RNS encoding of Figure 3b. The high-speed memory interface 435 and other control circuits not shown transfer converted RNS values from forward
converter 441a to DDR memory 445 using data path 444 as shown in Figure 4.
[0157] In general, binary values are converted to their minimum size RNS digit format in one preferred embodiment of the present invention. This is preferred since a high
efficiency design demands that storage requirements of RNS values be minimized, i.e., so they closely approximate the storage costs of the binary source format, which
is double floating-point in one example. In other words, it is an objective of the current invention to store RNS matrix data in its“non- extended” short format to save
storage and power whenever possible and feasible.
[0158] In the design of Figure 4, non-extended RNS data is stored in DDR4 memory 445, while fully extended data is stored“internally” in data caches 425, 430. When RNS
data is accessed from DDR4 memory 445, it is passed through base extension unit 443a before being routed to data cache memory 430. Assuming the RNS fixed-point
encoding of Figure 3b, the RNS data accessed from DDR4 memory 445 is four digits wide; after the RNS value exits the base extension pipeline 443a it will be eight digits
wide. The extended digits are known to be“redundant”, and do not affect the value of the fixed-point operand. When data is read from DDR memory 445 for matrix
multiplication, the extended digits of Figure 3b are added by the base extension unit 443 a when the basic data type format 351 is accessed from DDR4 memory 445,
resulting is a fully extended RNS word format Y 350. If fixed point RNS data types 351 of Figure 3a are read from DDR memory 445 for transferring data to a host system,
the fixed-point RNS data type may be transferred directly using data path 444 without being processed by base extension unit 443a. In this case, the short fixed-point RNS
data type 351 of Figure 3a is passed through reverse converter 442a to convert the RNS value to a corresponding binary or floating-point format before being transmitted
to the host system via the PCIe bus interface 440.
[0159] In the block diagram of Figure 4, forward and reverse converter pipelines 44la, 442a are simply shown as a block symbol. However, various forward and reverse
converter apparatus may be present in a typical design. For example, forward conversion pipeline 44 la and reverse conversion pipeline 442a may accommodate more
than one type of binary format, such as binary fixed-point format, or floating-point format. Floating point formats may include options for single precision or double
precision conversion, for example. Forward converter 441a and reverse converter 442a may support more than one type of RNS fixed point RNS format as well. For
example, more than one RNS fixed -point configuration may be supported, so that more than one selection of precision, or more than one choice of overall range is
supported. More details as to preferred implementations of these important pipelines is provided later.
There are several advantages of the converter and base extension arrangement of Figure 4. One advantage is it reduces memory storage and memory access power by
reducing the size of the stored RNS number format. Another advantage is the reduction in the size of the conversion pipelines 441a, 442a and base extension pipeline(s)
443a, since no pipeline handles more than four digits of mixed -radix conversion in our example RNS encoding of Figure 3b. The tradeoff incurred is that internal memory
caches 425, 430 and data routing circuits 435, 410, 420 supports fully extended RNS values, i.e., an eight-digit wide RNS format 350 in the example encoding of Figure
3b. The extended RNS word format 350 is generally required during multiplication, since the range of the non-normalized product may exceed the range of the RNS fixedpoint type format 351 of Figure 3 a.

https://patents.google.com/patent/WO2019213070A1/en?q=(high+radix)&oq=high+radix

10/38

2/14/26, 1:57 PM

WO2019213070A1 - Systems and methods for matrix multiplication, normalization, and reverse conversion - Google Patents

[0160] In some embodiments not shown, complex number formats are supported by pairing two RNS fixed-point types 351, i.e., a real part and a complex part. This
variation affects conversion pipelines, normalization pipelines, and multiplier- accumulators among other functions and apparatus. The present disclosure focuses on
signed fixed-point arithmetic to demonstrate unique and novel inventions herein; it is obvious to those skilled in the art of modular circuit design that a matrix multiplier
operating on complex RNS fixed-point formats is a direct extension of the techniques and methods taught herein. It is understood that an RNS matrix multiplier of the
present invention may be extended to support complex RNS fixed-point values as opposed to scalar fixed- point numbers.
[0161] Figure 5 provides more detail to the arithmetic circuitry of the systolic matrix multiplier 400a of Figure 4. For means of clarity, Figure 5 illustrates a 2x2 matrix
multiplier, with “continuation dots” describing any number of additional arithmetic elements, such as arithmetic element 401. Figure 5 is provided to illustrate a basic data
flow within the matrix multiplier 400a, and to suggest basic register-transfer level (RTL) design. However, it is noted that many other design techniques and technologies
can be used to implement the systolic multiplier- accumulator array 400a not discussed herein. For example, semi-systolic architecture technology can replace RTL
technology in the diagram of Figure 5.
[0162] In Figure 5, matrix data A port 4l5b and matrix data B port 405b is re-drawn from Figure 4 and now details a plurality of symbols indicating register FIFOs, such as
register FIFO 501. The register FIFO’s of the matrix A data port 4l5b deliver an RNS value to the row ports 510, 506 of a plurality of processing elements 401, 403. The
register FIFO’s of the matrix B data port 405b deliver an RNS value to the column ports 500, 514 of a plurality of processing elements 401, 402. The output of row port
register 510 of processing element 401 drives the row input 515 of the next processing element 402 of the row, assuming a 2x2 element matrix of Figure 5. The output of
column port register 500 of processing element 401 drives the input port 505 of the next column processing element 403 in succession. Therefore, matrix A and matrix B
input data is transferred through the systolic array 400a, from processing element to processing element in both the row and column directions simultaneously.
[0163] In Figure 5, control signals (not shown) control the clearing and loading of each accumulator of each processing element at the proper time, such as accumulator
register 520 of processing element 401. In many designs, these control signals are implemented as systolic structures to simplify the routing of control signals to all
processing elements. The output of processing elements 401, 402, 403, 404, is a product summation of some number of RNS operands delivered to their respective input
ports after a clear or load operation, such as input ports 500, 510 of processing element 401. The product summation represents a non-normalized product summation in
one preferred embodiment of the present invention. The non-normalized product summation contained in accumulator register 520 is transferred to a holding register
525 before being transferred to read-out register 530 in one embodiment. Therefore, a non-normalized matrix is output from some number of rows of the systolic array
400a, such as illustrated by row path 546. The row data exiting the row read-out register 545 of the matrix multiplier 400a enters a pipelined MOD function 454a in one or
more embodiments, so that RNS digits are fully reduced to modular format before being transferred to a product normalization pipeline 455a of Figure 4. [0164] It should
be clear that each row of the matrix multiplier 400a operates in a similar manner to that described above. It should be clear that the matrix multiplier 400a can be
designed to provide a column result, or both a column and row result. Moreover, many timing details are not disclosed which result in special features being supported by
the matrix multiplier 400a. For example, the matrix multiplier 400a of Figure 5 can support multiplication of matrices of different sizes, or matrices with dimensions that
are smaller than the dimensions of the hardware matrix 400a, or matrices with dimensions larger than the dimensions of the hardware matrix 400a. More about these
variations are described later, but it is known to those skilled in the art of systolic architecture design that many variations exist in terms of supporting arithmetic and
matrix operations.
[0165] Important to the invention at hand is the interpretation of the flow of RNS data through the matrix multiplier 400a, and through the arithmetic circuits of each
processing element 401, 402, 403, 404 of Figure 5. Equation (16) suggests a choice in the design of the matrix multiplier 400a and the design of the processing elements
401, 402, 403, 404 of Figure 5. For example, the flow of RNS data from the FIFO 511 to the input row register 510 of processing element 401 may be interpreted as an
RNS word, such as RNS word 350 of Figure 3a, or may be interpreted as an RNS digit, such as RNS digit Di 300 of Figure 3a. The interpretation of the data flow as a“word”
is normal and intuitive. For example, when using binary arithmetic to implement the matrix multiplier 400a, each input data port 500, 510 is ideally the width of the binary
operand, such as 32 bits for single precision floating point values. For RNS arithmetic, a preferred embodiment exists when the data flow into each data port 500, 510 is
the width of a single RNS digit, or Q bits. In this preferred embodiment, the matrix multiplier 400a is referred to as a“digit matrix multiplier”, and the entire structure 400a
is dedicated to modular circuitry which computes“digit results” of the modulus M, of the jth RNS digit.
[0166] Figure 6 is provided to describe details of an RNS matrix multiplier structure wherein arithmetic is partitioned into separate and distinct RNS digit matrix
multipliers. In Figure 6, the matrix multiplier 400a of Figure 4 is replaced with matrix multiplier 400c which is comprised of a plurality of digit matrix multipliers 600, 605,
610, 615, wherein each digit matrix multiplier is dedicated to a distinct modulus M, of the fixed point RNS format of Figure 3a. The“continuation dots” 620 indicate that
any number of digit matrix multipliers, such as digit matrix multiplier 600, are supported other than those depicted in Figure 6. Moreover, each digit matrix multiplier 600,
605, 610, 615 of Figure 6 is similar or identical to matrix multiplier 400b described in Figure 5 wherein operand data input ports, such as input ports 500, 510, are Q bits
wide, and wherein processing elements, such as processing element 401, support a single RNS digit calculation. [0167] Matrix A data port 4l5a of Figure 4 is replaced
with matrix A data port 4l5c in Figure 6; likewise, matrix B data port 405a of Figure 4 is replaced with matrix B data port 405c in Figure 6. The matrix data port shows
details that are relevant to an RNS implementation. For example, the RNS word FIFO 630 is depicted as a plurality of digit FIFOs, such as digit FIFO 51 lc in Figure 6. Each
digit FIFO is partitioned according to a distinct modulus Mj shown in Figure 3a and is connected to its corresponding digit matrix multiplier by means of matching digit
modulus Mj, and by its row and column location.
[0168] RNS (modular) dot products exit the matrix multiplier 400c and enter a plurality of pipelined MOD functions such as pipelined MOD function 454c in one
embodiment. The MOD function 454c depicted in Figure 6 shows additional detail, since this function is applied on a digit by digit basis and for a specific modulus Mj.
Thus, the MOD function 454c of Figure 6 is comprised of a plurality of pipelined MOD functions, each associated to a matching digit modulus Mj. Dot products exit the
MOD function 454c and enter a normalize unit pipeline 455c. Note the normalize unit pipeline 455c is depicted as a single pipeline receiving all RNS digits of a fixed- point
RNS word 350 of Figure 3a. The product normalize pipeline 455c applies a normalization procedure mathematically like that described in equation (17) and (18).
Hardware details which support mathematics of positive and negative fixed-point RNS numbers are important to the present invention and are detailed later.
[0169] The next sections disclose variations to the design of the multiplier- accumulator processing elements of the matrix multiplier 400c, such as the“digit” multiplieraccumulator processing element 40 lc of Figure 6.
[0170] Matrix Multiplier Detailed Overview
[0171] The RNS matrix multiplier 400c of Figure 6 can be drawn as a flat 2-D schematic as in Figure 7. The RNS matrix multiplier of Figure 7 shows essential elements of
the matrix multiplier of the present invention, such as eight RNS digit matrix multiplier units, such as RNS digit matrix multiplier 700. Figure 7 shows a total of eight RNS
digit matrix multipliers to support the example RNS moduli set of Figure 3c. For example, digit matrix multiplier 710 is associated to the digit modulus Mi 363 of Figure 3c
and each additional digit matrix multiplier in counter-clockwise fashion is associated to a distinct digit modulus Mi, such that all modulus Mi 363 through Ms 369 of
Figure 3c is assigned a digit matrix multiplier in Figure 7. For example, the digit matrix multiplier 700 is assigned the digit modulus M3 by means of example.
[0172] The eight RNS digit matrix multipliers of Figure 7 allow high-precision fixed-point matrix multiplication to be performed without carry from RNS digit to RNS digit;
therefore, there is a benefit of partitioning (i.e., isolating) each RNS digit of the machine word into a distinct matrix multiplier 700 as shown in Figure 7. One benefit is that

https://patents.google.com/patent/WO2019213070A1/en?q=(high+radix)&oq=high+radix

11/38

2/14/26, 1:57 PM

WO2019213070A1 - Systems and methods for matrix multiplication, normalization, and reverse conversion - Google Patents

RNS digit encoding is generally a narrow binary bit-width, which means the matrix multiplier 700 is not burdened by wide-precision binary arithmetic at each MAC element
within the matrix multiplier 400. This results in smaller size, greater speed and higher efficiency for a single matrix multiplier.
[0173] The architecture of Figure 7 is quite amazing considering the apparatus supports a matrix multiplication without carry from digit to digit; only after the matrix
multiplication is performed in digit isolation does the non-normalized dot-product output from each digit matrix multiplier, such as matrix multiplier output 775, get regrouped to form an RNS word at the front end of each matrix row product normalizer, such as product normalizer pipeline 760. The RNS word output of the product
normalization unit 760 is fully normalized and may re-enter matrix calculation and is therefore de-grouped into eight single digit transmission lanes, such as digit operand
bus 780, and routed back to each of the eight isolated digit matrix multipliers, such as digit matrix multiplier 710.
[0174] Unlike other RNS computation apparatus of the past, the RNS TPU of the present invention supports iterative, multiplicative calculation. In other words, normalized
data exiting the normalization pipeline 760 of Figure 7 can be de-grouped into separate digit buses 780 and re routed to the operand cache and staging circuit 712 and
714 of the associated digit matrix multiplier 710. The normalized RNS data 780 looks like any other fixed -point data 712, 714 input to the matrix multiplier array 716 and
can therefore be“re-processed” within the context of iterative calculations. For binary arithmetic, the same holds true but is often taken for granted. The RNS arithmetic of
the present invention is referred to as a“true arithmetic” in terms of its precision and ability to support multiplicative iteration. Moreover, the RNS TPU of the present
invention is highly accurate since it does not truncate each (extended) product before and during summation, and only truncates (normalizes) after product summation is
complete.
[0175] Also critical to the operation of the RNS TPU of Figure 7 is the high-speed forward conversion pipeline unit 730 and the high-speed reverse conversion pipeline unit
740. High-speed forward conversion 730 of matrix data is important to maintain matrix data flow to the matrix multiplier since if high-speed data cannot be delivered to
the matrix multiplier of Figure 7, the benefits of speed and efficiency of the RNS TPU technology of the present invention cannot be fully realized. Also, if data results of
the matrix multiplier cannot be delivered and converted from RNS to binary format at high-speed by reverse conversion pipeline 740, the benefits of the RNS TPU
technology cannot be used in real-time. It is shown in the disclosure that high-speed forward and reverse conversion technology, which are integrated alongside the
normalization pipelines of Figure 7 is crucial to the matrix multiplier of the present invention. [0176] In Figure 7, high-speed data interfaces and transmission technologies
are employed to deliver matrix data 720 to the forward converter(s) 730 generally at the highest rates possible and are illustrated in a general sense by data receiver
interface 725. Also, high-speed interfaces and data transmission technologies 745 are employed to transmit matrix product results 750 and other processed data from
the matrix multiplier after such data is converted to binary format using high speed reverse conversion pipeline(s) 740. In Figure 7, control elements supporting the
synchronization of data values is not shown. Such control circuitry is complex but generally understood by experts in the context of advanced processor design.
Moreover, the matrix multiplier of Figure 7 does not show the MOD function units of Figure 4, such as MOD function 454a. For purposes of illustrating the novel methods
that follow, the MOD function of Figure 4 is assumed integrated into the front-end of the word normalization units, such as word normalization unit 760 of Figure 7.
[0177] In Figure 8a, one embodiment of the present invention is characterized. As depicted in Figure 8a, multiplier-accumulator element 40ld illustrates bus widths of
major arithmetic data paths. The input port registers 500, 510 accept an RNS digit encoded into a Q-bit binary word as depicted by the width of data paths 825, 830. A
binary multiplier 800 is provided to perform multiplication of the input data stored in registers 500, 510 on each clock cycle during a multiply accumulate operation. The
binary multiplier 800 outputs a product result with twice the width of the operands, which is 2*Q as shown. A binary adder 805 is paired with a register 810 to form an
accumulator function. The register 810 stores the accumulated result and is often referred to as an “accumulator”.
[0178] As shown in Figure 8a, the adder 805 accepts a 2Q bit wide product from multiplier 800 at one input, and an accumulator value of width 2Q+s using feedback data
path 806 at the other input. The accumulator 810 is 2Q+s bits wide because during accumulation of“digit” dot products, the product summation may grow quickly and
exceed the binary encoding width of 2Q bits. When the product summation is completely processed by multiplier 800, adder 805, and accumulator 810, the product
summation is transferred to holding register 815 which therefore must be 2Q+s bits wide; likewise, all digit dot product read-out registers will also be 2Q+s bits wide. The
value of s depends on the number of products summed, however, a conservative value is equal to the log (base 2) of the number of product terms summed.
[0179] One advantage of digit multiplier- accumulator element 40ld is that it can be synthesized using standard binary arithmetic. A common issue confronting modular
circuit designers is the lack of so-called“dedicated” modular circuit libraries. Often, modular circuit designers construct modular circuits from binary component libraries
because general purpose, customizable modular circuits are not available as component primitives in most ASIC and FPGA component libraries. Therefore,
embodiments of the present invention teach advanced techniques for building high speed modular circuits using standard binary circuits and libraries. However, modular
circuits naively constructed from standard binary circuits often lead to difficult problems and are slower in operation than their binary equivalents.
[0180] For purposes of comparison and analysis, this disclosure defines an“ideal” modular circuit as a circuit which takes the same or less resources as an equivalent
binary circuit with equivalent input width and operates at the same speed or faster than an equivalent width binary circuit. For example, an“ideal” modular accumulator of
bit width Q will operate as fast as a Q bit binary accumulator and will require the same or less circuitry to implement. It should not be a surprise that“ideal” modular
circuits seldom exist in practice, especially if such ideal circuits support a wide choice of modulus. It is therefore a goal of this disclosure to provide solutions which
approximate ideal modular circuitry to the best extent possible so that practical modular computation systems can be constructed using readily available binary
component libraries. While this approach is important, the use of modular circuits constructed or synthesized using standard binary circuits and technology is not a
limitation of the present invention.
[0181] The modular digit accumulator solution of Figure 8a is novel because it uses a standard binary accumulator circuit and provides for modular digit accumulation by
delaying and then applying the MOD function as a final operation, as depicted by the MOD function 454c of Figure 6. Thus, modular digit accumulation is achieved with a
binary accumulator value that grows as products are summed; thus, the MOD function applied after summation must process a“congruent” digit result of 2Q+s bits wide.
Such a MOD function will be disclosed later, but its circuitry is relatively large because the input operand width is large; that is, the 2Q+s bit wide RNS digit is reduced to a
fully modular digit value of Q bits wide. This is one disadvantage of the solution of Figure 8a.
[0182] To simplify disclosure and analysis, Figure 8b shows a block diagram and overall processing relationship for the basic data flow of the multiplier- accumulator
function of processing element 40ld in Figure 8a. The block diagram of Figure 8b shows a binary multiplier 800 which receives two Q bit-wide operands. The output of
multiplier 800 is 2Q bits wide and is passed to the binary accumulator function block 835. The binary accumulator block 835 of Figure 8b consists of the binary adder
805, the accumulator 810 and the feedback path 806 of Figure 8a. The output of the binary accumulator 835 of Figure 8b is up to 2Q+log2(K) bits wide, where K equals
the maximum number of products summed.
[0183] Another disadvantage of multiplier-accumulator of Figures 8a and 8b is the wide bit-width of the final product accumulation. For example, holding register 815 and
read-out shift register 820 are 2Q+s bits wide. This more than doubles the number of wires which must exist in each row of the matrix multiplier 400b of Figure 5.
Moreover, while the binary accumulator 835 is fast, since it is ideal, its width is more than twice as wide as an ideal modular accumulator of Q bits wide.
[0184] To minimize the data width of the accumulator 810 of Figure 8a, the modular accumulator 40le of Figure 9a is disclosed. In Figure 9a, a“modular” multiplier 900
multiplies two operands, each Q bits wide. The product result output by modular multiplier 900 is also Q bits wide; this is a“true” modular multiplier, so multiplier 900 is
designated with a modular multiply symbol‘|x|’ denoting multiplication and parallel brackets indicating a MOD function. The modular accumulator of Figure 9a includes a
modular adder 905, designated with a plus sign with parallel brackets‘|+|’; the modular adder accepts two operands, each Q bits wide, and produces a fully modular result,

https://patents.google.com/patent/WO2019213070A1/en?q=(high+radix)&oq=high+radix

12/38

2/14/26, 1:57 PM

WO2019213070A1 - Systems and methods for matrix multiplication, normalization, and reverse conversion - Google Patents

of Q bits wide, and such that the output is bounded so 0 < Dj < Mj. This is referred to in this disclosure as a“fully reduced” modular result. The fully reduced modular result
of adder 905 is summed to the accumulator register 910 of Figure 9a; the fully modular summation 905 is then stored into the accumulator 910 in each summation cycle,
or accumulation cycle, as a fully reduced summation never exceeding Q bits wide.
[0185] As discussed previously,“ideal” modular circuits seldom exist in practice. However, for highly pipelined circuits, such as the matrix multiplier of the present
invention, it is possible to overcome delays of modular circuits at the expense of increased circuit area, and at the expense of increased pipeline depth. For this reason, a
primary objective of the present invention is to reduce the amount of circuitry required and reduce the number of pipeline stages required for modular multiplication and
modular accumulation.
[0186] In Figure 9b, a block diagram for the modular multiplier-accumulator 40 le of Figure 9a is provided. In the block diagram of Figure 9b, modular circuits are
constructed from binary arithmetic, look-up table (LUT) memory and other logical circuits. In Figure 9b, product summations are computed using a binary multiplier 925a
which receives two Q-bit operands as input. The output of the binary multiplier is 2*Q bits wide, so must be reduced by modular reduction circuitry 930a. The modular
reduction unit 930a may be specific to a modulus 940, whose value is denoted by the letter M. The function provided by modular reduction 930a is to reduce the
congruent input value modulo M. In this case, the modular reduction circuit 930a reduces the 2Q bit-wide operand to a Q +n bit result. As will be shown in one preferred
embodiment, modular reduction circuit 930a utilizes“additive reduction”, with values for typically ranging from 1 to log2(Q) bits. Therefore, since

is not equal to zero, the modular reduction unit 930a may not reduce the input value to a“fully reduced” modular result, ex., a modular result bounded by Q bits, and further
bounded such that digit value D is less than its modulus M (0 < Dj < Mj). In other words, the output of modular reduction unit 930a is a binary value greater than or equal
to Q bits wide and congruent to digit value D modulo M (D > M), or it is fully modular with respect to the modulus M (D < M). (Assuming efficient modulus M of range
2(Q_1) < M < 2q).
[0187] A modular accumulator 935a comprises the last processing block of the modular- accumulator of Figure 9b. Internal components of the modular accumulator
935a of the prior art comprises binary arithmetic circuits and binary logic circuits, such as binary adders, binary subtractor, LUTs, registers and bus selector as will be
discussed later. The output of the modular accumulator 935a is a“fully” reduced modular digit result of Q bits. More specifically, the output of the modular accumulator
935a is a legal RNS digit such that Di < Mi using nomenclature of Figure 3a. Therefore, when using the multiplier- accumulator element 40le of Figure 9a, the output is
already fully modular, so there is no need to support MOD function 454a of Figure 4.
[0188] In a novel and unique apparatus of the present invention to be introduced later, a modular accumulator of the form of Figure 9b is constructed such that output Y
of the modular accumulator 935a is not fully reduced, but is congruent to the fully reduced digit value Y modulo m. Therefore, the output Y of the modular accumulator
935a of Figure 9b is not encoded using Q bits but is encoded as Q+n bits in a similar fashion as the output bit- width of the modular reduction unit 930a.
[0189] Figure lOa and lOb illustrate detail of the modular reduction block 930a and modular accumulator 935a of Figure 9b by means of example. There are many
techniques and apparatus for modular reduction in the prior art, however, for a highly pipelined RTL design of the present invention, a significant amount of modular
reduction is implemented using binary adders 1025, 1030, 1050, 1065, 1075, and look-up tables LUT1 1008, LUT2 1010, LUT3 1015, LUT4 1020, LUT5 1066, as shown in
Figure lOa in one preferred embodiment. Because the design is highly pipelined, there are registers inserted into the circuit, such as register“reg2” 1035; furthermore, LUT
blocks of Figure lOa, such as LUT4 block 1020, may include a registered output or input and therefore LUTs of Figure lOa act much like a pipeline register with a LUT
function as an output. In the many circuit descriptions which follow, the LUT and register form a common pipeline STAGE to reference operation for disclosure, however,
the invention is not limited to such synchronization in practice.
[0190] Figure lOa shows the first stage 930b of a modular reduction circuit 930a of Figure 9b. The modular reduction stage 930b of Figure lOa is shown following and
connected to the output of an 18x18 bit binary multiplier 925b by means of example. Only the output 925b of the binary multiplier is shown for clarity.
[0191] Modular reduction using the circuit 1000 of Figure lOa is referred to as “additive reduction”. This term is somewhat contradictory, but adequately describes a
plurality of binary adders used to reduce a 36-bit wide value 925b to a l9-bit wide value 1069 congruent to a value modulo M 1072, and output from the last adder stage
1068. In the example of Figure lOa, the RNS digit encoding width is Q=l 8-bits, and a typical digit value D of a modulus value M is bounded such that it occupies no more
than the full 18-bits encoding width expressed by the relation 1044. By the word“reduction”, it is meant the digit value D (congruent modulo M) encoded in a 36-bit wide
input register 925b is converted to a l9-bit wide digit value D 1069 which is congruent modulo M, where D is the same value as a fully reduced input value 925b.
[0192] The operation of additive reduction circuit 1000 of Figure lOa is based on the mathematical relation provided in equation (22),
\a + b \m = \(\a\m + \b \m) \m (22)
[0193] which states that the MOD of the sum of the quantity‘a’ plus the quantity V is equal to the MOD of the sum of the MOD of‘ a’ and the MOD of b provided all MOD
functions are with respect to the same modulus‘m’. More specifically, the method of additive reduction computes the term within the parenthesis on the right side of
equation (22). It does not compute the “outermost” final MOD function required to bring the right side equal to the left side of equation (22); this final MOD function
cannot be computed using only additive reduction in the general case.
[0194] In Figure lOa, the modular reduction stage 930b is connected to receive the 36-bit product result of binary multiplier 925b of the modular multiplier 930b of Figure
9b. Since the operands (not shown) of the binary multiplier 925b are in general Q bits wide, the output is as wide as 2*Q bits wide, and in the example configuration of
Figure lOa is therefore 36 bits wide. In the example of Figure lOa, the upper 20 bits of product register 925b is partitioned into four groups of 5 bits each 1001, 1002,
1003, 1004, 1005. Each 5-bit group represents a portion of the stored product 925b, since each 5-bit group of register 925b represents a weighted truncated binary value.
The product stored in input register 925b is reduced using a series of LUT’s 1006, 1008, 1010, 1020; each LUT is appropriately programmed to convert the weighted value
of a truncated portion of one or more bits of the product register 925b to a Q-bit value modulo M.
[0195] For example, LUT1 1008 receives a portion of the product 925a via bit position 16 through 20, denoted as bits“20: 16”, i.e., a common notation used in hardware
languages like Verilog. LUT1 1008 is configured such that data output from LUT1 is an 18-bit value equal to the weighted value of bits“20: 16” modulo M. Likewise, the
product 925b bits“25:21” are converted to an 18- bit value modulo M using LUT2 1010. The output of LUT1 1008 and LUT2 1010 is summed using binary adder 1025
which produces a l9-bit result sum which is stored in pipeline register reg3 1040. Likewise, the value of bits“30:26” 1004 is converted by LUT3 1015 modulo M, and the
value of bits“35:31” is converted by LUT4 1020 modulo M, and both converted values are summed using binary adder 1030 producing a l9-bit sum result stored in
pipeline register reg4 1045. The sum of binary adders 1025, 1030 produces a 20-bit result stored in register reg6 1060.
[0196] In the example of Figure lOa, it is not necessary to apply a LUT translation to the value represented by the least significant Q-l bits, denoted as bits“15:0” 1001 in
the example of Figure lOa. The reason is the value of the least significant Q-l bits of product 925b is always less than modulus M provided the modulus M is bounded by
Inequality Relation 23a, as provided in the example configuration 1044 for optimal moduli,
2

(<

3-1) < M £ 2Q (23a)

https://patents.google.com/patent/WO2019213070A1/en?q=(high+radix)&oq=high+radix

13/38

2/14/26, 1:57 PM

WO2019213070A1 - Systems and methods for matrix multiplication, normalization, and reverse conversion - Google Patents

[0197] The least significant bits“15:0” of product 925b is transferred to pipeline register regl 1006 to remain synchronized in time with the LUT stages 1008, 1010, 1015,
1020 in one embodiment. Likewise, the least significant value of the product stored in regl 1006 is transferred to pipeline register reg2 1035, and then to pipeline register
reg5 1055 to stay in synchronization with pipeline registers reg3 1040, reg4 1045 and pipeline register reg7 1064 respectively. Binary adder 1062 sums the result of
pipeline register reg6 1060 and reg5 1055 to produce a 21 -bit congruent result modulo M. The output of the binary adder 1062 may be as wide as 21 bits, and remains
congruent to a fully reduced digit value D modulo M.
[0198] In one preferred embodiment of the present invention, the output of binary adder 1068 is used as the final output to modular reduction stage 930b. In this case, the
output of modular reduction block 1000 serves to implement reduction block 930a of Figure 9b, which is labeled as “Q+n” bits wide so that‘n’ equals 2l-Q=3 bits wide. In
this case, the output 1069 of modular reduction unit 1000 is referred to as“partially reduced” with respect to modulus M.
[0199] In the embodiment shown in Figure lOa, the output of binary adder 1069 is further reduced using an additive technique described above followed by a subtractive
circuit 1070 to guarantee the digit result 1079 is fully modular with respect to modulus M 1072. The 2l-bit output of adder 1062 is partitioned so that its most significant
output bits 1063 are converted modulo M using LUT5 1066, which is summed to the least significant seventeen bits of adder 1062 stored in pipeline register reg7 1064
using binary adder 1068. In this embodiment, the goal is to reduce the product result 1000 to the smallest value possible, which in this case is encoded within 19 bits as
shown by the output 1069 of adder 1068.
[0200] The output of binary adder 1068 cannot be reduced further (modulo M) using only additive reduction alone. The reason is the sum of two values modulo M cannot
be guaranteed to be less than the modulus M. For example, the output of adder 1062 is partitioned into two values, each value modulo M, but the sum of these two
values, i.e., the output of adder 1068, cannot be guaranteed to be less than M. By using a much larger LUT to replace LUT5 so as to convert nearly all, or significantly all
bits of the adder output 1062, it is possible to have the output bound by Q- bits wide or even by the modulus M, but this is very inefficient because the LUT5 input address
would be 2l-bits wide in this case. In this example, using a LUT with a larger input address 1063 of >(2l-(Q-l)=4) bits does not use memory efficiently because it only
reduces the output by a single bit for every doubling of the LUT memory, and this still does not guarantee a fully modular result such that D < M unless the LUT input 1063
supports all 21 -bits of the adder 1062 output.
[0201] In Figure lOa, to process the congruent result 1069 into a fully modular result 1079, a subtractive reduction circuit 1070 is used. The subtractive reduction circuit
1070 as shown will reduce a value that is less than twice the modulus M to a fully reduce value. In terms of range equation, the acceptable range of input 1069 of value D
for the modular reduction circuit 1070 is,
0 £ D < (2 * M) (23b)
[0202] Figure lOa shows a final stage 1070 of modular reduction unit 930a of Figure 9b for the design example when n = 0, i.e., the output of the modular reduction block
930a is Q bits by design. The final stage 1070 is referred to as“subtractive reduction”. The technique of subtractive reduction ensures the output of modular reduction unit
930a of Figure 9b is fully modular, so its output is Q bits wide (and n=0 ,) and so that its output value, D, is bounded so that D < M. As the name suggests, subtractive
reduction uses subtraction to perform modular reduction. In Figure lOa, subtractive reduction stage 1070 is comprised of a circuit which performs subtractive reduction
by implementing the following function,

[0203] Function (24) is applied when the value being reduced, Dm, is bounded by less than two times the modulus M. This is convenient, since the additive reduction
circuitry 1000 of Figure lOa produces a result bounded by less than two times the modulus M.
[0204] In Figure lOa and assuming the example configuration 1044, a congruent digit value D is input into pipeline register reg8 1071 received from the output of binary
adder 1068 of Figure lOa. The entire 19-bit output of reg8 1071 is directed to subtractor unit 1074, and the least significant 18 bits of reg8 1071 is directed to bus selector
unit 1078. If the value contained in reg8 1071 is greater than the value of the modulus 1072, denoted as M, the borrow signal 1075 is not asserted so that the output of
subtractor 1074 is passed to pipeline register reg9 1078. Otherwise, the value contained in reg8 1071 is not greater than the modulus M 1072 so the borrow signal 1075
is asserted, and therefore the value of pipeline register reg8 1071 is passed directly to reg9 1078 by selector 1076.
[0205] There are many variations for the additive modulo reduction circuit 1000 and the subtractive modulo reduction circuit 1070 of Figure lOa which should be obvious
to those skilled in the art of modular digital circuit design. For example, additional pipeline stages may be inserted to improve overall speed of the circuit, such as adding
pipeline stages between the subtract unit 1074 and the bus selector unit 1076 of Figure lOa, or adding pipeline stages to break up adders and subtractors into smaller
units with less bit-width. It is well known such optimizations exist, and there are well known tradeoffs, such as increasing pipeline depth. Other variations for modular
reduction circuitry will include different numbers and sizes of LUT’s, adders, subtractors, and registers and different amounts of pipeline depth.
[0206] Referring back to Figure 9b, the output of modular reduction circuit 930a is connected to the input of modular accumulator 935a. In one embodiment under
discussion, the output of modular reduction unit 930a is fully modular, so that its output bus is bounded by Q bits, and further such that its output value D is bounded by
its modulus M. (The case when the value n of Figure 9b is equal to zero, i.e., Q+n=Q). One perceived advantage of using full modular reduction is that modular
accumulator 935a can accumulate an input within legal RNS digit range; therefore, if the accumulator is also in legal digit range, then a basic binary adder followed by a
subtractive reduction circuit like subtractive reduction circuit 1070 of Figure lOa can form the basis for the modular accumulator 935a.
[0207] In Figure lOb, an arithmetic schematic of a modular accumulator 935b is shown which is based on a binary adder 1080, and a subtractive reduction circuit
comprised of subtractor 1084 and bus selector 1088 which implement the logic of equation (24). The output of the bus selector 1088 is stored into a special register
called an“accumulator”, designated as register Acc 1090. As in the case of a binary accumulator, the modular accumulator Acc 1090 accumulates the value of each digit
D input via bus 1079. After summing the input digit D 1079 to the value of accumulator Acc 1090 using adder 1080, a modular reduction is performed on the resulting
sum using subtractor 1084 and selector 1088 in a manner explained in the section describing the subtractive reduction unit 1070 of Figure lOa. Therefore, the bit-width of
an accumulated modular result Acc 1090 does not grow during any number of accumulation cycles, and summation 1090 may be transferred to a digit value output
register R 1095 after any number of accumulations in the embodiment of Figure lOb.
[0208] Also shown in Figure lOb is an accumulator reset signal 1091 which is required to reset the accumulator 1090 before a new sequence of digit values D input 1079
is accumulated. Another option, not shown in Figure lOb, is a load input signal. The load input signal may be used instead of the reset signal 1091 to load the first digit
value D before subsequent digits are summed by the accumulator 1090. One disadvantage of a reset signal 1091 is it adds a“dead” arithmetic cycle into the arithmetic
pipeline and data flow. However, the reset signal 1091 is generally a faster circuit than a load circuit, and in many embodiments it is therefore more expedient to waste a
summation cycle because it is offset by the increased performance of accumulator circuits using a reset signal 1091. This is particularly true if the number of products
summed is large. For the embodiments of the present invention, either the load circuit or reset circuit feature may be used, or neither circuit may be used, or both circuit
features may be used together.
[0209] There are several disadvantages of the modular accumulator of Figure lOb. One disadvantage is data flow between the adder 1080 and the accumulator 1090
cannot be easily pipelined. This may seem counter-intuitive since it was discussed earlier how the subtractive reduction circuit 1070 of Figure lOa can support increased

https://patents.google.com/patent/WO2019213070A1/en?q=(high+radix)&oq=high+radix

14/38

2/14/26, 1:57 PM

WO2019213070A1 - Systems and methods for matrix multiplication, normalization, and reverse conversion - Google Patents

pipelining. The problem for the modular accumulator circuit 935b of Figure lOb is illustrated by the faulty modular accumulator circuit 1100 in Figure 11, which illustrates
the same modular accumulator circuit 1070 of Figure lOb but with an extra pipeline register 1106 inserted by means of example. The problem stems from the fact that
the accumulator 1125 output must be fed back into adder 1105 for immediate summation with an input from pipeline register 1101. Delaying summation 1105 by one
clock cycle before feeding the summation into the accumulator 1125 creates a“racing accumulators” effect, such that two values are accumulated, one value
accumulating digit values on an even clock, and another value accumulating digits on an odd clock. This presents problems which get worse when adding more pipeline
stages, such as adding a pipeline stage (not shown) between the subtractor 1110 and the bus selector 1120. When adding such pipeline stages the number of racing
accumulators increases, significantly complicating the design of a working modular accumulator circuit.
[0210] Figure 12 shows an improved modular accumulator 935c comprised of two adders in parallel instead of two adders in sequence as in Figure lOc. One adder 1205
supports two inputs, while a second adder 1210 supports three inputs and a wider input port to support the complement of the accumulator modulus M, i.e., -M 1215. In
Figure 12, the modular accumulator 935c takes advantage of two adders that operate in tandem, therefore decreasing delay and increasing speed of operation. The
three-input adder 1210 is required to support summation and subtraction simultaneously. Like the circuit of Figure lOc, if the borrow signal 1211 is not asserted, then the
value output from adder 1210 is passed to the accumulator 1225 by bus selector 1220, otherwise, if the borrow signal is asserted, then the value output from adder 1205
is passed by bus selector 1220 and latched by accumulator 1225. [0211] One advantage of the modular accumulator 935c of Figure 12 versus the modular accumulator
935b of Figure lOb is faster operation since arithmetic summation and subtraction circuits of Figure 12 operate in parallel and represent less overall delay. Another
advantage of the circuit of Figure 12 is the decreased number of logic levels which helps mitigate the problem of not being able to easily insert pipeline registers into the
accumulator feedback loop data flow as discussed earlier.
[0212] However, the modular accumulator 935c of Figure 12 also has a major problem since it cannot be easily pipelined. To understand why, it helps to consider the
case of a pipelined binary accumulator circuit of Figure 13 which does not have this problem. For high-speed pipelined accumulators, it is common practice to sub-divide
an accumulator into multiple stages, where stages supporting the least significant bits are processed before stages of the next significant bits. For example, an 18-bit
binary accumulator 1350 can be broken into three stages, with two least significant stages 1352, 1354 each of 6 bits and a high order stage 1356 of 7 bits width. The
processing loop of the accumulator is then expanded to support three accumulation pipeline (clock) stages 1320, 1330, 1340 instead of one. Because binary carry
circuits propagate from least significant stage 1320 to the next significant stage 1330 via carry signal 1324, it is possible to implement fast binary accumulator pipelines
because the binary counter width of each stage is small, such as accumulator stage 1331, and since the result of each pipeline stage is only dependent on the result of
the prior stage. While this is convenient for binary arithmetic, it is not a solution for modular accumulation up to this present disclosure.
[0213] When considering the modular accumulator 935c circuit of Figure 12, it is not a simple matter to pipeline the adders 1205, 1210, in a similar manner as described
in Figure 13 since partitioning the binary adder 1210 into smaller adders has an inherent problem; that is, the result of the first stage is dependent on the result of the last
stage of the pipeline. To be successful, the partitioning of a binary adder into smaller stages depends on the ability to“close” the accumulation feedback into as short a
path as possible. In the example of the first binary accumulator stage 1320 of Figure 13, the circuit can feedback 1321 its corresponding accumulator value 1323 in one
clock cycle, wherein each successive accumulator stage 1330, 1340 is then staggered by one clock cycle. In the case of the modular accumulator 935c, it is not feasible
to“close” the accumulator feedback of each stage to a single clock cycle, since the result of the first stage depends on knowing the result of the last stage of the pipeline.
[0214] In one preferred embodiment of the present invention, the modular accumulator 935d of Figure l4a is used to overcome the limitation of the modular accumulator
935c of Figure 12. In Figure l4a, modular accumulator 935d is shown with an input digit 1400 bit-width of Q+3 bits, and an output digit Y 1434 bit-width of Q+3 bits by
means of a generalized example. The significance of this configuration will be explained shortly. One important characteristic of the modular accumulator 935d is that its
operation is based on a method of combined additive reduction and gated truncation based modular reduction; this is a unique and novel technique of the present
invention. Other unique features of the modular accumulator 935d of Figure l4a is the three accumulator feedback paths 1431, 1437, 1438 and the“Ena runc” control
signal 1439, among others.
[0215] The operation of the modular accumulator 935d is explained when an input value D, congruent modulo M, is received on input bus 1400. The input value 1400
supports a binary width of Q+3 bits; the two most significant bits 1426 is routed to LUT 1410 while the least significant Q+l bits is routed to pipeline register 1402. The
output of the LUT 1410 converts the combined value of the three most significant bits of accumulator 1430 plus the most significant two bits 1426 of input value 1400
modulo M and returns an equivalent 18-bit value modulo M entering the binary adder 1415. The most significant Q+l bits of the input value 1400 stored in pipeline
register 1402 is transferred to the other input of binary adder 1415.
[0216] The output of adder 1415 is up to Q+2 bits wide and is connected to pipeline register 1420 which in turn is connected to an input of binary adder 1425 where the
input value is summed with the value contained in the accumulator register 1430. The following stages of the novel and unique modular accumulator 935d significantly
differ from the operation of modular accumulators 935c, 935b. For example, the accumulator 1430 is now Q+3 bits wide“[Q+2:0]” and is divided up into two accumulator
sections, a low-order section 1432 supporting accumulator bits“[Q-l :0]”, and a high-order section 1433 supporting accumulator bits“[Q+2:Q]”. The low-order accumulator
section 1432 is Q bits wide, and acts much like an“ideal” 18-bit binary accumulator. The output 1431 of the low-order accumulator section 1432 is routed and connected
to the low-order bits of the binary adder input bus 1426 thereby creating a“primary” accumulator feedback path 1431.
[0217] The high-order accumulator section 1433 contains the most significant bits of accumulator 1430. In a typical embodiment of Figure l4a, three high-order bits 1433
of the accumulator 1430 are connected to two additional accumulator feedback paths 1437, 1438 via a fast gate function logically comprised of AND gates 1435, 1436,
by means of example. The AND gates 1435, 1436 each have one input tied to the truncation enable (“Enajxunc”) control input 1439 in such a way that only one gate is
active at once. For example, when the truncation enable control 1439 is high (asserted), the gate 1435 passes the high order accumulator bits 1433 to the input of LUT
1410; simultaneously, AND gate 1436 transmits a“zero” value via the accumulator feedback path 1437 such that the high order input bits of adder 1425 input bus 1426
are zero.
[0218] Therefore, truncation of the high order accumulator bits 1433 of the accumulator 1430 is performed by gating to zero the high-order input bits 1437 such that they
are not re-calculated by adder 1425 in the next clock cycle. However, the value of the truncated bits 1433 are not lost in this case, since their value is converted by LUT
1410 to an equivalent value 1411 modulo M and re-enters the“outer” accumulator loop defined by the circular data path created by feedback path 1438. In this case,
when the truncation enable control 1439 is asserted, the accumulator 1430 is seen to accumulate congruent values in a“modular accumulation mode”.
[0219] Alternatively, when the truncation enable control 1439 is low (de-asserted), the high order accumulator bits 1433 are passed back via AND gate 1436 and feedback
path 1437 to the high order bits of input bus 1426 of binary adder 1425. Simultaneously, the AND gate 1435 transmits a“zero” value via feedback path 1438 to the input of
LUT 1410. The LUT is programmed so that when all feedback path 1438 bits are zero, there is no contribution from the accumulator 1430 to LUT function 1410; its output
1411 is zero. In the embodiment shown in Figure l4a, during a summation operation, the truncation enable control 1439 is de-asserted three clock cycles before
summation terminates; therefore, the values accrued in the pipeline registers 1402, 1420 and the value output from LUT 1410 are summed by adder 1425 and are
accumulated by the full width of accumulator 1430; full width accumulation is to include all Q+2 bits. In this case, while the truncation enable control 1439 is de-asserted,
the accumulator 1430 is seen to accumulate congruent values in a“binary accumulation mode”.
[0220] As shown in the embodiment of Figure l4a, after the last three clock cycles of summation in binary accumulation mode, the accumulation is complete, and all bits
of accumulator 1430 are read out data path 1434 for further processing by the matrix multiplier of the present invention. Also shown in Figure l4a is a reset input 1450
connected to the accumulator 1430. Details of the reset are not provided, but is conventionally understood that when asserted, the entire accumulator 1430 will be

https://patents.google.com/patent/WO2019213070A1/en?q=(high+radix)&oq=high+radix

15/38

2/14/26, 1:57 PM

WO2019213070A1 - Systems and methods for matrix multiplication, normalization, and reverse conversion - Google Patents

cleared. The reset control 1450 can be of various types, such as a synchronous reset control, or alternatively an asynchronous reset control. In some embodiments of the
present invention not shown, the reset signal 1450 may be replaced with a“load accumulator” control which loads the accumulator 1430 with the first value of a
summation from binary adder 1425.
[0221] Figures l4b and l4c illustrate timing diagrams for the modular accumulator of Figure l4a. In Figure l4b, a timing diagram illustrating the control of a modular
accumulator which supports an accumulator reset control input 1510 is shown. At cycle D+l, accumulator 1515 begins to accumulate in a modular fashion while the
ENA_TRUNC control input 1505 is asserted. When the ENA_TRUNC control input 1505 is asserted, the modular accumulator 935d of Figure l4a performs a modular
accumulation in what is referred to as a“modular accumulation mode”. Since the pipeline depth of the accumulator 935d is two stages, the ENA_TRUNC control 1505 is
de- asserted at cycle D+5, i.e., two clocks before summation is complete at cycle D+7. During the last two clocks before summation is complete, (during clock cycle D+5
and D+6) the ENA_TRUNC control is de-asserted, and the modular accumulator 935d of Figure l4a operates in what is referred to as“binary accumulation mode”. The
reason is the binary accumulation mode allows the accumulator 1430 of Figure l4a to overflow at some future summation cycle, whereas during the modular
accumulation mode, there is never a possibility of overflow of accumulator 1430 because the high order bits 1433 are repeatedly truncated at each summation cycle.
[0222] In Figure l4b, the modular accumulator supports an accumulator reset input 1510. The accumulator reset 1510 is asserted at the same clock cycle as the
accumulator summation is ready at clock cycle D+7. The next accumulator value at cycle D+8 is zero and the process of accumulation starts at cycle D+9. In Figure l4c,
the modular accumulator is cleared implicitly using a LOAD control input 1530. The waveform of Figure l4c shows the LOAD control input asserted at the same cycle as
the product summation 1535 is complete. However, the next product summation (accumulation) starts in the next clock cycle D+8, one clock earlier than the clock cycle
D+9 of Figure l4b. However, the reset signal may be implemented easier than a LOAD input within the context of digital circuitry and therefore may be preferable in some
FPGA embodiments if the wasted clock period is insignificant to the decrease in overall speed due to the delays of a LOAD circuit.
[0223] There are many interesting aspects and properties of the high-speed modular accumulator 935d of Figure l4a which contribute to a preferred embodiment. For
one, a modular reduction function is comprised of a truncation operation of the accumulator 1430 in combination with a specially programmed LUT 1410. In one
preferred embodiment, the truncation function is the same for all modulus Mj, but the programming of LUT 1410 is specific to each modulus Mj using the nomenclature
developed in Figure 3 a.
[0224] One advantageous property of LUT 1410 is its relatively small size, which in the embodiment of Figure l4a consists of a (2L5* 18=) 576 bits memory table. For
example, LUTs supporting five inputs are common primitives in modem FPGA and ASIC libraries. The programming (data) of the LUT also depends on the input bit
assignments which define the address inputs 1426, 1438 to the memory LUT 1410. For example, in one possible variation of LUT 1410, the input bits 1426 are assigned
to the least significant bits of the input address of LUT 1410, while the feedback path bits 1438 are assigned to the most significant address input bits of LUT 1410.
These types of variations are often arbitrary and known by those skilled in the art and do not materially affect the novel invention of the present disclosure.
[0225] Equations can be derived for any variation of address input connection and used to program RAM or ROM based LUT 1410. For the example provided earlier,
whereas bits 1426 are assigned to least significant address inputs, and bits 1438 are assigned to the most significant address inputs of LUT 1410, the following equation
(24) generates data for each address location, A, of LUT 1410 for a given Modulus, Mj:
LUT(A) = [((A & 0x3) * (2AQ)) + (((A & 0xlc)»2) * (2AQ))] % Mj (25)
[0226] Another major advantage of the modular accumulator 935d of Figure l4a is the high-speed operation of the accumulator 1430 during modular accumulation mode;
since the primary feedback path 1431 is a direct connection to a single binary adder 1425 which in turn is directly connected to the input of accumulator 1430 to re-load
data directly to the accumulator 1430. Note there is no other intermediate component or function, such as a bus selector unit 1220 of Figure 12. Moreover, there is no
other signal dependency in the primary accumulator feedback path 1431, such as the borrow signal 1211 of the modular accumulator 935c of Figure 12.
[0227] When the modular accumulator 935d of Figure l4a operates in binary accumulation mode, the truncation enable signal 1439 is de-asserted, so the upper three bits
of the accumulator 1433 are fed back to the binary adder 1425 through the delay of a single gate function 1436, illustrated as AND gate 1436 of Figure l4a. This
architecture is conveniently fast, since the delay of carry from the least significant bits to the most significant bits of binary adder 1425 are in tandem to the short delay
of a single gate function 1436. Therefore, when operating in binary accumulator mode, the accumulator 1430 is operating at the full speed of a Q+3 bits wide binary
accumulator, and is feeding back this entire accumulator width, consisting of both the lower section 1432 and the upper section 1433, into the binary adder input 1426.
[0228] Another major advantage of the modular accumulator 935d of Figure l4a results from the fact that the requirement for“decision logic” of modular accumulators
935b, 935c has been eliminated. For example, decision logic consisting of a subtractor, a borrow signal, and a bus selector is eliminated. Therefore, the ability to partition
and pipeline the binary adders 1425, 1415 of Figure l4a is now possible. The binary adder 1425 and accumulator 1430 may be further partitioned to provide for high
speed operation. For example, in one embodiment, the digit width Q is 18 bits, and the accumulator 1430 is Q+3=2l bits wide. Since 21 is evenly divisible by three, one
embodiment implements a three-stage binary accumulator 1430, with each stage being seven bits wide.
[0229] Another significant advantage of the modular accumulator of Figure l4a is the ability to accept a congruent digit value D as input 1400, as opposed to a“fully”
modular digit value D. In Figure l4a, the modular accumulator 935d accepts a digit value which can occupy Q+3 bits encoding, or stated another way, the modular
accumulator 935d can accept digit values D that are 3 bits more than the digit width Q. The consequence is the modular reduction unit 930a of Figure 9b can output a
congruent digit value D of up to Q+3 bits, so that circuitry requirements for modular reduction unit 930b of Figure lOa is reduced to that of the modular circuitry 1000 or
less. For example, referring to the modular reduction unit 1000 of Figure lOa, the LUT5 1066, register reg7 1064, and binary adder 1068 can be eliminated, and the output
of binary adder 1062 is used to form a 21 -bit wide congruent output.
[0230] To further clarify the advantages of accepting a congruent digit value and other unique features, an alternate embodiment of the modular multiplier of the present
invention is provided in Figure l5a. The embodiment of the modular accumulator of Figure l5a uses three-input adders 1520, 1530, 1540 instead of the two-input adders
1415, 1425 of Figure l4a. The three-input adder is a common primitive in more advanced FPGA devices which support 6-input look-up table memory functions, such as
the Cyclone- V series FPGA devices from Intel (formerly known as Altera). This is important, since readily available technology is used to implement the modular
accumulator of the present invention.
[0231] In Figure l5a, a pipelined modular reduction unit 930c is shown for a typical 18-bit wide RNS digit and is comprised of several 3-input adders 1520, 1530 and
several look-up tables LUT1 1512, LUT2 1514, LUT3 1526, and LUT4 1528 and several pipeline registers, regl 1511, reg2 1514, reg3 1515, and reg4 1521. It is understood
the 18-bit wide example of Figure l5a is for illustrative purposes only, and that other binary widths other than 18-bit may be constructed without materially changing the
present invention. Because the example uses 18-bit wide RNS digits, the input 1500 of the product of two RNS digits may be up to 36 bits wide. The function of the
modular reduction unit 930c is to reduce the 36-bit wide product 1500 into an equivalent 21- bit output 1533 having a value congruent modulo M, where M is a general
modulus. Note the output 1533 of modular reduction unit 930c is not bound by less than two times the modulus M as was the output 1069 of the modular reduction
circuit 1000 of Figure lOa. Therefore, circuitry for modular reduction is reduced; this is important since modular reduction circuitry for modular multiplication grows as the
square of the hardware matrix dimensions of matrix multiplier 400 of Figure 4. However, an alternate embodiment for modular reduction may include circuitry to
implement a full MOD function, since this does not affect the operation or need for the modular accumulator 935e.

https://patents.google.com/patent/WO2019213070A1/en?q=(high+radix)&oq=high+radix

16/38

2/14/26, 1:57 PM

WO2019213070A1 - Systems and methods for matrix multiplication, normalization, and reverse conversion - Google Patents

[0232] In Figure l5a., the modular accumulator 935e is likewise comprised of a 3-input adder 1540 and look-up table LUT5 1536 which is programmed to implement
a“running MOD” function, or more precisely, a“running congruence” summation function at the output 1559. Like Figure l4a, the LUT5 1536 is programmed to account for
the modular summation of the value contributed by the upper two bits 1535 of the incoming congruent operand and the value contributed by the truncated two bits 1557
of accumulator 1550. Equation (25) or similar is used to encode the LUT5 1536 for each modulus M and for each digit accumulator 935e of a matrix multiplier. Another
improvement of Figure l5a is the reduction of truncation bits via feedback bus 1537 to only two bits wide (versus three) since the value in accumulator 1550 never
exceeds a 20-bit value (“[19:0]”) during the modular accumulation mode (during accumulation when ENA_TRUNC is high). Therefore, LUT5 1537 supports only 4 input
address bits and is therefore half the size of the five address LUT 1410 of Figure l4a. Note the control signal name“Ena tmnc feedback” 1556 may also be called by other
names, such as“Trunc ena”, or“Ena tmnc”.
[0233] The use of 3-input adder 1540 reduces the number of pipeline delay stages for modular accumulator 935e of Figure l5a which results in two pipelined stages
1538, 1555 of accumulation versus three. The correct operation of the“Ena Truncation” (ENA_TRUNC) control signal 1556 accounts for the reduced number of stages by
being de-asserted one clock before summation is complete. This differs from the circuit of Figure l4a where the ENA_TRUNC control 1439 is de- asserted two clocks
before final summation is complete. During the last cycle of summation, it is possible the summation within accumulator 1550 can exceed 20 bits in this example circuit
for certain general modulus M, therefore, an extra high order accumulator bit 1554 is provided. Therefore, the accumulator 1550 output bus is 1559 is 21 bits wide and
will transmit a 2l-bit modular summation congruent modulo M after product summation is complete.
[0234] The Figure l5b is included to show a sample modular digit accumulation. In Figure l5b a table is shown having eight states illustrated in Row 1567 through Row
1574. The example modulus chosen is the value 177147 in decimal, and a series of 18-bit wide RNS digits labeled Digit A and Digit B 1562 are multiplied using a binary
multiplier (not shown) producing a 36-bit binary product 1500 of Figure l5a whose value is listed in column 1563 of Figure l5b. At row 1567, the accumulator is initialized
at zero. At row 1568, the accumulator sum 1566 takes on the modular reduction of the first multiplication result, i.e., value 0xB798 of column 1565. As modular (and
congruent) products are produced, the modular sum 1565 changes in each stage as does the accumulation value 1566. At row 1572, the Ena_Trunc control line 1561 is
de-asserted, and the modular accumulator 1566 operates in binary accumulation mode for two cycles as shown in row 1572 and 1573. The accumulation during binary
accumulation mode shows the value in the accumulator 1566 will grow without bounds if summation continues and no other action is taken. In a typical application, the
summation 1566 is transported after two clock periods of binary accumulation mode, and another product summation starts in modular accumulation mode after an
active reset control 1560 is asserted for one clock period. As seen in row 1573, the accumulator value is at its maximum value and grew significantly in value during the
period Ena_Trunc control 1561 is de-asserted.
[0235] Figure l5c is used to disclose detailed operation of the modular multiplier- accumulator circuit of Figure l5a comprising a binary multiplier 1500, a modular
reduction circuit 930c, and a modular accumulator 935e with support for gated truncation and feedback of accumulator data using Ena_Trunc 1579 (Trunc_ena and
Enajrunc are identical) and reset control 1578 of the accumulator. The same example digit data multiplied and summed in Figure l5b is multiplied and summed in Figure
l5c over two complete cycles. The signal names of the detailed waveform of Figure l5c correspond to the registers and FUTs of Figure l5a by means of reference and
disclosure. One difference that Figure l5c shows that cannot be seen in Figure l5b is the latency of the modular accumulation operation. For example, the modular digit
data for Digit A 1576 and Digit B 1577 appears in clock cycle 1 through clock cycle 6, but the accumulated modular result Accum 1593 doesn’t appear until clock cycle
12. The control of the Trunc ena control signal as well as the accumulator reset control input must be properly timed to coincide with data that is flowing within the
pipeline structure.
[0236] Another interesting feature of the waveform of Figure l5c is the truncation address bits 1592, which are seen to change as the value in the accumulator change.
During modular accumulator mode, the Trunc_ena control signal 1579 is asserted, and the accumulator value at clock periods 7 through 10 show the accumulator value
1593 stable and not growing out of bounds. Except when the Trunc_ena signal is de-asserted at clock period 10 and 11, then binary accumulation mode is operating, and
the accumulator value 1593 grows like an arithmetic binary count sequence, and not in a modular sequence as is the case when operating in modular accumulator mode.
Of significance is that if the accumulator can overflow in binary accumulation mode, then information is lost, and the digit summation result is in error, however, during
modular accumulation, information is not lost, since the truncated value doesn’t overflow, it is re-routed back and its value re-mapped into the outer accumulation loop in
a modular fashion.
[0237] As previously discussed, modular accumulator circuits of Figure lOb and Figure 12 do not allow simple methods of pipelining, so that wide binary accumulators
cannot be broken into smaller“cascaded accumulators” such as that shown in Figure 13. This is a problem, since partitioning a wide accumulator into a plurality of
multiple smaller accumulators is a well-known technique for enabling faster execution at the expense of added pipeline circuitry. To overcome this problem, and to
provide another variation of the unique modular accumulator apparatus of the present invention, a highly pipelined 18 -bit modular accumulator of the present invention
is disclosed in Figure 16.
[0238] In Figure 16. by means of example, an 18-bit modular accumulator is supported using a 22- bit modular accumulator 1650, which consists of three distinct stages,
two stages which are 7 bits wide, and a third accumulator stage of 8 bits wide. In this manner, a modular accumulator uses three high-speed accumulators instead of the
one slower 21 -bit accumulator of Figure l5a. Modular data is staged into the pipeline using input register 1600 which is diagrammatically shown divided into three data
bit sections, the first section consisting of the first 7 bits [6:0] 1602 of the input, the second section consisting of the data bits [13:7] 1604, and a third input section
consisting of data bits [20: 14] 1606. The last stage of the modular accumulator of Figure 16 is the output register 1650 which is also shown as two 7-bit sections 1654,
1656, and one high order 8-bit section consisting of sub-sections 1657, 1658, 1659. The output is therefore larger than the input in this example circuit, consisting of 22
bits wide congruent data. Other embodiments may differ by adjusting the data widths of various components which is obvious to those skilled in this art.
[0239] In the apparatus of Figure l6a, the novel innovations of the present invention are present. For example, the feedback LUT5 1536 of Figure l5a is shown split into
three LUTs, LUT5A 1610, LUT5B 1628, and LUT5C 1640 each providing the LUT5 data bits associated to each respective high-speed accumulator stage 1636, 1646, 1657
respectively. For example, LUT 1610 provides the LUT data for bits [6:0], LUT 1628 provides the data bits [11:6], and the last LUT 1640 provides the data bits [17: 12].
Therefore, while the LUT is split into three smaller LUTs, the number of memory bits is the same as LUT5 1536 which supports the full 18-bit data bits [17:0].
[0240] In Figure 16, the modular accumulator differs in that the accumulator counter is broken into three distinct stages, each stage offset by one clock cycle. For
example, the first 7-bit accumulator stage 1636 is comprised of adder 1632 and accumulation feedback data bus 1635. A carry bit bus 1637 transmits a carry bit data to
the next 7-bit accumulator adder stage 1642 which is added to the second stage accumulator 1646 in the next clock cycle. Like Figure l5a, the last 8-bit accumulator
stage comprised of register sections 1657, 1658, 1659 is coupled to a truncation gating circuit 1665 which provides gating of the upper accumulator bits [19: 18] 1658 to
feedback LUTs 1610, 1628, 1640 when the Trunc_Ena control signal 1668 is asserted. Because of the additional accumulator stages, the number of pipeline stages for
the modular accumulator of Figure l6a is increased over that of Figure l5a. This impacts the time when the Trunc_Ena control 1668 is de- asserted as discussed earlier.
[0241] Normalization Pipeline Units
[0242] In this section, the design of high-speed product normalization pipeline units, such as the pipelined normalization unit 455a of Figure 4, is disclosed. The product
normalization unit performs an important function for general purpose computation in the residue number format and is necessary to properly normalize dot products
which exit the matrix multiplier unit 400a of Figure 4 in the present invention. It is a claim of this disclosure that proper application of“true fixed point RNS normalization”
of RNS fractional formats provides a critical ingredient enabling general purpose computation in RNS format. To that end, equations (1) through (80) provide new
arithmetic expressions to describe RNS fixed-point processing which aids modular computation design. [0243] In U.S . Patent No. 9,081 ,608, a normalization unit for

https://patents.google.com/patent/WO2019213070A1/en?q=(high+radix)&oq=high+radix

17/38

2/14/26, 1:57 PM

WO2019213070A1 - Systems and methods for matrix multiplication, normalization, and reverse conversion - Google Patents

normalizing the intermediate product of two fractional RNS formats is described in Figure l5c of that patent. This type of normalization is referred to as“true fixed-point
RNS normalization” because it normalizes, or divides by, the range of the fractional RNS digits; this is illustrated mathematically for positive fractions using equations
(11) through (15) and equations (17) through (18) of the present disclosure. To handle normalization of negative values U.S. Patent No. 9,081,608 discloses an apparatus
which uses two normalization apparatus operating in parallel, one normalization unit processing the original intermediate product, the other processing the complement
of the intermediate product. Only the processing of the shorter of the two is kept in this scheme, as it is guaranteed to be positive and valid. If the complemented value
processing is shorter, the processed result is complemented, since the normalized result is known negative. In the scheme of U.S. Patent No. 9,081,608, the
normalization apparatus performs a comparison of both values as they are processed to determine which is smaller in magnitude so that the properly normalized value
is kept, and the improperly normalized value is discarded. Since direct normalization only works with positive encoded values, and not negative values encoded with the
method of complements, the apparatus of the prior art keeps the smaller of two processed values thus ensuring the kept value is correctly processed. If the kept
processed value was the complemented value, this indicates the answer is negative, and the kept processed value is“re-complemented” as a result.
[0244] It is noted the need to perform normalization on signed RNS values pose significant complexity in terms of design of RNS normalization circuits. For example, a
pipelined normalization unit 455b employing the method of U.S. Patent No. 9,081,608 is illustrated as the block diagram of Figure l7a. In the apparatus depicted in Figure
l7a, a non-normalized value 1700 is input via data bus 1710 and data bus 1705; both data buses are wide enough to support a full range of RNS digits, such as eight 18 bit wide digits of the example fixed -point RNS format of Figure 3a. One copy of the non-normalized value is input via data bus 1710 and routed to delay register 1720, the
other copy is transmitted via data bus 1705 to a complement function 1715. The delay function does not perform an arithmetic function, it only delays the input value so
that it remains in synchronization with the non-normalized value routed to complement function 1715.
[0245] Both the value delayed in block 1720 and the value processed in complement function 1715 exit simultaneously and enter a normalization unit 1735 and 1725
respectively. The normalization unit is comprised of an RNS to mixed-radix converter stage followed by a mixed -radix to RNS converter stage illustrated by the two
triangular regions denoted by dotted lines 1736, 1726 of normalization pipeline unit 1735, 1725 respectively. Each normalization pipeline unit produces a series of mixedradix digits transmitted to a comparator pipeline 1730 using digit comparison buses, such as digit comparison buses 1740, 1745. The result of the comparison of the
RNS value processed in normalization pipeline 1735 is compared with the RNS (complement) value processed in normalization pipeline unit 1725; the result of the
comparison is stored in an optional pipeline storage (delay) unit 1765.
[0246] Moreover, each normalization pipeline 1735, 1725 performs a decomposition of the input RNS value to a mixed radix format, then performs a division by the
fractional range using truncation in mixed radix format, and then performs a recombination of the truncated mixed-radix value to RNS format. The mathematics of this
processing is expressed in equations (17) through (18) for positive values only, since if the value is detected to be negative, it is discarded by selector 1775. Only one of
the processed values exiting the normalization pipeline units 1735, 1725 is selected as a result using selector 1775, which is controlled depending on the sign of the
processed values comparison stored in sign result register 1765. It should be noted the pipeline design produces a unique result every clock cycle or better, so that
multiple values are processed through the pipeline at once.
[0247] The normalization pipeline 455b of Figure l7a consists of two normalization pipeline units each comprising an RNS to mixed-radix converter 1725, 1735 and each
further comprising a mixed-radix to residue converter 1726, 1736. A non-normalized intermediate product or intermediate product summation value in RNS word format
enters the normalization pipeline 455b and is transmitted to a delay element 1720 and a word complement unit 1715. The delay element 1720 is used to delay the unmodified RNS word to remain in pipeline synchronization with the RNS complement produced by word complement unit 1715. The original RNS value enters a residue to
mixed -radix converter 1735 and the complement of the value enters the residue to mixed- radix converter 1725.
[0248] During synchronized conversion of the two values in Figure l7a, a comparator pipeline 1730 determines if the original residue value or its complement is larger in
magnitude; the comparison results in a sign determination flag 1765 being set if the un-complemented value is larger than the complemented value, which indicates the
original value undergoing processing is negative. If the sign determination flag 1765 is set, it implies the result is negative so selector 1775 passes the complement of
the result of mixed-radix to residue converter pipeline 1726 to the result output register 1780. Otherwise, if the sign determination flag is not set, it implies the result is
positive, and the mixed-radix to residue result 1736 is passed by selector 1775 to the output register 1780.
[0249] Figure 17a is re-drawn in more detail in Figure 17b. In Figure l7b, the normalization pipeline 1735 consists of a pipelined RNS to mixed-radix converter 1737 and a
pipelined mixed- radix to RNS converter 1736. Likewise, the normalization pipeline 1725 which processes the complement of the non-normalized value consists of RNS
to mixed-radix converter 1727 and mixed-radix to RNS converter 1726. Also shown in Figure l7b is a pipelined comparison unit 1730 which compares mixed-radix digits
via digit comparison buses, such as digit comparison buses 1745, 1740, 1755, 1750. Like Figure l7a, the selector 1775 of Figure l7b decides which processed value to
pass to the output 1780. Inspecting the operation of the normalization unit 455b of Figure l7b it is clear that at least half of the circuitry of Figure l7b is wasted during
operation since it produces an incorrect result, and this implies that 50% of the power is also wasted. Figure l7b is included to reveal the complexity of RNS normalization
pipelines, and to reveal the large resource requirements and potential loss of efficiency of a design using the methods of U.S. Patent No. 9,081,608.
[0250] One method of reducing the circuitry required to perform normalization is to attempt to “correct” a negative value that has been normalized using the
normalization pipeline 1735 of Figure l7b. In the prior art, the negative value is in-correctly processed and cannot be used according to equation (28). In a new and novel
method and apparatus of the present invention, a single normalization pipeline is used to normalize values despite their sign.
[0251] As a review as to the validity of this new approach, the following generalized mathematical description of fixed-point arithmetic is provided. First the sign of an
RNS value which adheres to the method of complement is defined as,

[0252] It follows that the complement of a non-zero RNS value Y can be computed using,

[0253] In a new method recently developed and now disclosed for the first time, a high-speed normalization is performed using a single mixed radix converter which
guarantees normalization regardless of the sign of the intermediate product. To clarify the problem of normalizing negative values, equation (27) for a negative value is
substituted into equation (15), so that a normalization of a negative intermediate value is attempted as,

[0254] In this case the result YR is not correct since the normalization of equation Error! Reference source not found, is derived for positive numbers only. In other words,
dividing the complement (RT-YIP) directly by the fractional range RF does not normalize the negative number. In equation (28) the fractional range RF is dividing both the

https://patents.google.com/patent/WO2019213070A1/en?q=(high+radix)&oq=high+radix

18/38

2/14/26, 1:57 PM

WO2019213070A1 - Systems and methods for matrix multiplication, normalization, and reverse conversion - Google Patents

positive representation of the intermediate value as well as the total range of the number system, so the formula doesn’t work as intended. Another minor issue with (28)
is the ceiling function, which is used to denote round-up, but should be replaced by a floor function symbol to indicate round-down for negative numbers.
[0255] The problem of directly normalizing negative values is solved using a correction factor C which is determined by adding both the positive value and negative value
calculation of (14), since adding complements should produce zero, so,

[0256] To simplify the meaning of this equation, one case is easy to solve, that is, the case when YIP is evenly divided by RF. In this case, a“perfect correction constant” Cp
is therefore equal to,
R,
c =— = R W
P RF (30)
[0257] The correction constant is used by deriving a positive value from an improperly processed negative value as,

[0258] And next forming a“correct” complement for the corrected positive value of equation (31) using equation (27) the properly normalized negative result is,

[0259] Which can be re-organized by observing the subtraction of Rw causes an underflow of the machine word (mod RT) SO the need for the MOD brackets to maintain
mathematical correctness,

[0260] Therefore, equation (33) above shows how an in-correctly processed negative value is corrected to a positive answer which is then cast back as a result in the
negative range of a larger target number system, i.e. the base extended number system whose range is RT. The two correction operations of equation (32) are merged
together as one subtraction operation (33) with corresponding wrap-under of the target number system whose range is RT. Therefore, the novel and unique method of
applying a correction factor Cp to a negative value that is processed using a single mixed-radix pipeline is supported in theory and is not to be confused with the wellknown “complement” operation 1715 of Figure l7b, since applying a complement function to an improperly normalized negative value does not yield a correct answer.
[0261] For the more general case, a math substitution is made so the round-up and round-down brackets can be removed. To create a substitution, a value e is defined
which represents the truncated value during normalization,
YIP = ZIR + e RF | ZIP 0 < e < RF
(34)
[0262] Therefore, the non-normalized intermediate product Yip is expressed as a sum of a value Zip which is evenly divisible by RT, and a value e which represents that
portion to be truncated. To make this derivation, the complement of the truncated value e is required, and is provided as,
E = RF - E 0 < e < R F
(35)
[0263] Therefore, a new “general correction constant” Cg is derived using the following substitution of (34) and (35) into (29) to provide,

[0264] where the floor function brackets denote integer function only (not round down) since the truncation remainders is now included in the equations. The general
correction constant is therefore, 2e
cg = Rw - 1+
R, 0 < e < RF (37)
[0265] The general correction constant Cg accounts for the values influencing rounding. After careful analysis of the meaning of equation (37) above, it is shown that the
perfect correction factor Cp is equivalent to the general correction constant Cg provided the rules for rounding of positive values is used for the case of negative values.
The one exception is when the truncated remainder is equal to RF/2, known as the tie-splitting case. In this case, the sign of the intermediate value dictates the polarity of
the rounding value. Therefore, the summary for the mathematics of the new form of normalization is now,

https://patents.google.com/patent/WO2019213070A1/en?q=(high+radix)&oq=high+radix

19/38

2/14/26, 1:57 PM

WO2019213070A1 - Systems and methods for matrix multiplication, normalization, and reverse conversion - Google Patents

[0266] with a single bit round-up value ru calculated using,
0, if [MIPJF < - sign(x), if [MIPJF = - (39)

1, if [MIPJF > [0267] Equation (38) describes a new procedure for fixed-point arithmetic in RNS which can be explained as follows. If the value of Mip is less than RT/2, the intermediate
value is positive, and the standard procedure for normalization applies, i.e., the mixed-radix value is truncated and converted back to RNS; a round-up value is also added
if the truncated mixed-radix digits exceed a rounding trigger value RF/2. If the value of Mip is greater than RT/2, the intermediate value is negative, and the standard
procedure for normalization is performed followed by a subtraction of the whole range value Rw, a round-up value is also added to the value if the truncated digits exceed
RF/2.
[0268] It might be argued that correction of an improperly normalized negative number is simply a standard complement function, however, as demonstrated in this
disclosure, this is not correct. In both fixed-radix and RNS number systems, a complement is derived by subtracting the equivalent positive value from the entire range of
the number system, RT, as provided in (27). For the problem of correction of negative values normalized as if they are positive values, equation (38) indicates the whole
number range Rwbe used for correction; moreover, the order of subtraction is different, i.e., the correction constant is subtracted from the“improperly” normalized
negative value. If a basic complement function is used instead, the incorrectly processed value is subtracted from the total range RT, thereby complementing the
incorrect result and so the result is still not correct.
[0269] For the objective of correcting an improperly normalized negative value of equation (28), it is noted equation (38) contains two case terms each of which operate
on the same value Mip, which implies a single pass mixed-radix conversion be used instead of two mixed-radix conversions of the prior art, or the two mixed -radix
conversions suggested by equation (20). However, for the correction to be applied per the case clause of equation (38), the normalization apparatus must also detect the
sign polarity of the normalized value Mip. The sign polarity of the value MIP can be detected by measurement of the magnitude of Mip using a mixed-radix conversion
comparison approach. In one preferred embodiment of the present invention, the pipeline normalization unit performs a mixed-radix conversion for several reasons, one
reason is to determine the comparison result of the magnitude of Mip versus half the total range RT/2 SO the comparison case of equation (38) can be determined.
[0270] Figure l8a and Figure l8b discloses a preferred embodiment for a pipelined normalization unit 455c which may be used to implement the normalization pipeline
455a of Figure 4. Figure l8a is one example only and is suitable for use with the eight-digit fixed-point example number system of Figure 3a. Many other variations are
possible, which is plainly evident to those skilled in the art of modular circuit design. The preferred normalization pipeline 455c of Figure l8b requires less than half the
circuitry versus the pipelined normalization unit 455b of Figure l7b. This significantly improves efficiency of the RNS matrix multiplier design since values are not
processed and then discarded as in the pipeline 455b of Figure l7b.
[0271] Figure l8a shows a preferred pipelined normalization unit 455c consisting of four major sections; an RNS to mixed-radix conversion unit 1830, a pipelined
comparison unit 1810, a pipelined mixed-radix to RNS converter 1820, and a negative value correction circuit 1840. A non-normalized input is stored at input register
1800 and is available in nine pipeline stages at the output register 1850 as shown in Figure l8b by means of example. Each major section of the pipelined normalization
unit 455c is comprised of one or more functional elements. Figure l8b is provided to provide detailed explanation of Figure l8a.
[0272] The next sections introduce each basic functional element of the normalization pipeline 455c of Figure l8a and l8b and a typical FPGA or ASIC solution for each
functional element before disclosing a normalization data example of Figure 18f.
[0273] The functional element symbols used in the present disclosure may be a modular arithmetic function or may be a binary arithmetic function. To identify each type,
this disclosure adopts a convention so that modular arithmetic elements are denoted using arithmetic or alphanumeric symbols enclosed by MOD brackets | |, so that the
symbols“|+|” and“|x|” are used to denote a modular adder and a modular multiplier, respectively. To denote a standard binary adder or multiplier, only the“+” and“x” symbol
is shown on the element block symbol.
[0274] To organize and abbreviate the complexity of the modular circuits of the present invention, it is preferable to adopt symbols for the“processing elements” (PE) that
comprise the modular circuits. PEs hide the complexity of their internal operation and are useful building blocks as they may be re-usable in more than one design or
application. For example, Figure l8c illustrates a symbol for a modular“subtract then multiply” (|SM|) processing element l863a. The basic |SM| symbol is typically
denoted by a trapezoid symbol l863a in Figure l8c and represents a modular operation on an RNS digit R l86la and an RNS (or mixed-radix) digit A l862a. The |SM|
function is provided in equation form denoted within dotted circle 1860; note that the equation 1860 includes a MOD function associated to a digit modulus Mi. In some
illustrations, the subscript of the MOD operation of the trapezoid symbol 1863 a indicates the modulus of the digit operation, as is the case of symbol l863a by means of
example. In other illustrations of the present invention, the subscript is omitted, and the modulus of the digit processing element is implied by its position in the pipeline
or circuit.
[0275] The |SM| function l863a is a common task in mixed radix conversion, since at each stage of mixed radix conversion, a digit value A l862a is subtracted from a
residue digit R 186 la. The |SM| symbol may be expanded to disclose more of its internal operation using an equivalent modular circuit enclosed by dotted lines l863b.
The processing element |SM| is therefore seen to be a modular subtraction 1865 followed with a modular multiplication 1867. Note the modular multiplier 1867 is
a“multiply by constant” variation, which requires less circuitry than an arbitrary modular multiplier, (such as arbitrary modular multiplier consisting of stages 925a, 930a
of Figure 9b., which is required within the matrix multiplier 400a of Figure 4.) The value of the multiplicative constant used in each processing element is dependent on
the modulus of the digit, and on the position of the PE in the circuit. For most applications, the multiplication is performed using a constant M 1 1866, which represents
the multiplicative inverse of a“dividing modulus” MA with respect to the modulus of the digit processing element, Mi, in this example.
[0276] The |SM| processing element l863a of Figure l8c appears in numerous locations within the RNS to mixed-radix conversion pipeline unit 1830 of Figure l8b, such as
|SM| processing unit 1816. The specific |SM| processing element 1816 subtracts the value of the incoming RNS digit Ri from the incoming RNS digit R2 and multiplies the
result by the multiplicative inverse of Mi with respect to M2, which is implied by the relative position of the PE 1816 in the pipeline section 1830. The result of the digit
operation is stored in digit register a2 1817. This is a common digit operation during mixed radix conversion and forms the basis for schematic representation and

https://patents.google.com/patent/WO2019213070A1/en?q=(high+radix)&oq=high+radix

20/38

2/14/26, 1:57 PM

WO2019213070A1 - Systems and methods for matrix multiplication, normalization, and reverse conversion - Google Patents

definition of each mixed radix conversion cycle, or“STAGE” denoted using STAGE 1 plaque symbol 1801 which designates the first pipeline stage or cycle of mixed radix
conversion for purposes of reference and disclosure. Labeling specific stages of the pipeline of Figure l8b is helpful to aid the disclosure but may not coincide with
pipeline stage references in practice.
[0277] There are many circuits that may be used to implement a modular subtract then multiply |SM| processing element. In one example of the present invention, the
|SM| processing element is implemented from common Verilog or VHDL components descriptions available in FPGA and ASIC libraries. For example, the |SM| processing
element l863a of Figure 18c may be constructed using commonly available“binary” library components as shown in the schematic of Figure 19. (At the time of this
writing, modular library components are not readily available, so that virtually all FPGA and ASIC libraries consist of only“binary” components.)
[0278] In Figure 19, an example implementation of the |SM| processing element l863b of Figure l8c is shown as a pipelined |SM| processing element enclosed within
dotted circle l863c. Figure 19 shows each pipeline stage is designated using stage plaques, such as SFTB-STAGE 1 plaque 1901. The stages are labeled as“sub-stage” to
differentiate them from the primary pipeline cycle stages introduced earlier, but this designation is arbitrary. At SFTB-STAGE 1, 1901, the incoming digit value A, l862c, is
subtracted from the incoming digit value R, l86lc using two subtractors, 1911, 1912. The result of the subtraction which is positive is passed to register reg5 1913 via
selection circuit 1910. The subtraction is modular, so that a“wrap-under” is detected and corrected in circuit section 1910. Note the modular subtraction is limited in
range such that the digit A is generally restricted to be less than the modulus M 1905 of digit R l86lc.
[0279] Because of the range restriction of the modular subtraction of a basic |SM| element, mixed- radix conversion is typically performed on the least value digit
modulus Mi of digit Ri first and proceeds with successively larger value digit modulus up to the“highest value” modulus Ms of digit Rs of the normalization pipeline 1830
of Figure l8b. In the sample embodiment of the TPU invention, the hardware pipeline 1830 organizes digit modulus Mi through Ms from least value to most value 1800
corresponding to the sample RNS modulus set of Figure 3c such that the first RNS digit converted by mixed-radix conversion pipeline 1830 starts with Ri and progresses
digit- by-digit at each STAGE of the pipeline until digit Rs is processed at STAGE 7 1807.
[0280] It should be noted that other embodiments of the RNS to mixed-radix converter 1830 may perform mixed-radix conversion on digits of varying size and sequence,
such that modular subtraction comprising a subtractor circuit 1900 of Figure 19 is generally not sufficient.
[0281] With the modular subtraction complete, the |SM| processing element l863c of Figure 19 multiplies the result of the modular subtraction 1913 with a pre-set
constant designated as M 1. One method for performing the modular multiplication by a constant value M 1 is to use a look-up table (LUT), or alternatively a series of
smaller LUTs, such as LUT1, LUT2, and LUT3 as shown enclosed in circuit section 1915. Each LUT1, LUT2, LUT3 of section 1915 translates the input value to be the
product of the input value 1916 with the constant M 1 and reduced modulo Mu In a typical embodiment, the input value data bits 1916 are split between separate
memory devices as shown by the 18-bit address bus 1916 divided into three buses of 6-bit address bits each, since it is only the significance of the input (address) bits at
each LUT that are translated to a product by a constant M 1, and reduced modulo nn. Therefore, as additional LUTs are used to perform the modular multiplication,
additional values modulo nn must be summed and further reduced modulo mu
[0282] Equations to fill each LUT of circuit section 1915 with the required data patterns for modular multiplication by a constant M 1 are disclosed in equations (40), (41)
and (42),
LUTl( c) = ((JC & 0x3f) * M 1) % mi (40)
LUT2(JC) = ((( & 0x3 f) « 6) * M 1) % m, (41)
LUT3(JC) = (((JC & 0x3 f) « 12) * M 1) % m, (42)
[0283] The tradeoff of using more LUTs results in less memory bits required but at the cost of more modular results to sum and then reduce. These tradeoffs are known
by skilled modular circuit designers and may be made without affecting the invention at hand. Lor example, in the circuit of Ligure 19 the output of three LUTs are shown
feeding a 3 -input adder 1917 in circuit section 1920. The final modular result of a modular multiplication is restricted to be less than the digit modulus mu but cannot be
so restricted when using more than LUT since it is possible the sum of more than one value modulo mi may exceed the value mi. This problem is solved using the circuit
section 1920 where the result of the sum of three values modulo mi is further reduced using LUT4 1918 and reg6 1919 at sub-stage 4, 1904. In the circuit section 1920,
the LUT4 1918 translates the upper 4 bits of the summation 1917 modulo mi thereby producing a value modulo mu and this value is summed to the least significant
sixteen bits of the summation 1917 using circuit section 1925.
[0284] The summation of LUT4 1918 and reg6 1919 is a modular sum, so that circuit section 1925 provides addition modulo mu using constant mi 1926. The circuit of
Ligure 19 handles a single digit modulus mi so the constant mi 1926 matches the constant mi for programming LUT entries using equations (40), (41), an (42). In circuit
section 1925, two summations are made, one using a two-input adder 1928 and the other a three-input adder 1927 with the third input of adder 1927 assigned to the
negative value of the digit modulus mi 1926. Using the sign of the result of adder 1927, the selector unit 1930 steers the correct modular result to be stored in output
register R l864c. In other words, if the result of adder 1927 is positive it is the correct result, otherwise, the correct result is sourced from adder 1928, passed by selector
1930 and stored in output register R l864c.
[0285] Another common processing element comprising the normalization pipeline unit 455c of Figure 18b is a“modular multiply then add” PE element 1822 denoted
using the |MA| symbol. The |MA| processing element is also shown in Figure l8d as a trapezoidal symbol l873a designated with the |MA| symbol. Additional detail of an
equivalent |MA| processing element is enclosed within dotted lines l873b which shows a modular multiplier 1876, modular adder 1877 and multiplier constant 1875. The
functional equation for the modular multiply-add |MA| processing element l873a is enclosed in dotted lines 1870. In practice, the multiply-add processing element l873b
performs modular multiplication of an incoming digit A l872b with a preset constant value P 1875 using modular multiplier 1876, and then performs a modular addition
of the modular product with the incoming value R 187 lb using modular adder 1877.
[0286] The modular multiply-add function 1870 is typically used to reconvert mixed-radix digits back to RNS format, therefore, |MA| processing elements appear in the
mixed-radix to RNS conversion pipeline 1820 of Figure l8b. One method used for reconversion is to compute the respective weight, or“positional significance” of each
mixed-radix digit and sum all computed values. In a fixed-radix number system like decimal, we often cite a digit having a“positional power”, but mathematically speaking,
the mixed-radix number is not a series of powers but a series of products, so more specifically each mixed-radix digit has an associated“positional product” which is
designated by the constant P 1875. The constant P 1875 of Figure l8d is generally different in value for each |MA| processing element of pipeline 1820 since the digit
modulus is different for each digit column in a pipeline, and since the positional power at each pipeline STAGE is different.
[0287] In Figure l8b, the |MA| processing element 1822 receives a residue digit from a previous stage digit storage register b4 1821 and adds to it the modular product of
the weighted mixed-radix digit received via bus 1823 and a constant value representing the digit power, such as constant P 1875 of Figure l8d. During the re-combination
of mixed-radix“weighted powers” into RNS, certain digit powers may be zero in specific locations of the converter pipeline 1820, and therefore the |MA| processing
element is replaced with a delay line, such as delay line 1824. The delay line 1824 is required to maintain synchronization between digits needing processing by a |MA|
processing element, and those digits only requiring delay storage.

https://patents.google.com/patent/WO2019213070A1/en?q=(high+radix)&oq=high+radix

21/38

2/14/26, 1:57 PM

WO2019213070A1 - Systems and methods for matrix multiplication, normalization, and reverse conversion - Google Patents

[0288] In one embodiment of the present invention, the multiply-add |MA| processing element l873b of Figure l8d is synthesized using of the schematic of the |MA|
processing element l873c of Figure 20. Figure 20 shows a detailed schematic block diagram of a pipelined multiply-add |MA| processing element constructed from
standard binary library components and suitable for the example TPU apparatus of the present invention. Figure 20a shows the |MA| processing unit divided into a
plurality of sub-stages, such as SUB-STAGE 1, 2001, to help clarify the operation and data flow of the pipeline l873c.
[0289] In Figure 20, the value of the input digit A l872c is multiplied by a constant P using three LUTs LUT1, LUT2, LUT3 shown enclosed within dotted lines 2000. Any
number of LUTs may be used to perform the multiplication by a constant; for example, as many as eighteen l-bit memory tables, or as few as one LUT can be used. There
are subtle tradeoffs in selecting the number of LUTs known by those skilled in the art of modular circuit design. Less memory has a tradeoff of more summation terms,
while less LUTs result in less summation terms but at the expense of more LUT memory bits.
[0290] In the example |MA| circuit l873c of Figure 20, three LUTs are used to perform a modular multiplication by a constant value P. The input operand A l872c is 18 bits
wide which is partitioned into three 6-bit buses, each bus representing a portion of the total value to be multiplied by constant P. The product of P and the equivalent
value of the 6-bit bus is pre-computed modulo rrn and stored into the associated LUT memory location. Equations which define the data patterns in each LUT enclosed in
dotted lines 2000 is given by means of example as,
LUTl( c) = ((a: & 0x3f) * P) % rrn (43)
LUT2G) = ((( & 0x3 f) « 6) * P) % rrn (44)
LUT3(JC) = (((JC & 0x3 f) « 12) * P ) % « (45)
[0291] Because three LUTs are used, the final answer is first derived by taking the sum of the three modulo results using the 3-input adder circuit 2010 and storing the
result in reg3 2011. The |MA| processing element l873c must also perform a modular addition with the additive operand R l87lc. The operand R l87lc may be delayed
using one or more registers, such as register 2013,
2014, so that operand R is staged in synchronization with the associated sum reg3 2011 to be summed using adder circuit 2015 and stored in register reg4 2016. The
value stored in reg4 2016 is congruent to the desired modular digit result referred for this discussion as‘X’ and is 20 bits wide. To reduce the congruent value X farther,
reduction circuit 2020, 2025 and selector circuit 2030 is included. The operation of the reduction circuit 2020 and 2025 is like the reduction circuit 1920 and 1925 of
Figure 19 respectively and will not be explained here. The output of selector circuit 2030 sends the modular result to the output register R l874c of the multiply-add
processing element l873c by means of example.
[0292] The normalization pipeline 455c of Figure l8b is also comprised of a pipelined comparison unit 1810 which in turn is comprised of a series of comparison
processing elements denoted with a“C” and a series of sign data storage registers denoted with an“s”, ex., such as comparison processing element 1811 and sign
storage register 1813. The RNS to mixed-radix converter pipeline circuit 1830 produces a mixed -radix digit at each pipeline stage, such as pipeline STAGE 1 1801 and
transmits each mixed-radix digit via a series of mixed-radix digit buses, such as bus 1818. The pipelined circuit produces a mixed -radix digit at each digit bus on every
clock cycle in one embodiment. For example, at the start 1800 of the pipeline 1830, a mixed radix digit (equivalent to RNS digit Ri ) is transmitted on digit bus 1818 which
is connected to comparison processing element 1811. As an RNS value progresses through each STAGE 1 1801 through STAGE 7 1807 of the mixed-radix conversion
pipeline 1830, a mixed-radix digit is generated and transmitted via a mixed-radix digit bus and connected to a corresponding comparator processing element. In this
manner, an RNS value 1800 is converted to mixed-radix format and is simultaneously compared to one or more constants in a typical embodiment of the normalize
pipeline 455c of Figure l8b of the present invention.
[0293] One reason to compare the RNS value processed in the mixed radix pipeline section 1830 of Figure l8b is to determine if a round-up is to be applied to the result
1850. In one embodiment of the present invention, a round-up unit stored in register n through rg is added to the result 1850 if the fractional digits are greater than or
equal to half the fractional range, RF, as described by equation (39). In the example apparatus of Figure l8b, the first two mixed-radix digits generated by pipeline unit
1830 comprise the value of the fractional range, RF, since two digits are assigned to the fractional range in this example which supports the example number system of
Figure 3 a.
[0294] In Figure l8b, the first mixed radix digit is sourced as the initial Ri digit register 1826 and transmitted via bus 1818 to comparator 1811. The comparator 1811
compares the value of the mixed -radix digit to a constant digit C and the result is stored in sign bit register storage s 1813. The constants stored in each comparator
processing element C are mixed-radix digit values which represent a constant value organized least significant digit first starting with comparator 1811 and progressing
through each STAGE of the comparator pipeline 1810 until comparator 1833.
[0295] The comparator unit l883a, 1883b of Figure l8e may be implemented using standard binary components and using the schematic of comparator processing unit
l883c of Figure 21. In Figure 21, the comparator processing unit l883c is comprised of a digit value input A l882c, and a prior stage sign state input l88lc and produces a
sign state output S l884c. The sign state is arbitrarily encoded, however, in one embodiment, two bits are used to encode up to three sign states using logic function
1885, Figure l8e, i.e., a greater than state is encoded to a binary 2, a lesser than state is encoded to a binary zero, and an equal state is encoded to a binary one. Using this
two-bit encoding, an equal state can be detected which is mapped to a negative value, or alternatively, to an error condition. [0296] The comparator processing pipeline
1810 of Figure l8a is designed to compare least significant digits first; as a consequence, the comparator processing unit l883c of Figure 21 will “over- write” the sign
state calculated by a prior stage comparison unless the digit comparison result is“equal”. If the comparison of the input value A l882c is equal to constant C 2104, the
selector circuit 2120 passes the prior stage sign state reg3 2112 to output sign state register S l884c. Otherwise, if the comparator output 2113 is not an“equal” result,
i.e., the comparator 2111 result is greater than or lesser than, the result of the comparison 2111 is passed to output sign state l884c via selector 2121. For the first
comparator element 1811 of Figure l8b, the prior sign stage is set to an equal sign state as indicated by the“=” constant symbol 1825. The digit comparison element
l883c may also include a delay circuit 2100 to delay the value of the input digit A l882c and prior sign state l88lc to coincide with the result of other processing elements
within the normalization pipeline 455c of Figure l8b.
[0297] Continuing the disclosure and discussion of Figure l8b, at pipeline STAGE 1 1801 the second mixed-radix digit is sourced from register <¾ 1817 and is transmitted
via digit bus 1815 to comparator processing element 1812. If the digits are equal the result of the prior comparator processing element 1811 stored in register 1813 is
passed to the next sign bit storage register at pipeline STAGE 2 1802. In the example of the present invention, the result of the comparison of two mixed-radix digits
produces a rounding bit, or round-up unit ru, which is transmitted from comparison processing element 1812 via round-up bus 1814 to each digit n through rg of the initial
value stage of recombination pipeline 1820 (STAGE 2, 1802) of the mixed-radix to RNS conversion pipeline 1820.
[0298] Because of the method used to recombine and convert mixed-radix digits back to an RNS value within the pipeline section 1820 of the present example, the carry
unit may be added at the initial cycle at pipeline STAGE 2 1802 . Other embodiments may add the carry unit at the final stage, or at some other convenient stage of the
normalization pipeline unit 455c. However, the initial STAGE 2, 1802, as shown, is convenient and represents optimal distance in many cases.
[0299] Many details are left out of Figure l8b, such as register to register data flows that may help in re-timing, etc. These advanced topics, such as signal fan-out and bus
loading are known to those skilled in the art of circuit design.

https://patents.google.com/patent/WO2019213070A1/en?q=(high+radix)&oq=high+radix

22/38

2/14/26, 1:57 PM

WO2019213070A1 - Systems and methods for matrix multiplication, normalization, and reverse conversion - Google Patents

[0300] Another reason to perform a comparison of the RNS value undergoing mixed-radix conversion (MRC) in pipeline section 1830 is to determine the sign polarity of
the value. To perform this task, the comparison must include all digits of the value, which is why the comparison pipeline unit 1810 extends downwards to compare each
mixed-radix digit as they are generated, including the first mixed radix digit Ri l826 and progressing to the last mixed radix digit as 1834 transmitted via bus 1832
connected to the last comparator element 1833. The result of the complete comparison is stored in sign storage register 1831. In some embodiments not shown, the
sign result is converted to“sign bits” associated with or stored alongside the result 1850, i.e., to support encoding of sign magnitude notation for output 1850.
[0301] Some details of the design of comparison pipeline unit 1810 may differ depending on several factors, one factor being the chosen number system modulus. For
example, in many cases, the same digit constants used to compare half the fractional range RF are used during the comparison of half the total range RT. In some cases,
if a modulus based on a power of two is implemented as the final fractional digit, a single high-order bit of the mixed-radix digit may be used to indicate round-up, and the
comparator processing elements used for fractional digit comparison, such as comparator element 1811, are removed. Note, however, the sign comparators generally
remain if fully signed normalization is supported.
[0302] The normalization pipeline 455c of Figure l8b. is a preferred embodiment and includes advanced features, such as negative value correction circuitry 1840 which
allows the normalization unit 455c to support a single pass mixed-radix conversion cycle regardless of the sign of the intermediate product 1800. (Note that negative
correction circuitry is not to be confused with arithmetic complement circuitry). The negative value correction circuitry 1840 is activated when the final sign range
comparison state 1831 indicates the magnitude of the converted RNS value 1800 exceeds the end of the positive number range according to the sign(x) function of
equation (26). In this case, the correction constant equal to the whole range Rw 1842 is subtracted from the fractional range digits Ri, R2 using a modular subtractor, such
as modular subtractor 1844 and the result is stored in the final output register 1850 for further processing. If the converted RNS value 1800 is positive, the sign range
comparison state 1831 indicates a positive value so that the correction circuit 1840 is not activated, i.e., the correction circuit 1840 subtracts the value zero 1841 from
the result stored in output register 1850.
[0303] Note that since the correction constant Rw 1842 is a product of all whole digit moduli, the RNS digit values associated with the whole range of the constant Rw are
zero; therefore, there are no modular subtractors required for whole digits within the correction unit 1840.
[0304] Figure 18f shows a data dump table of the normalization pipeline 455c of Figure l8b for a positive input value 1800 which is held stable at the input 1800 for a
period of 9 pipeline stages. Holding the input data to a pipeline unit provides a means to see the progression of a single data value as it is processed at each stage of a
pipeline. In actual applications, each data value at each stage of a pipeline is independent in the methods disclosed herein. At the start row 2260 of the table of Figure
22b, the intermediate product of the fixed-point value“3.14159265” and the value “2.71828182” which is expressed in RNS format in the starting row is the number
895456035220579262479io. The intermediate product number doesn’t mean much in a casual glance, but this value is normalized to a smaller RNS integer 1896 which
represents the correct answer“8.53973422” in RNS fixed-point format as shown by the normalized RNS value of Row 384a of Figure 3c.
[0305] Improved and Streamlined Normalization Pipeline
[0306] An alternate embodiment and a further streamlined version of the normalization pipeline unit 455c of Figure l8b is shown as the normalization pipeline unit 455d
of Figure 22a.
[0307] An overview of the normalization pipeline unit 2200 of Figure 22a. is as follows. The non- normalized input 2200 is coupled to an RNS to mixed-radix converter
pipeline 2220. The mixed- radix converter 2220 generates mixed-radix digits at each stage of the pipeline, such as at pipeline stage 2 designated by the STAGE 2 plaque
2203 and dotted line. Mixed-radix digits are transmitted from the mixed-radix converter pipeline 2220 to a pipelined comparison unit 2212 using digit buses, such as digit
bus 2221 and digit bus 2222. The mixed-radix digit buses are also connected to a mixed-radix to RNS recombination unit 2230. Note the mixed-radix to RNS
recombination unit 2230 of Figure 22a is significantly smaller than the mixed-radix to RNS recombination unit 1820 since the mixed-radix to RNS converter 2230 only
includes support for recombination of the fractional digits (n, rz) of the RNS fixed point number system.
[0308] In Figure 22a, the reconversion of the whole digits of the RNS fractional number R3 through Rs is not required by noting the pipeline storage registers a3 through a8
of the mixed-radix converter 2220 at pipeline STAGE 2, 2203, represent the final reconverted RNS digits R3 through Rs 2250 but without rounding added. The reason is the
normalized result represents a division by the fractional range RF, according to equation (15); therefore, after“dividing out” both fractional digit modulus during mixedradix conversion, the remaining RNS digits as through as of pipeline STAGE 2, 2203, represent the desired whole RNS digits of the answer but without any rounding
applied. In terms of RNS terminology, therefore only the fractional digits n, rz are required to be “base extended” using the mixed-radix to RNS digit recombination pipeline
2230.
[0309] The streamline circuit of Figure 22a adds a rounding value to each whole digit of stage 2, 2203, using a modular adder, such as modular adder (or modular
increment unit) denoted by a trapezoid with a |+l| designation 2223, and stores each modular sum to storage registers R3 2225 through Rs 2226 of STAGE 3, 2204. As
shown each modular adder |+l| will add a single rounding unit if the rounding comparison result storage u 2224 is true. Storage registers R3 through Rs of pipeline STAGE
3, 2204, through STAGE 7, 2208, route the RNS digits from STAGE 2, 2203 to the final stage registers 2240, at STAGE 8 2209 and generally serve to delay RNS digits to
maintain synchronization with other associated data in the pipeline. [0310] The normalization pipeline 455d of Figure 22a supports an RNS number system as shown in
the example number system of Figure 3c wherein two digits, Ri, R2 are assigned to the fractional range 370. As previously stated, each stage of the mixed-radix converter
unit 2220 produces a mixed -radix digit which is transmitted to each stage of the mixed-radix to RNS recombination pipeline 2230; each digit is converted to an equivalent
RNS value and summed with the prior stage digit value of the same digit modulus (column). This recombination of mixed-radix digits back into RNS digit data is
performed using multiply-add |MA| processing elements at each stage of the recombination pipeline 2230, such as |MA| processing elements 2231, 2232.
[0311] A normalized RNS result is produced by the normalization pipeline 455d of Figure 22a at pipeline STAGE 8, 2209, and is stored in digit register n 2242 through rg
2244 of result register 2240. A sign state associated with the result 2240 is stored in sign polarity state storage register s 2241 and is used to control the correction value
passed by selector 2246 to modular subtract processing elements 2247, 2248. If the processed value 2240 is a positive value as indicated by sign stage 2241, a zero
value is passed by selector 2246 which is subtracted from the result before the result is stored in final result register 2250; thus, positive values are not affected by the
correction circuit. Alternatively, if the sign state register 2241 indicates a negative value, the selector 2246 passes a correction value Rw, 2249, which is subtracted from
the register result 2240 before the result is stored in the final register 2250. Note because the correction constant Rw has zeroes in all whole digit positions R3 through
Rg, there is no requirement for a modular subtraction unit |S| for whole digits in correction processing stage 2245.
[0312] The normalization pipeline 455d of Figure 22a requires less than 30% of the resources versus the normalization pipeline unit 455b of Figure l7b obtained by“unrolling” or“pipelining” prior art methods. The significant reduction of circuitry results in better overall efficiency and speed. Furthermore, the normalization pipeline circuit
of Figure 22a is implemented using standard binary logic and requires only general-purpose logic resources in FPGA devices, such as 6-input LUTs (MLABs), arithmetic
logic modules (ALMs) and Logic Elements (LEs). This is important, since“hard IP resources” such as hardwired 18x18 binary multipliers within FPGA devices are reserved
for use within the matrix multiplier 400a of Figure 4.
[0313] It should be noted the resource requirements of the streamlined product normalization pipeline 455d of Figure 22a provides an excellent balance for modern RNS
based computation since it takes relatively few matrix multipliers to make up for the“cost” of the array of modular multipliers within pipeline elements such as the |SM|

https://patents.google.com/patent/WO2019213070A1/en?q=(high+radix)&oq=high+radix

23/38

2/14/26, 1:57 PM

WO2019213070A1 - Systems and methods for matrix multiplication, normalization, and reverse conversion - Google Patents

and |MA| elements; one reason is the normalizer pipeline multipliers are of the “multiply by a constant” variety, and they are significantly smaller in IC area than an
arbitrary binary or modular multiplier needed for the matrix multiplier unit 400. A second reason is the normalization unit 455d pipeline depth and digit input- width are
independent of the matrix row and column dimensions.
[0314] In Figure 22b., an arithmetic example of the operation of normalization pipeline 455d of Figure 22a is provided. Because pipeline designs may operate on a
plurality of data at once, for analysis and illustration it is instructive to hold the input data to a single data value for all clock cycles, so that the propagation of data
derived from the same input can be seen at each stage of the pipeline. In actual operation, the data values at any instant in the pipeline will hold data that is generally unrelated in each stage of the pipeline.
[0315] The normalization pipeline arithmetic example of Figure 22b begins with an RNS input data value listed in the first row 2260 which is the example RNS value in row
383b of the sample data Table 360 of Figure 3c. This data value is the RNS (integer) product of data value “3.14159256” of Row 382 and data value“2.71828182” of Row
383a of Figure 3c. The product of Row 383b is herein called an“intermediate product” since it represents a value which must be normalized; the value is re-listed in Row
2260 in Figure 22b as the starting data value for the present example.
[0316] The RNS intermediate product value at row 2260 is processed at each stage of the normalization pipeline 455d corresponding to STAGE 1, 2261, through STAGE 9,
2269, of Figure 22b. In the present example of Figure 22c., the data result is determined to be positive by the sign comparator pipeline result column 2074 which
indicates a“lesser than” sign at STAGE 8 row 2268. Because the positive result does not require a correction, the processed data at STAGE 8 2268 is passed to the final
STAGE 9, 2269, without correction applied. Since correction only affects fractional digits Ri of column 2275 and R2 of column 2276, the resulting fractional digits of
STAGE 8, 2268 are passed unchanged to STAGE 9, 2269, as a final fractional digit result 2079. The final fractional digits 2279 is part of the overall RNS answer when
combined with the final whole digit result 2277.
[0317] In Figure 22b., during processing of the fractional digits Ri, column 2275, and R2 column 2276, at row 2262 of STAGE 2, a value of“one” is generated by the
rounding comparator which is stored as the initial starting values 2080 for fractional digit Ri and R2 at row 2063 of STAGE 2. In some embodiments, a sign flag
associated with the sign of the normalized result 2277, 2279 may be stored based upon the sign comparison result 2278.
[0318] In Figure 22c, the complement of the starting value of Figure 22b is used. The complement is the value that would be processed in the case of a negative answer.
For example, the product of 3.14159256 * -2.71828182 provides a negative intermediate product indicated by residue digits column Ri 2292 through column Rs 2293 and
is in Row 2282 labeled Start of Figure 22c. By means of example, data values for each stage of the pipeline processing is again indicated in each Row 2283 STAGE 1
through Row 2291 STAGE 9 with the assumption that the input data value of Row 2282 is held for 9 clocks. This allows the progression of a single processed value to be
tabulated as it progresses through the pipeline normalization unit 455d of Figure 22a.
[0319] In the example of Figure 22c, the negative value of the present example is processed using mixed radix conversion and has a different set of outcomes from the
example of Figure 22b; specifically, the rounding value for fractionally associated digits Ri and R2 2296 are set to zero at Row 2284, and the sign comparison result S of
column 2294 and Row 2290 STAGE 8 is“greater than” which indicates the processed value is negative. In Figure 22c, a negative result occurs because the mixed radix
digits generated by mixed radix converter 2220 of Figure 22a are treated as a mixed-radix number, and the comparison pipeline 2212 of Figure 22a determines this
number is greater than the mixed radix constants listed in column 2295, also treated as a mixed-radix number. Note the last residue digit Rs of Row 2289 STAGE 7
contains the value 0x3FFFA which is larger than the constant OxlFFFD, column 2295 of Row 2289, so the residue value reduced into mixed radix is larger than half the
total numeric range, RT, which indicates the value is negative according to equation (26).
[0320] Significant to the present invention is the data transition from row 2290 STAGE 8 to row 2291 STAGE 9 of Figure 22c, since a correction to the fractional digits Ri
and R2 of column 2296 is made before storing the fractional digits at STAGE 9, row 2291 if the result is negative. This is also shown in the circuit of Figure 22a; the
selector 2246 passes the whole range value Rw to the subtractors 2247, 2248 so that a modular subtraction of the value Rw is made to the STAGE 8 result 2209 registers
labeled n 2242 and r22243. Referring to Figure 22c, the“whole” digits of the result 2297 remain un-modified in the transition from STAGE 8 to STAGE 9 and were
transferred from STAGE 2 of row 2284 using pipeline registers in some embodiments to be latched as a final result, as indicated in the present example enclosed by
dotted lines 2297.
[0321] The novel inventions and apparatus of Figure 22a as further illustrated in examples of Figure 22b and 22c are numerous. For example, the normalization pipeline
455d of Figure 22a only performs mixed-radix to RNS recombination on fractionally associated digits; in contrast, the normalization unit 455c of Figure l8b performs RNS
recombination on all digits using recombination unit 1820, which requires more circuit resources. For an 8 -digit TPU design as shown herein for purposes of illustration,
there are only two factional digits Ri, R2 that must support re-combination of mixed -radix digits. Furthermore, RNS data representing negative values are directly
normalized by the normalization unit 455d of Figure 22a; direct normalization of negative values produces an incorrect result which is then“corrected” by subtracting a
suitable correction constant. This novel and innovative RNS feature means only one mixed -radix conversion unit or pipeline is required for high-speed operation
supporting both positive and negative numbers as direct input.
[0322] Base Extension Unit for Signed Operands
[0323] Referring to the matrix multiplier accelerator card 406 of Figure 4, a base extend unit 443a is shown between DDR4 DRAM memory 445 and High-Speed Memory
Interface 435. The base extend unit 443a is supported in some embodiments of the present invention to decompress, i.e., or base extend, the short word format RNS
data stored in memory and output a fully extended format RNS word required by the matrix multiplier 400a. The TPU design of the present invention may support any
number of digits for an extended RNS word, but the example used in the present disclosure supports 8 digits in a fully extended format, as shown in Figure 3a and 3b.
Without the use of base extend unit 443a, storing and retrieving extended RNS data from memory device 445, such as DDR4 DRAM in one embodiment, will require at
least twice as much memory storage, and twice as much energy compared with storing and retrieving“short” RNS format 351 data of Figure 3a consisting of only 2
fractional digits 370 and two whole digits 372 as shown in Figure 3b and by means of example.
[0324] The assumption for the operation of the base extend unit 443a is that RNS operands are sufficiently represented in a reduced digit format. This is to say that the
longer“extended” RNS format 350 of Figure 3a consists of one or more redundant digits 330, 340. Not shown in Figure 4 is that redundant digits of the extended RNS
data format are“truncated”, i.e. discarded, before the RNS value is stored to memory via memory bus 444. RNS digits are restored, or“base extended” when data is moved
from data memory 445 through base extend unit 443a; the base extend unit 443a outputs a fully extended RNS format which is moved to its intended location by the
memory data router 435.
[0325] In the example embodiment of the present invention, a base extend unit 453b of Figure 23a is shown as a working example for means of disclosing key elements
of the present invention. The base extend unit 453b of Figure 23 a is a pipelined unit and consists of a pipelined mixed-radix conversion unit 2310, a pipelined comparator
unit 2320, a pipelined mixed-radix to RNS recombination unit 2330, and a negative range correction unit 2345.
[0326] Short format RNS data is presented as input at register 2300 and is converted to mixed- radix in the converter unit 2310. At pipeline STAGE 2, 2302, the first mixedradix digit generated is transmitted via bus 2312 and is received into the recombination register 2331. As explained in similar circuits of this disclosure, the mixed-radix

https://patents.google.com/patent/WO2019213070A1/en?q=(high+radix)&oq=high+radix

24/38

2/14/26, 1:57 PM

WO2019213070A1 - Systems and methods for matrix multiplication, normalization, and reverse conversion - Google Patents

digits generated are simultaneously compared to a series of stored or hard- wired mixed-radix digits representing the sign range of the number format using comparator
pipeline 2320; the series of digit comparisons, such as comparison 2320, results in a sign determination of the value presented at register input 2350 at STAGE 5, 2306,
indicated by the state of the sign storage register S 2341. If the value detected by comparator unit 2320 is negative, the sign flag register S 2341 is set and directs
selector unit 2346 to gate the value of the quantity (RT-RS) 2349 to the modular adders, such as modular adder 2347, to correct for the proper offset for the negative
value range.
[0327] The base extend unit of Figure 23a is a novel invention. One property of RNS is the truncation of long format negative values to short format preserves the“method
of complements” encoding, albeit in a smaller range Rs. However, when base extending the short format value, the base extend cannot account for the loss of range in
returning to a two’s complement value with respect to the larger range RT of the extended RNS format. The base extend unit 453b applies the negative range correction
2349 when a short format value is detected to be negative. To perform this function, the base extend unit measures the sign of a short format value using a sign range
with respect to Rs, i.e., the comparator 2320 uses a constant Rs/2; the sign range Rs/2 is stored in mixed- radix format, typically in digit format, and applied as an
operand to comparators of pipeline unit 2320. In one embodiment, the mixed-radix range comparison digits (not shown) are stored within or alongside comparators of
the comparison pipeline 2320. When the value input at register 2300 is detected to be negative, an offset value of RT-RS is restored to the result 2340 using modular
adders 2347 thereby producing a corrected, base-extended negative result 2350, at final STAGE 6, 2306.
[0328] Equations for deriving the correction constant for the case of base extending a negative value in short fixed -point RNS format is,
RT - YX = CBE + (Rs— Uc) (46)
[0329] The base extend correction constant CBE is therefore,
CBE = RT ~ Rs (47)
[0330] In Figure 23b an example base extend operation is shown for purpose of disclosure. The value used in the present example is a short format fixed-point RNS value
of Row 384b of Figure 3c which is the same value of Row 384a except that only the first four digits are defined and is equivalent to the decimal value“8.53973422”. This
allows the short format fixed-point value to be stored in a memory location which is smaller than that required for the long format number of Row 384a of Figure 3c.
However, when the short format data value is moved from memory into the matrix multiplier unit 400a, it must be fully extended, i.e., all eight RNS digits must be valid in
this example.
[0331] The example value is four digits wide comprising digit Ri 2367 through R4 2368 and is held for 6 clocks at the input Stage 1 2360 of Figure 23b representing data
held at input register 2300 of Figure 23a. In this example case, holding the data for 6 pipeline stages allows the data progression of a single data value to be listed stage
by stage, such as STAGE 1 2360 through STAGE 6 2365. In actual operation, each stage of the base extend unit of Figure 23a may contain processed data derived from
different input values. During mixed radix conversion, the example value is determined to be less than the sign range constant indicated by the mixed-radix digits of
column 2370 during mixed radix conversion. During the same mixed radix conversion, the digits undergoing base extend are recombined in digit columns Rs 2371
through Rs 2372 of Figure 23b.
[0332] At STAGE 5 2364 of Figure 23b, the mixed-radix conversion and mixed-radix digit recombination are complete and sign flag S of column 2369 indicates a lesser
than state 2373. Because the value is positive, the extended digits Rs through Rs are correct, and are passed unchanged to STAGE 6 2365. Note the values of the digits
are correctly base extended according to the long format example value of Row 384a of Figure 3c.
[0333] In the example of Figure 23c, the complement of the starting value of Figure 23b is used. In this case, the first four digits of column Ri 2387 through column R4
2388 is equal to the first four digits of the complement of the value“8.53973422” of Row 384c of Figure 3c, which represents the decimal value“-8.53973422”. The issue
which arises is the result at STAGE 5 2384 is base extended, but only to the magnitude represented by the four digits alone. However, the sign flag S of column 2389 is
set to“greater than” (>) 2393 at STAGE 5 2384 which indicates the value is negative with respect to the“short” negative value range Rs/2.
[0334] The minimum range of the negative numbers of a fully extended data value starts at the value Rx/2, which is significantly greater than the maximum range of the
negative numbers of a short format data Rs, so that the value which is base extended using four digits cannot produce a value in the extended negative range. The
reason is the range information of the negative representation of the long format fixed-point RNS word was lost during truncation of the value to short format. To recover
the lost range information for the case of extending negative short format fixed-point value (or integer) value, the correction constant 2395 derived in equation (47) is
added to the result of STAGE 5 2384 since the sign flag S 2393 is set to greater than, and so a final result at STAGE 6 2394 is produced which is correctly offset to an
equivalent negative value relative to the long (extended) format range RT.
[0335] The base-extend unit 443a of Figure 4 in combination with the technique of storing short format RNS values to memory 445 provides a unique strategy in RNS
processor design, since it relieves the high storage requirement of fully extended values and preserves the complement of truncated RNS values during short format
storage and restores the complement after base extension to a fully extended value when the short format value is detected to be negative.
[0336] Forward conversion pipeline unit
[0337] Important to the present invention is the high-speed forward converter 44la of Figure 4. Many methods for forward conversion exist in the prior art of RNS
research, however, complete pipelined forward converters which handle signed fixed-point or floating point forward conversion is not known in the prior art and is
disclosed for the first time in this present invention.
[0338] In Figure 24, a block diagram for a forward converter 44lb is disclosed which converts a binary floating-point format value from register source 2405 to a signed
RNS fixed point value output to register destination 2460. IEEE standard floating-point formats are in common use and many Verilog and VHDL libraries exist to support
common FPU operations including format conversions. One common format conversion is a floating point to fixed point converter 2400 of Figure 24. The floating-point
to binary fixed -point conversion unit 2400 converts a binary floating point format and outputs a sign state signal 2401, a positive integer representing the whole portion
of the floating-point value via bus 2402, and a positive fraction representing the fractional portion of the floating-point value via bus 2403. In one embodiment, the sign
state transmitted via sign signal 2401 is latched into sign register 2404 and the signed whole integer and fractional binary values are latched into a register 2405 which
stores the fixed-point equivalent value of the floating point value in sign magnitude format.
[0339] A controller unit not shown synchronizes the latched sign data 2404 and the latched integer value 2406 and the latched fractional value 2407 by transmitting them
in tandem to sign delay unit 2410, forward integer conversion pipeline 2420a, and forward fractional conversion pipeline 2430a respectively. The sign state delay line
2410 is used to keep sign data for each processed data element in synchronization. The forward integer conversion pipeline 2420a converts a binary value input using
bus 2406 and outputs an RNS integer value at bus 2421 which is multiplied by the fractional range RF 2446 using modular multiplier 2440. The result of the modular
multiply unit 2440 is transmitted to modular adder 2450.

https://patents.google.com/patent/WO2019213070A1/en?q=(high+radix)&oq=high+radix

25/38

2/14/26, 1:57 PM

WO2019213070A1 - Systems and methods for matrix multiplication, normalization, and reverse conversion - Google Patents

[0340] The forward fractional conversion pipeline 2430a converts a binary fractional value input via bus 2407 and outputs an equivalent fractional RNS value via bus 2431
to modular adder 2445 where it is summed with the result of rounding function 2435 which in turn derives rounding data via the forward fractional conversion pipeline
2430a as shown in Figure 24. The output of modular adder 2445 is transmitted to be summed with the output of modular multiplier 2440 using modular adder 2450.
Modular adder 2450 performs a modular summation of the scaled whole RNS value and the rounded fractional RNS value in one embodiment of the present invention.
[0341] Continuing with Figure 24, the output 2451 of the modular adder 2450 represents a positive value since the binary input 2406, 2407 is always a positive value. If
the sign flag associated with a processed value 2451 indicates the value should be negative, a complement unit 2455 is enabled by the associated sign state via signal
path 2411 so that the positive value is complemented, and therefore encoded into a negative number suitable for RNS processing. In some embodiments, a sign flag is
also included with the fixed -point RNS output 2460 but is not shown in the block diagram of Figure 24.
[0342] In the forward conversion embodiment of Figure 24, a complement unit 2455 unit is used instead of a correction unit, since the value converted is always positive.
Therefore, there is little need for the negative value correction circuitry described in other circuits of the present invention. This is because floating point format is
typically encoded in a binary sign magnitude notation format.
[0343] However, in some cases it is desirable to convert two’s complement binary format directly into a signed RNS fixed-point format; one embodiment is disclosed in
block diagram of Figure 25. The forward converter of Figure 25 accepts two’s complement format directly, and if negative, applies a correction factor 2556 to adjust the
incorrectly processed negative values into correctly signed values in RNS.
[0344] The correction factor for fractionally associated digits is ascertained by considering the conversion of a positive binary fraction n of a binary fractional range 2N to
a positive residue fraction r of an RNS fractional range RF,
n_r
(48)
2N — Rp
[0345] Therefore, a positive residue fraction r is derived using,
(n * Rp )
r = (49)
2N
[0346] An issue occurs in the conversion equation (49) above when a negative value encoded using two’s complement is directly converted since the converted value is
not equal to the complement of the equation (49) since, ) * Rp
r = RT (n * RF , (2 N-n
2N ÷ 2N n > 0 (50) [0347] By solving for a forward fractional correction constant CFF (that will be subtracted) we have, n (n * RF) (2 N-n) * RF
K

T 2N - N LFF (51)

[0348] and reducing yields,

CFF = 1.0 = RF (53)
[0349] The final interpretation of the correction constant, CFF, in equation (53) above is that CFF is equal to -1.0, which is equivalent to subtracting the fractional range RF,
according to the definition of the value 1.0 in Row 385 of Figure 3b. This interpretation accounts for the fact the modulo RT function in equation (52) is equivalent to
under- flow of the machine word (i.e., word wrap-under) and so is removed by convenience of notation in equation (53). In other words, a modular subtraction of the value
RF will invoke an under-flow of the machine word as it normally does.
[0350] The forward converter of Figure 25 uses two conversion pipelines in tandem, an integer conversion pipeline 2520, and a fractional converter pipeline 2530. This
provides more speed than the use of a single converter pipeline which attempts a full conversion of a binary value having both an integer and fractional quantity. If the
binary input value is a positive quantity, the forward integer conversion pipeline converts the magnitude of the positive value and converts it to the same magnitude value
in RNS which is correct. However, if the binary input value is a negative quantity, the forward integer conversion pipeline 2520 converts the magnitude of the two’s
complement of the integer portion, which is not the correct negative representation in RNS.
[0351] To correct for the case of directly converting negative values, a forward integer correction constant CFI is derived for the integer portion 2502 of the two’s
complement value 2501 of Figure 25. To derive this constant, it is necessary to understand the binary ranges of the two’ s complement fixed-point value of the binary
source operand 2501 ; the binary integer range (2l) and binary fractional range (2 ) of the source operand are expressed as a product of the total range as,

[0352] It can be shown that if the input integer value 2502 is obtained by truncation of the fractional bits as suggested in Figure 25, the negative integer value is encoded
in a two’s complement value with respect to the integer range (2l), and not the total range. Therefore, when converting a negative encoded integer value k, 2502, the
forward integer conversion pipeline 2520 converts an offset value, which can be corrected by subtracting the offset value. By observing the complement and its positive
value should cancel to zero, and by observing the correction constant is negative if it is to be added to correct the incorrectly converted answer, the correction constant
CFI, can be calculated,
CFI = (2i - k) + k (55)
CFI = 2l (56)
[0353] Once the negative encoded value of k, i.e. (2L— /c), is converted to RNS, the equation for applying the correction constant in RNS to provide a correctly encoded
negative value in RNS accounts for the underflow in the RNS system, so that the subtraction is performed in RNS using a modular subtraction,

https://patents.google.com/patent/WO2019213070A1/en?q=(high+radix)&oq=high+radix

26/38

2/14/26, 1:57 PM

WO2019213070A1 - Systems and methods for matrix multiplication, normalization, and reverse conversion - Google Patents

[0354] In other words, a modular subtraction of the correction constant CFI results in an underflow with respect to the RNS word size, which is RT, and this operation
corrects the value of the incorrectly converted negative integer to a properly encoded negative value in RNS.
[0355] The forward converter apparatus of Figure 25 converts both the fractional portion and integer portion of the fixed-point input value 2501 which is summed using
modular adder 2550. In one embodiment of the present invention shown in Figure 25, the integer conversion constant CFI and the fractional conversion constant CFF are
combined. Because the converted integer value is scaled by the fractional range RF using modular multiplier 2540, the integer conversion constant for correcting negative
values is also scaled by RF, SO that a total forward correction constant CFT 2556 is subtracted using a single modular subtractor 2560 from the sum of the improperly
converted negative fractional value 2546 and the improperly converted and scaled negative value of the integer value 2541,
CFT = CFF T R * CFI (58)
CFT = (2l + 1 ) * RF (59) [0356] One embodiment for the forward integer conversion pipeline 2520b of Figure 25 is shown using the forward integer conversion pipeline
2520c of Figure 26. In Figure 26, an input binary integer value 2600 is stored and partitioned into a plurality of binary digits Bo 2606 through B3 2609 with each digit
typically associated to a binary radix; for example, each of the four digits Bo 2606 through B3 2609 in one embodiment will support a l6-bit encoding, i.e., or binary radix
of 216, which allows for a total binary integer conversion range of 264, and a signed integer range of 263. In each stage of the integer conversion pipeline 2420c an input
binary digit is converted to a series of RNS digits, such as RNS digits Ri through R6, using a plurality of RNS digit recombination units, such as RNS digit recombination
unit 2620.
[0357] For example, at STAGE 1 2601 of Figure 26, the most significant binary digit B3 is transmitted via bus 2610 to the input stage of each of the RNS digit
recombination units Ri 2611 through R6 2612. At each subsequent stage of the forward integer conversion pipeline 2420c, the most significant binary digit remaining is
added to the current summation of the residue digits after the summation is scaled by a binary range representing the width of the binary digits Bi 2606 through B3 2609.
In the next stage, at STAGE 2, 2602, of the pipeline 2420c, the binary digit B2 2625 is transferred to the carry-in registers of each residue recombination digit unit. Each
residue recombination unit, such as recombination unit 2620, multiplies the residue digit R at STAGE 1 by a modular constant representing the binary range of the
previous binary digit modulo m, such as binary digit range constant |2n| 2621, where n represents the binary width of a single binary digit in this context. In a typical
embodiment, each binary digit B3 2609 through Bo 2606 is the same width, so the constants are the same for each stage of the conversion unit for each digit modulus;
however, the invention is not restricted to having binary digits of the same width.
[0358] A forward fractional conversion pipeline unit 2530b of Figure 25 is shown. The forward fractional conversion pipeline 2530b converts an incoming binary fractional
value 2503 to an RNS fractional value 2531. The converter 2530b performs two types of conversions, a format conversion, which is converting binary format to RNS
format, and a fractional range conversion, which converts the binary value represented in a binary fractional space to an RNS value in an RNS fractional space.
[0359] The‘fractional space’ may be fixed in some embodiments or may be adjustable, i.e. dynamic, in other embodiments. For means of disclosure, it is easiest to
describe the operation of converter 2530b using a fixed binary input fractional space, and a fixed output RNS fractional space. It is noted that circuit details of a dynamic
fractional range converter are not included in this disclosure but are understood by those skilled in the art of modular circuit design, and further those designers that
understand the fixed range conversion operation of fractional converter pipeline 2530b of the present invention. [0360] Figure 27a discloses details of an example
forward fractional conversion pipeline 2430c which may be used in a full forward conversion unit 44 lb, 44 lc to perform the function of forward fractional conversion
pipeline 2430a, 2430b respectively. In Figure 27a, a fractional binary input 2700 is sub-divided into a plurality of binary digits Bo 2711 through B3 2712. In the context of
the example, each binary digit Bo 2711 through B3 2712 is 8-bits wide for a total of 32 bits of binary fractional range; one way to describe the binary fractional range is
through the definition of its fractional unit,
1
binary fractional unit =— =
2 (4*8) 232 (60)
[0361] The fractional range is always expressed as“1.0”, and a binary fractional value within a fractional range is any fractional value expressed where the numerator can
take a value between zero and (2,4“S| - 1) and the denominator is fixed at a value of 232 in the example apparatus.
[0362] In the Figure 27a, the forward fractional converter pipeline consists of a pipelined binary fraction to RNS fractional space converter unit 2720 coupled to an RNS
digit recombination unit 2750 comprising a plurality of modular RNS digit recombination units, such as digit recombination unit 2755. The output of the plurality of RNS
digit recombination units 2780 comprising digits Ri 2781 through R4 2784 latch a converted binary fractional value at 2700 in RNS format and after nine stages of
pipeline processing, each stage designated in Figure 27a such as STAGE 1, 2702. The RNS fractional range is different than the binary fractional range, and may be
expressed using the smallest unit of fractional measure called“ ump or unit of most precision,

[0363] In general terms, fractional range conversion is facilitated by finding a suitable residue number, r, which completes an approximation of the binary fraction
represented by n, so that, r _ n
(62)
Rp— 2N
[0364] So that an RNS machine word value r which approximates a fractional binary quantity n is calculated as,

[0365] The fractional space conversion unit 2720 of Figure 27a processes the input value n of equation (63) at input port 2700 and multiplies the value n by a plurality of
modulus Mi 2721, M2 2723, M3 2724, and M4 2725 as the value proceeds through the pipeline. For example, the binary digit Bo 2711 is multiplied using multiplier 2726 by
the modulus value Mi 2721 and the result is added to value zero 2722 during the processing of STAGE 1, 2702. In a typical embodiment, the adder 2727 is removed and

https://patents.google.com/patent/WO2019213070A1/en?q=(high+radix)&oq=high+radix

27/38

2/14/26, 1:57 PM

WO2019213070A1 - Systems and methods for matrix multiplication, normalization, and reverse conversion - Google Patents

replaced with a bus wire but is shown for purposes of symmetry since the carry in stage 2722 will not be zero in subsequent binary digit stages, such as carry in register
Ci, 2729. In fact, the high order result from the multiplier 2726 is routed to the carry register Ci 2729 of the next digit processing column, i.e. processing of the Bi digit
2732 whereas the digit value contained in register Bi 2732 is multiplied by the modulus value Mi 2728 and summed with the value of carry register Ci 2729.
[0366] The multiply-sum ladder network of the fractional space conversion unit 2720 employs binary multipliers and binary adders as opposed to modular multipliers and
modular adders. The function of the binary multiplier-adder network 2720 is to simultaneously multiply a binary fractional value 2700 by each modulus Mi 2721 through
M4 2735 at each stage of the pipeline 2720 thus producing a binary product, while also dividing the binary product by the binary fractional range 2N as expressed in
equation (63). Division by the binary fractional range 2N is accomplished by the novel pipeline circuit 2720 by successive shifting of the binary product left by exactly the
number of binary digits representing N bits, which is N=32 bits, the number of bits represented by the four binary digits Bo 2711 through B3 2712 by means of the present
example.
[0367] The output of the fractional space conversion unit 2720 is a plurality of digit values transmitted via digit value buses 2756, 2757, 2758, 2759, to RNS digit
recombination pipeline 2750. The RNS word recombination pipeline 2750 may consist of any number of individual digit recombination circuits, such as individual digit
recombination circuit 2755, so that a completely extended RNS word size is supported, or so that a short format word is supported. In the example of Figure 27a, four
fractionally associated RNS digits Ri 2781 through R4 2784 is shown as an output result 2780 of the fractional conversion pipeline 2430c for means of brevity of the
illustration. It is understood that additional RNS digits are supported by employing additional individual RNS digit recombination units 2755, each unit associated with
and supporting a distinct pair-wise prime modulus, m,.
[0368] For the example system of Figure 27a, four moduli Mi 365 (=0x1312D), M2 366 (=0x20000) M3 367 (=0x2B3FB), M4 368 (=0x3FFDF) of Figure 3c are associated
with an RNS fractional range which differs from the example system of Figure 3c. In the example of Figure 27a, instead of two RNS fractionally associated digits, the
example uses four fractionally associated RNS digits to illustrate the conversion circuit 2430c capabilities in more detail, and to explain the effects of converting and
rounding in cases where the binary and RNS fractional ranges differ significantly. Moreover, in some embodiments the quantity of binary input digits 2700 and RNS
output digits 2780 is equal, and the encoding width of binary digits and RNS digits is equal or approximately equal. These embodiments provide a balanced circuit so that
multipliers and adders have operands of approximately the same binary encoding width.
[0369] In Figure 27b, an example conversion problem is provided which corresponds to the circuit of Figure 27a. In Figure 27b, an example binary fractional
value“0.14159265” is represented by a binary integer (hex) number 0x243F6A88 since the binary fractional range is 0x100000000, and since 0x243F6A88 divided by
0x100000000 is 0.14159265346825 as shown using calculations enclosed within dotted circle 2798. The binary integer input value 2700b is partitioned into four 8-bit
binary digits Bo 271 lb through B3 27 l2b and for the purposes of this example, the input value is held for a period of execution of nine pipeline stages, such as STAGE 1,
2702. The effect of holding input data 2720b is to show the state sequence of a data value as it undergoes processing at each stage of the pipelines 2720b, 2750b.
[0370] In the example of Figure 27b, data is processed by the fractional space conversion unit 2720b and output to the RNS digit recombination unit 2750b; in the
example circuit of Figure 27a, an extra register stage is wasted, i.e., not needed, but was included for clarity in the drawing. For example, the circuit has duplicate RNS
values Ri through R4 for STAGE 4 2705 and STAGE 5 2706 as can be seen from examination of Figure 27a. As RNS digits are processed in further stages, the RNS value
is multiplied by a constant of a modulus Mi, such as modulus M2 2760 of Figure 27a, and summed with a digit value from a digit bus, such as digit bus 2757. In Figure
27b, at pipeline processing STAGE 8, 2709, the final RNS recombination result 2770 is produced but is not rounded.
[0371] At the STAGE 9, a rounded RNS fraction is produced and stored in output register 2780 in Figure 27b. A rounding constant is generated using LUT table 2791 of the
rounding circuit 2790 in one embodiment. The rounding value is not necessarily a single unit as in the case of fractional normalization but may be a significantly larger
value if the RNS fractional range is significantly larger than the input binary fractional range, which is the case in the example presented. Moreover, the rounding value, r,
2792 may not be a constant value, but may be one of a plurality of values stored in look-up table memory 2791 or may be calculated using a formula applied to the
remainder of the conversion; the remainder is derived from the value of binary remainder digit B3 2736 in some embodiments. A fully rounded RNS fractional result 2780
is produced when a rounding value, r, 2792, is transmitted via bus 2793 and summed to each digit Ri through R4 of result 2770 using modular adders, such as modular
adder 2771 of Figure 27a. [0372] In the example of Figure 27b, a rounding value is derived using the upper two bits of binary remainder register B3 2736 of Figure 27a and
multiplying these bits with the ratio of the RNS fractional range to the binary fractional range divided by the range of the truncated bits minus one. In the example, the
RNS fractional range is many times the binary fractional range, so that the input binary value is converted too accurately. While the conversion is highly accurate without
rounding, a properly interpreted value of the binary fractional input value 2700b is not preserved, since the rounding constant in the binary fractional space will be much
larger in the RNS fractional space.
[0373] Therefore, a whole unit of RNS fractional rounding can be approximated by the ratio of the RNS fractional range to the binary fractional range; moreover, some or
all of the most significant bits of B3 2736 can be used to index a sub-divided whole unit of RNS fractional rounding; an example calculation is shown in the equation
enclosed in dotted lines 2799. In the example provided, the two most significant bits of B3 2736b are used to scale a rounding value; since the value of the remainder
digit is 0xB5, the upper two bits equal 2, so a value of 2 is multiplied by the ratio of the RNS to binary fractional range, which is then sub-divided into three equal rounding
values according to 2799. Other more linear rounding conventions can be formulated and are left to design specifics. In other embodiments, a fractional rounding value in
RNS, rums, is expressed and indexed as i number of most significant bits of a j bit binary digit B3 2736a, ru rns

[0374] Scaled rounding provides a means to more accurately convert fractional values of a low resolution (small range) to a target value in a high resolution (large range)
or vice-versa. Therefore, the example output value 0.14159265362 is closer to the intended value 0.14159265358 compared to 0.14159265346 because the rounding
constant 2792b is scaled to account for a significant difference in the input value range 232 versus the output value range R/ of the conversion example of Figure 27b.
Scaled rounding circuit 2790 as disclosed herein is a significant advancement and novel feature in the art of fractional range conversion integrated into a binary to RNS
format conversion apparatus 2430c of Figure 27a.
[0375] Figure 28 is provided to illustrate components of a fractional forward converter supporting a 32-bit binary fraction input 2800 comprising two 16-bit input binary
digits Bo 2801 and Bi 2802 which can support the TPU example of the present disclosure. The output 2850 of the forward fractional converter unit 2430c is comprised of
eight RNS digits Ri 2851 through Rs 2852 which in one embodiment represent a fully extended word conversion. In other embodiments, the output 2850 includes only
four digits Ri through R4 to support a short RNS word format which occupies less memory storage than a fully extended RNS word.
[0376] Like the apparatus of Figure 27a, the forward fractional conversion pipeline of Figure 28 includes a binary fraction to RNS fraction space conversion unit 2810
coupled with an RNS word recombination unit 2830 comprising a plurality of digit recombination units, such as digit recombination unit 2840. Because the example TPU
of the present disclosure supports two RNS digits associated to the fractional range, binary input digits Bo 2801 and Bi 2802 typically share the same or approximate

https://patents.google.com/patent/WO2019213070A1/en?q=(high+radix)&oq=high+radix

28/38

2/14/26, 1:57 PM

WO2019213070A1 - Systems and methods for matrix multiplication, normalization, and reverse conversion - Google Patents

RNS digit encoding width, so that the input and output ranges of conversion are approximately equal. The RNS output 2850 may support more than two digits overall,
such as digits Ri through Rs, because RNS fixed point format (11) requires the fractional portion to be fully extended to all digits before it is summed with the integer
conversion 2541 using modular adder 2550 in Figure 25 by means of example.
[0377] When the input ranges and output ranges of fractional conversion are approximately equal, the rounding circuitry is simplified to the application of a single, unscaled unit of value; in this case, the rounding logic and rules resemble those of fractional product normalization. In Figure 28, the rounding unit, r, 2816 is derived from
the most significant bit of the remainder digit Bi 2815, is added to the result of the RNS word recombination unit 2830 using modular adders, such as modular adder
2831, before being transmitted to the converter output 2850.
[0378] One embodiment for the forward conversion pipeline unit 44la of Figure 4 is shown in more detail using forward conversion pipeline 44 lc in Figure 29. The
diagram of Figure 29 is provided to illustrate a typical embodiment of the forward fractional conversion pipeline 44 lb of Figure 24 which includes integration of the
component blocks shown therein. For example, an unsigned binary fractional value comprising fractional digits Bo 2901 and Bi 2902, and further comprising integer
value digits B2 2903 and B3 2904 is input at input port 2900. At some number of clock cycles later, the result representing the converted binary digits appears at output
port 2990 represented in RNS format comprising two fractionally associated digits Ri 2991 and R2 2992 and two digits R3 2993 and R4 2994 associated to a whole
number range.
[0379] As shown in previous illustrations, Figure 29 shows a fractional space converter 2910 coupled to an RNS word recombination unit 2950. In addition, a forward
integer conversion unit 2930 performs modular multiplication by a binary scaling factor 2931 and subsequent modular addition to provide a two-digit RNS integer
conversion. The RNS integer result is scaled by the fractional range constant RF 2941 using a modular multiplier, such as modular multiplier 2942, and then stored in a
pipeline storage register W, such as register 2961, before summed using a three- input modular adder, such as modular adder 2963. Only two modular multipliers are
needed, because fractionally associated digits are always zero. [0380] One input of the modular adder may accept a carry bit signal 2977 generated from the state of the
sign stored in delay register 2976, 2975. A recombination result which includes a scaled whole portion and a fully extended RNS fractional portion is stored in 2970. If the
sign flag 2975 indicates the answer is negative, the positive RNS word 2970 is complemented using a plurality of conditional digit complement units, such as digit
complement unit 2978. Output port 2990 stores a fully converted 32.32 binary fractional value in RNS format, which is four RNS digits Ri 2991 through R4 2994
representing, or approximating, a 32.32 fixed point representation, and is converted to a short format in the example of this disclosure.
[0381] It is to be understood by those practiced in the art of modular circuit design that additional circuits may be added to Figure 29 to produce a fully extended result
2990 consisting of eight RNS digits for some embodiments and when the converted value is to be immediately processed within the matrix multiplier 400a of the
example of the present disclosure.
[0382] Reverse Converter Pipeline Unit
[0383] The matrix multiplier 400a of Figure 4 processes product summations and passes non- normalized RNS data via bus 453 to a MOD function 454a and then
through a normalization pipeline 455a. Normalized data may then pass through a non-linear ReLU function 465 during neural network processing and may be stored back
to memory 445 via data bus 411, data cache 430, memory interface 435, and high-speed memory bus 444. In many cases, data is returned to the host system (not
shown) via reverse converter 442a and PCIe interface 440. The PCIe interface may be substituted with a high speed optical network interface in some embodiments, for
example, employing such interface standards such as SFP+ or QSFP+ gigabit network interfaces.
[0384] When RNS fixed point data is returned to the host system, it is generally converted to binary format. For the example TPU design of the present disclosure, RNS
fixed point format data is converted to 32.32 binary format by reverse converter 442a; the binary format is suitable for direct use by a host system, or the binary format
may be converted to floating-point format (not shown). The examples of this disclosure will focus on the conversion of signed fixed-point RNS format to signed fixedpoint binary format. Conversion of signed fixed-point binary format to floating-point format can be supported with the apparat herein and is well known and so not
described. The present disclosure will discuss two important RNS reverse converter variations, the first reverse converter 442b of Figure 30 relies on a valid sign bit in
RNS format, and the second reverse converter 442c of Figure 31 converts a signed RNS fixed -point format directly, regardless if the value is negative, and regardless of
knowledge of the sign of the data value. The second embodiment provides an advantage for applications since the sign bit for each RNS data value need not be stored
within the TPU accelerator card 406. [0385] The mathematics for reverse conversion are introduced by considering that a binary value n must be found to complete a
binary fraction equivalent to the RNS fraction, n _ r
(65)
2N — Rp
[0386] Conversion requires we find a value n which completes a binary fractional value that is

T
equivalent, or approximately equal to, the fractional value— of the RNS value 3002, so that,

[0387] In one embodiment, the block diagram of Figure 30 is used to perform the processing of equation (66). In Figure 30, an RNS fixed-point value is input to register
3002, and additionally, a sign bit indicating the sign polarity of the data value 3002 is input to sign register 3004. The RNS input value 3002 is first processed by
multiplying by the binary fractional range 2N using modular multiplier 3010. If the product is positive as indicated by the sign register 3016 it is passed unchanged by the
conditional complement unit 3015 to the reverse converter pipeline 3030 where the RNS value undergoes division by the fractional range RF during format conversion to
binary. Therefore, equation (66) describing reverse fractional range conversion is fulfilled during the processing of the RNS to binary format conversion using the
apparatus 442b.
[0388] If the RNS input value is negative, the sign bit stored in sign register 3016 enables the RNS word complement unit 3015 so that a complement of the product is
sent to the reverse conversion pipeline 3030, else if the sign bit is not set, the complement unit is not activated, and it passes the product unchanged to the reverse
converter pipeline 3030. The reverse converter pipeline 3030 is designed to convert positive encoded values only, so it is important the sign bit 3016 correctly reflects the
correct sign encoding of the RNS value (i.e., using method of complements). If the sign bit 3016 does not correspond to the encoding of the value, then an error in
conversion will occur, since the converter 3030 will convert a negative value directly.

https://patents.google.com/patent/WO2019213070A1/en?q=(high+radix)&oq=high+radix

29/38

2/14/26, 1:57 PM

WO2019213070A1 - Systems and methods for matrix multiplication, normalization, and reverse conversion - Google Patents

[0389] During conversion of the scaled RNS value by the reverse fractional conversion unit 3030, a residue to mixed-radix converter 3040 converts the incoming value to
mixed-radix format and stores the mixed radix number to the register 3055. Next, a mixed-radix to binary converter pipeline 3060 processes the mixed-radix value input
from storage register 3055 and outputs a binary value 3084 to binary adder 3085 to add a round up value ru 3086 generated by the sign delay and rounding comparator
pipeline 3020. If the sign bit storage 3092 indicates the value is negative, the converted and rounded value output from adder 3085 is complemented using conditional
complement unit 3090.
[0390] Once again, the conversion unit 3030 only handles the processing of positive encoded values, so the apparatus of Figure 30 handles sign processing exterior to
the main conversion unit 3030 using two complement units 3015, 3090. The issue with the converter unit 3030 is the conversion of negative encoded values will be
incorrect since the direct division of a negative encoded value does not provide a properly scaled result. The residue to mixed-radix conversion unit 3040 measures only
the fractional digits using data paths 3041, 3042 so that a rounding value may be generated.
[0391] In one preferred embodiment, a novel invention of the present disclosure is shown in Figure 31. The reverse conversion unit 442c is unique since it does not
require a sign bit 3104 to be input; this means the matrix multiplier circuitry of Figure 4 does not need to include support for sign bits for each data value 3100 to be
output to the host system. However, this is not a limitation, since clearly sign bits such as sign bit 3104 can be supported in some embodiments of the present design but
will not be used by the reverse converter unit 442c of Figure 31. Reduction of sign bit storage 3004 of Figure 30 implies that sign information is not required, and this
means sign bit and sign information need not be attached or transported alongside RNS data encoded using method of complements; this results in reduction of circuitry
and memory, so the TPU 406 as disclosed does not need support for sign bit storage, such as sign bit 3104 of Figure 30, and other sign bit storage, such as within
memory storage elements 445, 435 of Figure 4.
[0392] The overall architecture of reverse fractional conversion unit 442c differs from the reverse conversion pipeline unit 442b of Figure 30 since the front-end word
complement unit 3015 is removed as well as the input sign storage register or delay line 3016 from the Figure 30. In addition, the back-end word complement unit of
3090 of Figure 30 is replaced with a word correction unit 3190 driven by a sign storage bit 3192 generated by the sign detection and rounding comparator pipeline unit
3120. The reverse fractional conversion unit 442c of Figure 31 is more flexible than the reverse fractional converter unit 442b of Figure 30 because the sign of the input
RNS value 3102 is always detected by the sign detection and rounding comparator pipeline 3120 so that there is no reliance on redundant sign bit information, such as
sign bit information 3104 represented alongside the RNS fixed-point value 3102.
[0393] In the operation of the converter 442b of Figure 30, there is an error condition generated if the sign information 3004 and method of complement encoding of the
input RNS word 3002 differ for some reason. If a negative number, i.e. a negative RNS value encoded using method of complements, is processed using the reverse
fraction processing unit 3030, the result 3084 will be incorrect. The reason is the direct conversion of a negative number results in the expression of equation (66) above
into,
(RT- (2 N*r))
n ¹ (67)
RF
[0394] The incorrectly processed value of n using equation (67) above can be corrected by subtracting a reverse conversion correct constant CR which can be expressed
by observing the negative and positive values of a converted result should cancel to zero, so that,
(Rt- (2w *r)) -(2w* r)CR Rp + (
RP 68)
[0395] For the negative term of equation (68), the round-up INT bracket is replaced with a round- down INT bracket to denote round-down of negative values, i.e., brackets
in equation (68) denote symmetrical rounding towards negative and positive infinity. Assuming a perfect divide result by RF, the round-up and round-down brackets of
equation (68) above can be removed, so that correction constant CR is equal to the whole number range Rw,

[0396] For the perfect case, using equation (69) above, the correction constant can be applied to correct an incorrectly processed negative value 3102 into a correctly
processed equivalent positive value using,

[0397] The positive value of equation (70) above can be re-converted to a negative value in the binary number system by observing the rules for calculating the method of
complements in the binary number system, so that a negative value machine number n. can be recovered from a positive machine number n+ which is first recovered
using,
(2 N* r) (RT- (2 w*r))
71+
Rp Rw Rp (71)
[0398] so that a negative n. is recovered by processing the complement of a positive n+ with respect to a G-bit binary number system with a range of 2T expressed as,
(RT~ (2 w*r))
71. 9 T R w for 0 < (2W * r) < RT, (72)
RF
[0399] Which can be re-written using a MOD function as,

https://patents.google.com/patent/WO2019213070A1/en?q=(high+radix)&oq=high+radix

30/38

2/14/26, 1:57 PM

WO2019213070A1 - Systems and methods for matrix multiplication, normalization, and reverse conversion - Google Patents

[0400] The MOD function with respect to the value 2T in equation (73) above is needed to represent an underflow in the binary number system with a range of Tbits. Most
arithmetic circuits perform this wrap-under function without specifically designing provisions for MOD function of equation (73) above, so with care it is removed out of a
need for convenience so that equation (73) above is rewritten as,

[0401] Equation (74) is interpreted by observing an incorrectly processed negative result of equation (67) can be converted to a correctly processed negative result by
subtracting Rw with underflow in the target number system (binary in this case). To determine when a value is to be corrected, the input value 3102 must be compared
against the sign range during processing by reverse conversion pipeline unit 3130 and sign detection and rounding comparator pipeline 3120 which are coupled via
mixed-radix digit buses 3141 through bus 3142 and digit bus 3143 through bus 3144.
[0402] To understand the effects of rounding on equation (70) above, a substitution is made such that the value converted is represented using two new variable
quantities, Z, which evenly divisible by RF, and e, which is the remainder of division by RF, SO that
(2W * r) = Z + e 0 < e < RF, RF\ Z (75)
[0403] And to represent an associated complement using the same substitution, we observe, e = RF— e, 0 < e < RF, R | Z (76)
[0404] Therefore, the effect of rounding can be determined using equation (70) and substitution of equations (75) and (76) to find a new general reverse correction
constant CRG,

[0405] Reducing equation (77) above by removing the round up and round down brackets and replacing with a single INT bracket, and by removing ratio terms which
divide evenly from the INT bracket and canceling terms, the new general reverse conversion correction factor CRG is,

[0406] The form of general correction constant CRG is the same as the general correction constant Cg for negative product normalization according to equation (37). The
interpretation of equation (78) is like the interpretation of equation (37) since it demonstrates the rules for rounding a positive converted value can be applied to negative
values that are directly processed using the reverse fractional conversion unit 3130 of Figure 31. The apparatus of Figure 31 incorporates a single rounding comparator
circuit 3120 to determine both rounding magnitude as well as sign magnitude. A correction constant is applied if the value is determined to be negative, and a rounding
value is added if the remainder is determined to generate a rounding constant according to,

[0407] with a basic conversion rounding value ru calculated using,

[0408] The correction factor 3191 is shown in Figure 31 and is fed to the word correction unit 3190 which is equivalent to a binary word subtract unit that subtracts the
value Rw 3191 from the converted value 3187. The binary subtraction supports a“wrap-under” according to the MOD function of equation (73) since the value Rw is
always larger than the converter value. In one embodiment shown in Figure 32c to be discussed later, the correction constant Rw is encoded as a negative value and
added to the conversion result 3187 of Figure 31; therefore, the correction unit 3190 is a binary adder in this embodiment.
[0409] A detailed diagram of the reverse fractional conversion pipeline 442c of Figure 31 is shown in Figure 32a, Figure 32b, and Figure 32c. The first stage of the reverse
fractional conversion pipeline 442c of Figure 31 consisting of a modular multiply 3110 with the constant 2N 3111 is shown in Figure 32a with an equivalent converter
input port 3215 at STAGE 1, 3201 which transmits the input data to a modular word multiplier consisting of a plurality of digit modular multipliers, such as digit modular
multiplier 3218, and multiplies the input data by a binary range scaling constant 2N 3216. The output of the modular multiply scaling is stored in an RNS word register
3235 at pipeline STAGE 2 3202.
[0410] The register RNS register 3235 of Figure 32a is repeated for convenience in Figure 32b to indicate a continuation of the pipeline circuit into Figure 32b. In Figure
32b, the scaled input RNS data 3235 is fed into a mixed-radix conversion pipeline 3240 which performs several important functions including division by the fractional
range RF, and conversion to a mixed-radix word format 3255. Moreover, a sign and rounding comparator 3220 is coupled to the mixed-radix converter 3240 so that mixedradix digits are transmitted via digit data buses, such as digit data bus 3243, so the converted data value may be compared to a sign range constant and a rounding
range constant during conversion processing.
[0411] At pipeline STAGE 2, 3202, the diagram shows six RNS digits input, i.e., two digits Ri 3239 and R2 3238 associated to the fractional range RF, and four digits R3
3237 through R6 3236 associated to a whole range Rw. The converter 3240 must support an RNS range large enough to process at least a short word RNS format times
the binary fractional range 2N. In the example of the present disclosure, since the RNS short word format is four RNS digits, and since the binary fractional range is 32
bits which represents a range of two additional RNS digits, a total of six RNS digits 3235 is required for the input to converter 3240. In operation, the converter 3240 will
process six RNS digits 3235 and generate six mixed-radix digits, but the first two mixed-radix digits are essentially truncated (discarded) resulting in four mixed radix
digits 3255. The first two RNS digits are not completely discarded, the value of the two mixed-radix digits help determine the rounding value r 3225 using digit
comparators 3222, 3224.

https://patents.google.com/patent/WO2019213070A1/en?q=(high+radix)&oq=high+radix

31/38

2/14/26, 1:57 PM

WO2019213070A1 - Systems and methods for matrix multiplication, normalization, and reverse conversion - Google Patents

[0412] It should be clear that many other embodiments with different number of RNS input digits 3235 be supported for a wide variety of reasons, but these reasons do
not materially affect the invention of the present disclosure.
[0413] In the Figure of 32b, as an RNS input word 3235 is converted to mixed-radix format 3255, a sign comparison function is provided by the sign comparator unit 3220
using digit comparator 3222 through digit comparator 3227. At STAGE 8, 3208, of the converter 3240, a four-digit mixed-radix word 3255 is stored, and a round-up value
ru is stored in register 3230 and sign data S is stored in register 3229. The sign data S may also have mixed-radix word 3255“zero detect” information stored alongside
the sign polarity data to indicate when the conversion operation is a “perfect” operation (a perfect division by RF), since advanced rounding with non-symmetrical scaling
may require“zero detection” of the mixed-radix word as indicated by the arrow connection from the sign data S 3229 and the ru value register 3230.
[0414] In Figure 32c, the mixed-radix word stored in register 3255 of Figure 32b is repeated for convenience and indicates the reverse conversion pipeline diagram
continues from Figure 32b to Figure 32c by means of example. In Figure 32c, the mixed-radix word 3255, previously illustrated by mixed-radix word 3155 of Figure 31,
undergoes a second stage mixed-radix to binary format conversion 3160 also previously indicated in Figure 31. In Figure 32c, the mixed-radix word 3255 begins
conversion to binary using a mixed-radix to binary fractional format conversion unit 3260 in one preferred embodiment of the present invention. The mixed-radix to binary
fractional format converter 3260 is comprised of a pipelined mixed-radix digit shift circuit 3262 coupled to a mixed- radix to binary converter 3265. Also included is a sign
state storage pipeline 3220b, sometimes referred to herein as a sign flag delay line.
[0415] The mixed-radix digit shift circuit 3262 works by shifting a mixed-radix digit left in each successive pipeline stage. For example, at pipeline STAGE 8 3207 the most
significant mixed- radix digit A6 3257 is transmitted into the converter 3265 using digit bus 3258 at pipeline STAGE 8 3208b followed by next most significant mixed-radix
digit As 3263 transmitted via bus 3264 at pipeline STAGE 9 3209. At each pipeline STAGE 8 3208b, STAGE 9 3209, STAGE 10 3210, and STAGE 11 3211, a mixed-radix
digit of the mixed-radix word 3255 is processed by the mixed- radix to binary conversion pipeline 3265 in the order of most significant A6 3257 to least significant A 3
3256 mixed-radix digit.
[0416] The mixed-radix to binary conversion pipeline 3265 supports several important functions. For one, the converter 3265 supports conversion of mixed-radix format
to binary format. The converter 3265 does not support fractional space conversion per se, since fractional space conversion is accomplished using the modular scaling
by 2N of Figure 32a in conjunction with division by RF using RNS to mixed-radix conversion unit 3240 of Figure 32b. However, the mixed-radix to binary conversion
pipeline 3265 of Figure 32c does support rounding, which in some embodiments may be quite complex if the range of the input operand 3255 is significantly different
than the range of the output binary format 3295.
[0417] For purposes of clarity, rounding circuitry is shown in the conversion apparatus of the example of Figure 32c, but is simplified to assume approximately equal input
and output number ranges as in the case of the TPU example of the present disclosure. For example, rounding data 3230b generated in the sign generation and rounding
comparator 3220a of Figure 32b is transmitted using rounding data pipeline registers such as rounding data register ru 3232 and summed with the converted value using
binary adder, such as 3-input binary adder 3274. As mentioned, rounding values may be larger than a single unit if the fractional range of the binary output 3295 of Figure
32c is significantly larger than the fractional range of the RNS input 3215 of Figure 32a. On the other hand, if the fractional range of the output 3295 is significantly
smaller than the fractional range of the input 3215, some embodiments will typically support a single bit of rounding, since the source precision is high enough so not to
need rounding constants greater than one applied to the output 3295.
[0418] In some designs, if the output fractional range is significantly larger than the source or input fractional range, a look-up table (LUT) memory (not shown) can be
used to source one of a plurality of rounding constants usually indexed by the value of the last fractional digit converted to mixed-radix, which is <¾ 3241 in the example
of Figure 32b. In many cases, only the most significant bits of <¾ 3241 are used to index a rounding constant for a LUT, and these rounding bits are transported by the
sign and round value delay unit 3220b. During conversion of negative numbers, a large rounding constant (greater than 1) representing a“single rounding unit” is pre
biased if the fractional division is not“perfect”, and a“complement” of the high order bits of <22 3241, having been delayed with appropriate pipeline registers, is used to
index the same LUT as used for positive converted values. In one embodiment, the LUT sources fractional amounts of the rounding unit in a linear fashion according to
the value of the high order bits of <22 3241. However, during“perfect” negative results, i.e. all fractional mixed-radix digits are equal to zero, the LUT must not gate
(source) a pre-biased“unit rounding” since the correction constant for the perfect case as provided in equation (69) is only Rw[0419] In addition to format conversion and rounding, the mixed-radix to binary pipeline 3265 of Figure 32c supports signed value processing using the means of a
correction factor Rw represented as a negative binary number value -Rw and partitioned into four binary digits 3270-3273. The use of a correction constant 3191 and
correction circuit 3190 was previously discussed in Figure 31 and its use is supported by mathematics derived in equations (67) through (78). The novel reverse
conversion circuit of Figure 32c supports the correction of improperly processed negative values using the previously measured sign of the converted value 3255 stored
in sign storage pipeline 3220b, such as sign storage register 3231.
[0420] For means of example and referring to Figure 32c, at pipeline STAGE 12, 3212, the sign of the converted value is stored within the sign register 3231 and controls
the gating of the least significant digit of the constant -Rw 3270 so that if the converted value is negative, a correction constant is added using binary adder 3274,
otherwise zero is added. Correction constants 3271, 3272, 3273 are gated in a similar manner at each STAGE 31 3213, STAGE 14 3214, and STAGE 15 3215 respectively
such that an entire correction constant -Rw is added to the result of the conversion before the converted result is stored in binary output register 3295. The reverse
mixed- radix to binary converter unit 3265 supports advanced operation so that only a single RNS to mixed-radix conversion pipeline 3240 of Figure 32b is required.
[0421] In the prior art, two RNS to mixed -radix converters 3240 of Figure 32b is required because a converted value and its complement must be processed to ensure at
least one conversion operates on a positive value and produces a correct positive result; only then can a correctly processed positive value be processed into a negative
result by means of a complement unit 3090 of Figure 30. The correction unit 3190 of Figure 31 is significant and novel as it allows pipelined reverse converter operation
using only a single RNS to mixed-radix converter pipeline 3240 of Figure 32b. In some embodiments, an overflow flag ov 3293 is supported to indicate if the RNS value is
greater than the allowable range of the output binary format 3295. [0422] The mixed-radix to binary conversion pipeline 3265 of Figure 32c processes the mixed- radix
word 3255 by summing each mixed-radix digit in sequence while also multiplying by a sequence of modulus constants, such as modulus constant M3 3266, using a
binary multiplier then add unit (MA), such as binary multiply then add MA units 3267, 3269. The binary multiply-then- add unit MA 3267 multiplies by the initial summation
of zero so the MA element 3267 may be replaced with an adder in actual embodiments but was left in the illustration to disclose symmetry of the pipeline 3265. The
multiply-then-add MA element 3269 multiplies the binary result Bo 3268 of the previous stage by the modulus constant M4 3274 then adds the digit value A5 3263 via
digit bus 3264. The least significant portion of the output of the MA element 3269 is stored in the Bo digit 3275 of the pipeline STAGE 10, 3210, and the most significant
portion of the output of the MA element 3269 is stored in the carry in register C of the same pipeline stage.
[0423] After eight stages of pipeline 3265 processing, such as stages STAGE 9, 3209, through STAGE 16, 3216 of Figure 32c, a mixed-radix word 3255 is converted to a
signed fractional binary value 3295 comprising a plurality of fractional digits, such as digits Bo 3296 and Bi 3297, and comprising a plurality of binary digits associated
with an integer value, such as binary digits B2 3298 and B3 3299. It is clear to those skilled in the art of modular circuit design that more, or less number, of mixed-radix
digits 3255 can be supported, and that more, or less number, of output binary digits 3295 can be supported. It is understood the dynamic ranges of the input value 3255
can be approximately the same, or may be significantly different, then the dynamic range of the output 3295, both in terms of the dynamic range of the fractional space
and the dynamic range of the integer number supported. If the fractional ranges differ, a scaled rounding technique may be employed when the dynamic range of the

https://patents.google.com/patent/WO2019213070A1/en?q=(high+radix)&oq=high+radix

32/38

2/14/26, 1:57 PM

WO2019213070A1 - Systems and methods for matrix multiplication, normalization, and reverse conversion - Google Patents

output fraction is greater than the input fraction. When an integer overflow occurs in the whole portion of the conversion, the ov flag 3293 is set to indicate a conversion
range error in the whole portion of the converted value.
[0424] To illustrate the data flow through the reverse converter pipeline 442c of Figure 31, which is further illustrated by the reverse converter pipeline spanning Figures
32a, 32b and 32c, an example data flow of the reverse converter is shown in Figure 33 for a positive value“8.53973422” 3300 which is an example positive fixed-point
RNS data value listed in Row 384a of Figure 3c. For the reverse converter, not all eight digits Ri through Rs, 370, 372, 374 are necessarily needed for conversion, however,
some embodiments may support all eight digits. For the diagram of Figure 33, the RNS data input 3300 of the converter pipeline is held constant for a period of 16
pipeline stages, such as pipeline STAGE 1, 3301. This provides a means to study the processing of a single data item as it progresses through the reverse converter
pipeline. In practice, the reverse converter pipeline supports a complete conversion on every clock cycle, so that illustrating independent data at each stage of the
pipeline is cumbersome for disclosure. [0425] In Figure 33 and for the example TPU of the present disclosure, six RNS digits Ri 3317 through R6 3318 are used as the
RNS input word 3300 for reverse conversion; four digits comprise the short RNS word format shown in Row 384b of Figure 3c, which is comprised of the fractionally
associated digits 370 and the whole associated digits 372. Two additional RNS digits are chosen to represent an extended range 374 (not four), so digits R5 and R6 are
arbitrarily chosen in the TPU design (any redundant two digits in this case). The extended range is necessary so the fixed-point RNS value can be scaled by the binary
fractional range 2N at the start of conversion; therefore, at least two digits are used to provide the required dynamic range for the scaling multiply 3218 of Figure 32a.
[0426] In Figure 33 and by means of example, at start of conversion an RNS fixed-point value enters the reverse converter at register port 3300. At pipeline STAGE 1,
3301, the RNS fixed point value 3300 is scaled, or multiplied, by the binary fractional range 2N using a circuit like Figure 32a and stored in word register 3320. At pipeline
STAGE 2, 3302 through STAGE 8, 3308, the scaled RNS value 3320 is reduced using mixed-radix conversion producing a four-digit mixed-radix word 3335 and further
producing a rounding constant of (+1) in round storage pipeline 3355 and a sign polarity state of“lesser than” in sign polarity comparator pipeline 3350 at pipeline STAGE
8, 3308. A lesser than state 3351 at pipeline STAGE 8, 3308, indicates the magnitude of the converted value is less than the beginning of the negative value range
indicated by sign compare constants 3360; in other words, the example value is positive.
[0427] The round comparator pipeline uses two mixed-radix constants 3465 and compares each constant to one of the first two (fractionally associated) mixed-radix
digits respectively; the comparison is least significant digit first, which results in a“greater than” value in the rounding comparator pipeline 3355 and this results in adding
a single unit (+1) to the converted answer at STAGE 12, 3312, which increases the binary digit Bo from the value“5A2” to the value“5 A3” within the mixed-radix to binary
pipeline 3380 of Figure 33.
[0428] At pipeline STAGE 8, 3308 of the RNS to mixed-radix converter 3330, a mixed-radix word is stored in register 3335; this register is analogous to register 3155 of the
reverse fractional conversion pipeline 442c of Figure 31. The mixed -radix word 3335 of Figure 33 is further processed by the mixed-radix digit shift circuit 3340 and
mixed-radix to binary conversion pipeline 3380. The mixed-radix word 3335 is converted to a binary format by successive additions and multiplications using multiplythen-add (MA) binary arithmetic elements, such as MA element 3266 of Figure 32c. The multiplications are performed with the prior state of a binary register, such as
binary register Bo 3268, and a modulus constant, such as M4 3274 of Figure 32c. Next, the MA element performs an addition with a mixed -radix digit, such as mixedradix digit As 3263. In other areas of the pipeline, the MA element performs an addition with a carry-in register generated from a prior stage, such as carry-in register C
3276.
[0429] In Figure 33, the operation of the mixed-radix to binary conversion pipeline 3380 is that a mixed-radix word 3335 is multiplied by a series of factors 3345
representing the value of each digit radix while simultaneously the mixed-radix digits are added and shifted 3340 at each pipeline STAGE 9, 3309, through STAGE 11,
3311, thereby affecting a binary starting value of zero 3381 at pipeline STAGE 9, 3309 and ending with a fully recombined value 3385 at STAGE 15, 3316. The radix of
each associated mixed-radix digit is shown as the constants affecting the multiplier- add elements MA in Figure 32c. The multiplier within an MA element is a“multiply by
a constant” function, and the value used as the constant operand is the value of a radix, such as M3 3266 and M4 3274 of Figure 32c. Note the radix values 3345 are
equal to modulus values M3 367 through M6 369 in Row 381 of Figure 3c.
[0430] Since the example of Figure 33 converts a positive value, there is no need to perform a correction on the converted result 3385, therefore, the correction constant
3370 applied is zero in value. Inspection of the output value demonstrates the output value 3385 is indeed the binary equivalent to the RNS fixed-point fractional
value“8.53973422” 3300. Several check values for the example of Figure 33 are provided enclosed in dotted lines 3390.
[0431] In Figure 34, a more complex example involving the conversion of a negative input value 3400 is shown. To better illustrate the operation, the complement of the
input data of Figure 33 is used, i.e., the RNS fixed-point equivalent of the value“-8.53973422” shown in Row 384c of Figure 3c. Therefore, the reverse fractional converter
must perform a correction on the converted value since it is negative.
[0432] In Figure 34, the mixed-radix conversion pipeline generates two mixed-radix digits which indicate the round-up value is (+0) 3455. After generating another four
mixed radix digits, the sign state is known at STAGE 8, 3408 which indicates a“greater than” sign 3451 in the present example; therefore, the converted value is greater
than the sign range constants 3460 which indicates the converted value is negative. The mixed-radix value generated 3435 is much larger than the mixed -radix word
3335 of Figure 33 since the value converted is negative, and negative encoded values lie at the upper end of the number system range. Therefore, the recombination of
the mixed-radix value 3435 within the mixed-radix to binary conversion pipeline 3480 takes longer to complete.
[0433] In the unique and novel apparatus of Figures 32a, 32b, 32c, and shown as an example in Figure 34, a correction circuit consisting of four binary digits 3470 and a
plurality of adders is equivalent to adding the negative value of Rw to the converted result. In the example of Figure 34, the binary constant -Rw is summed with the
converted value undergoing processing in pipeline 3480 to perform a correction to the improperly converted value such that a properly converted negative value arrives
at the converter output 3485. Note the conversion output word 3485 of Figure 34 is a binary complement of the output conversion word 3385 of Figure 33, therefore, the
example of Figure 34 checks out.
[0434] MOD conversion pipeline
[0435] The MOD function 454a of the matrix multiplier circuit 406 of Figure 4 may be required if modular accumulators 401, 402, 403, 404 compute a congruent result
and not a completely reduced “modular” result. A congruent result does not guarantee that the RNS digit D is less than its modulus M, i.e., (D < M). In a previous section,
the merits of a multiplier- accumulator which uses congruent accumulation, such as the modular accumulator circuit 935e of Figure l5a, is discussed. The unique and
novel modular accumulator 935e allows high-speed operation, and low LUT resource memory usage, but does so at the cost of computing only a residue digit with a
value that is congruent to the intended modular result.
[0436] In Figure 4, the normalization pipeline 455a as described in the present disclosure requires a fully modular result as input, since the first digit is used to subtract all
other digits and subtracting a congruent digit value is not correct. In some embodiments, a pipelined normalization unit 455a will accept a congruent digit, but a fully
modular RNS digit is required for mixed-radix digit subtraction and must be produced during processing, so in practice, a fully modular input to normalization pipeline
455a is typical of the state of the art.

https://patents.google.com/patent/WO2019213070A1/en?q=(high+radix)&oq=high+radix

33/38

2/14/26, 1:57 PM

WO2019213070A1 - Systems and methods for matrix multiplication, normalization, and reverse conversion - Google Patents

[0437] The pipelined MOD function 454a of Figure 4 may be implemented using the block diagram of MOD unit 454d of Figure 35. The MOD unit 454d is applied to a 21 bit input value Rin congruent to a value R, and outputs a modular digit value R such that R < Mi in a time of three pipeline sub-stages. In the pipeline SUB-STAGE 1 3502
the high-order five bits 3505 of the input digit value Rin 3500 is sent to LUT1 3510 where the value of the upper five bits modulo Mi is stored and sourced to adder 3522.
The low order l6-bits of the input digit Rin is stored to regl 3506 and summed using adder 3522. Other input bit combinations 3505 can be supported, for example l7-bits
of the digit R may be stored in regl 3406 which decreases LUT memory address 3505 to 4 bits.
[0438] The summation of two modular values modulo Mi is produced by adder 3522 and the same summation minus the modulus Mi 3521 is produced by adder 3524 at
pipeline SUB-STAGE 2 3503. If the sign of the value in register reg2 3528 indicates a negative result, it implies the result of adder 3522 is the correct result, therefore,
selector 3530 will pass the result in reg3 3526 to the output register 3540 at pipeline SUB_STAGE 3 3504. Otherwise, if the reg2 value is not negative, the sign signal 3529
is zero and the value contained in reg2 3528 is passed to output register R 3540 using selector 3530 at pipeline SUB_STAGE 3 3504.
[0439] While various embodiments of the invention have been described, it will be apparent to those of ordinary skill in the art that many more embodiments and
implementations are possible that are within the scope of this invention. In addition, the various features, elements, and embodiments described herein may be claimed
or combined in any combination or arrangement.

Patent Citations (24)
Publication number

Priority date

Publication date

Assignee

Title

EP1068565B1 *

1999-01-11

2009-09-30

SanDisk IL Ltd

Acceleration and security enhancements for elliptic curve and rsa
coprocessors

US8051124B2 *

2007-07-19

2011-11-01

Itt Manufacturing Enterprises, Inc.

High speed and efficient matrix multiplication hardware module

US8195735B2 *

1995-08-16

2012-06-05

Microunity Systems Engineering, Inc.

System and method to implement a matrix multiply unit of a
broadband processor

US20150339103A1 *

2012-05-19

2015-11-26

Eric B. Olsen

Product summation apparatus for a residue number arithmetic logic
unit

US4281391A

1979-01-15

1981-07-28

Leland Stanford Junior University

Number theoretic processor

US4588980A

1984-09-24

1986-05-13

Gte Communication Systems
Corporation

Residue to analog converter

US4948959A

1988-07-15

1990-08-14

The Boeing Company

Optical computer including pipelined conversion of numbers to
residue representation

US4910699A

1988-08-18

1990-03-20

The Boeing Company

Optical computer including parallel residue to binary conversion

JP2930325B2

1989-07-29

1999-08-03

ソニー株式会社

Digital signal processing circuit

US5050120A

1989-09-29

1991-09-17

The Boeing Company

Residue addition overflow detection processor

US4996527A

1989-09-29

1991-02-26

The Boeing Company

Pipelined residue to mixed base converter and base extension
processor

US4963869A

1989-09-29

1990-10-16

The Boeing Company

Parallel residue to mixed base converter

US5107451A

1990-01-30

1992-04-21

The Boeing Company

Method and apparatus for pipelined detection of overflow in residue
arithmetic multiplication

US7269261B1 *

1999-09-22

2007-09-11

Raytheon Company

Key escrow systems

US7523151B1

2000-05-12

2009-04-21

The Athena Group, Inc.

Method and apparatus for performing computations using residue
arithmetic

JP3532860B2

2001-01-22

2004-05-31

株式会社東芝

Arithmetic device, method, and program using remainder
representation

JP4279626B2

2003-07-31

2009-06-17

株式会社アドバンテスト

Remainder calculation system, scaling calculator, scaling
calculation method, program thereof and recording medium

RU2318238C1

2006-07-05

2008-02-27

Ставропольский военный институт
связи ракетных войск

Neuron network for transformation of residual code to binary
positional code

US8363830B2

2008-02-07

2013-01-29

Harris Corporation

Cryptographic system configured to perform a mixed radix
conversion with a priori defined statistical artifacts

US20110231465A1

2010-03-09

2011-09-22

Phatak Dhananjay S

Residue Number Systems Methods and Apparatuses

US9712185B2

2012-05-19

2017-07-18

Olsen Ip Reserve, Llc

System and method for improved fractional binary to fractional
residue converter and multipler

US9652200B2 *

2015-02-18

2017-05-16

Nxp B.V.

Modular multiplication using look-up tables

US9747546B2

2015-05-21

2017-08-29

Google Inc.

Neural network processor

Family To Family Citations

https://patents.google.com/patent/WO2019213070A1/en?q=(high+radix)&oq=high+radix

34/38

2/14/26, 1:57 PM

WO2019213070A1 - Systems and methods for matrix multiplication, normalization, and reverse conversion - Google Patents

IT201700008949A1 *

2017-01-27

2018-07-27

St Microelectronics Srl

OPERATING PROCEDURE FOR NEURAL NETWORKS, NETWORK,
EQUIPMENT AND CORRESPONDENT COMPUTER PRODUCT

* Cited by examiner, † Cited by third party

Cited By (54)
Publication number

Priority date

Publication date

Assignee

Title

US12112265B2

2020-12-18

2024-10-08

Analog Devices International Unlimited Company

Architecture for running convolutional networks on
memory and mips constrained embedded devices

US10489479B1

2016-09-12

2019-11-26

Habana Labs Ltd.

Matrix multiplication engine

US11321092B1

2017-11-08

2022-05-03

Habana Labs Ltd.

Tensor-based memory access

US10915297B1 *

2017-11-15

2021-02-09

Habana Labs Ltd.

Hardware accelerator for systolic matrix
multiplication

US10599429B2 *

2018-06-08

2020-03-24

Intel Corporation

Variable format, variable sparsity matrix
multiplication instruction

US11790245B2 *

2019-01-22

2023-10-17

International Business Machines Corporation

Cognitive machine learning for semantic network

US12124530B2 *

2019-03-11

2024-10-22

Untether Ai Corporation

Computational memory

DE102019107817A1 *

2019-03-27

2020-10-01

Dspace Digital Signal Processing And Control
Engineering Gmbh

Method for simulating a dynamic system

US11442700B2

2019-03-29

2022-09-13

Stmicroelectronics S.R.L.

Hardware accelerator method, system and device

US10990397B2 *

2019-03-30

2021-04-27

Intel Corporation

Apparatuses, methods, and systems for transpose
instructions of a matrix operations accelerator

US11132198B2

2019-08-29

2021-09-28

International Business Machines Corporation

Instruction handling for accumulation of register
results in a microprocessor

US11416580B2 *

2019-11-13

2022-08-16

Intel Corporation

Dot product multiplier mechanism

US12061910B2

2019-12-05

2024-08-13

International Business Machines Corporation

Dispatching multiply and accumulate operations
based on accumulator register index number

US11372644B2 *

2019-12-09

2022-06-28

Meta Platforms, Inc.

Matrix processing instruction with optional
up/down sampling of matrix

US11422803B2 *

2020-01-07

2022-08-23

SK Hynix Inc.

Processing-in-memory (PIM) device

CN111221577B *

2020-01-17

2020-12-29

中国人民解放军32802部队

Function reconstruction method for noncooperative linear feedback shift register

US11468002B2

2020-02-28

2022-10-11

Untether Ai Corporation

Computational memory with cooperation among
rows of processing elements and memory thereof

US11887647B2

2020-04-09

2024-01-30

Micron Technology, Inc.

Deep learning accelerator and random access
memory with separate memory access connections

US11461651B2

2020-04-09

2022-10-04

Micron Technology, Inc.

System on a chip with deep learning accelerator
and random access memory

US11355175B2

2020-04-09

2022-06-07

Micron Technology, Inc.

Deep learning accelerator and random access
memory with a camera interface

US11726784B2

2020-04-09

2023-08-15

Micron Technology, Inc.

Patient monitoring using edge servers having deep
learning accelerator and random access memory

US11874897B2 *

2020-04-09

2024-01-16

Micron Technology, Inc.

Integrated circuit device with deep learning
accelerator and random access memory

EA038389B1 *

2020-04-14

2021-08-20

федеральное государственное автономное
образовательное учреждение высшего
образования "Северо-Кавказский федеральный
университет"

Device to compare and determine the sign of the
numbers presented in the residual classes system

US11507817B2

2020-04-17

2022-11-22

Samsung Electronics Co., Ltd.

System and method for performing computations
for deep neural networks

US20230169144A1 *

2020-04-21

2023-06-01

Cambricon (Xi'an) Semiconductor Co., Ltd.

Operation method, processor, and related product

DE102020131666A1 *

2020-05-05

2021-11-11

Intel Corporation

Scalable multiplication acceleration of sparse
matrices using systolic arrays with feedback inputs

Family To Family Citations

https://patents.google.com/patent/WO2019213070A1/en?q=(high+radix)&oq=high+radix

35/38

2/14/26, 1:57 PM

WO2019213070A1 - Systems and methods for matrix multiplication, normalization, and reverse conversion - Google Patents

US11501151B2 *

2020-05-28

2022-11-15

Arm Limited

Pipelined accumulator

US11899745B1 *

2020-08-19

2024-02-13

Meta Platforms Technologies, Llc

Systems and methods for speech or text
processing using matrix operations

KR102694681B1 *

2020-08-31

2024-08-16

(주)제이엘케이

Normalized convolutional neural network system
and method of normalizing convolutional neural
network

CN114282162B *

2020-09-28

2025-03-07

中科寒武纪科技股份有限公司

Matrix multiplication operation method, electronic
device and storage medium

US20220180032A1 *

2020-12-03

2022-06-09

Dspace Gmbh

Method for simulating an electric motor

US12135955B2 *

2020-12-24

2024-11-05

Intel Corporation

Systems and methods for low latency modular
multiplication

US12020028B2 *

2020-12-26

2024-06-25

Intel Corporation

Apparatuses, methods, and systems for 8-bit
floating-point matrix dot product instructions

US12135954B2 *

2021-02-19

2024-11-05

Verisilicon Microelectronics (Shanghai) Co., Ltd.

Accumulation systems and methods

US12373677B2

2021-03-04

2025-07-29

Samsung Electronics Co., Ltd.

Neural processor and control method of neural
processor

CN112949845B *

2021-03-08

2022-08-09

内蒙古大学

Deep convolutional neural network accelerator
based on FPGA

US20240184523A1 *

2021-06-24

2024-06-06

Intel Corporation

METHODS AND APPARATUS TO PERFORM MIXED
RADIX FAST FOURIER TRANSFORM (FFT)
CALCULATIONS ON GRAPHICS PROCESSING
UNITS (GPUs)

US11791979B2 *

2021-07-08

2023-10-17

International Business Machines Corporation

Accelerated cryptographic-related processing with
fractional scaling

CN113805844B *

2021-09-14

2024-04-09

北京升哲科技有限公司

Data processing method, device, electronic device
and storage medium

CN114047903B *

2021-11-09

2025-01-07

上海交通大学

A mixed precision arithmetic unit for data stream
driven reconfigurable array

US20230359694A1 *

2022-05-05

2023-11-09

Xcelerium Inc

System and method for implementing variableprecision matrix multiplication using low-precision
digit matrix multiplier

US12242894B2 *

2022-05-24

2025-03-04

Khalifa University of Science and Technology

Technique for hardware activation function
computation in RNS artificial neural networks

US20240012615A1 *

2022-07-07

2024-01-11

International Business Machines Corporation

Fast modular multiplication of large integers

WO2024072251A1 *

2022-09-27

2024-04-04

Huawei Technologies Co., Ltd.

System and method for matrix multiplication

US20240152330A1 *

2022-11-02

2024-05-09

Kneron Inc.

K-cluster residue number system using look-up
tables with reduced data capacity for addition,
subtraction, and multiplication operations

CN116187407B *

2023-03-08

2025-06-13

西安电子科技大学

A system and method for implementing a systolic
array self-attention mechanism

US12200101B2 *

2023-06-14

2025-01-14

Trustees Of Boston University

Semi-custom accelerator device for bootstrappable
fully homomorphic encryption

US12362008B2

2023-09-25

2025-07-15

Kneron Inc.

Compute-In-Memory architecture using look-up
tables

WO2025085124A2 *

2023-10-19

2025-04-24

Massachusetts Institute Of Technology

Systolic ai processor compiler

CN117234458B *

2023-11-09

2024-02-23

深圳大普微电子股份有限公司

Multiplication array, data processing method,
processing terminal and storage medium

US12045309B1

2023-11-29

2024-07-23

Recogni Inc.

Systems and methods for performing matrix
multiplication with a plurality of processing
elements

US12008069B1

2023-11-29

2024-06-11

Recogni Inc.

Multi-mode architecture for unifying matrix
multiplication, 1×1 convolution and 3×3 convolution

US12007937B1 *

2023-11-29

2024-06-11

Recogni Inc.

Multi-mode architecture for unifying matrix
multiplication, 1×1 convolution and 3×3 convolution

CN120429015B *

2025-06-25

2025-08-29

众智齐芯(上海)科技有限公司

A pipeline divider calculation method and system
with variable precision and throughput

* Cited by examiner, † Cited by third party, ‡ Family to family citation

https://patents.google.com/patent/WO2019213070A1/en?q=(high+radix)&oq=high+radix

36/38

2/14/26, 1:57 PM

WO2019213070A1 - Systems and methods for matrix multiplication, normalization, and reverse conversion - Google Patents

Similar Documents
Publication

Publication Date

Title

US10649736B2

2020-05-12

Normalization unit for signed operands

US6081821A

2000-06-27

Pipelined, high-precision fast fourier transform processor

EP2761432A1

2014-08-06

Residue number arithmetic logic unit

JPH01266628A

1989-10-24

Apparatus and method for calculating division

US20220075598A1

2022-03-10

Systems and Methods for Numerical Precision in Digital Multiplier Circuitry

WO2022150058A1

2022-07-14

Numerical precision in digital multiplier circuitry

Tian et al.

2019

Ultra-fast modular multiplication implementation for isogeny-based post-quantum cryptography

US20090132630A1

2009-05-21

Method and apparatus for multiplying binary operands

CN104090737A

2014-10-08

Improved partial parallel architecture multiplying unit and processing method thereof

CA2329104C

2005-05-24

Method and apparatus for calculating a reciprocal

US7136893B2

2006-11-14

Decimal multiplication using digit recoding

Kelley et al.

2005

Parallelized very high radix scalable Montgomery multipliers

Villalba-Moreno

2016

Digit recurrence floating-point division under HUB format

CN100520708C

2009-07-29

An apparatus for square root computation

Parashar et al.

2017

Fast combinational architecture for a vedic divider

Azarmehr et al.

2010

High-speed and low-power reconfigurable architectures of 2-digit two-dimensional logarithmic number system-based recursive
multipliers

Cardarilli et al.

2002

A systolic architecture for high-performance scaled residue to binary conversion

Wang et al.

2007

A multiplier structure based on a novel real-time CSD recoding

Patronik et al.

2017

Design of residue generators with CLA/compressor trees and multi-bit EAC

Villalba-Moreno et
al.

2017

Floating point square root under HUB format

Hsiao et al.

2010

Low-cost design of reciprocal function units using shared multipliers and adders for polynomial approximation and Newton
Raphson interpolation

James et al.

2008

Fixed point decimal multiplication using RPS algorithm

GB2372353A

2002-08-21

Method and apparatus for calculating a reciprocal

James et al.

2009

High performance, low latency double digit decimal multiplier on ASIC and FPGA

Olsen

0

Hardware Matrix Multiplier for High Precision Neural Network Acceleration and its implementation in fixed-point RNS

Priority And Related Applications
Applications Claiming Priority (2)
Application

Filing date

Title

US15/972,021

2018-05-04

Residue number matrix multiplier

US15/972,021

2018-05-04

Legal Events
Date

Code

Title

Description

2019-12-18

121

Ep: the epo has been informed by wipo that ep was designated in this application

Ref document number: 19796575
Country of ref document: EP
Kind code of ref document: A1

https://patents.google.com/patent/WO2019213070A1/en?q=(high+radix)&oq=high+radix

37/38

2/14/26, 1:57 PM

WO2019213070A1 - Systems and methods for matrix multiplication, normalization, and reverse conversion - Google Patents

2020-11-05

NENP

Non-entry into the national phase

Ref country code: DE

2021-05-26

122

Ep: pct application non-entry in european phase

Ref document number: 19796575
Country of ref document: EP
Kind code of ref document: A1

Data provided by IFI CLAIMS Patent Services

About

Send Feedback

Public Datasets

Terms

https://patents.google.com/patent/WO2019213070A1/en?q=(high+radix)&oq=high+radix

Privacy Policy

Help

38/38

2/14/26, 1:58 PM

WO2019232091A1 - Radix-23 fast fourier transform for an embedded digital signal processor - Google Patents

Patents
Back to results

11 of 125,048

high radix

(high radix);

Radix-23 fast fourier transform for an embedded digital signal processor
Abstract
In some embodiments, a circuit may include an input configured to receive a signal and a radix-23
fast Fourier transform (FFT) processing element coupled to the input. The radix-23 FFT processing

WO2019232091A1
WIPO (PCT)

element may be configured to control variation of twiddle factors during calculation of a complete
FFT through a plurality of processing stages. The radix- 23 FFT processing element may be

Download PDF

Find Prior Art

Similar

configured to incorporate the twiddle factors and adder tree matrices of the calculation into a single
stage.

Other languages: French
Inventor: Radwan A JABER, Marwan A JABER, Daniel

Classifications

Massicotte

G06F17/142 Fast Fourier transforms, e.g. using a Cooley-Tukey type algorithm

Current Assignee : Jaber Technology Holdings Us Inc

Worldwide applications

Landscapes

2019 WO US

Physics & Mathematics

Application PCT/US2019/034452 events

Engineering & Computer Science
Show more

2019-05-29

Application filed by Jaber Technology Holdings
Us Inc

2019-12-05

Publication of WO2019232091A1

2020-11-29

Anticipated expiration

Status

Ceased

Info: Patent citations (5), Cited by (3), Legal events, Similar
documents, Priority and Related Applications
External links: Espacenet, Global Dossier, PatentScope, Discuss

Claims

Hide Dependent

WHAT IS CLAIMED IS:
1. A circuit comprising:
an input configured to receive a signal; and
a radix-23 fast Fourier transform (FFT) processing element coupled to the input and configured to control variation of twiddle factors during calculation of a complete FFT
through a plurality of processing stages of an FFT process, the radix-23 FFT processing element configured to incorporate the twiddle factors and adder tree matrices of the
calculation into a single stage.
2. The circuit of claim 1, wherein data input to the radix-23 FFT processing element and data output by the radix-23 FFT processing element are in natural order during each
stage of the plurality of processing stages of the FFT process.
3. The circuit of claim 1, wherein data within the radix-23 FFT processing element are grouped with their corresponding coefficients multipliers during each stage of the plurality
of processing stages of the FFT process.
4. The circuit of claim 1, wherein a total number of shifts during each stage in the plurality of processing stages of an FFT process configured to perform a decimation in time
(DIT) process is represented as rs.
5. The circuit of claim 1, wherein a total number of shifts during each stage in the plurality of processing stages of an FFT process configured to perform a decimation in
frequency (DIF) process is represented as r{s~s).
6. The circuit of claim 1, wherein trivial multiplication by one operations are avoided during the plurality of processing stages of the FFT process.
7. A circuit comprising:
an input configured to receive a signal; and
a radix-23 fast Fourier transform (FFT) processing element coupled to the input and configured to control variation of twiddle factors during calculation of a complete FFT
through one or more stages.

https://patents.google.com/patent/WO2019232091A1/en?q=(high+radix)&oq=high+radix&page=1

1/14

2/14/26, 1:58 PM

WO2019232091A1 - Radix-23 fast fourier transform for an embedded digital signal processor - Google Patents

8. The circuit of claim 7, wherein the radix-23 FT processing element is configured to incorporate the twiddle factors and adder tree matrices of the calculation into a single
stage.
9. The circuit of claim 7, wherein data input to the radix-23 FFT processing element and data output by the radix-23 FFT processing element are in natural order during each
stage of the one or more stages.
10. The circuit of claim 7, wherein the radix-23 FFT processing element is configured to:
determine data from the signal at the input;
group each data element from the determined data with its corresponding coefficient multiplier to form grouped data; and
process the grouped data to produce an output signal.
11. The circuit of claim 7, wherein the radix-23 FFT processing element is configured to perform a decimation in time (DIT) process having a number of shifts corresponding to a
size N of the input data divided by the radix.
12. The circuit of claim 7, wherein the radix-23 FFT processing element is configured to perform a decimation in frequency (DIF) process having a number of shifts
corresponding to a number of words minus a number of stages.
13. The circuit of claim 7, wherein the radix-23 FFT processing element avoids multiplication-by-one operations during the one or more stages of the FFT.
14. A circuit comprising:
an input configured to receive a signal; and
a radix-r fast Fourier transform (FFT) processing element coupled to the input, the radix-r FFT processing element configured to:
receive an input signal having a number of bits A;
reverse a bit order of the bits N;
decompose the bit order into groups of bits based on a base of a radix of the radix- r FFT processing element; and
process the groups of bits together with their coefficients to produce an output signal.
15. The circuit of claim 14, wherein the radix-r FFT processing element is configured to control variation of twiddle factors during calculation of an FFT through one or more
stages of an FFT process.
16. The circuit of claim 14, wherein the radix-r FFT processing element is configured to incorporate the twiddle factors and adder tree matrices of the calculation into a single
stage.
17. The circuit of claim 14, wherein data input to the radix-r FFT processing element and data output by the radix-r FFT processing element are in natural order during each stage
of the one or more stages.
18. The circuit of claim 14, wherein the radix-r FFT processing element is configured to:
determine data from the signal at the input;
group each data element from the determined data with its corresponding coefficient multiplier to form grouped data; and
process the grouped data to produce an output signal.
19. The circuit of claim 14, wherein the radix-r FFT processing element is configured to:
perform a decimation in time (DIT) process having a number of shifts corresponding to a size N of the input data divided by the radix; and
perform a decimation in frequency (DIF) process having a number of shifts
corresponding to a number of words minus a number of stages.
20. The circuit of claim 14, wherein the radix-r FFT processing element includes a radix-23 FFT processing element to avoid multiplication-by-one operations during processing
within the one or more stages.

Description

3
Radix-2 Fast Fourier Transform for an Embedded Digital Signal Processor

CROSS-REFERENCE TO RELATED APPLICATION(S)
[0001] The present disclosure is a non-provisional of and claims priority to ET.S.
Provisional Patent Application No. 62/677,610 filed on May 29, 2019 and entitled “Radix-23 Fast Fourier Transform for an Embedded Digital Signal Processor”, which is
incorporated herein by reference in its entirety.
FIELD
[0002] The present disclosure is generally related to devices, systems, and methods configured to determine a fast Fourier transform (FFT), and more particularly to a
radix-23 FFT that can be embedded in a digital signal processor (DSP).

https://patents.google.com/patent/WO2019232091A1/en?q=(high+radix)&oq=high+radix&page=1

2/14

2/14/26, 1:58 PM

WO2019232091A1 - Radix-23 fast fourier transform for an embedded digital signal processor - Google Patents

BACKGROUND
[0003] The Discrete Fourier Transform (DFT) is a mathematical procedure that is used in a wide variety of applications, from image processing to radio communications.
Further, the DFT can be implemented in computers or dedicated circuitry. Further, the DFT is at the center of the processing that takes place inside a digital signal
processor.
[0004] It is known that a DFT can be written as the sum of two discrete Fourier transforms, each of length N/2. One of the two DFTs can be formed from the evennumbered points of the original data of size N, and the other from the odd-numbered points. The Fast Fourier Transform allowed the DFT to be evaluated with a
significant reduction in the amount of calculation required, allowing the DFT of a sampled signal to be obtained rapidly and efficiently.
SUMMARY
[0005] In some embodiments, circuits, devices, systems, and methods described herein may enhance the efficiency of a DFT operation used to process input/output data
by avoiding trivial multiplication operations. In some embodiments, the circuits, devices, systems and methods may utilize a simple mapping from the three indices (FFT
stage, butterfly, and element) to the addresses of the input/output data with its corresponding multiplier coefficients.
[0006] In some embodiments, a radix-23 FFT can be used to reduce a computational load by reducing an amount of the coefficient’s multipliers (Twiddle Factors) utilized
to compute an FFT as compared to the conventional radix-2 FFT. In a particular embodiment, the radix-23 FFT can be configured to reduce the memory accesses, and
further, the
V2 y/2
multiplication by ± ^- ± _/ ^- can be also predicted where the number of arithmetical operation required for the complex multiplication can be reduced from 6 to 2, thereby
improving computational performance.
[0007] In some embodiments, a circuit may include an input configured to receive a signal and a radix-23 fast Fourier transform (FFT) processing element coupled to the
input. The radix-23 FFT processing element may be configured to control variation of twiddle factors during calculation of a complete FFT through a plurality of
processing stages. The radix- 23 FFT processing element may be configured to incorporate the twiddle factors and adder tree matrices of the calculation into a single
stage.
BRIEF DESCRIPTION OF THE DRAWINGS
[0008] FIG. 1 depicts a graph of a Discrete Fourier Transform (DFT) decomposition.
[0009] FIG. 2 depicts three stages in the computation of an 8-point Decimation in Time (DIT) DFT.
[0010] FIG. 3 depicts a graph of a basic butterfly computation for the DIT FFT algorithm.
[0011] FIG. 4 depicts a signal flow graph of an 8-point DIT FFT.
[0012] FIG. 5 depicts three stages of an 8-point DIF FFT algorithm.
[0013] FIG. 6 depicts a butterfly computation for a decimation in frequency (DIF) FFT algorithm.
[0014] FIG. 7 depicts stages of an 8-point DIF FFT algorithm. [0015] FIG. 8 depicts a radix-8 DIT butterfly, in accordance with certain embodiments of the present
disclosure.
[0016] FIG. 9 depicts a signal flow graph of an 8-point DIT FFT, in accordance with certain embodiments of the present disclosure.
[0017] FIG. 10 depicts a graph of the 8th root of unity, in accordance with certain embodiments of the present disclosure.
[0018] FIG. 11 depicts a graph of a Radix-23 FFT butterfly structure for a trivial computation, in accordance with certain embodiments of the present disclosure.
[0019] FIG. 12 depicts a graph of a Radix-23 FFT butterfly structure for a non-trivial computation, in accordance with certain embodiments of the present disclosure.
[0020] FIG. 13 depicts a graph of a percentage reduction of clock cycles as a function of the FFT length for a timing clock and a reference clock, in accordance with
certain embodiments of the present disclosure.
[0021] FIG. 14 depicts a block diagram of a signal processing system including a Radix- 23 FFT butterfly structure, in accordance with certain embodiments of the
present disclosure.
[0022] In the following discussion, the same reference numbers are used in the various embodiments to indicate the same or similar elements.
DETAILED DESCRIPTION OF ILLUSTRATIVE EMBODIMENTS
[0023] circuits, devices, systems, and methods described herein may enhance the efficiency of a DFT operation used to process input/output data by avoiding trivial
multiplication operations. In some embodiments, the circuits, devices, systems and methods may utilize a simple mapping from the three indices (FFT stage, butterfly,
and element) to the addresses of the input/output data with its corresponding multiplier coefficients.
[0024] In some embodiments, a radix-23 FFT can be used to reduce a computational load by reducing an amount of the coefficient’s multipliers (Twiddle Factors) utilized
to compute an FFT as compared to the conventional radix-2 FFT. In a particular embodiment, the radix-23 FFT can be configured to reduce the memory accesses, and
further, the
2 yf2
multiplication by ± -y ± ) ^-can be also predicted where the number of arithmetical operation required for the complex multiplication can be reduced from 6 to 2, thereby
improving computational performance.
[0025] In some embodiments, a circuit may include an input configured to receive a signal and a radix-23 fast Fourier transform (FFT) processing element coupled to the
input. The radix-23 FFT processing element may be configured to control variation of twiddle factors during calculation of a complete FFT through a plurality of

https://patents.google.com/patent/WO2019232091A1/en?q=(high+radix)&oq=high+radix&page=1

3/14

2/14/26, 1:58 PM

WO2019232091A1 - Radix-23 fast fourier transform for an embedded digital signal processor - Google Patents
3

processing stages. The radix- 2 FFT processing element may be configured to incorporate the twiddle factors and adder tree matrices of the calculation into a single
stage.
[0026] FIG. 1 depicts a graph 100 of a Discrete Fourier Transform (DFT) decomposition. The definition of the DFT is represented by the following equation

k E [0, N - 1], (Equation 1) where x[n] is the input sequence, X[k] is the output sequence, N is the transform length, wfik = e J N )n i is called the twiddle factor in butterfly
structure, and j2 = - 1. Both x[n] and X[k] are complex number sequences.
[0027] The graph 100 depicts a sixteen-bit input sequence at 102, which can be decomposed into two signals of eight bits each as shown at 104. It should be understood
that a decimation-in-time (DIT) FFT algorithm (sometimes called a“Cooley-Tukey FFT algorithm”) first rearranges the input elements into bit-reverse order, and then builds
up the output transform in log2N iterations. In the DIT process, the input data is subdivided into two sets of even-numbered and odd numbered data, as shown by the first
decomposition 104 in the graph 100. The two signals of eight bits can be further decomposed into four signals of four bits each, as shown at 106. The four signals of
four bits each can be decomposed into eight signals of two bits each, at 108. The eight signals can be further decomposed into sixteen signals of one bit each, at 110.
[0028] If N/2 is even, as it is when N is equal to power of 2, then the DFTs of each of the N/2 points can be computed by breaking each of the sums into two N/4 points
DFTs, which can be combined to yield the N/2 points DFTs. In the example of FIG. 1, an N point signal can be decomposed into N signals, each of which includes a single
point. In some embodiments, each stage may use an interlace decomposition, separating the even and odd numbered samples. If the system is configured to
decompose the four signals into eight signal point transforms, the system may decompose N into N/4 and N/4 into N/8 points transforms. The system may continue
until left with only 2 points transforms, this requires m stages where m = log2N, as shown in FIG. 2.
[0029] FIG. 2 depicts a system 200 including three stages 202, 204, and 206 in the computation of an 8-point Decimation in Time (DIT) DFT. At a first stage 202, a twopoint DFT receives two inputs and provides two outputs. At a second stage 204, the block combines four inputs from the first stage 202 and provides four outputs. At a
third stage 206, the block combines four-point DFTs to produce an eight-point DIT DFT.
[0030] FIG. 3 depicts a graph 300 of a basic butterfly computation for the DIT FFT algorithm. The graph 300 may include a summing node 302 including a first input
coupled to a node 304, a second input coupled to a node 306, and an output coupled to a node 308. The graph 300 may include a summing node 310 including a first
input coupled to a node 304, a second input coupled to a node 306, and an output coupled to a node 312. The graph 300 further includes a butterfly operation 314
coupled to the inputs 308 and 312. Other embodiments are also possible.
[0031] It is also possible to derive FFT algorithms that first go through a set of log2 N iterations on the input data and rearrange the output values into bit-reverse order.
This type of FFT algorithm is sometimes referred to as a decimation-in-frequency (DIF) or Sande- Tukey FFT algorithm. An example of an 8-point DIT FFT is described
below with respect to FIG. 4.
[0032] FIG. 4 depicts a signal flow graph 400 of an 8-point DIT FFT. The output sequences X(k) are decimated (split) into the even-numbered samples and odd-numbered
samples. Then, the DIF is obtained by performing the butterfly computation (in place computation or post multiplication technique).
[0033] Briefly, the basic operation of a radix-r butterfly includes combining r inputs to provide r outputs via the following operation:
X = Br x, (Equation 2) where x = [x(o>, X(i), ... , X(r-i)]T is the input vector, X = [ X(o), X i ), . . . . , Xu- 1 ) ]T is the output vector, and T denotes the transpose of the vector.
[0034] The value Br is the rxr butterfly matrix, which can be expressed as follows:
Br— WNTr, (Equation 3) for the decimation in frequency (DIF) process. The value Br of the r xr butterfly matrix for the decimation in time (DIT) process can be expressed
as follows:
Br = TrWN (Equation 4) where, for both cases, the value Fwis defined as follows:
(Equation 5)

and

[0035] The signal flow graph 400 may include a first stage 402, a second stage 404, and a third stage 406, which may be configured to receive eight inputs and to
generate an eight- point DIF FFT output.
[0036] FIG. 5 depicts three stages of an 8-point DIF FFT algorithm 500. The algorithm 500 may include a first stage 502, a second stage 504, and a third stage 506. The
first stage 502 may receive eight inputs and may produce eight inputs for the second stage 504, which produces eight outputs. The third stage 506 may receive the eight
outputs of the second stage 504 and may produce the DIF FFT output.
[0037] FIG. 6 depicts a butterfly computation 600 for a decimation in frequency (DIF) FFT algorithm. The computation 600 may include a summing node 602 including a
first input coupled to a node 604, a second input coupled to a node 606, and an output coupled to a node 608. The computation 600 may further include a summing node
610 including a first input coupled to the node 604, a second input coupled to the node 606, and an output coupled to a node 612. The computation 600 may further
include a multiplication stage 614.
[0038] FIG. 7 depicts stages of an 8-point DIF FFT algorithm 700. The algorithm 700 may include a first stage 702, a second stage 704, and a third stage 706 that may
cooperate to sort the output data in normal order to provide an output in bit-reversed order.
[0039] One of the bottlenecks in most applications, where high performance is required, is the FFT/IFFT processor. Given that higher radix implementations are attractive
for reduction in computations, researchers have sought a higher radix butterfly implementation, because the higher radix will reduce automatically the communication

https://patents.google.com/patent/WO2019232091A1/en?q=(high+radix)&oq=high+radix&page=1

4/14

2/14/26, 1:58 PM

WO2019232091A1 - Radix-23 fast fourier transform for an embedded digital signal processor - Google Patents

load. However, the higher radix has typically added to the computational load. While attempts have been made to reduce the computational load by factoring the adder
matrix (or by simplification of adder tree), conventional attempts have not provided a complete solution for the FFT problem due to the increasing complexity of the
butterflies for higher radices introduced by the added multipliers in the butterfly’s critical path, as depicted in FIG. 8.
[0040] FIG. 8 depicts a radix-8 DIT butterfly 800, in accordance with certain embodiments of the present disclosure. In this example, the radix-8 DIT butterfly 800 may
include a plurality of multiplier nodes 802, which are each coupled to one of a plurality of inputs 804. The butterfly 800 may further include a plurality of summing nodes
806, 810, and 814, and additional multiplier nodes 808 and 812. In this example, the multiplier node 808B and the multiplier node 812A may be in a critical path and may
represent additional multipliers that may not be present in lower valued radices and thus add to the computational load. In FIG. 8, the dashed line may represent a
butterfly critical path.
[0041] It should be appreciated that the elements of the adder tree matrix Tr and the elements of the twiddle factor matrix both contain twiddle factors. By controlling the
variation of the twiddle factors during the calculation of a complete FFT, the twiddle factors and the adder tree matrices can be incorporated in a single stage of
calculation.
[0042] Therefore, by defining [Tr\i m as the element at the 7th line and

column in the matrix Tr as a result, Equation 6 can be rewritten as follows:

(Equation ?) where 7=0, 1,..., r- 1, m=0,l,...,r - 1 and [x]w represents the operation x modulo N. Further, by defining WN (m V S), the set of the twiddle factor matrix can be
determined as follows:
[IΈ/y] i,m(v,s)— diag(ww (o ,V,S)> ^N (i ,v,s)> tVjy (I— i,v, s))> (Equation 8) where the indices r is the FF s radix, v = 0,1, ... , V— 1 represents the number of words of size r (V =
^), and s = 0,1, ... , S is the number of stages (or iterations S = logr N— 1).
[0043] Finally, Equation 8 could be expressed for the different stages in an FFT process as follows: for l = m (Equation 9)

elsewhere for the DIF process. For the DIT process, Equation 8 can be expressed as follows: for l = m (10)

elsewhere for the DIT Process, where 7=0,1,..., r-l is the Ith butterfly’s output, m=Q,\,...,r-\ is the mth butterfly’s input, and [x] represents the integer part operator of x. [0044]
Consequently, the Ith transform output during each stage could be illustrated as follows:

(Equation 11) for the DIF process, and could be expressed as follows for the DIT process:

(Equation 12)
[0045] The read address generator (RAG), write address generator (WAG), and coefficient address generator (CAG) can be written for DIF and DIT processes, respectively.
The mth butterfly’s input of vth word x(m) at the sth stage (sth iteration) can be determined as follows:
N
RAG (m,v, 0) = m x - + v. (Equation 13)
[0046] For s> 0, the read address generator can determine the read address as follows:
(Equation 14)

for the DIF process, and for the DIT process, the read address generator can be determined as follows:
(Equation 15)

for the DIT process wherem = 0,1, ... , r— 1, v = 0,1, ... , V— 1 and s = 0,1,

S = logr N— 1 in which [x]w represents the operation x modulo A and [x] represents the integer

part operator of x.
[0047] For both cases, the Ith processed butterfly’s output X(i,v,S) for the vth word at the sth stage can be stored into the memory address location can be determined
according to the following equation:

(Equation 16)
In this example, the input data and the output data are in natural order during each stage of the FFT process according to an Ordered Input Ordered Output (OIOO)
algorithm. [0048] The coefficients multipliers (Twiddle Factors) can be determined during each stage. The coefficient address generator values can be fed to the mth
butterfly’s input of vth word X(m) at the sth stage (sth iteration), and can be determined according to the following equation:
(Equation 17)

https://patents.google.com/patent/WO2019232091A1/en?q=(high+radix)&oq=high+radix&page=1

5/14

2/14/26, 1:58 PM

WO2019232091A1 - Radix-23 fast fourier transform for an embedded digital signal processor - Google Patents

tor the DIF process, and according to the following equation for the DIT process:
(Equation 18)

[0049] By examining Equations 16 and 17, it can be observed that the data are grouped with their corresponding coefficients multipliers during each stage due to the fact
that the mth coefficient multiplier of the Ith butterfly’s output shift, if and only if, v (v = 0,1, ... , V— 1) will be equal to r(S-s) in the DIF process or v = rs in the DIT process.
As a result and since V = N/r = rS ; the total number of shifts during each stage in the DIT process would be rs, and the total number of shifts during each stage in the DIF
process is r(S-s). Therefore, by implementing a word counter r(S-s) (wordcounter = 0, 1, .. ,r(S-s) - 1) and a shifting counter rs (shiftcounter = 0,1, ... , rs - 1) in the DIT
process (or a word counter rs and a shifting counter r(S-s) in the DIF process), it is possible to obtain high efficiency DIT/DIF radix-r algorithms in which the access to the
coefficient multiplier’s memory is reduced compared to conventional radix-r DIT/DIF algorithms.
[0050] In addition, the occurrence of the multiplication by one (i.e. the elements of the twiddle factor matrix illustrated in Equation 8 are all equal to one) can be easily
predicted when the shifting counter in both cases is equal to zero (i.e. v < rs or v < r(S s)). By predicting when the shifting counter is equal to zero, the trivial multiplication
by one (wO) during the entire FFT process can be avoided.
[0051] With the same reasoning as above, the complexity of the DIT/DIF reading generators can be obtained and replaced with simple counters. Further reductions in
computation and further reductions in the coefficient multiplier’s memory access can also be realized. For simplicity and in order to reduce the complexity of the
equations that will follow, the terms can be defined as follows:

[0052] For the radix 2 case, Equation 12 at the sth stage can be rewritten as follows:

that could be simplified as follows:
(Equation 21)

where x denotes the input from the previous stage and X represents the transform output.
[0053] By replacing the term u/ 2^ s^Jwith the term A which is the value of the shifting counter that cannot exceed 2s - 1, Equation 21 may be written to have the final
form as follows:
(Equation 22)

For the first iteration (s = 0), the maximum value that v can attain i s V - 1. As a result, the term\v/V\ = Ais always zero; therefore, for the first iteration, Equation 22 can be
written as follows:
(Equation 23)

[0054] During the second iteration (s = 1), the term l is either zero or one as a result
Equation 22 and can be expressed as follows:

which could be simplified as follows:

[0055] Finally, for the third iteration (s = 2), the term l could have the following values 0, 1, 2 and 3, and, as a result, Equation 22 can be illustrated as follows:

https://patents.google.com/patent/WO2019232091A1/en?q=(high+radix)&oq=high+radix&page=1

6/14

2/14/26, 1:58 PM

WO2019232091A1 - Radix-23 fast fourier transform for an embedded digital signal processor - Google Patents

[0056] The matrices of Equation 26 may be simplified as follows:

and the signal flow graph of an 8 point DIT FFT according to Equation 27 is illustrated in FIG. 9.
[0057] FIG. 9 depicts a signal flow graph 900 of an 8-point DIT FFT, in accordance with certain embodiments of the present disclosure. The graph 900 may include a
plurality of summing nodes, generally indicated at 902. Further, the graph 900 can include reordering operations, generally indicated at 904. The graph 900 depicts a
plurality of summing nodes, generally indicated at 906, and two multiplier nodes 907 A and 907B. Further, the graph 900 may include a plurality of reordering operations,
generally indicated at 908. Additionally, the graph 900 can include multipliers 909A, 909B, and 909C and a plurality of summing nodes, generally indicated at 910.
[0058] The multiplication by -j at 907A and 907B in FIG. 9 can be easily incorporated in the additions by switching the real and imaginary parts of the data, and the
multiplication
V2 y/2
of the input data by ± - ± j— may cost 2 real multiplications. As a result, the total cost of real multiplication of the proposed structure can include 4 real multiplication
operations, as compared to the structure of FIG. 4 that would cost 20 real multiplication operations (i.e., 5 complex multiplications).
[0059] FIG. 10 depicts a graph 800 of the 8th root of unity, in accordance with certain embodiments of the present disclosure. The graph 800 depicts complex numbers
including imaginary (I) and real (R) components. In some embodiments, the complex numbers may result in a value of one when raised to some positive integer power n.
[0060] From Equations 23, 25, and 27, the first, second, and the third iterations of the DIT
FFT process may include only trivial multiplication operations. In order to predict the occurrence of the trivial multiplication in the rest of the iterations (i.e. s > 3), which is
a multiple of w8 as shown in FIG. 10, the following discussion introduces the term 2(s - 2)
(hereinafter referred to as a“separator”) that will subdivide 2s into 4 sub regions. The choice of the separator’s value will be based on the following equations. For Lemma
1, for all stages of the OIOOO FFT algorithm, the product of 2(s - 2) and 2(S - s) is always
= N/8Vs. This identity can be proven according to the following equations:
(Equation 28)

[0061] For different values of l, Equation 22 provides the following values:

V.
vi. · ·· 3 X 2(s_2) [
vii.
viii.

··· 2S[

https://patents.google.com/patent/WO2019232091A1/en?q=(high+radix)&oq=high+radix&page=1

7/14

2/14/26, 1:58 PM
[0062] For the 1

th

WO2019232091A1 - Radix-23 fast fourier transform for an embedded digital signal processor - Google Patents
case at the . „st’h

11

iteration (stage), Equation 22 can be expressed as follows:

(Equation 29)

For the iii case, Equation 22 can be expressed as follows:

[0063] For ;;
Vth and v 7iith cases, Equation 22 can be expressed, respectively, as follows:

wherea

+2

[0064] Therefore, for .v > 3, there are four sets of size / s v) words that have-^- (1 ± j), 1, and -j as trivial multiplications that can be grouped. Grouping the“trivial”
multiplications can yield the following expression:

and the resulting structure for this particular case is depicted in FIG. 11.
[0065] FIG. 11 depicts a graph 1100 of a Radix-23 FFT butterfly structure for a trivial computation, in accordance with certain embodiments of the present disclosure. The
graph 1100 may include summing nodes, generally indicated at 1103. The graph 1100 may include a complex multiplier node 1103 and can include summing nodes,
generally indicated at 1104. The graph 1100 may further include a trivial multiplier 1105 and can include summing nodes, generally indicated at 1106. The graph 1100
can further include a complex multiplier 1107 and can include summing nodes 1108, generally indicated at
1108.
[0066] For the other cases and by comparing the domains of 2, each domain of l can be represented as follows:

(Equation 34) where x = 0, 1, 2 and 3. Other cases can be expressed as follows:

[0067] By regrouping these four cases where each of which will share the same coefficient multiplier, the following expression may be realized:

https://patents.google.com/patent/WO2019232091A1/en?q=(high+radix)&oq=high+radix&page=1

8/14

2/14/26, 1:58 PM

WO2019232091A1 - Radix-23 fast fourier transform for an embedded digital signal processor - Google Patents

(2r(s~z +X)cc
where 1 G t ·· 2 ^(s-2) . [ i_. T ah aae^ entity w ** p!j, J in the fifth and the sixth terms of
Equation 36 can be simplified as follows:

(Equation 37). fr(s 2) + )a . (3r(x 2^+l)a
[0068] In this example, the domain for l for the entitiesww and can be defined as follows:
1 e 2(c-¾ ... i [. (Equation 38)
[0069] These entities could be expressed, respectively, as follows:

(Equation
39)

(Equation
40) where the variable conj in Equations 39 and 40 refers to the complex conjugate process. As a result, Equation 36 can be rewritten as follows:

(Equation 41)
[0070] From Equation 41, the FFT radix 23 butterfly can be derived as depicted and described below with respect to FIG. 12.
[0071] FIG. 12 depicts a graph of a Radix-23 FFT butterfly structure 1200 for a non-trivial computation, in accordance with certain embodiments of the present disclosure.
In this example, one complex coefficient multiplier (or twiddle factor) can be used for each of the eight complex inputs. In addition, the coefficient multiplier memory can
be accessed once for each 4x2s word (a set of two inputs) for the DIT process. For the DIF process, where s is the actual stage (iteration) of the FFT process and where S
represents a total number of stages of the FFT process, the coefficient multiplier memory can be accessed once for every 2(S v) word where (S = logi (N) - 1).

https://patents.google.com/patent/WO2019232091A1/en?q=(high+radix)&oq=high+radix&page=1

9/14

2/14/26, 1:58 PM

WO2019232091A1 - Radix-23 fast fourier transform for an embedded digital signal processor - Google Patents

[0072] In FIG. 12, the structure 1200 may include a complex multiplier node 1201 and can include summing nodes, generally indicated at 1202. The structure 1200 may
also include a complex multiplier node 1203 and summing nodes, generally indicated at 1204. Further, the structure 1200 can include a complex multiplier node 1205
and summing nodes, generally indicated at 1206. The structure 1200 can also include a complex multiplier node 1207 and summing nodes 1208, generally indicated at
1208.
[0073] Compared to conventional methods that require two memory accesses per four inputs and one memory access per two inputs, the FFT radix-23 butterfly structure
1200
V2" yf2 may use one memory access per eight inputs. Further, the multiplication by ± ± j ^ -can be predicted, where the number of arithmetical operations to complete the
complex multiplication can be reduced from six to two as shown in Tables 1 and 2 below. Further, the reduction in memory accesses to the coefficient multiplier’s
memory is illustrated in Table 3 for different FFT sizes.
[0074] In Tables 1-3, a conventional method #1 (“DIT”) refers to a method described in Y. Wang and al,“Novel Memory Reference Reduction Methods for FFT
Implementations on DSP Processors”, IEEE Transactions on signal processing, Vol. 55, No. 5, May 2007. Further, a conventional method #2 (“TMS”) refers to DIF radix-2
FFT code taken from "TMS320C64x DSP Library Programmer’s Reference", Literature Number: SPRU565B, Oct. 2003, (code DSP-radix-2, p. 4-9, 4-10).
Table 1 : Comparison in terms of real multiplication between conventional methods versus the Radix-23 FFT method

Table 2: Comparison in terms of real addition between the conventional methods versus the Radix-23 FFT method

Table 3: Comparison in terms of memory accesses to the coefficient multiplier in the conventional methods versus the Radix-23 FFT method where each complex access
is counted as 1 :

[0075] Table 4 reveals simulation results of the conventional methods versus the Radix- 23 FFT method where the term“Loss” is defined as the ratio of the conventional
method over the Radix-23 FFT method.
Table 4: Comparative results in term of clock cycle of the conventional methods versus the Radix-23 FFT method for different FFT sizes

https://patents.google.com/patent/WO2019232091A1/en?q=(high+radix)&oq=high+radix&page=1

10/14

2/14/26, 1:58 PM

WO2019232091A1 - Radix-23 fast fourier transform for an embedded digital signal processor - Google Patents

The ratio of the conventional method over the Radix-23 FFT method is described below with respect to FIG. 13.
[0076] FIG. 13 depicts a graph 1300 of a percentage reduction of clock cycles as a function of the FFT length for a TMS clock and a DIT clock, in accordance with certain
embodiments of the present disclosure. The percentage reduction in clock cycles appears to increase substantially linearly as the FFT length (N) increases for the
implementation of the Radix-23 FFT method as compared to the reference. At a FFT length of log2(l2), the Radix-23 FFT method provides a 60% rejection in clock cycles
as compared to the reference algorithm.
Table 5 : Comparison of the coefficients multiplier’s memory requirement of the conventional methods versus the Radix-23 FFT method where the size is computed in
term of bytes

[0077] As can be seen from Table 5, the method described herein achieves a significant reduction in the coefficient multiplier’s memory requirements in terms of bytes. In
particular, the method described herein achieves a memory size reduction of one less than the number of bytes divided by 8, as compared to the DIT reduction of two
less than half of the number of bytes.
[0078] FIG. 14 depicts a block diagram of a signal processing system 1400 including a Radix-23 FFT butterfly structure, in accordance with certain embodiments of the
present disclosure. The system 1400 may include a digital signal processing (DSP) circuit 1402 having an input coupled to an analog-to-digital converter 1404, which may
be configured to provide digital input stream to the DSP circuit 1402. The DSP circuit 1402 may further include an output coupled to a processor core 1406 or to another
circuit or device. Other embodiments are also possible. [0079] In some embodiments, the DSP circuit 1402 may include a low-pass filter 1408 including an input coupled
to the output of the ADC 1404 and including an output. The DSP circuit 1402 may further include a radix-23 FFT module 1410 including an input coupled to the low pass
filter 1408 and including an output coupled to the processor cor 1406 through an input/output (I/O) interface 1412.
[0080] In conjunction with the systems, methods, and devices described above with respect to FIGs. 1-14 provides an efficient ordered input, ordered output radix 23
algorithm that reduces the complexity and the computational effort in comparison to conventional methods. Furthermore, the systems, methods, and devices
demonstrate a significant improvement in execution time in term of clock cycles compared to the conventional methods. In certain embodiments, the systems, methods,
and devices may be configured to predict the 8th root of unity and to reduce the memory size needed to stock the coefficient multiplier to N/8. Accordingly, each of these
improvements may contribute, individually and collectively, to an efficiency gain with respect to the processor, which may be realized in terms of faster processing,
reduced memory consumption, reduced power consumption, and other improvements.
[0081] Implementations that may be used within the scope of the present disclosure may be illustrated by way of the following clauses:
Clause 1 : A circuit comprising an input configured to receive a signal; and a radix-23 fast Fourier transform (FFT) processing element coupled to the input and configured
to control variation of twiddle factors during calculation of a complete FFT through a plurality of processing stages of an FFT process, the radix-23 FFT processing
element configured to incorporate the twiddle factors and adder tree matrices of the calculation into a single stage.
Clause 2: The circuit of clause 1, wherein data input to the radix-23 FFT processing element and data output by the radix-23 FFT processing element are in natural order
during each stage of the plurality of processing stages of the FFT process. Clause 3 : The circuit according to any of the preceding clauses, wherein data within the radix23 FFT processing element are grouped with their corresponding coefficients multipliers during each stage of the plurality of processing stages of the FFT process.
Clause 4: The circuit according to any of the preceding clauses, wherein a total number of shifts during each stage in the plurality of processing stages of an FFT process
configured to perform a decimation in time (DIT) process is represented as rs.
Clause 5: The circuit according to any of the preceding clauses, wherein a total number of shifts during each stage in the plurality of processing stages of an FFT process
configured to perform a decimation in frequency (DIF) process is represented as r(S-s).
Clause 6: The circuit according to any of the preceding clauses, wherein trivial multiplication by one operations are avoided during the plurality of processing stages of
the FFT process.
Clause 7: A circuit comprising an input configured to receive a signal; and a radix-23 fast Fourier transform (FFT) processing element coupled to the input and configured
to control variation of twiddle factors during calculation of a complete FFT through one or more stages.
Clause 8: The circuit according to any of the preceding clauses, wherein the radix-23 FFT processing element is configured to incorporate the twiddle factors and adder
tree matrices of the calculation into a single stage.
Clause 9: The circuit according to any of the preceding clauses, wherein data input to the radix-23 FFT processing element and data output by the radix-23 FFT processing
element are in natural order during each stage of the one or more stages.
Clause 10: The circuit according to any of the preceding clauses, wherein the radix-23 FFT processing element is configured to determine data from the signal at the
input; group each data element from the determined data with its corresponding coefficient multiplier to form grouped data; and process the grouped data to produce an
output signal. Clause 11 : The circuit according to any of the preceding clauses, wherein the radix-23 FFT processing element is configured to perform a decimation in
time (DIT) process having a number of shifts corresponding to a size N of the input data divided by the radix.

https://patents.google.com/patent/WO2019232091A1/en?q=(high+radix)&oq=high+radix&page=1

11/14

2/14/26, 1:58 PM

WO2019232091A1 - Radix-23 fast fourier transform for an embedded digital signal processor - Google Patents

Clause 12: The circuit according to any of the preceding clauses, wherein the radix-23 FFT processing element is configured to perform a decimation in frequency (DIF)
process having a number of shifts corresponding to a number of words minus a number of stages.
Clause 13: The circuit according to any of the preceding clauses, wherein the radix-23 FFT processing element avoids multiplication-by-one operations during the one or
more stages of the FFT.
Clause 14. A circuit comprising an input configured to receive a signal; and a radix-r fast Fourier transform (FFT) processing element coupled to the input. The radix-r FFT
processing element may be configured to receive an input signal having a number of bits N; reverse a bit order of the bits N; decompose the bit order into groups of bits
based on a base of a radix of the radix-r FFT processing element; and process the groups of bits together with their coefficients to produce an output signal.
Clause 15: The circuit according to any of the preceding clauses, wherein the radix-r FFT processing element is configured to control variation of twiddle factors during
calculation of an FFT through one or more stages of an FFT process.
Clause 16: The circuit according to any of the preceding clauses, wherein the radix-r FFT processing element is configured to incorporate the twiddle factors and adder
tree matrices of the calculation into a single stage.
Clause 17: The circuit according to any of the preceding clauses, wherein data input to the radix-r FFT processing element and data output by the radix-r FFT processing
element are in natural order during each stage of the one or more stages.
Clause 18: The circuit according to any of the preceding clauses, wherein the radix-r FFT processing element is configured to determine data from the signal at the input;
group each data element from the determined data with its corresponding coefficient multiplier to form grouped data; and process the grouped data to produce an output
signal.
Clause 19: The circuit according to any of the preceding clauses, wherein the radix-r FFT processing element is configured to perform a decimation in time (DIT) process
having a number of shifts corresponding to a size N of the input data divided by the radix; and perform a decimation in frequency (DIF) process having a number of shifts
corresponding to a number of words minus a number of stages.
Clause 20: The circuit according to any of the preceding clauses, wherein the radix-r FFT processing element includes a radix-23 FFT processing element to avoid
multiplication-by-one operations during processing within the one or more stages.
[0082] Although the present invention has been described with reference to preferred embodiments, workers skilled in the art will recognize that changes may be made in
form and detail without departing from the scope of the invention.

Patent Citations (5)
Publication number

Priority date

Publication date

Assignee

Title

US20010032227A1 *

2000-01-25

2001-10-18

Jaber Marwan A.

Butterfly-processing element for efficient fast fourier transform method and apparatus

US20080215656A1 *

2006-09-26

2008-09-04

Oki Electric
Industry Co., Ltd.

Fast fourier transform circuit and fast fourier transform method

US20090013021A1 *

2007-07-06

2009-01-08

Mediatek Inc.

Variable length fft system and method

US20100011046A1 *

2006-12-08

2010-01-14

Samsung
Electronics Co.,
Ltd.

Apparatus and method for variable fast fourier transform

US20140280420A1 *

2013-03-13

2014-09-18

Qualcomm
Incorporated

Vector processing engines having programmable data path configurations for
providing multi-mode radix-2x butterfly vector processing circuits, and related vector
processors, systems, and methods

Family To Family Citations
* Cited by examiner, † Cited by third party

Cited By (3)
Publication number

Priority date

Publication date

Assignee

Title

CN113434811A *

2021-06-29

2021-09-24

河北民族师范学
院

Improved-2 ^6 algorithm and 2048-point FFT processor IP core used by FFT processor IP
core

CN119356733A *

2023-06-06

2025-01-24

华为技术有限公
司

Method, processor and computing device for performing FFT

CN120067503A *

2023-11-30

2025-05-30

华为技术有限公
司

Method, device and computing equipment for performing FFT (fast Fourier transform)
calculation

Family To Family Citations

* Cited by examiner, † Cited by third party, ‡ Family to family citation

Similar Documents
Publication

Publication Date

Title

https://patents.google.com/patent/WO2019232091A1/en?q=(high+radix)&oq=high+radix&page=1

12/14

2/14/26, 1:58 PM

WO2019232091A1 - Radix-23 fast fourier transform for an embedded digital signal processor - Google Patents

Nussbaumer

1981

The fast Fourier transform

Mersereau et al.

2003

A unified treatment of Cooley-Tukey algorithms for the evaluation of the multidimensional DFT

Garrido

2016

A new representation of FFT algorithms using triangular matrices

US20010032227A1

2001-10-18

Butterfly-processing element for efficient fast fourier transform method and apparatus

Jones

2022

Regularized Fast Hartley Transform

US20180373677A1

2018-12-27

Apparatus and Methods of Providing Efficient Data Parallelization for Multi-Dimensional FFTs

WO2019232091A1

2019-12-05

Radix-23 fast fourier transform for an embedded digital signal processor

EP0174995A4

1986-08-21

COMPUTER AND METHOD FOR PROVIDING A DISCRETE BRACEWELL TRANSFORM.

Singh et al.

2017

Design of radix 2 butterfly structure using vedic multiplier and CLA on xilinx

US7761495B2

2010-07-20

Fourier transform processor

Palmer et al.

2004

A parallel FFT architecture for FPGAs

Arambepola

1983

Discrete Fourier transform processor based on the prime-factor algorithm

US20080126462A1

2008-05-29

Optimized multi-mode DFT implementation

EP1426872A2

2004-06-09

Linear scalable FFT/IFFT computation in a multi-processor system

US20180373676A1

2018-12-27

Apparatus and Methods of Providing an Efficient Radix-R Fast Fourier Transform

CN111291315A

2020-06-16

Data processing method, device and equipment

Bautista et al.

2024

A hardware-efficient 1200-point FFT architecture that combines the prime factor and Cooley-Tukey algorithms

Sundararajan et al.

2002

Vector computation of the discrete Fourier transform

Ajmal et al.

2016

FPGA based area optimized parallel pipelined radix-2 2 feed forward FFT architecture

Jaber et al.

2009

A new FFT concept for efficient VLSI implementation: Part I-Butterfly processing element

Uzun et al.

2003

Towards a general framework for an FPGA-based FFT coprocessor

Meher et al.

2010

Efficient systolic designs for 1-and 2-dimensional DFT of general transform-lengths for high-speed wireless communication
applications

Kaur et al.

2015

Design and Simulation of 32-Point FFT Using Mixed Radix Algorithm for FPGA Implementation

Chavan et al.

2017

VLSI Implementation of Split-radix FFT for High Speed Applications

Çerri et al.

2015

FFT implementation on FPGA using butterfly algorithm

Priority And Related Applications
Applications Claiming Priority (2)
Application

Filing date

US201862677610P

2018-05-29

US62/677,610

2018-05-29

Title

Legal Events
Date

Code

Title

Description

2020-01-15

121

Ep: the epo has been informed by wipo that ep was designated in this application

Ref document number: 19811282
Country of ref document: EP
Kind code of ref document: A1

2020-12-01

NENP

Non-entry into the national phase

https://patents.google.com/patent/WO2019232091A1/en?q=(high+radix)&oq=high+radix&page=1

Ref country code: DE

13/14

2/14/26, 1:58 PM
2021-06-16

WO2019232091A1 - Radix-23 fast fourier transform for an embedded digital signal processor - Google Patents
122

Ep: pct application non-entry in european phase

Ref document number: 19811282
Country of ref document: EP
Kind code of ref document: A1

Data provided by IFI CLAIMS Patent Services

About

Send Feedback

Public Datasets

Terms

Privacy Policy

https://patents.google.com/patent/WO2019232091A1/en?q=(high+radix)&oq=high+radix&page=1

Help

14/14


